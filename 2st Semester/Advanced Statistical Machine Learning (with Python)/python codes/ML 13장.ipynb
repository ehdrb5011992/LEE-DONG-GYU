{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 50,000개의 영화 평론 데이터를 불러옴.\n",
    "# https://ai.stanford.edu/~amaas/data/sentiment/에서 Internet movie database를 받은 후\n",
    "# 아래의 프로그램을 수행함.\n",
    "import tarfile\n",
    "tar = tarfile.open('C://Users//pupub//Desktop//python prog//aclImdb_v1.tar.gz','r:gz') # gz파일이 저장된 경로로 변경\n",
    "tar.extractall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 자료를 행렬 형태로 바꿈.\n",
    "import pyprind\n",
    "import pandas as pd\n",
    "import os\n",
    "basepath = 'C://Users//pupub//Desktop//aclImdb'\n",
    "labels = {'pos':1, 'neg':0}\n",
    "pbar = pyprind.ProgBar(50000)\n",
    "df = pd.DataFrame()\n",
    "for s in ('test', 'train'):\n",
    "    for l in ('pos', 'neg'):\n",
    "        path = os.path.join(basepath, s, l)\n",
    "        for file in os.listdir(path):\n",
    "            with open(os.path.join(path, file), 'r', encoding='utf-8') as infile:\n",
    "                txt = infile.read()\n",
    "            df = df.append([[txt, labels[l]]], ignore_index=True)\n",
    "            pbar.update()\n",
    "df.columns = ['review', 'sentiment']    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 자료의 순서를 임의로 뒤섞어 csv 파일로 저장. \n",
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "df = df.reindex(np.random.permutation(df.index))\n",
    "df.to_csv('C://Users//pupub//Desktop//movie_data.csv', index = False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>In 1974, the teenager Martha Moxley (Maggie Gr...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>OK... so... I really like Kris Kristofferson a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>***SPOILER*** Do not read this, if you think a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  sentiment\n",
       "0  In 1974, the teenager Martha Moxley (Maggie Gr...          1\n",
       "1  OK... so... I really like Kris Kristofferson a...          0\n",
       "2  ***SPOILER*** Do not read this, if you think a...          0"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 저장된 파일 불러와 확인함.\n",
    "import pyprind\n",
    "import pandas as pd\n",
    "import os\n",
    "df = pd.read_csv('C://Users//pupub//Desktop//movie_data.csv', encoding='utf-8')\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        in 1974 the teenager martha moxley maggie grac...\n",
       "1        ok so i really like kris kristofferson and his...\n",
       "2         spoiler do not read this if you think about w...\n",
       "3        hi for all the people who have seen this wonde...\n",
       "4        i recently bought the dvd forgetting just how ...\n",
       "5        leave it to braik to put on a good show finall...\n",
       "6        nathan detroit frank sinatra is the manager of...\n",
       "7        to understand crash course in the right contex...\n",
       "8        i ve been impressed with chavez s stance again...\n",
       "9        this movie is directed by renny harlin the fin...\n",
       "10       i once lived in the u p and let me tell you wh...\n",
       "11       hidden frontier is notable for being the longe...\n",
       "12       it s a while ago that i have seen sleuth 1972 ...\n",
       "13       what is it about the french first they apparen...\n",
       "14       this very strange movie is unlike anything mad...\n",
       "15       i saw this movie on the strength of the single...\n",
       "16       there are some great philosophical questions w...\n",
       "17       i was cast as the surfer dude in the beach sce...\n",
       "18       i had high hopes for this one until they chang...\n",
       "19       set in and near a poor working class town in t...\n",
       "20       opulent sets and sumptuous costumes well photo...\n",
       "21       i saw the film and i got screwed because the f...\n",
       "22       i m getting a little tired of people misusing ...\n",
       "23       how offensive those who liked this movie have ...\n",
       "24       what else can you say about this movie except ...\n",
       "25       certain aspects of punishment park are less th...\n",
       "26       first of all i d like to tell you that i m int...\n",
       "27       you should not take what i am about to say lig...\n",
       "28       i love the jurassic park movies they are three...\n",
       "29       the first series of lost kicked off with a ban...\n",
       "                               ...                        \n",
       "49970    tom fontana s unforgettable oz is hands down o...\n",
       "49971    last weekend i bought this zombie movie from t...\n",
       "49972    i watched the first few moments on tcm a few y...\n",
       "49973    i saw this movie for the first time in 1988 wh...\n",
       "49974    al pacino kim basinger tea leoni ryan o neal r...\n",
       "49975    stanwyck at her villainous best robinson her e...\n",
       "49976    an allegation of aggravated sexual assault alo...\n",
       "49977    i thought this movie was wonderfully plotted i...\n",
       "49978    just like most people i couldn t wait to see t...\n",
       "49979    dark comedy gallows humor how does one make a ...\n",
       "49980     probably will contain spoilers after a succes...\n",
       "49981    i must be that one guy in america that didn t ...\n",
       "49982    the plot a trucker kristofferson battles a cor...\n",
       "49983    the ladies man is laugh out loud funny with a ...\n",
       "49984    well the artyfartyrati of cannes may have like...\n",
       "49985    the director was probably still in his early l...\n",
       "49986    you know when you re on the bus and someone de...\n",
       "49987    five minutes into this movie you realize that ...\n",
       "49988     if you ve been laughing too much for a long t...\n",
       "49989    i love dissing this movie my peers always try ...\n",
       "49990    ok i think the tv show is kind of cute and it ...\n",
       "49991    big disappointment clash by night is much to t...\n",
       "49992    cassidy kacia brady puts a gun in her mouth bl...\n",
       "49993    with rapid intercutting of scenes of insane pe...\n",
       "49994    when girlfight came out the reviews praised it...\n",
       "49995    ok lets start with the best the building altho...\n",
       "49996    the british heritage film industry is out of c...\n",
       "49997    i don t even know where to begin on this one i...\n",
       "49998    richard tyler is a little boy who is scared of...\n",
       "49999    i waited long to watch this movie also because...\n",
       "Name: review, Length: 50000, dtype: object"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 정보를 가지지 않은 것으로 판단 되는 것을 사전에 정리\n",
    "import re\n",
    "def preprocessor(text):\n",
    "    text = re.sub('<[^>]*>', '', text) # text에서 <[^>]*>과 일치하는 데이터를 공백으로 바꾸는 명령어\n",
    "    emoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)',\n",
    "    text)\n",
    "    text = (re.sub('[\\W]+', ' ', text.lower()) # 단어가 아닌 모든 기호는 공백으로 대체\n",
    "    +' '.join(emoticons).replace('-', '')) # emoticons을 빈공간 뒤에 배치\n",
    "    return text\n",
    "df['review'] = df['review'].apply(preprocessor)\n",
    "df['review']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['runners', 'like', 'running', 'and', 'thus', 'they', 'run']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 단어를 분류\n",
    "def tokenizer(text):\n",
    "    return text.split()\n",
    "tokenizer('runners like running and thus they run')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['runner', 'like', 'run', 'and', 'thu', 'they', 'run']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 단어의 원뿌리로 재표현\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "porter = PorterStemmer()\n",
    "def tokenizer_porter(text):\n",
    "    return [porter.stem(word) for word in text.split()]\n",
    "tokenizer_porter('runners like running and thus they run')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\pupub\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['runner', 'like', 'run', 'run', 'alot']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 정보가 없는 단어 제거\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')\n",
    "[w for w in tokenizer_porter('a runner likes running and runs alot')[-10:] if w not in stop]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 영화에 대한 평가를 positive or negative로 분류\n",
    "X_train = df.loc[:25000, 'review'].values\n",
    "y_train = df.loc[:25000, 'sentiment'].values\n",
    "X_test = df.loc[25000:, 'review'].values \n",
    "y_test = df.loc[25000:, 'sentiment'].values\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf = TfidfVectorizer(strip_accents=None,lowercase=False,preprocessor=None)\n",
    "param_grid = [{'vect__ngram_range': [(1,1)],'vect__stop_words': [stop, None],'vect__tokenizer': [tokenizer,tokenizer_porter],'clf__penalty':['l1', 'l2'],'clf__C': [1.0, 10.0, 100.0]},\n",
    "               {'vect__ngram_range': [(1,1)],'vect__stop_words': [stop, None],'vect__tokenizer': [tokenizer, tokenizer_porter],'vect__use_idf':[False],\n",
    "                'vect__norm':[None], 'clf__penalty': ['l1', 'l2'],'clf__C': [1.0, 10.0, 100.0]}]\n",
    "lr_tfidf = Pipeline([('vect', tfidf),('clf', LogisticRegression(random_state=0))])\n",
    "gs_lr_tfidf = GridSearchCV(lr_tfidf, param_grid,scoring='accuracy',cv=5, verbose=1,n_jobs=1)\n",
    "gs_lr_tfidf.fit(X_train, y_train) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Best parameter set: %s ' % gs_lr_tfidf.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 검증데이터의 정밀도\n",
    "print('CV Accuracy: %.3f' % gs_lr_tfidf.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 초모수 선택 후 시험데이터에 적용한 결과\n",
    "print('Test Accuracy: %.3f' % clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# out-of-core learning을 영화리뷰 데이터에 적용\n",
    "\n",
    "# csv 형태인 자료가 online 자료라고 가정한 후 \n",
    "# 자료의 cleaning과 bag-of-words 모형을 조합해 문서를 특성변수화함.\n",
    "import numpy as np\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')\n",
    "def tokenizer(text):\n",
    "    text = re.sub('<[^>]*>', '', text)\n",
    "    emoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)', text.lower())\n",
    "    text = re.sub('[\\W]+', ' ', text.lower()) + ' '.join(emoticons).replace('-', '')\n",
    "    tokenized = [w for w in text.split() if w not in stop]\n",
    "    return tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "def stream_docs(path):\n",
    "    with open(path, 'r', encoding='utf-8') as csv:\n",
    "        next(csv) # skip header\n",
    "        for line in csv:\n",
    "           text, label = line[:-3], int(line[-2])\n",
    "           yield text, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "def get_minibatch(doc_stream, size):\n",
    "    docs, y = [], []\n",
    "    try:\n",
    "        for _ in range(size):\n",
    "            text, label = next(doc_stream)\n",
    "            docs.append(text)\n",
    "            y.append(label)\n",
    "    except StopIteration:\n",
    "        return None, None\n",
    "    return docs, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "vect = HashingVectorizer(decode_error='ignore',n_features=2**21,preprocessor=None,tokenizer=tokenizer)\n",
    "clf = SGDClassifier(loss='log', random_state=1, n_iter=1)\n",
    "doc_stream = stream_docs(path='C://Users//pupub//Desktop//movie_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "import pyprind\n",
    "pbar = pyprind.ProgBar(45)\n",
    "classes = np.array([0, 1])\n",
    "for _ in range(45):\n",
    "    X_train, y_train = get_minibatch(doc_stream, size=1000)\n",
    "    if not X_train:\n",
    "        break\n",
    "    X_train = vect.transform(X_train)\n",
    "    clf.partial_fit(X_train, y_train, classes=classes)\n",
    "    pbar.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "X_test, y_test = get_minibatch(doc_stream, size=5000)\n",
    "X_test = vect.transform(X_test)\n",
    "print('Accuracy: %.3f' % clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "clf = clf.partial_fit(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 5000)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Latent Dirichlet Allocation\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count=CountVectorizer(stop_words='english', max_df=.1, max_features=5000)#전체 단어중 비중이10%이상이면 제거\n",
    "X=count.fit_transform(df['review'].values)\n",
    "\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "lda=LatentDirichletAllocation(n_components=10, random_state=123, learning_method='batch')\n",
    "X_topics=lda.fit_transform(X)\n",
    "lda.components_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 1:\n",
      "horror original comedy black house\n",
      "Topic 2:\n",
      "worst minutes guy script money\n",
      "Topic 3:\n",
      "book dvd read version watched\n",
      "Topic 4:\n",
      "family performance father beautiful mother\n",
      "Topic 5:\n",
      "series episode tv kids comedy\n",
      "Topic 6:\n",
      "murder police wife john plays\n",
      "Topic 7:\n",
      "documentary camera effects audience sense\n",
      "Topic 8:\n",
      "music song songs musical role\n",
      "Topic 9:\n",
      "horror effects guy dead budget\n",
      "Topic 10:\n",
      "action war game fight american\n"
     ]
    }
   ],
   "source": [
    "n_top_words=5\n",
    "feature_names=count.get_feature_names()\n",
    "for topic_idx, topic in enumerate(lda.components_):\n",
    "    print(\"Topic %d:\" % (topic_idx+1))\n",
    "    print(\" \".join([feature_names[i]\n",
    "                  for i in topic.argsort()[:-n_top_words -1:-1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Horror movie #1:\n",
      " spoilers extremely brutal police drama set in san francisco involving a sting operation that goes terribly wrong a cop det falon sam elliott mistakenly and savagely beats to death an undercover policeman winch mike watson thinking that he murdered his partner det sam levinson mike burstyn a partner ...\n",
      "\n",
      "Horror movie #2:\n",
      "this first rate western tale of the gold rush brings great excitement romance and james stewart to the screen the far country is the only one out of all five stewart mann westerns that is often overlooked stewart yet again puts a new look on the ever present personalities he had in the five stewart  ...\n",
      "\n",
      "Horror movie #3:\n",
      "the fourth of five westerns anthony mann did with james stewart this one involves a hard bitten cattleman named jeff webster who takes a cattle drive from wyoming to alaska via seattle he hooks up in seattle with his partners ben tatum walter brennan and rube morris jay c flippen that he has sent ah ...\n"
     ]
    }
   ],
   "source": [
    "horror=X_topics[:, 5].argsort()[::-1]\n",
    "for iter_idx, movie_idx in enumerate(horror[:3]):\n",
    "    print('\\nHorror movie #%d:' % (iter_idx+1))\n",
    "    print(df['review'][movie_idx][:300],'...') # df에서 열이름이 review인 자료의 movie_idx 행에서 0~299 문자. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
