"","id","submitted","updated","title","abstract","authors","affiliations","link_abstract","link_pdf","link_doi","comment","journal_ref","doi","primary_category","categories"
"1","cond-mat/9312070v1","1993-12-17 14:13:38","1993-12-17 14:13:38","A Calculation of the Exponent $¥ì$ for the Gauge Glass model","  Removed according to the author's request :
  Dear Administrator,
  Please,please could you remove the paper,tentatively assigned the number
9312070, with title ""A calculation of the exponent mu for the gauge glass
model"" and just submitted to the cond-mat board as it contains an important
error. Mike Moore
","M. A. Moore|S. Murphy","","http://arxiv.org/abs/cond-mat/9312070v1","http://arxiv.org/pdf/cond-mat/9312070v1","http://dx.doi.org/10.1103/PhysRevB.50.3450","11 pages, LATEX, 4210","","10.1103/PhysRevB.50.3450","cond-mat","cond-mat"
"2","astro-ph/9604182v1","1996-04-30 18:49:37","1996-04-30 18:49:37","Clustering of the Diffuse Infrared Light from the COBE DIRBE maps. I.
  $C(0)$ and limits on the near-IR background","  This paper is devoted to studying the CIB through its correlation properties.
We studied the limits on CIB anisotropy in the near IR (1.25, 2.2, and 3.5 \um,
or $J,\;K,\;L$) bands at a scale of 0.7\deg\ using the COBE\footnote{ The
National Aeronautics and Space Administration/Goddard Space Flight Center
(NASA/GSFC) is responsible for the design, development, and operation of the
{\it COBE}. Scientific guidance is provided by the {\it COBE} Science Working
Group. GSFC is also responsible for the development of the analysis software
and for the production of the mission data sets.} Diffuse Infrared Background
Experiment (DIRBE) data. In single bands we obtain the upper limits on the
zero-lag correlation signal $C(0)= \langle(\nu \delta I_\nu)^2\rangle < 3.6
\times 10^{-16},\; 5.1 \times 10^{-17},\; 5.7 \times 10^{-18}$ \w2m4sr2 for the
$J,K,L$ bands respectively. The DIRBE data exhibit a clear color between the
various bands with a small dispersion. On the other hand most of the CIB is
expected to come from redshifted galaxies and thus should have different color
properties. We use this observation to develop a `color subtraction' method of
linear combinations of maps at two different bands. This method is expected to
suppress the dominant fluctuations from foreground stars and nearby galaxies,
while not reducing (or perhaps even amplifying) the extragalactic contribution
to $C(0)$. Applying this technique gives significantly lower and more isotropic
limits.
","A. Kashlinsky|J. C. Mather|S. Odenwald|M. Hauser","","http://arxiv.org/abs/astro-ph/9604182v1","http://arxiv.org/pdf/astro-ph/9604182v1","http://dx.doi.org/10.1086/177900","44 pages postcript; includes 5 tables, 14 figures. Astrophysical
  Journal, in press","Astrophys.J. 470 (1996) 681","10.1086/177900","astro-ph","astro-ph"
"3","physics/9802034v1","1998-02-18 08:18:32","1998-02-18 08:18:32","IBM PC running Solaris x86 as an affordable NFS file server platform for
  a workgroup","  The possibility of using an IBM PC-compatible computer as an NFS file server
for a moderate-size workgroup has been explored. A pilot project currently
under way at LBNL has proven that this platform does indeed provide a viable
alternative to commercially available file servers at a fraction of the cost
without significant sacrifices in performance and reliability.
  From a $1,500 worth of hardware, we have built a Solaris x86 file server with
19.2 GB of disk space, expandable to 112 GB for another $6,200, which has
proven to be a worthy contender in performance to its proprietary counterparts.
The server has not exhibited any software of hardware problems during the four
months it has been in service. Overall, the platform appears to be well suited
to the needs of an experimental particle physics workgroup. We recommend it for
use in the BaBar collaboration.
  This paper focuses on aspects of building a properly configured dedicated
file server that are not adequately covered in the Solaris Installation and
System Administration guides, such as choosing right hardware and performance
optimization. Parts of it may also be of interest to prospective buyers of IBM
PC-compatible systems and to Solaris SPARC administrators.
","Alexandre Telnov","","http://arxiv.org/abs/physics/9802034v1","http://arxiv.org/pdf/physics/9802034v1","","17 pages. This article is a slightly modified version of BaBar Note
  #404. If you are affiliated with BaBar, you should get the original note from
  the BaBar Note Repository","","","physics.gen-ph","physics.gen-ph"
"4","astro-ph/9807214v1","1998-07-21 10:19:56","1998-07-21 10:19:56","The measurement of the sky brightness at Merate Observatory","  We describe the problems met using the telescopes of Merate Observatory, the
branch of Brera Astronomical Observatory. From the point of view of the data
analysis, we discuss how to introduce a satisfactory correction of the variable
extinction coefficient, giving also some examples. We also measured the sky
brightness, obtaining $V$=18.2 mag/arcsec$^2$. The light pollution is
responsible for such a bright background and we present pictures showing its
effects. We are limiting it trying to persuade public administrations to reduce
the light scattered toward the sky by using cutoff lamps and putting out
commercial searchlights. The need of a law which safeguards astronomical
activities is stressed.
","Ennio Poretti|Marco Scardia","Osservatorio Astronomico di Brera|Osservatorio Astronomico di Brera","http://arxiv.org/abs/astro-ph/9807214v1","http://arxiv.org/pdf/astro-ph/9807214v1","","8 pages, 2 ps figures, 2 ps pictures showing effects of different
  light pollution. To be published in Memorie della Societa' Astronomica
  Italiana"", Asiago Autumn Rendez-vous ""Light pollution""","","","astro-ph","astro-ph"
"5","cs/9811027v1","1998-11-20 14:33:36","1998-11-20 14:33:36","Push vs. Pull in Web-Based Network Management","  In this paper, we show how Web technologies can be used effectively to (i)
address some of the deficiencies of traditional IP network management
platforms, and (ii) render these expensive platforms redundant. We build on the
concept of embedded management application, proposed by Wellens and Auerbach,
and present two models of network management application designs that rely on
Web technologies. First, the pull model is based on the request/response
paradigm. It is typically used to perform data polling. Several commercial
management platforms already use Web technologies that rely on this model to
provide for ad hoc management; we demonstrate how to extend this to regular
management. Second, the push model is a novel approach which relies on the
publish/subscribe/distribute paradigm. It is better suited to regular
management than the pull model, and allows administrators to conserve network
bandwidth as well as CPU time on the management station. It can be seen as a
generalization of the paradigm commonly used for notification delivery.
Finally, we introduce the concept of the collapsed network management platform,
where these two models coexist.
","J. P. Martin-Flatin","","http://arxiv.org/abs/cs/9811027v1","http://arxiv.org/pdf/cs/9811027v1","","21 pages, submitted to IM'99,
  http://tcomwww.epfl.ch/~jpmf/papers/tr_1998_022.pdf","","","cs.NI","cs.NI|C.2.3; C.2.4"
"6","cs/9903019v2","1999-03-30 21:28:07","1999-03-31 22:44:35","Workflow Automation with Lotus Notes for the Governmental Administrative
  Information System","  The paper presents an introductory overview of the workflow automation area,
outlining the main types, basic technologies, the essential features of
workflow applications. Two sorts of process models for the definition of
workflows (according to the conversation-based and activity-based
methodologies) are sketched. Later on, the nature of Lotus Notes and its
capabilities (as an environment for workflow management systems development)
are indicated. Concluding, the experience of automating administrative
workflows (developing a Subsystem of Inter-institutional Document Management of
the VADIS project) is briefly outlined.
","Saulius Maskeliunas","Institute of Mathematics & Informatics, Vilnius","http://arxiv.org/abs/cs/9903019v2","http://arxiv.org/pdf/cs/9903019v2","","8 pages, 7 figures","","","cs.HC","cs.HC|H.4.1; H.5.3; J.1"
"7","astro-ph/9904229v2","1999-04-18 14:17:05","1999-04-21 17:56:35","On the Importance of PhD Institute in Establishing a Long-Term Research
  Career in Astronomy","  We have examined the success rates of 19 American, Canadian, Australian, and
Dutch graduate programs in producing astronomers. A 20-year baseline was
considered (1975-1994), incorporating 897 PhD recipients. The major conclusion
from our study is that the fraction of PhD graduates still involved in
astronomical research is surprisingly insensitive to the institutional source
of one's PhD. With few exception, 55-75% of graduates, regardless of PhD
source, remain active in the astronomical research community. While it remains
true that graduates of so-called ``prestigious'' programs preferentially
populate the same, it is also clear that an abundance of opportunities exist at
smaller ``non-prestigious'' institutions, liberal arts colleges, government,
and industry. The latter, of course, generally carry enhanced administrative
and/or teaching duties, but, on the other hand, do not entirely preclude a role
in the research community. A Kepler-Meier survival analysis of two disparate
institutes demonstrates that ``success'' is a dynamical entity, and that blind
consideration of a 20-year baseline sample can mask important recent trends.
Within ten years of PhD receipt, an equilibrium is reached in which 45% of the
graduates are in identifiably permanent positions, 20% remain in soft-money
positions, and 35% have left research entirely. Graduates of American
universities are 2-3 times more likely to find permanent employment in the USA
than Canadian or Australian graduates are within their respective institute's
country. While the number of American, Canadian, and Dutch PhDs have grown 20%
during the past decade, the growth in Australia has been closer to 70%.
","Brad K. Gibson|Michelle Buxton|Emanuel Vassiliadis|Maartje N. Sevenster|D. Heath Jones|Rebecca K. Thornberry","CASA|MSSSO|MSSSO|MSSSO|MSSSO|OHS","http://arxiv.org/abs/astro-ph/9904229v2","http://arxiv.org/pdf/astro-ph/9904229v2","","LaTeX, 8 pages, 1 figure, requires baasar.sty, to appear in the
  Bulletin of the American Astronomical Society (no changes to text; have
  simply added the previous statement letting the reader know where the paper
  is being published!)","","","astro-ph","astro-ph"
"8","physics/9905023v2","1999-05-07 14:10:17","1999-05-28 13:52:42","Progetto per la costituzione del centro di divulgazione della cultura
  scientifica a Sesto Fiorentino","  Two Italian non-profit organizations (Set and Galea) are working together to
realize a center (Cdcs) supported by the town administration of Sesto
Fiorentino (a little town not far from Florence, Italy), for the divulgation of
the scientific culture in the schools and among people. This is their project:
the key idea is to let the non-profit organizations intervent in the public
school system by creating cooperative structures in which young scientists can
work.
","Massimo Materassi|Marco Ulivi","S.E.T.|Galea","http://arxiv.org/abs/physics/9905023v2","http://arxiv.org/pdf/physics/9905023v2","","18 pages, no figures","","","physics.ed-ph","physics.ed-ph"
"9","cs/9908012v1","1999-08-17 01:09:54","1999-08-17 01:09:54","Safe Deals Between Strangers","  E-business, information serving, and ubiquitous computing will create heavy
request traffic from strangers or even incognitos. Such requests must be
managed automatically. Two ways of doing this are well known: giving every
incognito consumer the same treatment, and rendering service in return for
money. However, different behavior will be often wanted, e.g., for a university
library with different access policies for undergraduates, graduate students,
faculty, alumni, citizens of the same state, and everyone else.
  For a data or process server contacted by client machines on behalf of users
not previously known, we show how to provide reliable automatic access
administration conforming to service agreements. Implementations scale well
from very small collections of consumers and producers to immense client/server
networks. Servers can deliver information, effect state changes, and control
external equipment.
  Consumer privacy is easily addressed by the same protocol. We support
consumer privacy, but allow servers to deny their resources to incognitos. A
protocol variant even protects against statistical attacks by consortia of
service organizations.
  One e-commerce application would put the consumer's tokens on a smart card
whose readers are in vending kiosks. In e-business we can simplify supply chain
administration. Our method can also be used in sensitive networks without
introducing new security loopholes.
","H. M. Gladney","","http://arxiv.org/abs/cs/9908012v1","http://arxiv.org/pdf/cs/9908012v1","","31 pages, 6 figures","","","cs.CR","cs.CR|cs.DL|D.4.6; H.2.7; K.6.5"
"10","hep-ph/9911220v2","1999-11-03 14:08:05","1999-12-12 19:02:45","Kt effect for the top quark production in hadronic collisions","  Withdrawn by arXiv administration because authors have forged affiliations
and acknowledgements, and have not adequately responded to charges
[hep-th/9912039] of unattributed use of verbatim material.
","N. Mebarki|Z. Belghobsi|K. Benhizia","Constantine University, Constantine, Algeria|E.N.S de Jijel, Algeria.|Universite de Batna, Algeria","http://arxiv.org/abs/hep-ph/9911220v2","http://arxiv.org/pdf/hep-ph/9911220v2","","WITHDRAWN by arXiv administration","","","hep-ph","hep-ph"
"11","hep-th/9911044v2","1999-11-06 15:53:52","1999-12-12 18:56:33","Quantum Nonsymmetric Gravity and The Superfiber Bundle Formalism","  Withdrawn by arXiv administration because authors have forged affiliations
and acknowledgements, and have not adequately responded to charges
[hep-th/9912039] of unattributed use of verbatim material.
","N. Mebarki|A. Maireche","","http://arxiv.org/abs/hep-th/9911044v2","http://arxiv.org/pdf/hep-th/9911044v2","","WITHDRAWN by arXiv administration","Bulg.J.Phys.28:1-14,2001","","hep-th","hep-th"
"12","hep-th/9911045v2","1999-11-06 16:01:47","1999-12-12 18:56:51","N=1,D=4 Quantum Nonsymmetric supergravity","  Withdrawn by arXiv administration because authors have forged affiliations
and acknowledgements, and have not adequately responded to charges
[hep-th/9912039] of unattributed use of verbatim material.
","N. Mebarki|A. Maireche","","http://arxiv.org/abs/hep-th/9911045v2","http://arxiv.org/pdf/hep-th/9911045v2","","WITHDRAWN by arXiv administration","","","hep-th","hep-th"
"13","hep-th/9911046v2","1999-11-06 16:08:03","1999-12-12 18:56:55","Symmetries of Quantum Nonsymmetric Gravity","  Withdrawn by arXiv administration because authors have forged affiliations
and acknowledgements, and have not adequately responded to charges
[hep-th/9912039] of unattributed use of verbatim material.
","N. Mebarki|A. Maireche|A. Boudine|A. Benslama","","http://arxiv.org/abs/hep-th/9911046v2","http://arxiv.org/pdf/hep-th/9911046v2","","WITHDRAWN by arXiv administration","","","hep-th","hep-th"
"14","hep-th/9911047v2","1999-11-06 16:08:46","1999-12-12 18:56:58","Gauged BRST Transformations of Quantum Nonsymmetric Gravity","  Withdrawn by arXiv administration because authors have forged affiliations
and acknowledgements, and have not adequately responded to charges
[hep-th/9912039] of unattributed use of verbatim material.
","N. Mebarki|A. Maireche","","http://arxiv.org/abs/hep-th/9911047v2","http://arxiv.org/pdf/hep-th/9911047v2","","WITHDRAWN by arXiv administration","","","hep-th","hep-th"
"15","hep-th/9911048v2","1999-11-06 16:14:20","1999-12-12 18:57:02","Quantum Nonsymmetric Gravity Geometric Commutation Relations","  Withdrawn by arXiv administration because authors have forged affiliations
and acknowledgements, and have not adequately responded to charges
[hep-th/9912039] of unattributed use of verbatim material.
","N. Mebarki|A. Maireche","","http://arxiv.org/abs/hep-th/9911048v2","http://arxiv.org/pdf/hep-th/9911048v2","","WITHDRAWN by arXiv administration","Bulg.J.Phys.26:97-108,1999","","hep-th","hep-th"
"16","hep-th/9911049v2","1999-11-06 16:15:27","1999-12-12 18:57:09","QNGT Sixteen Dimensional GL(4,R)-like Superalgebra","  Withdrawn by arXiv administration because authors have forged affiliations
and acknowledgements, and have not adequately responded to charges
[hep-th/9912039] of unattributed use of verbatim material.
","N. Mebarki|A. Maireche|M. Haouchine","","http://arxiv.org/abs/hep-th/9911049v2","http://arxiv.org/pdf/hep-th/9911049v2","","WITHDRAWN by arXiv administration","","","hep-th","hep-th"
"17","cs/0003075v1","2000-03-23 16:19:44","2000-03-23 16:19:44","On the theory of system administration","  This paper describes necessary elements for constructing theoretical models
of network and system administration. Armed with a theoretical model it becomes
possible to determine best practices and optimal strategies in a way which
objectively relates policies and assumptions to results obtained. It is
concluded that a mixture of automation and human, or other intelligent
incursion is required to fully implement system policy with current technology.
Some aspects of the author's immunity model for automated system administration
are explained, as an example. A theoretical framework makes the prediction that
the optimal balance between resource availability and garbage collection
strategies is encompassed by the immunity model.
","Mark Burgess","","http://arxiv.org/abs/cs/0003075v1","http://arxiv.org/pdf/cs/0003075v1","","About 38 pages american size, 4 figures","","","cs.OH","cs.OH|H.1.m"
"18","cs/0006016v1","2000-06-08 18:08:19","2000-06-08 18:08:19","The X-Files: Investigating Alien Performance in a Thin-client World","  Many scientific applications use the X11 window environment; an open source
windows GUI standard employing a client/server architecture. X11 promotes:
distributed computing, thin-client functionality, cheap desktop displays,
compatibility with heterogeneous servers, remote services and administration,
and greater maturity than newer web technologies. This paper details the
author's investigations into close encounters with alien performance in
X11-based seismic applications running on a 200-node cluster, backed by 2 TB of
mass storage. End-users cited two significant UFOs (Unidentified Faulty
Operations) i) long application launch times and ii) poor interactive response
times. The paper is divided into three major sections describing Close
Encounters of the 1st Kind: citings of UFO experiences, the 2nd Kind: recording
evidence of a UFO, and the 3rd Kind: contact and analysis. UFOs do exist and
this investigation presents a real case study for evaluating workload analysis
and other diagnostic tools.
","Neil J. Gunther","","http://arxiv.org/abs/cs/0006016v1","http://arxiv.org/pdf/cs/0006016v1","","13 pages; Invited Lecture at the High Performance Computing
  Conference, University of Tromso, Norway, June 27-30, 1999","Proc. Hiper'99 Vol.1, p.156","","cs.PF","cs.PF|cs.DC|C.2.4;C.4;D.4.8;D.4.9;H.3.4;H.5.2;I.6.8"
"19","cs/0007012v1","2000-07-07 15:13:09","2000-07-07 15:13:09","Using Learning-based Filters to Detect Rule-based Filtering Obsolescence","  For years, Caisse des Depots et Consignations has produced information
filtering applications. To be operational, these applications require high
filtering performances which are achieved by using rule-based filters. With
this technique, an administrator has to tune a set of rules for each topic.
However, filters become obsolescent over time. The decrease of their
performances is due to diachronic polysemy of terms that involves a loss of
precision and to diachronic polymorphism of concepts that involves a loss of
recall.
  To help the administrator to maintain his filters, we have developed a method
which automatically detects filtering obsolescence. It consists in making a
learning-based control filter using a set of documents which have already been
categorised as relevant or not relevant by the rule-based filter. The idea is
to supervise this filter by processing a differential comparison of its
outcomes with those of the control one.
  This method has many advantages. It is simple to implement since the training
set used by the learning is supplied by the rule-based filter. Thus, both the
making and the use of the control filter are fully automatic. With automatic
detection of obsolescence, learning-based filtering finds a rich application
which offers interesting prospects.
","Francis Wolinski|Frantz Vichot|Mathieu Stricker","","http://arxiv.org/abs/cs/0007012v1","http://arxiv.org/pdf/cs/0007012v1","","13 pages, 12 figures, Content-based Multimedia Information Access,
  RIAO 2000","","","cs.CL","cs.CL|cs.AI|H.3.3; I.2.6"
"20","cs/0106058v1","2001-06-28 23:08:48","2001-06-28 23:08:48","Enabling the Long-Term Archival of Signed Documents through Time
  Stamping","  In this paper we describe how to build a trusted reliable distributed service
across administrative domains in a peer-to-peer network. The application we use
to motivate our work is a public key time stamping service called Prokopius.
The service provides a secure, verifiable but distributable stable archive that
maintains time stamped snapshots of public keys over time. This in turn allows
clients to verify time stamped documents or certificates that rely on formerly
trusted public keys that are no longer in service or where the signer no longer
exists. We find that such a service can time stamp the snapshots of public keys
in a network of 148 nodes at the granularity of a couple of days, even in the
worst case where an adversary causes the maximal amount of damage allowable
within our fault model.
","Petros Maniatis|T. J. Giuli|Mary Baker","","http://arxiv.org/abs/cs/0106058v1","http://arxiv.org/pdf/cs/0106058v1","","25 pages, 10 figures, unpublished","","","cs.DC","cs.DC|cs.CR|C.2.4; K.6.5; H.3.4"
"21","cs/0107034v1","2001-07-26 15:19:00","2001-07-26 15:19:00","NEOS Server 4.0 Administrative Guide","  The NEOS Server 4.0 provides a general Internet-based client/server as a link
between users and software applications. The administrative guide covers the
fundamental principals behind the operation of the NEOS Server, installation
and trouble-shooting of the Server software, and implementation details of
potential interest to a NEOS Server administrator. The guide also discusses
making new software applications available through the Server, including areas
of concern to remote solver administrators such as maintaining security,
providing usage instructions, and enforcing reasonable restrictions on jobs.
The administrative guide is intended both as an introduction to the NEOS Server
and as a reference for use when running the Server.
","Elizabeth D. Dolan","","http://arxiv.org/abs/cs/0107034v1","http://arxiv.org/pdf/cs/0107034v1","","45 pages including front matter, 3 figures","","","cs.DC","cs.DC|C.2.4"
"22","cs/0109021v2","2001-09-17 22:01:01","2001-09-17 23:01:32","Competing DNS Roots: Creative Destruction or Just Plain Destruction?","  The Internet Domain Name System (DNS) is a hierarchical name space that
enables the assignment of unique, mnemonic identifiers to Internet hosts and
the consistent mapping of these names to IP addresses. The root of the domain
name system is the top of the hierarchy and is currently managed by a
quasi-private centralized regulatory authority, the Internet Corporation for
Assigned Names and Numbers (ICANN). This paper identifies and discusses the
economic and policy issues raised by competing DNS roots. The paper provides a
precise definition of root-competition and shows that multiple roots are a
species of standards competition, in which network externalities play a major
role. The paper performs a structural analysis of the different forms that
competing DNS roots can take and their effects on end-user compatibility. It
then explores the policy implications of the various forms of competition.
  The thesis of the paper is that root competition is caused by a severe
disjunction between the demand for and supply of top-level domain names. ICANN
has authorized a tiny number of new top-level domains (7) and subjected their
operators to excruciatingly slow and expensive contractual negotiations. The
growth of alternate DNS roots is an attempt to bypass that bottleneck. The
paper arrives at the policy conclusion that competition among DNS roots should
be permitted and is a healthy outlet for inefficiency or abuses of power by the
dominant root administrator.
","Milton L. Mueller","","http://arxiv.org/abs/cs/0109021v2","http://arxiv.org/pdf/cs/0109021v2","","","","","cs.CY","cs.CY|K.4.m"
"23","cs/0109044v1","2001-09-22 04:22:13","2001-09-22 04:22:13","Analyzing ENUM Service and Administration from the Bottom Up: The
  addressing system for IP telephony and beyond","  ENUM creates many new market opportunities and raises several important
policy issues related to the implementation and administration of the ENUM
database and services. Recent World Telecommunications Policy Forum 2001 dealt
with the emergence of ENUM as an important numbering issue of IP telephony.
This paper prepares some important emerging issues of ENUM administration and
policy by taking an empirical research approach from the bottom up.
  We will identify potential key ENUM services, and estimating the size of the
service market opportunities created by the availability of PSTN-IP addressing
and mapping mechanisms, particularly in the context of IP telephony. Also, we
analyze the possible administrative models and relationship scenarios among
different ENUM players such as Registry(ies), Registrars, Telephone Service
Providers, ENUM Application Service Providers, etc. Then, we will assess the
effects of various administrative model architectures of ENUM service by
looking at the market opportunities and motivations of the players. From the
empirical findings, we will draw the implications on transactions among
different kinds of ENUM service providers. Finally, the results of the model
analysis will be used for the discussion of policy related issues around the
ENUM and IP telephony services.
  Keywords: IP Telephony, ENUM, Internet Policy, Numbering and Addressing
System, Service and Market Study, Administration Model, Empirical Market Study.
","Junseok Hwang|Milton Mueller|Gunyoung Yoon|Joonmin Kim","","http://arxiv.org/abs/cs/0109044v1","http://arxiv.org/pdf/cs/0109044v1","","29th TPRC Conference","","","cs.CY","cs.CY|K.4.m Miscellaneous"
"24","cs/0109064v1","2001-09-24 15:28:07","2001-09-24 15:28:07","Commonalities: The R.E.A. and High-Speed Rural Internet Access","  This paper explores commonalities between the creation of the Rural
Electrification Administration and the similar dilemma of providing an
affordable infrastructure for high-speed Internet access in places where profit
incentives do not exist. In the case of the R.E.A., the necessity for an
aggressive federal initiative to wire rural America, where the market for
electricity had failed, is revisited as the missing incentives are identified
and explored. We then examine the incentive-poor similarities between rural
electrification and rural high-speed Internet access through how consumers
currently and prospectively gain access to broadband Internet service. The
regulatory environment created by the Telecommunications Act of 1996 and the
Federal Communications Commission is considered. Although the FCC is required
(Section 254.b.3) to take regulatory measures to ensure comparable and
affordable access to the Internet for all Americans, the historical
similarities and comparative analysis of rural electrification and high-speed
Internet access suggests the goal of universal service is unlikely to be met in
the near future. Regulatory disincentives to build such networks are present,
driven in part by market realities and in part by competitive restrictions in
the Telecommunications Act of 1996. Finally, we pose the question of whether a
federal effort equivalent to the R.E.A. is needed to ensure that residents of
sparsely populated areas, like their predecessors in the 1930s, are not
comparatively disadvantaged in the first decades of the 21st century. The paper
concludes with a proposal to accelerate the deployment of broadband
infrastructure in rural America.
","Laurence J. Malone","","http://arxiv.org/abs/cs/0109064v1","http://arxiv.org/pdf/cs/0109064v1","","29th TPRC Conference, 2001","","","cs.CY","cs.CY|K.4.m Miscellaneous"
"25","cs/0109073v1","2001-09-24 18:48:33","2001-09-24 18:48:33","E-Business and SMEs: Preliminary Evidence from Selected Italian
  Districts","  The debate on the Information Society shows large agreement on the assumption
that the promised benefits will fully display only if the diffusion of ICTs and
the Internet will involve all the actors of the socio-economic system.
Accordingly, special emphasis is put on the participation of small and medium
enterprises (SMEs), but also on public administrations (PAs) as promoters and
catalysts of private initiatives. As for SMEs, public intervention concerns
both the promotion of fully competitive e-markets and the solution of market
failures. However, effective and efficient intervention requires specific
information on SMEs' approach to e-commerce, often depending upon specific
sector and local condition and in most cases still lacking. In order to
identify the need and the scope for public intervention, the paper focuses on a
peculiar SMEs-intensive productive environment: the manufacturing industrial
district, which traditionally constitutes an examples of winning SMEs' network,
characterised by common industrial culture and intense input-output
interactions. The paper presents empirical evidence from the Italian districts
of Como (textile industry) and Lumezzane (metalwork industry). The research
results show that pro-active entrepreneurs are creatively exploring the
opportunities offered by the Internet to promote their businesses. However, it
is also clear that the transition to the Internet economy still involves a
reduced percentage of potential participants, and that institutional actions
are needed in order to foster a larger participation.
","Lucia Piscitello|Francesca Sgobbi","","http://arxiv.org/abs/cs/0109073v1","http://arxiv.org/pdf/cs/0109073v1","","29th Research Conference on Communication, Information and Internet
  Policy, October 27-29, 2001, Alexandria, Virginia Area: Economic Growth and
  Development","","","cs.CY","cs.CY|A.0; J.4"
"26","cs/0109091v2","2001-09-24 21:53:55","2001-10-01 19:26:03","E PLURIBUS ENUM: Unifying International Telecommunications Networks and
  Governance","  ENUM effectively bridges the telephone and Internet worlds by placing
telephone numbers from the ITU Rec. E.164 public telecommunication numbering
plan into the Internet Domain Name System (DNS) as domain names. ENUM
potentially presents significant public policy issues at both the domestic and
international levels. Ultimately, it should not matter whether ENUM is
approached as a telecommunications issue or an Internet issue because: (1) they
are becoming the same thing technically, and (2) they engage the same global
public interests. For the same reasons as apply to traditional
telecommunications, and even to the Internet itself, public oversight of ENUM
naming, numbering, and addressing resources is justified both by technical
necessity and the interests of consumer protection (particularly personal
privacy) and competition at higher service layers. A single, coordinated global
DNS domain for at least Tier 0 (the international level) of the ENUM names
hierarchy should be designated by public authorities. Many of the technical
characteristics and policy considerations relevant at the ENUM Tier 0 and 1
zones are also directly applicable to the Internet's IP address space and DNS
root (or Tier 0) zone - key shared elements of the Internet's logical
infrastructure. Despite the fundamentally international nature of the
Internet's logical infrastructure layer, and the purported privatization of
administration of its IP address space and the DNS, Internet governance is not
yet truly international. The ENUM policy debate illustrates the need for
authoritative international public oversight of public communications network
logical infrastructure, including that of traditional telecommunications, the
Internet, and ENUM.
","Craig McTaggart","","http://arxiv.org/abs/cs/0109091v2","http://arxiv.org/pdf/cs/0109091v2","","29th TPRC Conference, 2001","","","cs.CY","cs.CY|cs.NI|K.4.m Miscellaneous"
"27","cs/0110013v1","2001-10-03 09:24:17","2001-10-03 09:24:17","A Proposal for Dynamic Access Lists for TCP/IP Packet Filering","  The use of IP filtering to improve system security is well established, and
although limited in what it can achieve has proved to be efficient and
effective.
  In the design of a security policy there is always a trade-off between
usability and security. Restricting access means that legitimate use of the
network is prevented; allowing access means illegitimate use may be allowed.
Static access list make finding a balance particularly stark -- we pay the
price of decreased security 100% of the time even if the benefit of increased
usability is only gained 1% of the time.
  Dynamic access lists would allow the rules to change for short periods of
time, and to allow local changes by non-experts. The network administrator can
set basic security guide-lines which allow certain basic services only. All
other services are restricted, but users are able to request temporary
exceptions in order to allow additional access to the network. These exceptions
are granted depending on the privileges of the user.
  This paper covers the following topics: (1) basic introduction to TCP/IP
filtering; (2) semantics for dynamic access lists and; (3) a proposed protocol
for allowing dynamic access; and (4) a method for representing access lists so
that dynamic update and look-up can be done efficiently performed.
","Scott Hazelhurst","","http://arxiv.org/abs/cs/0110013v1","http://arxiv.org/pdf/cs/0110013v1","","12 pages. Shortened version appeared in SAICSIT 2001","","","cs.NI","cs.NI|cs.LO|F2.2; F4.1; F4.2; I2.4"
"28","gr-qc/0110023v5","2001-10-04 06:19:56","2007-08-22 19:12:54","Exact Solution of Photon Equation in a Nonstationary Godel-Type
  Cosmological Universe","  This paper has excessive overlap with the following papers also written by
the authors or their collaborators: gr-qc/0207026, gr-qc/0502059,
gr-qc/0502061, and gr-qc/0510038.
","Ali Havare|Taylan Yetkin|Oktay Aydogdu","","http://arxiv.org/abs/gr-qc/0110023v5","http://arxiv.org/pdf/gr-qc/0110023v5","http://dx.doi.org/10.1142/S0218271804004888","This submission has been withdrawn by arXiv administrators due to
  inappropriate text reuse from external sources","Int.J.Mod.Phys.D13:935-944,2004","10.1142/S0218271804004888","gr-qc","gr-qc"
"29","hep-ph/0110403v1","2001-10-31 12:56:18","2001-10-31 12:56:18","Report on the PCaPAC 2000 Workshop","  In October 2000, the third PCaPAC (PCs and Particle Accelerator Controls)
workshop took place at DESY. This paper presents a summary of the workshop. The
workshop reviewed existing and new PC-based accelerator control systems, from
small-scale to large-scale installations. It demonstrated convincingly the
advantage of modern, commercial mass-market products used for accelerator
controls. Disadvantages of these technologies were reported as well.
Large-scale PC systems inherently bring administrative concerns into the
picture. In this vein, special emphasis was given to system administration for
distributed systems. A major topic of the workshop was the integration of
different control system approaches as well as the integration of different
platforms within the same control system. In particular, PC-based concepts
offer the simple opportunity to interface to commercial SCADA systems. In
addition, large emphasis was given to the presentation of future developments
including the next network trends to data exchange via SOAP and XML.
","R. Bacher","","http://arxiv.org/abs/hep-ph/0110403v1","http://arxiv.org/pdf/hep-ph/0110403v1","","ICALEPCS 2001 TUBT005","eConf C011127 (2001) TUBT005","","hep-ph","hep-ph"
"30","cs/0111028v1","2001-11-09 14:29:16","2001-11-09 14:29:16","The ESRF TANGO control system status","  TANGO is an object oriented control system toolkit based on CORBA presently
under development at the ESRF. IN this paper, the TANGO philosophy is briefly
presented. All the existing tools developed around TANGO will also be
presented. This include a code genrator, a WEB interface to TANGO objects, an
administration tool and an interface to LabView. Finally, an xample of a TANGO
device server for OPC device is given.
","JM. Chaize|A. Goetz|WD. Klotz|J. Meyer|M. Perez|E. Taurel|P. Verdier","","http://arxiv.org/abs/cs/0111028v1","http://arxiv.org/pdf/cs/0111028v1","","3 pages","eConf C011127 (2001) TUAP004","","cs.DC","cs.DC|C.2.4;D.1.3"
"31","math/0203245v1","2002-03-23 10:37:39","2002-03-23 10:37:39","A Generator System of Invariant differential forms","  We obtain a generator system of the algebra of $\mathrm{GL}(V)$-invariant
differential forms on $\mathrm{End}_{\bf k} (V)$. The proof uses the Weyl-Schur
reciprocity.
","Tensai Bakabon","","http://arxiv.org/abs/math/0203245v1","http://arxiv.org/pdf/math/0203245v1","","(from arXiv administration) Prank submission from Yoshinaga Masahiko
  <math-ys@kusm.kyoto-u.ac.jp>, 3 pages","","","math.GM","math.GM"
"32","hep-ph/0205006v2","2002-05-01 12:32:33","2002-05-06 03:15:34","QCD as a theory of hadrons (from partons to confinement)","  This submission is withdrawn by arXiv administrators. arXiv does not accept
book advertisements.
","Stephan Narison","LPMT-Montpellier Univ. France","http://arxiv.org/abs/hep-ph/0205006v2","http://arxiv.org/pdf/hep-ph/0205006v2","","Summary of a book withdrawn by arXiv administrators","Camb.Monogr.Part.Phys.Nucl.Phys.Cosmol.17:1,2002","","hep-ph","hep-ph"
"33","gr-qc/0207026v3","2002-07-04 09:57:03","2007-07-26 21:04:42","The Massless DKP Equation and Maxwell Equations in Bianchi Type III
  Spacetimes","  This paper has been removed by arXiv administrators because it plagiarizes
P.K. Jena, P.C. Naik and T. Pradhan, ""Photon As The Zero Mass Limit Of Dkp
Field,"" J. Phys. A {\bf 13}, 2975 (1980) [not cited within submission]. In
addition, the following submissions by the authors and their collaborators all
contain a great deal of overlap: gr-qc/0502059, gr-qc/0502061, gr-qc/0207026,
hep-th/0110228, and hep-th/0207088.
","T. Yetkin|A. Havare","","http://arxiv.org/abs/gr-qc/0207026v3","http://arxiv.org/pdf/gr-qc/0207026v3","","This submission has been withdrawn by arXiv administrators due to
  inappropriate text reuse from external sources","Chin.J.Phys.41:475,2003","","gr-qc","gr-qc"
"34","astro-ph/0208177v1","2002-08-08 07:54:37","2002-08-08 07:54:37","WhiteDwarf.org - Establishing a permanent endowment for the Whole Earth
  Telescope","  White Dwarf Research Corporation is a 501(c)(3) non-profit organization
dedicated to scientific research and public education on topics relevant to
white dwarf stars. It was founded in 1999 in Austin, Texas to help fulfill the
need for an alternative research center where scarce funding dollars could be
used more efficiently, and to provide a direct link between astronomers who
study white dwarf stars and the general public. Due to its administrative
simplicity, WDRC can facilitate the funding of multi-institutional and
international collaborations, provide seamless grant portability, minimize
overhead rates, and actively seek non-governmental funding sources. I describe
the motivation for, and current status of, one of the long-term goals of WDRC:
to establish a permanent endowment for the operation of the Whole Earth
Telescope. I pay particular attention to fund-raising efforts through the
website at http://WhiteDwarf.org/donate/
","Travis S. Metcalfe","White Dwarf Research Corporation","http://arxiv.org/abs/astro-ph/0208177v1","http://arxiv.org/pdf/astro-ph/0208177v1","http://dx.doi.org/10.1515/astro-2017-0053","5 pages, to appear in proceedings of the 6th Whole Earth Telescope
  workshop","Baltic Astron. 12 (2003) 295","10.1515/astro-2017-0053","astro-ph","astro-ph"
"35","physics/0208046v3","2002-08-11 11:34:50","2004-02-25 13:48:42","Peer review in context","  Scientific publishing is in a transition between the old paper-bound, static
forms and the new electronic media with its interactive, dynamic possibilities.
This takes place in the context of imploding library budgets and exploding
magazine costs. The scientists as authors, reviewers and editors of scientific
journals are exposed to an increased pressure by the their administrations and
the public towards quantification, objectification and certification of
scientific achievements. The ``publication roulette'' resulting from
low-quality editorial procedures often amounts to malign censorship, which not
only is experienced as a frustration by the authors, but is also delaying and
hampering the progress of science. It also leads to a waste of funds under the
cover of pseudo-objectivity and pseudo-legitimacy of financial decisions.
Different solutions are outlined and discussed. As concerns scientific
publishing, an e-print service should be established, which, in continuation of
existing e-servers such as arxiv.org, is operated either directly by the United
Nations Educational, Scientific and Cultural Organization, or by an
international consortium. In order to become generally accepted by the
scientists, certification criteria must be provided, which would make it
possible to successfully pursue a scientific career besides the traditional
peer reviewed print publications.
","Karl Svozil","","http://arxiv.org/abs/physics/0208046v3","http://arxiv.org/pdf/physics/0208046v3","","22 pages, presented at ODOK '03 in Salzburg, Austria, September
  23-26, 2003 (see http://voeb.uibk.ac.at/odok2003/svozil.pdf for a German
  version), and at the INST conference ""The Unifying Aspects of Cultures"",
  Vienna, November 7-9, 2003","TRANS. Internet-Zeitschrift f\""ur Kulturwissenschaften. No.
  15/2003. WWW: http://www.inst.at/trans/15Nr/03_2/svozil15.htm","","physics.soc-ph","physics.soc-ph"
"36","cs/0210002v1","2002-10-01 10:41:52","2002-10-01 10:41:52","GridBank: A Grid Accounting Services Architecture (GASA) for Distributed
  Systems Sharing and Integration","  Computational Grids are emerging as new infrastructure for Internet-based
parallel and distributed computing. They enable the sharing, exchange,
discovery, and aggregation of resources distributed across multiple
administrative domains, organizations and enterprises. To accomplish this,
Grids need infrastructure that supports various services: security, uniform
access, resource management, scheduling, application composition, computational
economy, and accountability. Many Grid projects have developed technologies
that provide many of these services with an exception of accountability. To
overcome this limitation, we propose a new infrastructure called Grid Bank that
provides services for accounting. This paper presents requirements of Grid
accountability and different models within which it can operate and proposes
Grid Bank Services Architecture that meets them. The paper highlights
implementation issues with detailed discussion on format for various
records/database that the GridBank need to maintain. It also presents protocols
for interaction between GridBank and various components within Grid computing
environments.
","Alexander Barmouta Rajkumar Buyya","","http://arxiv.org/abs/cs/0210002v1","http://arxiv.org/pdf/cs/0210002v1","","12 pages","","","cs.DC","cs.DC|C.2.4"
"37","hep-th/0301234v2","2003-01-29 08:50:48","2003-02-03 19:22:17","The two-loop effective action and some symmetries of the Riemann tensor
  and the Kalb-Ramond H field","  Withdrawn by arXiv administration because the text and equations were
plagiarized almost entirely verbatim from hep-th/9610131 .
","Suayyip Salim Ozkurt","","http://arxiv.org/abs/hep-th/0301234v2","http://arxiv.org/pdf/hep-th/0301234v2","","WITHDRAWN by arXiv administration due to plagiarism of hep-th/9610131","","","hep-th","hep-th"
"38","cs/0302017v1","2003-02-12 22:10:04","2003-02-12 22:10:04","A Proposal to Separate Handles from Names on the Internet","  Networked communications inherently depend on the ability of the sender of a
message to indicate through some token how the message should be delivered to a
particular recipient. The tokens that refer messages to recipients are
variously known as routes, addresses,handles, and names} ordered by their
relative nearness to network topology vs. human meaning. All four sorts of
token refer in some way to a recipient, but they are controlled by different
authorities and their meanings depend on different contextual parameters.
  Today's global Internet employs dynamically determined routes, IP addresses,
and domain names. Domain names combine the functions of handles and names. The
high value of domain names as names leads to substantial social and legal
dispute about their assignment, degrading their value as handles. The time has
come to provide a distinct open network handle system (ONHS), using handles
that are not meaningful in natural language and are therefore not subject to
the disputes surrounding the use of names.
  A handle service may be deployed easily as a handle domain within the current
Domain Name System. In order to minimize the administrative load, and maximize
their own autonomy, netizens may use public-key cryptography to assign their
own handles.
","Michael J. O'Donnell","","http://arxiv.org/abs/cs/0302017v1","http://arxiv.org/pdf/cs/0302017v1","","","","","cs.NI","cs.NI|C.2.3"
"39","hep-ph/0302267v2","2003-02-28 05:36:35","2003-04-09 08:26:31","The CP Violation Problem","  Withdrawn by arXiv administration because the text and equations were
plagiarized from chapters 1 and 6 of the BaBar Physics Book
http://www.slac.stanford.edu/pubs/slacreports/slac-r-504.html
  See also hep-ph/0304045
","Ramy Naboulsi","","http://arxiv.org/abs/hep-ph/0302267v2","http://arxiv.org/pdf/hep-ph/0302267v2","","WITHDRAWN by arXiv administration due to plagiarism of chapters 1 and
  6 of the BaBar Physics Book","","","hep-ph","hep-ph"
"40","cs/0303033v3","2003-03-30 18:46:46","2004-11-21 23:54:22","A Digital Preservation Appliance Based on OpenBSD","  The LOCKSS program has developed and deployed in a world-wide test a system
for preserving access to academic journals published on the Web. The
fundamental problem for any digital preservation system is that it must be
affordable for the long term. To reduce the cost of ownership, the LOCKSS
system uses generic PC hardware, open source software, and peer-to-peer
technology. It is packaged as a ``network appliance'', a single-function box
that can be connected to the Internet, configured and left alone to do its job
with minimal monitoring or administration. The first version of this system was
based on a Linux boot floppy. After three years of testing it was replaced by a
second version, based on OpenBSD and booting from CD-ROM.
  We focus in this paper on the design, implementation and deployment of a
network appliance based on an open source operating system. We provide an
overview of the LOCKSS application and describe the experience of deploying and
supporting its first version. We list the requirements we took from this to
drive the design of the second version, describe how we satisfied them in the
OpenBSD environment, and report on the initial
","David S. H. Rosenthal","","http://arxiv.org/abs/cs/0303033v3","http://arxiv.org/pdf/cs/0303033v3","","12 pages","Proceedings of BSDcon, 2003","","cs.DC","cs.DC|cs.DL|D.4.5"
"41","hep-ph/0304039v2","2003-04-04 06:38:19","2003-04-09 08:11:58","Theory of Hadronic Decays in B Meson System in the SM","  Withdrawn by arXiv administration because the text and equations were
plagiarized from chapter 10 of the BaBar Physics Book
http://www.slac.stanford.edu/pubs/slacreports/slac-r-504.html
  See also hep-ph/0304045
","Ramy Naboulsi","","http://arxiv.org/abs/hep-ph/0304039v2","http://arxiv.org/pdf/hep-ph/0304039v2","","WITHDRAWN by arXiv administration due to plagiarism of chapter 10 of
  the BaBar Physics Book","","","hep-ph","hep-ph"
"42","hep-ph/0304040v2","2003-04-04 06:43:13","2003-04-09 23:53:36","A Review of Heavy-Quark and Chiral Perturbation Theory","  Withdrawn by arXiv administration because the text and equations were
plagiarized from chapter 10 of the BaBar Physics Book
http://www.slac.stanford.edu/pubs/slacreports/slac-r-504.html
  See also hep-ph/0304045
","Ramy Naboulsi","","http://arxiv.org/abs/hep-ph/0304040v2","http://arxiv.org/pdf/hep-ph/0304040v2","","WITHDRAWN by arXiv administration due to plagiarism of chapter 10 of
  the BaBar Physics Book","","","hep-ph","hep-ph"
"43","hep-ph/0304041v2","2003-04-04 06:47:10","2003-04-09 23:55:29","B^0bar{B}^0 Mixing in the SM","  Withdrawn by arXiv administration because the text and equations were
plagiarized from chapter 11 of the BaBar Physics Book
http://www.slac.stanford.edu/pubs/slacreports/slac-r-504.html
  See also hep-ph/0304045
","Ramy Naboulsi","","http://arxiv.org/abs/hep-ph/0304041v2","http://arxiv.org/pdf/hep-ph/0304041v2","","WITHDRAWN by arXiv administration due to plagiarism of chapter 11 of
  the BaBar Physics Book","","","hep-ph","hep-ph"
"44","hep-ph/0304042v2","2003-04-04 06:50:54","2003-04-09 08:16:56","CP Violation and the Problem Measuring phi_1","  Withdrawn by arXiv administration because the text and equations were
plagiarized from chapter 7 of the BaBar Physics Book
http://www.slac.stanford.edu/pubs/slacreports/slac-r-504.html
  See also hep-ph/0304045
","Ramy Naboulsi","","http://arxiv.org/abs/hep-ph/0304042v2","http://arxiv.org/pdf/hep-ph/0304042v2","","WITHDRAWN by arXiv administration due to plagiarism of chapter 7 of
  the BaBar Physics Book","","","hep-ph","hep-ph"
"45","hep-ph/0304043v2","2003-04-04 06:57:18","2003-04-10 00:15:09","CP-Violation in B Mesons and Physics Beyond the SM","  Withdrawn by arXiv administration because the text and equations were
plagiarized from chapter 13 of the BaBar Physics Book
http://www.slac.stanford.edu/pubs/slacreports/slac-r-504.html
  See also hep-ph/0304045
","Ramy Naboulsi","","http://arxiv.org/abs/hep-ph/0304043v2","http://arxiv.org/pdf/hep-ph/0304043v2","","WITHDRAWN by arXiv administration due to plagiarism of chapter 13 of
  the BaBar Physics Book","","","hep-ph","hep-ph"
"46","hep-ph/0304044v2","2003-04-04 07:00:00","2003-04-09 23:57:43","Theory of Inclusive Decays of Heavy Hadron","  Withdrawn by arXiv administration because the text and equations were
plagiarized from chapter 11 of the BaBar Physics Book
http://www.slac.stanford.edu/pubs/slacreports/slac-r-504.html
  See also hep-ph/0304045
","Ramy Naboulsi","","http://arxiv.org/abs/hep-ph/0304044v2","http://arxiv.org/pdf/hep-ph/0304044v2","","WITHDRAWN by arXiv administration due to plagiarism of chapter 11 of
  the BaBar Physics Book","","","hep-ph","hep-ph"
"47","physics/0305005v2","2003-05-01 20:39:22","2003-05-05 21:50:14","Software Scalability Issues in Large Clusters","  The rapid development of large clusters built with commodity hardware has
highlighted scalability issues with deploying and effectively running system
software in large clusters. We describe here our experiences with monitoring,
image distribution, batch and other system administration software tools within
the 1000+ node Linux cluster currently in production at the RHIC Computing
Facility.
","A. Chan|R. Hogue|C. Hollowell|O. Rind|T. Throwe|T. Wlodek","","http://arxiv.org/abs/physics/0305005v2","http://arxiv.org/pdf/physics/0305005v2","","Poster presentation from CHEP2003, La Jolla, CA, USA, March 2003, 8
  pages, LaTeX, 9 low-resolution eps figures. Reference code TUDP002. High
  resolution figures can be found in
  http://www.rhic.bnl.gov/RCF/Organization/People/TonyChan.shtml","","","physics.comp-ph","physics.comp-ph"
"48","hep-ex/0305032v1","2003-05-14 20:05:34","2003-05-14 20:05:34","Parallel Reconstruction of CLEO III Data","  Reconstruction of one run of CLEO III raw data can take up to 9 days to
complete using a single processor. This is an administrative nightmare, and
even minor failures result in reprocessing the entire run, which wastes time,
money and CPU power. We leveraged the ability of the CLEO III software
infrastructure to read and write multiple file formats to perform
reconstruction of a single run using several CPUs in parallel. Using the Sun
Grid Engine and some Perl scripts, we assign roughly equal-sized chunks of
events to different CPUs. The Raw data are read from an Objectivity/DB
database, but the reconstruction output is written to a temporary file, not the
database. This takes about 6 hours. Once all the chunks have been analyzed,
they are gathered together in event-number order and injected into
Objectivity/DB. This process takes an additional 6 to 18 hours, depending on
run size. A web-based monitoring tool displays the status of reconstruction.
Many benefits accrue from this process, including a dramatic increase in
efficiency, a 20% increase in luminosity processed per week, more predictable
and manageable processor farm load, reduction in the time to stop the processor
farm from up to 9 days to less than 24 hours, superior fault tolerance, quicker
feedback and repair times for bugs in the reconstruction code, and faster
turn-around of early runs for data quality and software correctness checks.
","Gregory J. Sharp|Christopher D. Jones","","http://arxiv.org/abs/hep-ex/0305032v1","http://arxiv.org/pdf/hep-ex/0305032v1","","Talk from the 2003 Computing in High Energy and Nuclear Physics
  (CHEP03), La Jolla, Ca, USA, March 2003, 5 pages, LaTeX, 1 eps figure. PSN
  MODT001","","","hep-ex","hep-ex"
"49","cs/0305054v1","2003-05-29 13:29:27","2003-05-29 13:29:27","A Monitoring System for the BaBar INFN Computing Cluster","  Monitoring large clusters is a challenging problem. It is necessary to
observe a large quantity of devices with a reasonably short delay between
consecutive observations. The set of monitored devices may include PCs, network
switches, tape libraries and other equipments. The monitoring activity should
not impact the performances of the system. In this paper we present PerfMC, a
monitoring system for large clusters. PerfMC is driven by an XML configuration
file, and uses the Simple Network Management Protocol (SNMP) for data
collection. SNMP is a standard protocol implemented by many networked
equipments, so the tool can be used to monitor a wide range of devices. System
administrators can display informations on the status of each device by
connecting to a WEB server embedded in PerfMC. The WEB server can produce
graphs showing the value of different monitored quantities as a function of
time; it can also produce arbitrary XML pages by applying XSL Transformations
to an internal XML representation of the cluster's status. XSL Transformations
may be used to produce HTML pages which can be displayed by ordinary WEB
browsers. PerfMC aims at being relatively easy to configure and operate, and
highly efficient. It is currently being used to monitor the Italian
Reprocessing farm for the BaBar experiment, which is made of about 200 dual-CPU
Linux machines.
","M. Marzolla|V. Melloni","","http://arxiv.org/abs/cs/0305054v1","http://arxiv.org/pdf/cs/0305054v1","","Talk from the 2003 Computing in High Energy and Nuclear Physics
  (CHEP03), La Jolla, Ca, USA, March 2003, 10 pages, LaTeX, 4 eps figures. PSN
  MOET006","ECONFC0303241:MOET006,2003","","cs.PF","cs.PF|B.8.2; C.2.3"
"50","cs/0305059v1","2003-05-30 10:07:12","2003-05-30 10:07:12","EU DataGRID testbed management and support at CERN","  In this paper we report on the first two years of running the CERN testbed
site for the EU DataGRID project. The site consists of about 120 dual-processor
PCs distributed over several testbeds used for different purposes: software
development, system integration, and application tests. Activities at the site
included test productions of MonteCarlo data for LHC experiments, tutorials and
demonstrations of GRID technologies, and support for individual users analysis.
This paper focuses on node installation and configuration techniques, service
management, user support in a gridified environment, and includes
considerations on scalability and security issues and comparisons with
""traditional"" production systems, as seen from the administrator point of view.
","E. Leonardi|M. W. Schulz","","http://arxiv.org/abs/cs/0305059v1","http://arxiv.org/pdf/cs/0305059v1","","Talk from the 2003 Computing in High Energy and Nuclear Physics
  (CHEP03), La Jolla, Ca, USA, March 2003, 7 pages, LaTeX. PSN THCT007","ECONF C0303241:THCT007,2003","","cs.DC","cs.DC|C.1.4; C.2.4; D.4.7"
"51","cs/0305061v1","2003-05-30 11:42:37","2003-05-30 11:42:37","A Secure Infrastructure For System Console and Reset Access","  During the last years large farms have been built using commodity hardware.
This hardware lacks components for remote and automated administration.
Products that can be retrofitted to these systems are either costly or
inherently insecure. We present a system based on serial ports and simple
machine controlled relays. We report on experience gained by setting up a
50-machine test environment as well as current work in progress in the area.
","Andras Horvath|Emanuele Leonardi|Markus Schulz","","http://arxiv.org/abs/cs/0305061v1","http://arxiv.org/pdf/cs/0305061v1","","Conference for Computing in High Energy and Nuclear Physics
  (CHEP2003), March 24-28, 2003, La Jolla, California","","","cs.DC","cs.DC|K.6.4; C.2.4"
"52","cs/0306064v2","2003-06-13 13:38:01","2003-06-28 09:38:27","Exploiting peer group concept for adaptive and highly available services","  This paper presents a prototype for redundant, highly available and fault
tolerant peer to peer framework for data management. Peer to peer computing is
gaining importance due to its flexible organization, lack of central authority,
distribution of functionality to participating nodes and ability to utilize
unused computational resources. Emergence of GRID computing has provided much
needed infrastructure and administrative domain for peer to peer computing. The
components of this framework exploit peer group concept to scope service and
information search, arrange services and information in a coherent manner,
provide selective redundancy and ensure availability in face of failure and
high load conditions. A prototype system has been implemented using JXTA peer
to peer technology and XML is used for service description and interfaces,
allowing peers to communicate with services implemented in various platforms
including web services and JINI services. It utilizes code mobility to achieve
role interchange among services and ensure dynamic group membership. Security
is ensured by using Public Key Infrastructure (PKI) to implement group level
security policies for membership and service access.
","Muhammad Asif Jan|Fahd Ali Zahid|Mohammad Moazam Fraz|Arshad Ali","Centre for European Nuclear Research|Foundation University, Islamabad, Pakistan|Foundation University, Islamabad, Pakistan|National University of Science and Technology, Pakistan","http://arxiv.org/abs/cs/0306064v2","http://arxiv.org/pdf/cs/0306064v2","","The Paper Consists of 5 pages, 6 figures submitted in Computing in
  High Energy and Nuclear Physics, 24-28 March 2003 La Jolla California. CHEP03","","","cs.DC","cs.DC|H 3.4"
"53","cs/0306094v1","2003-06-16 06:12:04","2003-06-16 06:12:04","BaBar - A Community Web Site in an Organizational Setting","  The BABAR Web site was established in 1993 at the Stanford Linear Accelerator
Center (SLAC) to support the BABAR experiment, to report its results, and to
facilitate communication among its scientific and engineering collaborators,
currently numbering about 600 individuals from 75 collaborating institutions in
10 countries. The BABAR Web site is, therefore, a community Web site. At the
same time it is hosted at SLAC and funded by agencies that demand adherence to
policies decided under different priorities. Additionally, the BABAR Web
administrators deal with the problems that arise during the course of managing
users, content, policies, standards, and changing technologies. Desired
solutions to some of these problems may be incompatible with the overall
administration of the SLAC Web sites and/or the SLAC policies and concerns.
There are thus different perspectives of the same Web site and differing
expectations in segments of the SLAC population which act as constraints and
challenges in any review or re-engineering activities. Web Engineering, which
post-dates the BABAR Web, has aimed to provide a comprehensive understanding of
all aspects of Web development. This paper reports on the first part of a
recent review of application of Web Engineering methods to the BABAR Web site,
which has led to explicit user and information models of the BABAR community
and how SLAC and the BABAR community relate and react to each other. The paper
identifies the issues of a community Web site in a hierarchical,
semi-governmental sector and formulates a strategy for periodic reviews of
BABAR and similar sites.
","Ray Cowan|Yogesh Deshpande|Bebo White","","http://arxiv.org/abs/cs/0306094v1","http://arxiv.org/pdf/cs/0306094v1","","Talk from the 2003 Computing in High Energy and Nuclear Physics
  (CHEP03), La Jolla, Ca, USA, March 2003, 8 pages, PDF, PSN MONT006","","","cs.IR","cs.IR|H.4.m"
"54","cs/0307021v1","2003-07-08 16:58:57","2003-07-08 16:58:57","Tools and Techniques for Managing Clusters for SciDAC Lattice QCD at
  Fermilab","  Fermilab operates several clusters for lattice gauge computing. Minimal
manpower is available to manage these clusters. We have written a number of
tools and developed techniques to cope with this task. We describe our tools
which use the IPMI facilities of our systems for hardware management tasks such
as remote power control, remote system resets, and health monitoring. We
discuss our techniques involving network booting for installation and upgrades
of the operating system on these computers, and for reloading BIOS and other
firmware. Finally, we discuss our tools for parallel command processing and
their use in monitoring and administrating the PBS batch queue system used on
our clusters.
","A. Singh|D. Holmgren|R. Rechenmacher|S. Epsteyn","","http://arxiv.org/abs/cs/0307021v1","http://arxiv.org/pdf/cs/0307021v1","","Talk from the 2003 Computing in High Energy and Nuclear Physics
  (CHEP03), La Jolla, Ca, USA, March 2003, 4 pages, PDF. PSN TUIT005","ECONFC0303241:TUIT005,2003","","cs.DC","cs.DC|C.4; C.m"
"55","hep-ph/0309208v1","2003-09-17 19:36:24","2003-09-17 19:36:24","The Narrow Theta(1543)--A QCD Dilemma: Tube or Not Tube?","  We argue that a width of less than MeV of the new Theta resonance is
inconsistent with the observed ratio of resonance and background events in the
various photon initiated experiments if the latter can be described by K, K*,
etc., exchange. An evaluation of the Feynman diagrams which were believed to be
relevant is presented and supports the general claim in the one case where a
cross section has been given by the experimental group.
  More detailed arguments based on the flux tube model explaining the narrow
widths and the apparent conflict with the production rates are presented. We
predict narrow Tetra-quarks at mass ~ O(1-1.2 GeV) which the analysis of LEAR
may have missed.
","Aharon Casher|Shmuel Nussinov","Tel Aviv University and Univ. of South Carolina|Tel Aviv University and Univ. of South Carolina","http://arxiv.org/abs/hep-ph/0309208v1","http://arxiv.org/pdf/hep-ph/0309208v1","http://dx.doi.org/10.1016/j.physletb.2003.10.041","9 pages, 2 figures Please note that Nussinov has previously submitted
  entries to the ArXives through assistance of other individuals. The submitter
  of this paper (robinet@physics.umd.edu) is L. Robinette, administrative
  assistant to the Maryland TQHN Group; comments should be addressed to
  nussinov@ccsg.tau.ac.il","Phys.Lett. B578 (2004) 124-132","10.1016/j.physletb.2003.10.041","hep-ph","hep-ph"
"56","physics/0401104v1","2004-01-21 21:08:03","2004-01-21 21:08:03","The Enhancement of Confocal Images of Tissues at Bulk Optical Immersion","  The purpose of the present work is a theoretical examination of how localized
skin-tissue dehydration affects the depth of the confocal probing and what
depth of effective detection can be reached with the chemical administration of
skin tissues. A semi-infinite multilayer Monte Carlo model is used to estimate
spatial localization of the output signal offered by a confocal probe. A
solution of glycerol is taken in the capacity of innocuous osmotic agent.
Diffusion of this bio-compatible chemical agent into the skin temporarily
pushes water out of the tissues and results in the matching of the refractive
indices of skin structural elements. This temporarily decreases scattering and
increases transparency of topical skin layers, which allows for unrestricted
light to permeate deeper into the skin. The results of simulation show that
signal spatial localization offered by a confocal probe in the skin tissues
during their clearing is usable for the monitoring of deep reticular dermis and
improving the image contrast and spatial resolution. A discussion of the
optical properties of skin tissues and their changes due to diffusion of
glycerol into the skin is given. Optical properties of tissues and their
changes due to chemical administration are estimated based on the results of
experimental in vitro study with rat and human skin.
","I. V. Meglinski|D. Y. Churmakov|A. N. Bashkatov|E. A. Genina|V. V. Tuchin","","http://arxiv.org/abs/physics/0401104v1","http://arxiv.org/pdf/physics/0401104v1","","5 pages, 5 figures","Laser Physics 13 (2003) 65-69","","physics.med-ph","physics.med-ph|physics.optics"
"57","cs/0402025v1","2004-02-12 14:36:05","2004-02-12 14:36:05","A perspective on the Healthgrid initiative","  This paper presents a perspective on the Healthgrid initiative which involves
European projects deploying pioneering applications of grid technology in the
health sector. In the last couple of years, several grid projects have been
funded on health related issues at national and European levels. A crucial
issue is to maximize their cross fertilization in the context of an environment
where data of medical interest can be stored and made easily available to the
different actors in healthcare, physicians, healthcare centres and
administrations, and of course the citizens. The Healthgrid initiative,
represented by the Healthgrid association (http://www.healthgrid.org), was
initiated to bring the necessary long term continuity, to reinforce and promote
awareness of the possibilities and advantages linked to the deployment of GRID
technologies in health. Technologies to address the specific requirements for
medical applications are under development. Results from the DataGrid and other
projects are given as examples of early applications.
","V. Breton|A. E. Solomonides|R. H. McClatchey","","http://arxiv.org/abs/cs/0402025v1","http://arxiv.org/pdf/cs/0402025v1","http://dx.doi.org/10.1109/CCGrid.2004.1336598","6 pages, 1 figure. Accepted by the Second International Workshop on
  Biomedical Computations on the Grid, at the 4th IEEE/ACM International
  Symposium on Cluster Computing and the Grid (CCGrid 2004). Chicago USA, April
  2004","","10.1109/CCGrid.2004.1336598","cs.DB","cs.DB|cs.SE|H2.4,J.3"
"58","math/0402259v8","2004-02-16 14:43:10","2007-03-06 11:25:52","A New Approach to Renormalization, Using Zeta regularization","  In this paper we present a method to deal with divergences in perturbation
theory using the method of the Zeta regularization, first of all we use the
Euler-Mc Laurin Sum formula to associate the divergent integral to a divergent
sum in the form 1+2^{m}+3^{m}+... After that we find a recurrence formula for
the integrals and apply zeta regularization techniques to obtain finite results
for the divergent series. (Through all the paper we use the notation m, for the
power of the modulus of p, so we must not confuse it with the value of the mass
of the quantum particle) changes: added Bibliography about ramanujan
Resummation and references of Zeta regularization in Hardy,s Book ""divergent
series"".
","Jose Javier Garcia Moreta","","http://arxiv.org/abs/math/0402259v8","http://arxiv.org/pdf/math/0402259v8","","This submission has been withdrawn by arXiv administrators because of
  fraudulently claimed institutional affiliation and status","","","math.GM","math.GM|11.10.Gh 03.65.-w"
"59","nlin/0403042v1","2004-03-21 11:19:42","2004-03-21 11:19:42","The Structural Dynamics of Corruption: Artificial Society Approach","  Corruption has been an important issue as it becomes obstacle to achieve the
better and more efficient economic governmental system. The paper defines
corruption in two ways, as state capture and administrative corruption to grasp
the quintessence of the corruption cases modeled in dynamical computational
social system. The result of experiments through simulation is provided in
order to construct an understanding of structural properties of corruption,
giving way to consider corruption not as an isolated phenomenon, but
conclusively, as an interdisciplinary problem and should be handled in holistic
perspectives.
","Hokky Situngkir","","http://arxiv.org/abs/nlin/0403042v1","http://arxiv.org/pdf/nlin/0403042v1","","13 pages, 9 figures","","","nlin.AO","nlin.AO"
"60","hep-ph/0404226v2","2004-04-26 09:57:06","2004-06-22 20:10:18","Observation of the Second Tensor Nonet with the L3 Experiment","  This paper was withdrawn by arXiv administrators upon request of the
Chairperson and Spokesperson of the L3 Collaboration.
","V. A. Schegelsky|A. V. Sarantsev|V. A. Nikonov|A. V. Anisovich|M. P. Levtchenko","","http://arxiv.org/abs/hep-ph/0404226v2","http://arxiv.org/pdf/hep-ph/0404226v2","","This paper has been withdrawn","","","hep-ph","hep-ph"
"61","cs/0405006v3","2004-05-04 14:51:55","2005-01-20 15:52:59","Bi-criteria Algorithm for Scheduling Jobs on Cluster Platforms","  We describe in this paper a new method for building an efficient algorithm
for scheduling jobs in a cluster. Jobs are considered as parallel tasks (PT)
which can be scheduled on any number of processors. The main feature is to
consider two criteria that are optimized together. These criteria are the
makespan and the weighted minimal average completion time (minsum). They are
chosen for their complementarity, to be able to represent both user-oriented
objectives and system administrator objectives. We propose an algorithm based
on a batch policy with increasing batch sizes, with a smart selection of jobs
in each batch. This algorithm is assessed by intensive simulation results,
compared to a new lower bound (obtained by a relaxation of ILP) of the optimal
schedules for both criteria separately. It is currently implemented in an
actual real-size cluster platform.
","Pierre-Francois Dutot|Lionel Eyraud|Gregory Mounie|Denis Trystram","ID - IMAG|ID - IMAG|ID - IMAG|ID - IMAG","http://arxiv.org/abs/cs/0405006v3","http://arxiv.org/pdf/cs/0405006v3","","","ACM Symposium on Parallel Algorithms and Architectures (2004)
  125-132","","cs.DC","cs.DC|cs.DS|ACM F.2.2, ACM D.4.1"
"62","math/0405088v2","2004-05-06 11:32:11","2004-05-19 14:15:02","On the stability of dual scattering channel schemes","  This paper was withdrawn by arXiv administrators. It is an erroneous
duplicate submission of math.NA/0405095.
","Steffen Hein","","http://arxiv.org/abs/math/0405088v2","http://arxiv.org/pdf/math/0405088v2","","This paper has been withdrawn","","","math.NA","math.NA"
"63","hep-ph/0405117v2","2004-05-13 09:09:29","2004-06-22 20:12:55","The observation of the second nonet of tensor mesons with the L3
  experiment","  This paper was withdrawn by arXiv administrators upon request of the
Chairperson and Spokesperson of the L3 Collaboration.
","V. Schegelsky|A. Sarantsev|V. Nikonov|A. Anisovich|M. Levtchenko","","http://arxiv.org/abs/hep-ph/0405117v2","http://arxiv.org/pdf/hep-ph/0405117v2","","This paper has been withdrawn","","","hep-ph","hep-ph"
"64","cs/0405105v1","2004-05-27 13:46:36","2004-05-27 13:46:36","Study of Pakistan Election System as Intelligent e-Election","  The proposed election system lies in ensuring that it is transparent and
impartial.Thus while the electoral system may vary from country to country, It
has to take into account the peculiarities of every society while at the same
time incorporating remedies to problems prevailing in the system.
  The Electoral process expressed serious concerns regarding the independence
of the Election Commission of Pakistan, the restrictions on political parties
and their candidates, the misuse of state resources, some unbalanced coverage
in the state media, deficiencies in the compilation of the voting register and
significant problems relating to the provision of ID cards.
  The holding of a general election does not in itself guarantee the
restoration of democracy. The unjustified interference with electoral
arrangements, as detailed above, irrespective of the alleged motivation,
resulted in serious flaws being inflicted on the electoral process.
Additionally, questions remain as to whether or not there will be a full
transfer of power from a military to civilian administration.
  The Independent study research has following modules:
  Login/Subscription Module Candidate Subscription Module Vote casting Module
Administration Module Intelligent decision data analysis Module
","Muhammad Nadeem|Javaid R. Laghari","Szabist, Karachi|Szabist, Karachi","http://arxiv.org/abs/cs/0405105v1","http://arxiv.org/pdf/cs/0405105v1","","6 pages","","","cs.CY","cs.CY|H.2.8"
"65","cs/0408035v1","2004-08-14 16:05:47","2004-08-14 16:05:47","Monitoring, Analyzing, and Controlling Internet-scale Systems with ACME","  Analyzing and controlling large distributed services under a wide range of
conditions is difficult. Yet these capabilities are essential to a number of
important development and operational tasks such as benchmarking, testing, and
system management. To facilitate these tasks, we have built the Application
Control and Monitoring Environment (ACME), a scalable, flexible infrastructure
for monitoring, analyzing, and controlling Internet-scale systems. ACME
consists of two parts. ISING, the Internet Sensor In-Network agGregator,
queries sensors and aggregates the results as they are routed through an
overlay network. ENTRIE, the ENgine for TRiggering Internet Events, uses the
data streams supplied by ISING, in combination with a user's XML configuration
file, to trigger actuators such as killing processes during a robustness
benchmark or paging a system administrator when predefined anomalous conditions
are observed. In this paper we describe the design, implementation, and
evaluation of ACME and its constituent parts. We find that for a 512-node
system running atop an emulated Internet topology, ISING's use of in-network
aggregation can reduce end-to-end query-response latency by more than 50%
compared to using either direct network connections or the same overlay network
without aggregation. We also find that an untuned implementation of ACME can
invoke an actuator on one or all nodes in response to a discrete or aggregate
event in less than four seconds, and we illustrate ACME's applicability to
concrete benchmarking and monitoring scenarios.
","David Oppenheimer|Vitaliy Vatkovskiy|Hakim Weatherspoon|Jason Lee|David A. Patterson|John Kubiatowicz","","http://arxiv.org/abs/cs/0408035v1","http://arxiv.org/pdf/cs/0408035v1","","","","","cs.DC","cs.DC|cs.NI"
"66","cs/0411055v1","2004-11-17 13:27:17","2004-11-17 13:27:17","SDS : Une infrastructure d'installation de logiciels libres pour des
  organisations multi-sites","  Les developpements logiciels sur les systemes UNIX font de plus en plus appel
aux logiciels libres. Nous proposons une solution de deploiement et de controle
de ces logiciels libres au sein d'une grande organisation. Nous nous attachons
particulierement a resoudre les problemes lies au deploiement multi-sites ainsi
qu'a la gestion de configuration de ces deploiements. L'originalite de notre
approche repose sur sa capacite a etre mise en oeuvre et controlee par les
utilisateurs plutot que par les administrateurs, sans necessiter d'expertise
particuliere, et par les possibilites de deploiement dans des environnements
heterogenes.
  -----
  Free and open source software is more and more used for software developments
on UNIX systems. We are proposing a solution to control the deployment of free
software in the context of a large corporation, focusing on multi-site
deployment and configuration management. The originality of our approach rests
on its ability to be implemented and controlled by users rather than
administrators, without requiring any particular expertise, and on its facility
to be deployed in heterogeneous environments.
","Laurent Charles|Manuel Vacelet|Mohamed Chaari|Miguel Santana","","http://arxiv.org/abs/cs/0411055v1","http://arxiv.org/pdf/cs/0411055v1","","","DECOR04 (2004) 37-48","","cs.NI","cs.NI"
"67","cs/0411066v1","2004-11-18 19:00:35","2004-11-18 19:00:35","Using LDAP Directories for Management of PKI Processes","  We present a framework for extending the functionality of LDAP servers from
their typical use as a public directory in public key infrastructures. In this
framework the LDAP servers are used for administrating infrastructure
processes. One application of this framework is a method for providing
proof-of-possession, especially in the case of encryption keys. Another one is
the secure delivery of software personal security environments.
","V. Karatsiolis|M. Lippert|A. Wiesmaier","","http://arxiv.org/abs/cs/0411066v1","http://arxiv.org/pdf/cs/0411066v1","","9 pages, 1 figure","In Proceedings of Public Key Infrastructure: First European PKI
  Workshop: Research and Applications, EuroPKI 2004, volume 3093 of Lecture
  Notes in Computer Science, pages 126-134, June 2004","","cs.CR","cs.CR"
"68","cs/0411068v2","2004-11-18 19:40:59","2005-01-13 17:07:55","Planning for Directory Services in Public Key Infrastructures","  In this paper we provide a guide for public key infrastructure designers and
administrators when planning for directory services. We concentrate on the LDAP
directories and how they can be used to successfully publish PKI information.
We analyse their available mechanisms and propose a best practice guide for use
in PKI. We then take a look into the German Signature Act and Ordinance and
discuss their part as far as directories concerning. Finally, we translate
those to the LDAP directories practices.
","V. Karatsiolis|M. Lippert|A. Wiesmaier","","http://arxiv.org/abs/cs/0411068v2","http://arxiv.org/pdf/cs/0411068v2","","12 pages; 1 figure; accepted at QSIG2005 (see
  http://www-sec.uni-regensburg.de/sicherheit2005/index.shtml); camera ready
  version","Proceedings of ""Sicherheit 2005""; April 2005","","cs.CR","cs.CR"
"69","cs/0411088v1","2004-11-24 13:31:25","2004-11-24 13:31:25","Des correctifs de securite a la mise a jour","  The ever growing software complexity suggests that they will never be bugfree
and therefore secure. Software compagnies regulary publish updates. But maybe
because of lack of time or care or maybe because stopping application is
annoying, such updates are rarely if ever deployed on users' machines. We
propose an integrated tool allowing system administrators to deploy critical
security updates on the fly on applications running remotly without end-user
intervention. Our approach is based on an aspect weaving system, Arachne, that
dynamicaly rewrites binary code. Hence updated applications are still running
while they are updated. Our second tool Minerve integrates Arachne within the
standart updating process: Minerve takes a patch produced by dif and eventually
builds a dynamic patch that can later be woven to update the application on the
fly. In addition, Minerve allows to consult patches translated in a dedicated
language and hence eases auditing tasks.
","Nicolas Loriant|Marc Segura Devillechaise|Jean-Marc Menaud","OBASCO IRISA|OBASCO IRISA|OBASCO IRISA","http://arxiv.org/abs/cs/0411088v1","http://arxiv.org/pdf/cs/0411088v1","","","DECOR04 (2004) 65-76","","cs.NI","cs.NI"
"70","cs/0412068v1","2004-12-17 13:17:01","2004-12-17 13:17:01","ANTIDS: Self-Organized Ant-based Clustering Model for Intrusion
  Detection System","  Security of computers and the networks that connect them is increasingly
becoming of great significance. Computer security is defined as the protection
of computing systems against threats to confidentiality, integrity, and
availability. There are two types of intruders: the external intruders who are
unauthorized users of the machines they attack, and internal intruders, who
have permission to access the system with some restrictions. Due to the fact
that it is more and more improbable to a system administrator to recognize and
manually intervene to stop an attack, there is an increasing recognition that
ID systems should have a lot to earn on following its basic principles on the
behavior of complex natural systems, namely in what refers to
self-organization, allowing for a real distributed and collective perception of
this phenomena. With that aim in mind, the present work presents a
self-organized ant colony based intrusion detection system (ANTIDS) to detect
intrusions in a network infrastructure. The performance is compared among
conventional soft computing paradigms like Decision Trees, Support Vector
Machines and Linear Genetic Programming to model fast, online and efficient
intrusion detection systems.
","Vitorino Ramos|Ajith Abraham","","http://arxiv.org/abs/cs/0412068v1","http://arxiv.org/pdf/cs/0412068v1","","13 pages, 3 figures, Swarm Intelligence and Patterns (SIP)- special
  track at WSTST 2005, Muroran, JAPAN","","","cs.CR","cs.CR|cs.AI|H.3.3; I.2.11;I.5"
"71","cs/0412074v1","2004-12-17 15:38:19","2004-12-17 15:38:19","Threats of Human Error in a High-Performance Storage System: Problem
  Statement and Case Study","  System administration is a difficult, often tedious, job requiring many
skilled laborers. The data that is protected by system administrators is often
valued at or above the value of the institution maintaining that data. A number
of ethnographic studies have confirmed the skill of these operators, and the
difficulty of providing adequate tools. In an effort to minimize the
maintenance costs, an increasing portion of system administration is subject to
automation - particularly simple, routine tasks such as data backup. While such
tools reduce the risk of errors from carelessness, the same tools may result in
reduced skill and system familiarity in experienced workers. Care should be
taken to ensure that operators maintain system awareness without placing the
operator in a passive, monitoring role.
","Elizabeth Haubert","","http://arxiv.org/abs/cs/0412074v1","http://arxiv.org/pdf/cs/0412074v1","","13 pages, 1 figure","","","cs.HC","cs.HC|cs.OS|H.1.2"
"72","cs/0412097v1","2004-12-21 10:57:15","2004-12-21 10:57:15","The Computational Power of Benenson Automata","  The development of autonomous molecular computers capable of making
independent decisions in vivo regarding local drug administration may
revolutionize medical science. Recently Benenson at el (2004) have envisioned
one form such a ``smart drug'' may take by implementing an in vitro scheme, in
which a long DNA state molecule is cut repeatedly by a restriction enzyme in a
manner dependent upon the presence of particular short DNA ``rule molecules.''
To analyze the potential of their scheme in terms of the kinds of computations
it can perform, we study an abstraction assuming that a certain class of
restriction enzymes is available and reactions occur without error. We also
discuss how our molecular algorithms could perform with known restriction
enzymes. By exhibiting a way to simulate arbitrary circuits, we show that these
``Benenson automata'' are capable of computing arbitrary Boolean functions.
Further, we show that they are able to compute efficiently exactly those
functions computable by log-depth circuits. Computationally, we formalize a new
variant of limited width branching programs with a molecular implementation.
","David Soloveichik|Erik Winfree","","http://arxiv.org/abs/cs/0412097v1","http://arxiv.org/pdf/cs/0412097v1","http://dx.doi.org/10.1016/j.tcs.2005.07.027","18 pages","Theoretical Computer Science 344(2-3): 279-297, 2005","10.1016/j.tcs.2005.07.027","cs.CC","cs.CC|F.1.1; F.1.3"
"73","gr-qc/0502031v3","2005-02-08 11:49:56","2007-08-22 19:17:21","Inhomogeneous space-times and their energy distributions","  This paper also has excessove overlap with the following papers also written
by the authors or their collaborators: gr-qc/0502060, gr-qc/0606028,
gr-qc/0511095, gr-qc/0505078, gr-qc/0603044, gr-qc/0608014, gr-qc/0510123,
gr-qc/0607109, gr-qc/0602012, and others.
","O. Aydogdu|M. Salti|M. Korunur|A. Havare","","http://arxiv.org/abs/gr-qc/0502031v3","http://arxiv.org/pdf/gr-qc/0502031v3","","This submission has been withdrawn by arXiv administrators due to
  inappropriate text reuse from external sources","","","gr-qc","gr-qc"
"74","gr-qc/0502032v3","2005-02-08 12:02:51","2007-08-22 19:19:31","The Dynamics of Photon in the Shuwer: A Class of Solutions","  This paper has excessive overlap with the following papers also written by
the authors or their collaborators: gr-qc/0502059, gr-qc/0502061,
gr-qc/0510037, gr-qc/0510038, gr-qc/0207026, and others.
","M. Salti|O. Aydogdu|A. Havare|M. Korunur","","http://arxiv.org/abs/gr-qc/0502032v3","http://arxiv.org/pdf/gr-qc/0502032v3","","This submission has been withdrawn by arXiv administrators due to
  inappropriate text reuse from external sources","","","gr-qc","gr-qc"
"75","cs/0502052v1","2005-02-09 19:49:10","2005-02-09 19:49:10","Log Analysis Case Study Using LoGS","  A very useful technique a network administrator can use to identify
problematic network behavior is careful analysis of logs of incoming and
outgoing network flows. The challenge one faces when attempting to undertake
this course of action, though, is that large networks tend to generate an
extremely large quantity of network traffic in a very short period of time,
resulting in very large traffic logs which must be analyzed post-generation
with an eye for contextual information which may reveal symptoms of problematic
traffic. A better technique is to perform real-time log analysis using a
real-time context-generating tool such as LoGS.
","Dmitry Mogilevsky","","http://arxiv.org/abs/cs/0502052v1","http://arxiv.org/pdf/cs/0502052v1","","","","","cs.CR","cs.CR|cs.IR"
"76","gr-qc/0502042v5","2005-02-10 13:04:43","2007-08-22 18:08:30","Relative Energy-Momentum of the Bianchi-I Type Universes in Teleparallel
  Gravity","  This paper has been removed by arXiv administrators because it plagiarizes
gr-qc/0011027, gr-qc/0205028, and gr-qc/0303034.
","Mustafa Salti|Ali Havare","","http://arxiv.org/abs/gr-qc/0502042v5","http://arxiv.org/pdf/gr-qc/0502042v5","http://dx.doi.org/10.1007/s10509-005-5159-7","This submission has been withdrawn by arXiv administrators due to
  inappropriate text reuse from external sources","","10.1007/s10509-005-5159-7","gr-qc","gr-qc"
"77","gr-qc/0502043v4","2005-02-10 13:11:13","2007-07-26 18:49:01","The Momentum 4-Vector Imparted by Gravitational Waves in Bianchi-type
  Metrics","  This paper has been removed by arXiv administrators because it plagiarizes
gr-qc/0008034, ""The Energy Distribution of the Bianchi Type I Universe,"" by I.
Radinschi; Phys. Rev. D 41, 1086 (1990), ""Energy associated with a Kerr-Newman
black hole,"" by K.S. Virbhadra; and Braz. J. Phys. 30, 181 (2000), ""On The
Localization Of The Gravitational Energy,"" by N.P. Neto and P.I. Trajtenberg.
","A. Havare|M. Korunur|M. Salti","","http://arxiv.org/abs/gr-qc/0502043v4","http://arxiv.org/pdf/gr-qc/0502043v4","http://dx.doi.org/10.1007/s10509-008-9734-6","This submission has been withdrawn by arXiv administrators due to
  inappropriate text reuse from external sources","Astrophys.Space Sci.301:43-46,2006; Erratum-ibid.314:241,2008","10.1007/s10509-008-9734-6","gr-qc","gr-qc"
"78","gr-qc/0502058v2","2005-02-12 12:41:25","2007-08-22 19:22:16","Energy Distribution in LTB Space-time","  This paper has been removed by arXiv administrators because it overlaps
hep-th/0308070, and others.
  This paper also has excessive overlap with the following paper also written
by the authors or their collaborators: gr-qc/0506061.
","Mustafa Salti|Ali Havare","","http://arxiv.org/abs/gr-qc/0502058v2","http://arxiv.org/pdf/gr-qc/0502058v2","","This submission has been withdrawn by arXiv administrators due to
  inappropriate text reuse from external sources","","","gr-qc","gr-qc"
"79","gr-qc/0502059v3","2005-02-12 12:44:11","2007-07-26 21:01:49","On the Equivalence of the Massless DKP equation and the Maxwell
  Equations in the Shuwer","  This paper has been removed by arXiv administrators because it plagiarizes
P.K. Jena, P.C. Naik and T. Pradhan, ""Photon As The Zero Mass Limit Of Dkp
Field,"" J. Phys. A 13, 2975 (1980) [not cited within submission]. In addition,
the following submissions by the authors and their collaborators all contain a
great deal of overlap: gr-qc/0502059, gr-qc/0502061, gr-qc/0207026,
hep-th/0110228, and hep-th/0207088.
","Mustafa Salti|Ali Havare","","http://arxiv.org/abs/gr-qc/0502059v3","http://arxiv.org/pdf/gr-qc/0502059v3","http://dx.doi.org/10.1142/S0217732305015768","This submission has been withdrawn by arXiv administrators due to
  inappropriate text reuse from external sources","Mod.Phys.Lett.A20:451-465,2005","10.1142/S0217732305015768","gr-qc","gr-qc"
"80","gr-qc/0502060v3","2005-02-12 12:47:25","2007-07-26 19:36:47","Energy-Momentum in viscous Kasner-type Universe in Bergmann-Thomson
  Formulations","  This paper has been removed by arXiv administrators because it plagiarizes
gr-qc/0303034, ""The energy of the universe in teleparallel gravity,"" by T.
Vargas; and gr-qc/0011027, ""Viscous cosmologies in scalar-tensor theories for
Kasner type metrics,"" by M. Cataldo, S. del Campo and P. Salgado.
","Mustafa Salti|Ali Havare","","http://arxiv.org/abs/gr-qc/0502060v3","http://arxiv.org/pdf/gr-qc/0502060v3","http://dx.doi.org/10.1142/S0217751X05020926","This submission has been withdrawn by arXiv administrators due to
  inappropriate text reuse from external sources","Int.J.Mod.Phys.A20:2169-2177,2005","10.1142/S0217751X05020926","gr-qc","gr-qc"
"81","gr-qc/0502061v4","2005-02-12 12:51:31","2007-07-26 21:02:47","The Massless Spin-1 Particles in the Rotating Space-times","  This paper has been removed by arXiv administrators because it plagiarizes
P.K. Jena, P.C. Naik and T. Pradhan, ""Photon As The Zero Mass Limit Of Dkp
Field,"" J. Phys. A {\bf 13}, 2975 (1980) [not cited within submission]. In
addition, the following submissions by the authors and their collaborators all
contain a great deal of overlap: gr-qc/0502059, gr-qc/0502061, gr-qc/0207026,
hep-th/0110228, and hep-th/0207088.
","Mustafa Salti|Oktay Aydogdu|Ali Havare|Murat Korunur","","http://arxiv.org/abs/gr-qc/0502061v4","http://arxiv.org/pdf/gr-qc/0502061v4","","This submission has been withdrawn by arXiv administrators due to
  inappropriate text reuse from external sources","Fizika B15:71-84,2006","","gr-qc","gr-qc"
"82","cs/0503016v2","2005-03-07 23:17:23","2005-06-03 17:27:36","File-based storage of Digital Objects and constituent datastreams:
  XMLtapes and Internet Archive ARC files","  This paper introduces the write-once/read-many XMLtape/ARC storage approach
for Digital Objects and their constituent datastreams. The approach combines
two interconnected file-based storage mechanisms that are made accessible in a
protocol-based manner. First, XML-based representations of multiple Digital
Objects are concatenated into a single file named an XMLtape. An XMLtape is a
valid XML file; its format definition is independent of the choice of the
XML-based complex object format by which Digital Objects are represented. The
creation of indexes for both the identifier and the creation datetime of the
XML-based representation of the Digital Objects facilitates OAI-PMH-based
access to Digital Objects stored in an XMLtape. Second, ARC files, as
introduced by the Internet Archive, are used to contain the constituent
datastreams of the Digital Objects in a concatenated manner. An index for the
identifier of the datastream facilitates OpenURL-based access to an ARC file.
The interconnection between XMLtapes and ARC files is provided by conveying the
identifiers of ARC files associated with an XMLtape as administrative
information in the XMLtape, and by including OpenURL references to constituent
datastreams of a Digital Object in the XML-based representation of that Digital
Object.
","Xiaoming Liu|Lyudmila Balakireva|Patrick Hochstenbach|Herbert Van de Sompel","","http://arxiv.org/abs/cs/0503016v2","http://arxiv.org/pdf/cs/0503016v2","","12 pages, 1 figures (camera-ready copy for ECDL 2005)","","","cs.DL","cs.DL"
"83","astro-ph/0504001v1","2005-03-31 21:44:05","2005-03-31 21:44:05","The Color Selection of Quasars from Redshifts 5 to 10: Cloning and
  Discovery","  We present simulations of quasar colors, magnitudes, and numbers at redshifts
5<z<10 based on our discovery of ten new high-redshift quasars and the cloning
of lower redshift Sloan Digital Sky Survey (SDSS) quasars. The ten quasars have
redshifts ranging from z=4.7 to z=5.3 and i-magnitudes of 20.21 to 20.94. The
natural diversity of spectral features in the cloned sample allows more
realistic simulation of the quasar locus width than previously possible with
synthetic template spectra. Colors are generated for the z>6 epoch taking
advantage of the new UKIDSS near-infrared filter set, and we examine the
redshift intervals of maximum productivity, discussing color selection and
survey depth issues. On the basis of the SDSS sample, we find that the surface
density of z>4.7 quasars increases by a factor of 3X by extending 0.7
i-magnitudes deeper than the SDSS spectroscopic survey limit of i=20.2 --
correspondingly we predict a total of ~400 faint quasars in the SDSS main area
that have redshift z>4.7 and magnitudes $<20.9.
","Kuenley Chiu|Wei Zheng|Donald P. Schneider|Karl Glazebrook|Masanori Iye|Nobunari Kashikawa|Zlatan Tsvetanov|Michitoshi Yoshida|Jon Brinkmann","Johns Hopkins University|Johns Hopkins University|Penn State University|Johns Hopkins University|National Astronomical Observatory of Japan, Tokyo|National Astronomical Observatory of Japan, Tokyo|Johns Hopkins University|National Astronomical Observatory of Japan, Okayama|Apache Point Observatory","http://arxiv.org/abs/astro-ph/0504001v1","http://arxiv.org/pdf/astro-ph/0504001v1","http://dx.doi.org/10.1086/430525","14 pages, 11 Postscript figures, uses emulateapj.sty The Astronomical
  Journal, accepted, in press","Astron.J.130:13-22,2005","10.1086/430525","astro-ph","astro-ph"
"84","astro-ph/0505018v2","2005-05-02 12:39:53","2007-08-22 19:10:09","Rigidly Rotating Strange Quark Star","  This paper has been removed by arXiv administrators because it plagiarizes
gr-qc/0404032 and others.
","I. Yilmaz|H. Baysal","","http://arxiv.org/abs/astro-ph/0505018v2","http://arxiv.org/pdf/astro-ph/0505018v2","http://dx.doi.org/10.1142/S0218271805006158","This submission has been withdrawn by arXiv administrators due to
  inappropriate text reuse from external sources","Int.J.Mod.Phys.D14:697-706,2005","10.1142/S0218271805006158","astro-ph","astro-ph"
"85","q-bio/0505020v1","2005-05-10 14:40:30","2005-05-10 14:40:30","The contact network of patients in a regional healthcare system","  Yet in spite of advances in hospital treatment, hospitals continue to be a
breeding ground for several airborne diseases and for diseases that are
transmitted through close contacts like SARS, methicillin-resistant
Staphylococcus aureus (MRSA), norovirus infections and tuberculosis (TB). Here
we extract contact networks for up to 295,108 inpatients for durations up to
two years from a database used for administrating a local public healthcare
system serving a population of 1.9 million individuals. Structural and
dynamical properties of the network of importance for the transmission of
contagious diseases are then analyzed by methods from network epidemiology. The
contact networks are found to be very much determined by an extreme (age
independent) variation in duration of hospital stays and the hospital
structure. We find that that the structure of contacts between in-patients
exhibit structural properties, such as a high level of transitivity,
assortativity and variation in number of contacts, that are likely to be of
importance for the transmission of less contagious diseases. If these
properties are considered when designing prevention programs the risk for and
the effect of epidemic outbreaks may be decreased.
","Fredrik Liljeros|Petter Holme|Johan Giesecke","","http://arxiv.org/abs/q-bio/0505020v1","http://arxiv.org/pdf/q-bio/0505020v1","http://dx.doi.org/10.1080/08898480701612899","","Mathematical Population Studies 14, (2007) 269-284","10.1080/08898480701612899","q-bio.OT","q-bio.OT|physics.soc-ph|q-bio.PE"
"86","astro-ph/0505311v1","2005-05-14 23:32:57","2005-05-14 23:32:57","A Statistical Solar Flare Forecast Method","  A Bayesian approach to solar flare prediction has been developed, which uses
only the event statistics of flares already observed. The method is simple,
objective, and makes few ad hoc assumptions. It is argued that this approach
should be used to provide a baseline prediction for certain space weather
purposes, upon which other methods, incorporating additional information, can
improve. A practical implementation of the method for whole-Sun prediction of
Geostationary Observational Environment Satellite (GOES) events is described in
detail, and is demonstrated for 4 November 2003, the day of the largest
recorded GOES flare. A test of the method is described based on the historical
record of GOES events (1975-2003), and a detailed comparison is made with US
National Oceanic and Atmospheric Administration (NOAA) predictions for
1987-2003. Although the NOAA forecasts incorporate a variety of other
information, the present method out-performs the NOAA method in predicting mean
numbers of event days, for both M-X and X events. Skill scores and other
measures show that the present method is slightly less accurate at predicting
M-X events than the NOAA method, but substantially more accurate at predicting
X events, which are important contributors to space weather.
","M. S. Wheatland","","http://arxiv.org/abs/astro-ph/0505311v1","http://arxiv.org/pdf/astro-ph/0505311v1","http://dx.doi.org/10.1029/2004SW000131","9 pages, 9 figures, 3 tables","","10.1029/2004SW000131","astro-ph","astro-ph"
"87","gr-qc/0505078v3","2005-05-16 08:18:54","2007-07-26 18:35:25","Different Approaches for Moller's Energy in the Kasner-type Space-time","  This paper has been removed by arXiv administrators because it plagiarizes
gr-qc/0011027, ""Viscous cosmologies in scalar-tensor theories for Kasner type
metrics,"" by M. Cataldo, S. del Campo and P. Salgado; Phys. Rev. D 41, 1086
(1990), ""Energy associated with a Kerr-Newman black hole,"" by K.S. Virbhadra;
and gr-qc/0303034, ""The energy of the universe in teleparallel gravity,"" by T.
Vargas.
","Mustafa Salti","","http://arxiv.org/abs/gr-qc/0505078v3","http://arxiv.org/pdf/gr-qc/0505078v3","http://dx.doi.org/10.1142/S0217732305017901","This submission has been withdrawn by arXiv administrators due to
  inappropriate text reuse from external sources","Mod.Phys.Lett.A20:2175-2182,2005","10.1142/S0217732305017901","gr-qc","gr-qc"
"88","gr-qc/0505079v3","2005-05-16 08:21:11","2007-07-26 18:32:06","Energy of the Universe in Bianchi-type I Models in Moller's Tetrad
  Theory of Gravity","  This paper has been removed by arXiv administrators because it plagiarizes
gr-qc/0011027, ""Viscous cosmologies in scalar-tensor theories for Kasner type
metrics,"" by M. Cataldo, S. del Campo and P. Salgado.
","Oktay Aydogdu|Mustafa Salti","","http://arxiv.org/abs/gr-qc/0505079v3","http://arxiv.org/pdf/gr-qc/0505079v3","http://dx.doi.org/10.1007/s10509-005-7216-7","This submission has been withdrawn by arXiv administrators due to
  inappropriate text reuse from external sources","Astrophys.Space Sci.299:227-232,2005","10.1007/s10509-005-7216-7","gr-qc","gr-qc"
"89","cs/0506002v1","2005-06-01 01:30:09","2005-06-01 01:30:09","HepToX: Heterogeneous Peer to Peer XML Databases","  We study a collection of heterogeneous XML databases maintaining similar and
related information, exchanging data via a peer to peer overlay network. In
this setting, a mediated global schema is unrealistic. Yet, users/applications
wish to query the databases via one peer using its schema. We have recently
developed HepToX, a P2P Heterogeneous XML database system. A key idea is that
whenever a peer enters the system, it establishes an acquaintance with a small
number of peer databases, possibly with different schema. The peer
administrator provides correspondences between the local schema and the
acquaintance schema using an informal and intuitive notation of arrows and
boxes. We develop a novel algorithm that infers a set of precise mapping rules
between the schemas from these visual annotations. We pin down a semantics of
query translation given such mapping rules, and present a novel query
translation algorithm for a simple but expressive fragment of XQuery, that
employs the mapping rules in either direction. We show the translation
algorithm is correct. Finally, we demonstrate the utility and scalability of
our ideas and algorithms with a detailed set of experiments on top of the
Emulab, a large scale P2P network emulation testbed.
","Angela Bonifati|Elaine Qing Chang|Terence Ho|Laks V. S. Lakshmanan","Icar CNR, Italy|UBC, Canada|UBC, Canada|UBC, Canada","http://arxiv.org/abs/cs/0506002v1","http://arxiv.org/pdf/cs/0506002v1","","11 pages plus cover page","","","cs.DB","cs.DB|H.2.4; H.2.5"
"90","gr-qc/0506061v3","2005-06-10 14:57:02","2007-07-26 18:43:00","Energy and Momentum Associated with Kasner-type Universes","  This paper has been removed by arXiv administrators because it plagiarizes
gr-qc/0011027, ""Viscous cosmologies in scalar-tensor theories for Kasner type
metrics,"" by M. Cataldo, S. del Campo and P. Salgado; and gr-qc/0404108,
""Energy and Momentum Densities Associated with Solutions Exhibiting Directional
Type Singularities,"" by Ragab M. Gad.
","Mustafa Salti","","http://arxiv.org/abs/gr-qc/0506061v3","http://arxiv.org/pdf/gr-qc/0506061v3","http://dx.doi.org/10.1393/ncb/i2004-10169-7","This submission has been withdrawn by arXiv administrators due to
  inappropriate text reuse from external sources","Nuovo Cim.B120:53-60,2005","10.1393/ncb/i2004-10169-7","gr-qc","gr-qc"
"91","gr-qc/0506062v3","2005-06-10 15:00:40","2007-08-22 19:24:05","Exact Solutions of the Photon Equation in Anisotropic Spacetimes","  This paper has inappropriate amounts of overlap with the following papers
also written by the authors or their collaborators: gr-qc/0506135,
gr-qc/0207026, gr-qc/0502059, gr-qc/0502061, gr-qc/0510037, and others.
","Ali Havare|Murat Korunur|Oktay Aydogdu|Mustafa Salti|Taylan Yetkin","","http://arxiv.org/abs/gr-qc/0506062v3","http://arxiv.org/pdf/gr-qc/0506062v3","http://dx.doi.org/10.1142/S0218271805006754","This submission has been withdrawn by arXiv administrators due to
  inappropriate text reuse from external sources","Int.J.Mod.Phys.D14:957-972,2005","10.1142/S0218271805006754","gr-qc","gr-qc"
"92","math/0506402v2","2005-06-20 15:08:48","2007-07-24 21:22:30","Solutions for the quasi-linear equations in multipliers space","  This paper has been removed by arXiv administrators because it plagiarizes
math.AP/0501483, ""Quasilinear and Hessian equations of Lane-Emden type"" by
Nguyen Cong Phuc and Igor E. Verbitsky.
","Sadek Gala","","http://arxiv.org/abs/math/0506402v2","http://arxiv.org/pdf/math/0506402v2","","This paper has been withdrawn","","","math.AP","math.AP|35D05, 35Dxx"
"93","gr-qc/0506135v3","2005-06-29 13:52:18","2007-08-22 19:26:22","Creation of Photons in Anisotropic Space-Times","  This paper has been removed by arXiv administrators because of excessive
overlap with gr-qc/0506062.
","Murat Korunur|Ali Havare","","http://arxiv.org/abs/gr-qc/0506135v3","http://arxiv.org/pdf/gr-qc/0506135v3","","This submission has been withdrawn by arXiv administrators due to
  inappropriate text reuse from external sources","","","gr-qc","gr-qc"
"94","math/0507342v3","2005-07-17 09:47:08","2007-02-09 09:58:10","Queueing systems with many servers: Null controllability in heavy
  traffic","  A queueing model has $J\ge2$ heterogeneous service stations, each consisting
of many independent servers with identical capabilities. Customers of $I\ge2$
classes can be served at these stations at different rates, that depend on both
the class and the station. A system administrator dynamically controls
scheduling and routing. We study this model in the central limit theorem (or
heavy traffic) regime proposed by Halfin and Whitt. We derive a diffusion model
on $\mathbb {R}^I$ with a singular control term that describes the scaling
limit of the queueing model. The singular term may be used to constrain the
diffusion to lie in certain subsets of $\mathbb {R}^I$ at all times $t>0$. We
say that the diffusion is null-controllable if it can be constrained to
$\mathbb {X}_-$, the minimal closed subset of $\mathbb {R}^I$ containing all
states of the prelimit queueing model for which all queues are empty. We give
sufficient conditions for null controllability of the diffusion. Under these
conditions we also show that an analogous, asymptotic result holds for the
queueing model, by constructing control policies under which, for any given
$0<\epsilon <T<\infty$, all queues in the system are kept empty on the time
interval $[\epsilon, T]$, with probability approaching one. This introduces a
new, unusual heavy traffic ``behavior'': On one hand, the system is critically
loaded, in the sense that an increase in any of the external arrival rates at
the ``fluid level'' results with an overloaded system. On the other hand, as
far as queue lengths are concerned, the system behaves as if it is underloaded.
","Rami Atar|Avi Mandelbaum|Gennady Shaikhet","","http://arxiv.org/abs/math/0507342v3","http://arxiv.org/pdf/math/0507342v3","http://dx.doi.org/10.1214/105051606000000358","Published at http://dx.doi.org/10.1214/105051606000000358 in the
  Annals of Applied Probability (http://www.imstat.org/aap/) by the Institute
  of Mathematical Statistics (http://www.imstat.org)","Annals of Applied Probability 2006, Vol. 16, No. 4, 1764-1804","10.1214/105051606000000358","math.PR","math.PR|60K25, 68M20, 90B22, 90B36, 60F05, 49N25 (Primary)"
"95","gr-qc/0508018v2","2005-08-04 14:24:32","2007-07-26 18:38:06","Energy-Momentum in the Viscous Kasner-type Universe in Teleparallel
  Gravity","  This paper has been removed by arXiv administrators because it plagiarizes
gr-qc/0011027, ""Viscous cosmologies in scalar-tensor theories for Kasner type
metrics,"" by M. Cataldo, S. del Campo and P. Salgado; and gr-qc/0303034, ""The
energy of the universe in teleparallel gravity,"" by T. Vargas.
","Mustafa Salti","","http://arxiv.org/abs/gr-qc/0508018v2","http://arxiv.org/pdf/gr-qc/0508018v2","http://dx.doi.org/10.1007/s10509-005-5159-7","This submission has been withdrawn by arXiv administrators due to
  inappropriate text reuse from external sources","Astrophys.Space Sci.299:159-166,2005; Erratum-ibid.314:247,2008","10.1007/s10509-005-5159-7","gr-qc","gr-qc"
"96","gr-qc/0509022v4","2005-09-07 13:51:45","2007-08-22 19:28:15","Energy-Momentum of a Stationary Beam of Light in Teleparallel Gravity","  This paper has been removed by arXiv administrators because it overlaps
gr-qc/0303034.
  This paper also has excessive overlap with the following paper also written
by the authors or their collaborators: gr-qc/0606022, and others.
","Oktay Aydogdu|Mustafa Salti","","http://arxiv.org/abs/gr-qc/0509022v4","http://arxiv.org/pdf/gr-qc/0509022v4","http://dx.doi.org/10.1007/s10509-008-9735-5","This submission has been withdrawn by arXiv administrators due to
  inappropriate text reuse from external sources","Astrophys.Space Sci.302:61-65,2006; Astrophys.Space
  Sci.314:243Erratum),2008","10.1007/s10509-008-9735-5","gr-qc","gr-qc"
"97","gr-qc/0509023v3","2005-09-07 13:55:11","2007-08-22 18:21:12","Different Approaches to the Einstein Energy Associated with the de
  Sitter C-Space-time","  This paper has been removed by arXiv administrators because it plagiarizes
gr-qc/0404108, gr-qc/0306101, hep-th/0301046, and gr-qc/0303034.
","Mustafa Salti","","http://arxiv.org/abs/gr-qc/0509023v3","http://arxiv.org/pdf/gr-qc/0509023v3","","This submission has been withdrawn by arXiv administrators due to
  inappropriate text reuse from external sources","Acta Phys.Slov.55:563-572,2005","","gr-qc","gr-qc"
"98","gr-qc/0509047v5","2005-09-14 12:21:20","2007-08-22 19:30:54","Gravitational Energy-Momentum Density in Bianchi Type-II Space-times","  This paper has been removed by arXiv administrators because of overlap with
gr-qc/0303034.
  This paper also has excessive overlap with the following papers also written
by the authors or their collaborators: gr-qc/0601133, gr-qc/0602012,
gr-qc/0601070, gr-qc/0606028, and others.
","Oktay Aydogdu","","http://arxiv.org/abs/gr-qc/0509047v5","http://arxiv.org/pdf/gr-qc/0509047v5","http://dx.doi.org/10.1142/S0218271806008255","This submission has been withdrawn by arXiv administrators due to
  inappropriate text reuse from external sources","Int.J.Mod.Phys.D15:459-468,2006","10.1142/S0218271806008255","gr-qc","gr-qc"
"99","gr-qc/0509061v2","2005-09-15 15:10:48","2007-08-22 19:35:51","The Momentum 4-Vector of Static Bianchi-type Space-times in
  Tele-parallel Gravity","  This paper has excessive overlap with the following paper also written by the
authors or their collaborators: gr-qc/0511095.
","Mustafa Salti|Oktay Aydogdu","","http://arxiv.org/abs/gr-qc/0509061v2","http://arxiv.org/pdf/gr-qc/0509061v2","","This submission has been withdrawn by arXiv administrators due to
  inappropriate text reuse from external sources","","","gr-qc","gr-qc"
"100","gr-qc/0510038v2","2005-10-08 12:03:57","2007-08-22 19:45:44","Creation of Massless Spin-1 Particles in a Godel-type Metric","  This paper has inappropriate amounts of overlap with the following papers
also written by the authors or their collaborators: gr-qc/0502059,
gr-qc/0502061, gr-qc/0207026, and others.
","Oktay Aydogdu|Mustafa Salti","","http://arxiv.org/abs/gr-qc/0510038v2","http://arxiv.org/pdf/gr-qc/0510038v2","","This submission has been withdrawn by arXiv administrators due to
  inappropriate text reuse from external sources","","","gr-qc","gr-qc"
"101","gr-qc/0510037v2","2005-10-10 12:38:40","2007-08-22 19:38:30","Photon in Teleparallel Gravity in the Bianchi-type I Space-time","  This paper has been removed by arXiv administrators because it plagiarizes
gr-qc/0105122.
  This paper also has excessive overlap with the following paper also written
by the authors or their collaborators: gr-qc/0502059, gr-qc/0502061,
gr-qc/0510038, gr-qc/0502032, gr-qc/0207026, gr-qc/0506062, gr-qc/0505078, and
others.
","Mustafa Salti|Oktay Aydogdu","","http://arxiv.org/abs/gr-qc/0510037v2","http://arxiv.org/pdf/gr-qc/0510037v2","","This submission has been withdrawn by arXiv administrators due to
  inappropriate text reuse from external sources","","","gr-qc","gr-qc"
"102","gr-qc/0510123v3","2005-10-29 13:36:25","2007-08-22 19:54:55","Energy in Reboucas-Tiomno-Korotkii-Obukhov and Godel-type Space-times in
  Bergmann-Thomson's Formulations","  This paper has been removed by arXiv administrators because it overlaps
gr-qc/0303009, hep-th/0405047, gr-qc/0412120, and others.
  This paper also has excessive overlap with the following papers also written
by the authors or their collaborators: gr-qc/0607119, gr-qc/0607103,
gr-qc/0607115, and others.
","Oktay Aydogdu|Mustafa Salti|Murat Korunur","","http://arxiv.org/abs/gr-qc/0510123v3","http://arxiv.org/pdf/gr-qc/0510123v3","","This submission has been withdrawn by arXiv administrators due to
  inappropriate text reuse from external sources","Acta Phys.Slov.55:537-548,2005","","gr-qc","gr-qc"
"103","gr-qc/0511030v4","2005-11-06 21:28:05","2007-08-22 19:58:45","Energy of a Charged Wormhole","  This paper has been removed by arXiv administrators because it overlaps
gr-qc/0102077 and others.
  This paper also has excessive overlap with the following papers also written
by the authors or their collaborators: gr-qc/0603027, gr-qc/0606022,
gr-qc/0512080, gr-qc/0603108, gr-qc/0601141, gr-qc/0603063, gr-qc/0608050,
gr-qc/0607095, gr-qc/0607011, gr-qc/0607083, and others.
","Mustafa Salti|Oktay Aydogdu","","http://arxiv.org/abs/gr-qc/0511030v4","http://arxiv.org/pdf/gr-qc/0511030v4","http://dx.doi.org/10.1007/s10773-006-9158-4","This submission has been withdrawn by arXiv administrators due to
  inappropriate text reuse from external sources","Int.J.Theor.Phys.45:1891-1900,2006","10.1007/s10773-006-9158-4","gr-qc","gr-qc"
"104","gr-qc/0511095v5","2005-11-16 21:36:42","2007-07-26 18:46:13","On the Energy-Momentum in Closed Universes","  This paper has been removed by arXiv administrators because it plagiarizes
gr-qc/0011027, ""Viscous cosmologies in scalar-tensor theories for Kasner type
metrics,"" by M. Cataldo, S. del Campo and P. Salgado; gr-qc/0301022, ""A
generalisation of the Heckmann-Schucking cosmological solution,"" by I.M.
Khalatnikov and A.Y. Kamenshchik; and gr-qc/0303034, ""The energy of the
universe in teleparallel gravity,"" by T. Vargas.
","Mustafa Salti","","http://arxiv.org/abs/gr-qc/0511095v5","http://arxiv.org/pdf/gr-qc/0511095v5","http://dx.doi.org/10.1007/s10582-006-0078-6","This submission has been withdrawn by arXiv administrators due to
  inappropriate text reuse from external sources","Czech.J.Phys.56:177-189,2006","10.1007/s10582-006-0078-6","gr-qc","gr-qc"
"105","gr-qc/0512080v3","2005-12-13 20:42:28","2007-08-22 18:01:20","Energy in the Schwarzschild-de Sitter Spacetime","  This paper has been removed by arXiv administrators because it plagiarizes
gr-qc/0301090, gr-qc/0303034, gr-qc/0212018, and gr-qc/9601044.
","Mustafa Salti|Oktay Aydogdu","","http://arxiv.org/abs/gr-qc/0512080v3","http://arxiv.org/pdf/gr-qc/0512080v3","http://dx.doi.org/10.1007/s10702-006-0517-4","This submission has been withdrawn by arXiv administrators due to
  inappropriate text reuse from external sources","Found.Phys.Lett.19:269-276,2006","10.1007/s10702-006-0517-4","gr-qc","gr-qc"
"106","physics/0512202v1","2005-12-21 20:47:20","2005-12-21 20:47:20","Is it time for a science counterpart of the Benezet-Berman mathematics
  teaching experiment of the 1930's?","  Should teachers concentrate on critical thinking, estimation, measurement,
and graphing rather than college-clone algorithmic physics in grades K--12?
Thus far physics education research offers little substantive guidance.
Mathematics education research addressed the mathematics analogue of this
question in the 1930's. Students in Manchester, New Hampshire were not
subjected to arithmetic algorithms until grade 6. In earlier grades they read,
invented, and discussed stories and problems; estimated lengths, heights, and
areas; and enjoyed finding and interpreting numbers relevant to their lives. In
grade 6, with 4 months of formal training, they caught up to the regular
students in algorithmic ability, and were far ahead in general numeracy and in
the verbal, semantic, and problem solving skills they had practiced for the
five years before. Assessment was both qualitative -- e.g., asking 8th grade
students to relate in their own words why it is `that if you have two fractions
with the same numerator, the one with the smaller denominator is the larger';
and quantitative -- e.g., administration of standardized arithmetic
examinations to test and control groups in the 6th grade. Is it time for a
science counterpart of the Benezet/Berman Manchester experiment of the 1930's?
","Sanjoy Mahajan|Richard R. Hake","","http://arxiv.org/abs/physics/0512202v1","http://arxiv.org/pdf/physics/0512202v1","","plain TeX with eplain, 27 pages, 2 figures; based on an invited
  poster presented at the Physics Education Research Conference 2000: Teacher
  Education at http://www.sci.ccny.cuny.edu/~rstein/perc2000.htm","","","physics.ed-ph","physics.ed-ph"
"107","gr-qc/0601070v3","2006-01-17 19:08:11","2007-08-22 20:04:04","Moller Energy-Momentum Prescription for a Locally Rotationally Symmetric
  Space-Time","  This paper has inappropriate amounts of overlap with the following papers
also written by the authors or their collaborators: gr-qc/0608050,
gr-qc/0509047, gr-qc/0505078, gr-qc/0603108, gr-qc/0512080, gr-qc/0511030,
gr-qc/0606022, gr-qc/0603027, gr-qc/0603063, gr-qc/0607116, and others.
","Oktay Aydogdu","","http://arxiv.org/abs/gr-qc/0601070v3","http://arxiv.org/pdf/gr-qc/0601070v3","http://dx.doi.org/10.1142/S0217751X06030990","This submission has been withdrawn by arXiv administrators due to
  inappropriate text reuse from external sources","Int.J.Mod.Phys.A21:3845-3854,2006","10.1142/S0217751X06030990","gr-qc","gr-qc"
"108","cs/0601079v1","2006-01-18 01:08:49","2006-01-18 01:08:49","SCRUB-PA: A Multi-Level Multi-Dimensional Anonymization Tool for Process
  Accounting","  In the UNIX/Linux environment the kernel can log every command process
created by every user using process accounting. This data has many potential
uses, including the investigation of security incidents. However, process
accounting data is also sensitive since it contains private user information.
Consequently, security system administrators have been hindered from sharing
these logs. Given that many interesting security applications could use process
accounting data, it would be useful to have a tool that could protect private
user information in the logs. For this reason we introduce SCRUB-PA, a tool
that uses multi-level multi-dimensional anonymization on process accounting log
files in order to provide different levels of privacy protection. It is our
goal that SCRUB-PA will promote the sharing of process accounting logs while
preserving privacy.
","Katherine Luo|Yifan Li|Charis Ermopoulos|William Yurcik|Adam Slagell","","http://arxiv.org/abs/cs/0601079v1","http://arxiv.org/pdf/cs/0601079v1","","19 pages, 11 figures, 4 tables","","","cs.CR","cs.CR"
"109","gr-qc/0601133v2","2006-01-30 13:56:52","2007-08-22 20:06:34","Energy Density Associated with the Bianchi Type-II Space-Time","  This paper has been removed by arXiv administrators because of overlap with
gr-qc/9910015 and hep-th/0308070. It also has excessive overlap with the
following papers also written by the authors or their collaborators:
gr-qc/0509047, gr-qc/0607110, gr-qc/0602012, gr-qc/0606028, and others.
","Oktay Aydogdu|Mustafa Salti","","http://arxiv.org/abs/gr-qc/0601133v2","http://arxiv.org/pdf/gr-qc/0601133v2","http://dx.doi.org/10.1143/PTP.115.63","This submission has been withdrawn by arXiv administrators due to
  inappropriate text reuse from external sources","Prog.Theor.Phys.115:63-71,2006","10.1143/PTP.115.63","gr-qc","gr-qc"
"110","gr-qc/0601141v4","2006-01-31 15:48:31","2007-08-22 18:31:06","Teleparallel Gravitational Energy in the Gamma Metric","  This paper has been removed by arXiv administrators because it plagiarizes
gr-qc/0303034 and gr-qc/0212018.
","Mustafa Salti","","http://arxiv.org/abs/gr-qc/0601141v4","http://arxiv.org/pdf/gr-qc/0601141v4","http://dx.doi.org/10.1142/S0218271806008425","This submission has been withdrawn by arXiv administrators due to
  inappropriate text reuse from external sources","Int.J.Mod.Phys.D15:695-702,2006","10.1142/S0218271806008425","gr-qc","gr-qc"
"111","gr-qc/0602012v2","2006-02-02 20:13:17","2007-08-22 20:11:26","Energy Imparted by Gravitational Waves in Inhomogeneous Space-times","  This paper has been removed by arXiv administrators because of overlap with
gr-qc/0303034 and others.
  This paper also has excessive overlap with the following papers also written
by the authors or their collaborators: gr-qc/0509047, gr-qc/0601133,
gr-qc/0607126, gr-qc/0606080, gr-qc/0510123, gr-qc/0607115, gr-qc/0606028,
gr-qc/0607103, gr-qc/0509023, gr-qc/0607119, gr-qc/0603044, gr-qc/0511095,
gr-qc/0502031, and others.
","Oktay Aydogdu|Mustafa Salti","","http://arxiv.org/abs/gr-qc/0602012v2","http://arxiv.org/pdf/gr-qc/0602012v2","","This submission has been withdrawn by arXiv administrators due to
  inappropriate text reuse from external sources","","","gr-qc","gr-qc"
"112","gr-qc/0602070v3","2006-02-19 20:35:48","2007-08-22 20:13:54","Energy Distribution of the Universe in the Bianchi type II Cosmological
  Models","  This paper has been removed by arXiv administrators because it plagiarizes
gr-qc/9910015, gr-qc/0008034, and hep-th/0308070.
  This paper also has excessive overlap with the following papers also written
by the authors or their collaborators: gr-qc/0601070 and gr-qc/0509047.
","Oktay Aydogdu","","http://arxiv.org/abs/gr-qc/0602070v3","http://arxiv.org/pdf/gr-qc/0602070v3","http://dx.doi.org/10.1002/prop.200510271","This submission has been withdrawn by arXiv administrators due to
  inappropriate text reuse from external sources","Fortsch.Phys.54:246-251,2006","10.1002/prop.200510271","gr-qc","gr-qc"
"113","hep-ph/0603047v5","2006-03-06 13:19:55","2016-08-23 15:55:22","CP nonconservation in the leptonic sector","  In this paper we use an exact method to impose unitarity on moduli of the
neutrino PMNS matrix recently determined, and show how one could obtain
information on CP non-conservation from a limited experimental information. One
suggests a novel type of global fit by expressing all the theoretical
quantities in terms of convention independent parameters: the Jarlskog
invariant $J$ and the moduli $|U_{\alpha i}|$, able to resolve the positivity
problem of $|U_{e 3}|$. In this way the fit will directly provide a value for
$J$, and if it is different from zero it will prove the existence of CP
violation in the available experimental data. If the best fit result,
$|U_{e3}|^2<0$, from M. Maltoni {\em et al}, New J.Phys. {\bf 6} (2004) 122 is
confirmed, it will imply a new physics in the leptonic sector.
","Petre Dita","","http://arxiv.org/abs/hep-ph/0603047v5","http://arxiv.org/pdf/hep-ph/0603047v5","","This submission has been withdrawn by arXiv administrators because it
  is duplicated in arXiv:1101.4087","","","hep-ph","hep-ph|hep-lat|math-ph|math.MP|quant-ph"
"114","gr-qc/0603027v6","2006-03-08 21:38:18","2007-08-22 20:21:00","On the Moller Energy Associated with Black Holes","  This paper has been removed by arXiv administrators because of overlap with
gr-qc/9601044, gr-qc/0212018, hep-th/0310151, gr-qc/0011066, gr-qc/0404001,
gr-qc/0304081, gr-qc/0501002, and gr-qc/0109017.
  This paper also has excessive overlap with the following papers also written
by the authors or their collaborators: gr-qc/0606022, gr-qc/0512080,
gr-qc/0511030, gr-qc/0603063, gr-qc/0603108, gr-qc/0511095, gr-qc/0607095,
gr-qc/0608050, gr-qc/0601141, gr-qc/0607011, gr-qc/0607082, gr-qc/0603044,
gr-qc/0607083, and others.
","Mustafa Salti|Oktay Aydogdu","","http://arxiv.org/abs/gr-qc/0603027v6","http://arxiv.org/pdf/gr-qc/0603027v6","http://dx.doi.org/10.1007/s10773-006-9212-2","This submission has been withdrawn by arXiv administrators due to
  inappropriate text reuse from external sources","Int.J.Theor.Phys.45:2481-2495,2006","10.1007/s10773-006-9212-2","gr-qc","gr-qc"
"115","gr-qc/0603044v3","2006-03-13 20:05:48","2007-08-22 20:26:29","On the Gravitational Energy Associated with Spacetimes of Diagonal
  Metric","  This paper also has excessive overlap with the following papers also written
by the authors or their collaborators: gr-qc/0608014, gr-qc/0511095,
gr-qc/0505078, gr-qc/0502060, gr-qc/0603027, gr-qc/0606028, gr-qc/0607109,
gr-qc/0607110, gr-qc/0508018, gr-qc/0502031, and others.
","Murat Korunur|Ali Havare|Mustafa Salti","","http://arxiv.org/abs/gr-qc/0603044v3","http://arxiv.org/pdf/gr-qc/0603044v3","http://dx.doi.org/10.1007/s12043-007-0073-x","This submission has been withdrawn by arXiv administrators due to
  inappropriate text reuse from external sources","Pramana 68:735-748,2007","10.1007/s12043-007-0073-x","gr-qc","gr-qc"
"116","gr-qc/0603063v4","2006-03-15 19:12:05","2007-08-22 18:28:30","Energy Associated with the Gibbons-Maeda Dilaton Space-Time","  This paper has been removed by arXiv administrators because it plagiarizes
hep-th/0311050, gr-qc/0306101, gr-qc/9601044, gr-qc/0511106, gr-qc/0303034, and
gr-qc/0212018.
","Oktay Aydogdu|Mustafa Salti|Murat Korunur|Irfan Acikgoz","","http://arxiv.org/abs/gr-qc/0603063v4","http://arxiv.org/pdf/gr-qc/0603063v4","http://dx.doi.org/10.1007/s10702-006-1059-5","This submission has been withdrawn by arXiv administrators due to
  inappropriate text reuse from external sources","Found.Phys.Lett.19:709-721,2006","10.1007/s10702-006-1059-5","gr-qc","gr-qc"
"117","cs/0603075v1","2006-03-18 17:11:04","2006-03-18 17:11:04","Unmanaged Internet Protocol: Taming the Edge Network Management Crisis","  Though appropriate for core Internet infrastructure, the Internet Protocol is
unsuited to routing within and between emerging ad-hoc edge networks due to its
dependence on hierarchical, administratively assigned addresses. Existing
ad-hoc routing protocols address the management problem but do not scale to
Internet-wide networks. The promise of ubiquitous network computing cannot be
fulfilled until we develop an Unmanaged Internet Protocol (UIP), a scalable
routing protocol that manages itself automatically. UIP must route within and
between constantly changing edge networks potentially containing millions or
billions of nodes, and must still function within edge networks disconnected
from the main Internet, all without imposing the administrative burden of
hierarchical address assignment. Such a protocol appears challenging but
feasible. We propose an architecture based on self-certifying, cryptographic
node identities and a routing algorithm adapted from distributed hash tables.
","Bryan Ford","","http://arxiv.org/abs/cs/0603075v1","http://arxiv.org/pdf/cs/0603075v1","","7 pages, 3 figures","Second Workshop on Hot Topics in Networks (HotNets-II), November
  2003, Cambridge, MA","","cs.NI","cs.NI|cs.OS|C.2.1; C.2.2"
"118","q-bio/0603032v1","2006-03-27 10:25:17","2006-03-27 10:25:17","Feedback Medicine: Control Systems Concepts in Personalised, Predictive
  Medicine and Combinatorial Intervention","  In its broadest definition, systems biology is the application of a `systems'
way of thinking about and doing cell biology. By implication, this also invites
us to consider a systems approach in the context of medicine and the treatment
of disease. In particular, the idea that systems biology can form the basis of
a personalised, predictive medicine will require that much closer attention is
paid to the analytic properties of the feedback loops which will be set up by a
personalised approach to healthcare. To emphasize the role that feedback theory
will play in understanding personalised medicine, we use the term feedback
medicine to describe the issues outlined.In these notes we consider feedback
and control systems concepts applied to two important themes in medical systems
biology - personalised medicine and combinatorial intervention. In particular,
we formulate a feedback control interpretation for the administration of
medicine, and relate them to various forms of medical treatment.
","Peter Wellstead|Rick Middleton|Olaf Wolkenhauer","","http://arxiv.org/abs/q-bio/0603032v1","http://arxiv.org/pdf/q-bio/0603032v1","","22 pages, 10 figures","","","q-bio.TO","q-bio.TO|q-bio.QM"
"119","astro-ph/0603736v1","2006-03-27 17:40:41","2006-03-27 17:40:41","Asteroseismic inferences on GW Vir variable stars in the frame of new PG
  1159 evolutionary models","  This submission has been removed by arXiv administrators because it is a
duplicate of astro-ph/0603735.
","A. H. Corsico|L. G. Althaus","","http://arxiv.org/abs/astro-ph/0603736v1","http://arxiv.org/pdf/astro-ph/0603736v1","http://dx.doi.org/10.1051/0004-6361:20054199","","","10.1051/0004-6361:20054199","astro-ph","astro-ph"
"120","gr-qc/0603108v4","2006-03-28 14:40:04","2007-08-22 18:14:56","Energy Distribution in Reissner-Nordstrom anti-de Sitter black holes in
  Moller Prescription","  This paper has been removed by arXiv administrators because it plagiarizes
gr-qc/0010068, gr-qc/9911018, gr-qc/0303034, and gr-qc/0212018.
","Mustafa Salti|Oktay Aydogdu","","http://arxiv.org/abs/gr-qc/0603108v4","http://arxiv.org/pdf/gr-qc/0603108v4","http://dx.doi.org/10.1140/epjc/s2006-02550-6","This submission has been withdrawn by arXiv administrators due to
  inappropriate text reuse from external sources","Eur.Phys.J.C47:247-251,2006","10.1140/epjc/s2006-02550-6","gr-qc","gr-qc"
"121","cs/0603114v1","2006-03-29 05:25:41","2006-03-29 05:25:41","Using SMART for Customized Monitoring of Windows Services","  We focus on examining and working with an important category of computer
software called Services, which are provided as a part of newer Microsoft
Windows operating systems. A typical Windows user transparently utilizes many
of these services but is frequently unaware of their existence. Since some
services have the potential to create significant problems when they are
executing, it is important for a system administrator to identify which
services are running on the network, the types of processing done by each
service, and any interrelationships among the various services. This
information can then be used to improve the overall integrity of both the
individual computer where a questionable service is running and in aggregate an
entire network of computers.
  NCSA has developed an application called SMART (Services Monitoring And
Reporting Tool) that can be used to identify and display all services currently
running in the network. A commercial program called Hyena remotely monitors the
services on all computers attached to the network and exports this information
to SMART. SMART produces various outputs that the system administrator can
analyze and then determine appropriate actions to take. In particular, SMART
provides a color coordinated user interface to quickly identify and classify
both potentially hazardous services and also unknown services.
","Gregory A. Pluta|Larry Brumbaugh|William Yurcik","","http://arxiv.org/abs/cs/0603114v1","http://arxiv.org/pdf/cs/0603114v1","","15 pages, 10 figures, 2 tables","","","cs.NI","cs.NI|cs.CR"
"122","astro-ph/0604081v2","2006-04-04 20:33:38","2006-04-07 01:37:47","Stellar Properties of Embedded Protostars","  (Abridged) High dispersion spectrographs on large aperture telescopes have
recently allowed observers to study the stellar and accretion properties of
deeply embedded young stars, commonly referred to as Class I stars. We
summarize these newly determined properties and compare them with observations
of more optically revealed Class II (T Tauri) stars. Class I stars have
spectral types and stellar luminosities similar to those of Class II stars,
suggesting similar masses and ages. Estimates of stellar luminosity and age,
however, are especially uncertain given the large extinctions, scattered light
emission and continuum excesses typical of Class I stars. Several candidate
Class I brown dwarfs are identified. Class I stars appear to rotate more
rapidly than T Tauri stars, by roughly a factor of 2. Likewise, Class I disk
accretion rates are only a factor of two larger than those of T Tauri stars,
less than the mass infall rates predicted by envelope models by 1-2 orders of
magnitude. In at least a few cases the discrepancy appears to be caused by T
Tauri stars being misclassified as Class I stars because of their edge-on disk
orientation. Stars where the envelope density and infall velocity have been
determined directly and unambiguously imply that stellar mass is not acquired
in a steady-state fashion, but instead through brief outbursts of enhanced
accretion. If some Class I stars are in fact as old as T Tauri stars,
replenishment may be necessary to sustain the long-lived envelopes, possibly
via continued dynamical interactions with cloud material.
","R. J. White|T. P. Greene|G. W. Doppmann|K. R. Covey|L. A. Hillenbrand","University of Alabama in Huntsville|National Aeronautics and Space Administration at Ames Research Center|Gemini Observatory|University of Washington|California Institute of Technology","http://arxiv.org/abs/astro-ph/0604081v2","http://arxiv.org/pdf/astro-ph/0604081v2","","Review Chapter for Protostars and Planets V. 16 pages, 8 figures;
  Revised (4/6/06) to correct typo on pg. 2","","","astro-ph","astro-ph"
"123","cs/0604017v3","2006-04-06 01:04:05","2006-12-07 07:20:37","AS Relationships: Inference and Validation","  Research on performance, robustness, and evolution of the global Internet is
fundamentally handicapped without accurate and thorough knowledge of the nature
and structure of the contractual relationships between Autonomous Systems
(ASs). In this work we introduce novel heuristics for inferring AS
relationships. Our heuristics improve upon previous works in several technical
aspects, which we outline in detail and demonstrate with several examples.
Seeking to increase the value and reliability of our inference results, we then
focus on validation of inferred AS relationships. We perform a survey with ASs'
network administrators to collect information on the actual connectivity and
policies of the surveyed ASs. Based on the survey results, we find that our new
AS relationship inference techniques achieve high levels of accuracy: we
correctly infer 96.5% customer to provider (c2p), 82.8% peer to peer (p2p), and
90.3% sibling to sibling (s2s) relationships. We then cross-compare the
reported AS connectivity with the AS connectivity data contained in BGP tables.
We find that BGP tables miss up to 86.2% of the true adjacencies of the
surveyed ASs. The majority of the missing links are of the p2p type, which
highlights the limitations of present measuring techniques to capture links of
this type. Finally, to make our results easily accessible and practically
useful for the community, we open an AS relationship repository where we
archive, on a weekly basis, and make publicly available the complete Internet
AS-level topology annotated with AS relationship information for every pair of
AS neighbors.
","Xenofontas Dimitropoulos|Dmitri Krioukov|Marina Fomenkov|Bradley Huffaker|Young Hyun|kc claffy|George Riley","","http://arxiv.org/abs/cs/0604017v3","http://arxiv.org/pdf/cs/0604017v3","http://dx.doi.org/10.1145/1198255.1198259","Final journal version","ACM SIGCOMM Computer Communication Review (CCR), v.37, n.1,
  p.29-40, 2007","10.1145/1198255.1198259","cs.NI","cs.NI|C.2.5; C.2.1"
"124","cs/0605022v1","2006-05-05 18:16:22","2006-05-05 18:16:22","Toward a Collection-based Metadata Maintenance Model","  In this paper, the authors identify key entities and relationships in the
operational management of metadata catalogs that describe digital collections,
and they draft a data model to support the administration of metadata
maintenance for collections. Further, they consider this proposed model in
light of other data schemes to which it relates and discuss the implications of
the model for library metadata maintenance operations.
","Martin Kurth|Jim LeBlanc","","http://arxiv.org/abs/cs/0605022v1","http://arxiv.org/pdf/cs/0605022v1","","10 pages, 4 figures; submitted to DC 2006","","","cs.DL","cs.DL"
"125","cs/0605057v1","2006-05-15 13:41:27","2006-05-15 13:41:27","SLA-Based Coordinated Superscheduling Scheme and Performance for
  Computational Grids","  The Service Level Agreement~(SLA) based grid superscheduling approach
promotes coordinated resource sharing. Superscheduling is facilitated between
administratively and topologically distributed grid sites by grid schedulers
such as Resource brokers. In this work, we present a market-based SLA
coordination mechanism. We based our SLA model on a well known \emph{contract
net protocol}.
  The key advantages of our approach are that it allows:~(i) resource owners to
have finer degree of control over the resource allocation that was previously
not possible through traditional mechanism; and (ii) superschedulers to bid for
SLA contracts in the contract net with focus on completing the job within the
user specified deadline. In this work, we use simulation to show the
effectiveness of our proposed approach.
","Rajiv Ranjan|Aaron Harwood|Rajkumar Buyya","","http://arxiv.org/abs/cs/0605057v1","http://arxiv.org/pdf/cs/0605057v1","","","In Proceedings of the 8th IEEE International Conference on Cluster
  Computing (Cluster 2006), IEEE Computer Society Press, September 27 - 30,
  2006, Barcelona, Spain.","","cs.DC","cs.DC"
"126","math/0605407v2","2006-05-15 19:10:53","2006-05-17 15:37:38","A short visit into the Dehn filling Theory","  This article was withdrawn by the arXiv.org administrators since it
plagiarizes math.GT/0011056.
","Pratip Chakraborty|Unmesh Ghoshdastider|Puneet Sharma","","http://arxiv.org/abs/math/0605407v2","http://arxiv.org/pdf/math/0605407v2","","This article was withdrawn by the arXiv.org administrators since it
  plagiarizes math.GT/0011056","","","math.GT","math.GT"
"127","math/0605409v2","2006-05-15 19:28:34","2006-05-17 15:36:09","Comparing Singular Homology with respect to Isometry","  This article was withdrawn by the arXiv.org administrators since it
plagiarizes math.AT/0401211.
","Pratip Chakraborty|Unmesh Ghoshdastider","","http://arxiv.org/abs/math/0605409v2","http://arxiv.org/pdf/math/0605409v2","","This article was withdrawn by the arXiv.org administrators since it
  plagiarizes math.AT/0401211","","","math.GT","math.GT"
"128","math/0605570v5","2006-05-21 20:46:42","2007-03-06 11:23:50","An Approach to PI(x) and other Arithmetical function by Variational
  principles","  In this paper we present a method to derive Pi(x) and other Arithemtical
functions that can be generated by a Dirichlet series by variational
principles,we use a variational method to determine the solution for a Fredholm
integral equation of second kind, after that we propose (obtain) two integral
equations one for the Pi(x) and other for the arithmetical function
A(x)=Sum(n,x)a(n) so they can be solved by usual optimization method. Also some
conjectures on the value for the asymptotic value of the sum of f(t)=t^{n} are
given in the form Li(x^{n+1})
  Changes: Rayleigh-ritz Variational Methods added, we have also included a
brief description of how to accelerate the convergence of the series
Sum{p}f(x),Grammar changes.
","Jose Javier Garcia Moreta","","http://arxiv.org/abs/math/0605570v5","http://arxiv.org/pdf/math/0605570v5","","This submission has been withdrawn by arXiv administrators because of
  fraudulently claimed institutional affiliation and status","","","math.GM","math.GM|11.xx 45.xx 46.xx"
"129","hep-th/0605248v3","2006-05-25 07:42:02","2006-11-16 23:00:00","Cosmos Naturally Evolved to One Time Dimension and Three Spatial
  Dimensions","  Withdrawn by arXiv administrators because author has forged affiliations and
acknowledgments.
","Jia-Zhong Chen","","http://arxiv.org/abs/hep-th/0605248v3","http://arxiv.org/pdf/hep-th/0605248v3","","Withdrawn by arXiv administrators because author has forged
  affiliations and acknowledgments","","","hep-th","hep-th"
"130","math/0605663v3","2006-05-25 15:09:09","2007-03-06 11:19:56","A Brief Comment on Post inversion formula for the Laplace transform","  In this paper we comment the Post inversion formula for Laplace transform,
and its possible application to the branch of Analytic Number theory
(Arithmetical functions, RH and PNT), involving a condition in the form of
iterated limit to calculate the Riemann Hypothesis.
","Jose Javier Garcia MOreta","","http://arxiv.org/abs/math/0605663v3","http://arxiv.org/pdf/math/0605663v3","","This submission has been withdrawn by arXiv administrators because of
  fraudulently claimed institutional affiliation and status","","","math.GM","math.GM|47-xx, 44-xx, 65-xx"
"131","gr-qc/0606022v3","2006-06-05 14:27:53","2007-08-22 20:43:53","Moller's Energy in the Dyadosphere of a Charged Black Hole","  This paper has been removed by arXiv administrators because it plagiarizes
gr-qc/0304081, gr-qc/0212018, gr-qc/0501002, gr-qc/9601044, and gr-qc/0109017.
  This paper also has excessive overlap with the following papers also written
by the authors or their collaborators: gr-qc/0603027, gr-qc/0511030,
gr-qc/0512080, gr-qc/0607011, gr-qc/0603108, gr-qc/0601141, gr-qc/0603063,
gr-qc/0608050, gr-qc/0607095, gr-qc/0607082, gr-qc/0511095, gr-qc/0601070,
gr-qc/0505078, gr-qc/0607083, gr-qc/0607116, gr-qc/0612016, and others.
","Oktay Aydogdu|Mustafa Salti","","http://arxiv.org/abs/gr-qc/0606022v3","http://arxiv.org/pdf/gr-qc/0606022v3","http://dx.doi.org/10.1007/s12043-006-0068-z","This submission has been withdrawn by arXiv administrators due to
  inappropriate text reuse from external sources","Pramana 67:239-247,2006; Erratum-ibid 71:1368,2008","10.1007/s12043-006-0068-z","gr-qc","gr-qc"
"132","gr-qc/0606028v3","2006-06-07 06:36:34","2007-07-26 18:29:01","The Momentum 4-Vector in Bulk Viscous Bianchi Type-V Space-time","  This paper has been removed by arXiv administrators because it plagiarizes
gr-qc/0611031, ""Generation of Bianchi type V bulk viscous cosmological models
with time dependent Lambda-term,"" by A. Pradhan, K. Jotania and A. Rai; Phys.
Rev. D 41, 1086 (1990), ""Energy associated with a Kerr-Newman black hole,"" by
K. S. Virbhadra; and gr-qc/0303034, ""The energy of the universe in teleparallel
gravity,"" by T. Vargas.
","Oktay Aydogdu|Mustafa Salti","","http://arxiv.org/abs/gr-qc/0606028v3","http://arxiv.org/pdf/gr-qc/0606028v3","http://dx.doi.org/10.1007/s10582-006-0131-5","This submission has been withdrawn by arXiv administrators due to
  inappropriate text reuse from external sources","Czech J.Phys.56:789-798,2006","10.1007/s10582-006-0131-5","gr-qc","gr-qc"
"133","cs/0606063v1","2006-06-14 03:17:59","2006-06-14 03:17:59","FLAIM: A Multi-level Anonymization Framework for Computer and Network
  Logs","  FLAIM (Framework for Log Anonymization and Information Management) addresses
two important needs not well addressed by current log anonymizers. First, it is
extremely modular and not tied to the specific log being anonymized. Second, it
supports multi-level anonymization, allowing system administrators to make
fine-grained trade-offs between information loss and privacy/security concerns.
In this paper, we examine anonymization solutions to date and note the above
limitations in each. We further describe how FLAIM addresses these problems,
and we describe FLAIM's architecture and features in detail.
","Adam Slagell|Kiran Lakkaraju|Katherine Luo","","http://arxiv.org/abs/cs/0606063v1","http://arxiv.org/pdf/cs/0606063v1","","16 pages, 4 figures, in submission to USENIX Lisa","","","cs.CR","cs.CR"
"134","gr-qc/0606080v4","2006-06-19 08:55:19","2007-08-22 20:47:48","Energy Distribution in Szekeres Type I and II Space Times","  This paper has excessive overlap with the following papers also written by
the authors or their collaborators: gr-qc/0607103, gr-qc/0607119,
gr-qc/0607115, gr-qc/0607102, gr-qc/0602012, gr-qc/0702047, gr-qc/0607089,
gr-qc/0510123, and others.
","Sezgin Aygun|Melis Aygun|Ismail Tarhan","","http://arxiv.org/abs/gr-qc/0606080v4","http://arxiv.org/pdf/gr-qc/0606080v4","","This submission has been withdrawn by arXiv administrators due to
  inappropriate text reuse from external sources","Acta Phys.Polon.B37:2781-2794,2006","","gr-qc","gr-qc"
"135","cs/0606089v1","2006-06-20 16:53:02","2006-06-20 16:53:02","NVision-PA: A Tool for Visual Analysis of Command Behavior Based on
  Process Accounting Logs (with a Case Study in HPC Cluster Security)","  In the UNIX/Linux environment the kernel can log every command process
created by every user with process accounting. Thus process accounting logs
have many potential uses, particularly the monitoring and forensic
investigation of security events. Previous work successfully leveraged the use
of process accounting logs to identify a difficult to detect and damaging
intrusion against high performance computing (HPC) clusters, masquerade
attacks, where intruders masquerade as legitimate users with purloined
authentication credentials. While masqueraders on HPC clusters were found to be
identifiable with a high accuracy (greater than 90%), this accuracy is still
not high enough for HPC production environments where greater than 99% accuracy
is needed.
  This paper incrementally advances the goal of more accurately identifying
masqueraders on HPC clusters by seeking to identify features within command
sets that distinguish masqueraders. To accomplish this goal, we created
NVision-PA, a software tool that produces text and graphic statistical
summaries describing input processing accounting logs. We report NVision-PA
results describing two different process accounting logs; one from Internet
usage and one from HPC cluster usage. These results identify the distinguishing
features of Internet users (as proxies for masqueraders) posing as clusters
users. This research is both a promising next step toward creating a real-time
masquerade detection sensor for production HPC clusters as well as providing
another tool for system administrators to use for statistically monitoring and
managing legitimate workloads (as indicated by command usage) in HPC
environments.
","Charis Ermopoulos|William Yurcik","","http://arxiv.org/abs/cs/0606089v1","http://arxiv.org/pdf/cs/0606089v1","","25 pages, 13 Figures","","","cs.CR","cs.CR|cs.DC"
"136","math/0607095v5","2006-07-04 14:53:00","2007-03-06 11:11:56","Chebyshev Partition function: A connection between Statistical Physics
  and Riemann Hypothesis","  In this paper we present a method to obtain a possible self-adjoint
Hamiltonian operator so its energies satisfy Z(1/2+iE_n)=0, which is an
statement equivalent to Riemann Hypothesis, first we use the explicit formula
for the Chebyshev function Psi(x) and apply the change x=exp(u), after that we
consider an Statistical partition function involving the Chebyshev function and
its derivative so Z=Tr(exp(-BH), from the integral definition of the partition
function Z we try to obtain the Hamiltonian operator assuming that H=P^{2}+V(x)
by proposing a Non-linear integral equation involving Z(B=-iu) and V(x).
","Jose Javier garcia Moreta","","http://arxiv.org/abs/math/0607095v5","http://arxiv.org/pdf/math/0607095v5","","This submission has been withdrawn by arXiv administrators because of
  fraudulently claimed institutional affiliation and status","","","math.GM","math.GM|11.xx 45.xx 46.xx"
"137","gr-qc/0607011v2","2006-07-04 16:29:08","2007-08-22 20:50:54","Energy of a Stringy Charged Black Hole in the Teleparallel Gravity","  This paper has been removed by arXiv administrators because it plagiarizes
gr-qc/0212018, gr-qc/0501002, and gr-qc/9601044.
  This paper has excessive overlap with the following papers also written by
the authors or their collaborators: gr-qc/0603108, gr-qc/0606022,
gr-qc/0603027, gr-qc/0603063, gr-qc/0607083, gr-qc/0607095, gr-qc/0511030,
gr-qc/0512080, gr-qc/0601141, gr-qc/0608050, gr-qc/0607082, gr-qc/0511095,
gr-qc/0612016, and others.
","Mustafa Salti","","http://arxiv.org/abs/gr-qc/0607011v2","http://arxiv.org/pdf/gr-qc/0607011v2","http://dx.doi.org/10.1007/s10509-006-9208-7","This submission has been withdrawn by arXiv administrators due to
  inappropriate text reuse from external sources","Astrophys.Space Sci.306:201-204,2006","10.1007/s10509-006-9208-7","gr-qc","gr-qc"
"138","gr-qc/0607082v4","2006-07-20 16:28:30","2007-07-26 18:05:05","The Momentum Four-Vector in Brans-Dicke Wormholes","  This paper has been removed by arXiv administrators because it plagiarizes
K.K. Nandi and Y.Z. Zhang, ""An algorithm for generating rotating Brans-Dicke
wormhole solutions,"" gr-qc/0606012; and Ragab M. Gad, ""Energy and Momentum
Densities Associated with Solutions Exhibiting Directional Type Singularities,""
gr-qc/0404108.
","Nurettin Pirinccioglu|Irfan Acikgoz|Mustafa Salti","","http://arxiv.org/abs/gr-qc/0607082v4","http://arxiv.org/pdf/gr-qc/0607082v4","http://dx.doi.org/10.1007/s10773-006-9272-3","This submission has been withdrawn by arXiv administrators due to
  inappropriate text reuse from external sources","Int.J.Theor.Phys.46:1318-1333,2007","10.1007/s10773-006-9272-3","gr-qc","gr-qc"
"139","gr-qc/0607083v3","2006-07-20 16:34:39","2007-07-26 18:09:07","Relative Energy Associated with a White Hole Model of the Big Bang","  This paper has been removed by arXiv administrators because it plagiarizes
gr-qc/9803014, ""A White Hole Model of the Big Bang,"" by Philip Gibbs.
","Figen Binbay|Irfan Acikgoz|Mustafa Salti","","http://arxiv.org/abs/gr-qc/0607083v3","http://arxiv.org/pdf/gr-qc/0607083v3","","This submission has been withdrawn by arXiv administrators due to
  inappropriate text reuse from external sources","","","gr-qc","gr-qc"
"140","gr-qc/0607089v2","2006-07-21 07:53:48","2007-08-22 20:54:18","Energy and Momentum of The Szekeres Universes in Tele-parallel Gravity","  This paper has been removed by arXiv administrators because it plagiarizes
gr-qc/0603075 and others.
  This paper has excessive overlap with the following papers also written by
the authors or their collaborators: gr-qc/0606080, gr-qc/0607126,
gr-qc/0607109, gr-qc/0607110, and others.
","Sezgin Aygun|Ismail Tarhan|Husnu Baysal","","http://arxiv.org/abs/gr-qc/0607089v2","http://arxiv.org/pdf/gr-qc/0607089v2","","This submission has been withdrawn by arXiv administrators due to
  inappropriate text reuse from external sources","","","gr-qc","gr-qc"
"141","gr-qc/0607095v3","2006-07-22 15:47:27","2007-08-22 18:18:34","Electric and Magnetic Black Hole Solutions of Moller's Four-Momentum","  This paper has been removed by arXiv administrators because it plagiarizes
hep-th/0311050, gr-qc/0306101, gr-qc/9601044, gr-qc/0412111, hep-th/9604047,
and gr-qc/0212018.
","Figen Binbay|Nurettin Pirinccioglu|Mustafa Salti|Oktay Aydogdu","","http://arxiv.org/abs/gr-qc/0607095v3","http://arxiv.org/pdf/gr-qc/0607095v3","http://dx.doi.org/10.1007/s10773-007-9352-z","This submission has been withdrawn by arXiv administrators due to
  inappropriate text reuse from external sources","Int.J.Theor.Phys.46:2339-2350,2007","10.1007/s10773-007-9352-z","gr-qc","gr-qc"
"142","gr-qc/0607102v3","2006-07-24 06:15:42","2007-08-22 20:58:45","Energy Momentum Localization in Marder Space-Time","  This paper has been removed by arXiv administrators because it plagiarizes
hep-th/0308070, gr-qc/9910015, and others.
  This paper has excessive overlap with the following papers also written by
the authors or their collaborators: gr-qc/0607103, gr-qc/0606080,
gr-qc/0607119, gr-qc/0502043, gr-qc/0505079, gr-qc/0607115, gr-qc/0506061,
gr-qc/0607109, and others.
","Sezgin Aygun|Melis Aygun|Ismail Tarhan","","http://arxiv.org/abs/gr-qc/0607102v3","http://arxiv.org/pdf/gr-qc/0607102v3","http://dx.doi.org/10.1007/s12043-007-0002-z","This submission has been withdrawn by arXiv administrators due to
  inappropriate text reuse from external sources","Pramana 68:21-30,2007; Erratum-ibid 71:1367,2008","10.1007/s12043-007-0002-z","gr-qc","gr-qc"
"143","gr-qc/0607103v2","2006-07-24 06:32:21","2007-08-22 21:02:21","On the Energy Momentum in Bianchi Type I-III-V-VI0 Space-Time","  This paper has been removed by arXiv administrators because it plagiarizes
gr-qc/0303009, hep-th/0405047, gr-qc/0602107, hep-th/0311050, gr-qc/0412120,
gr-qc/0508005, and others.
  This paper has excessive overlap with the following papers also written by
the authors or their collaborators: gr-qc/0607119, gr-qc/0607115,
gr-qc/0606080, gr-qc/0607110, gr-qc/0607102, gr-qc/0510123, gr-qc/0607126,
gr-qc/0602012, and others.
","Sezgin Aygun|Melis Aygun|Ismail Tarhan","","http://arxiv.org/abs/gr-qc/0607103v2","http://arxiv.org/pdf/gr-qc/0607103v2","","This submission has been withdrawn by arXiv administrators due to
  inappropriate text reuse from external sources","","","gr-qc","gr-qc"
"144","gr-qc/0607104v2","2006-07-24 11:02:12","2007-08-22 21:05:18","Topological defect solutions in the spherically symmetric space-time
  admitting conformal motion","  This paper has excessive overlap with the following papers also written by
the authors or their collaborators: hep-th/0505013 and 0705.2930.
","Ihsan Yilmaz|Melis Aygun|Sezgin Aygun","","http://arxiv.org/abs/gr-qc/0607104v2","http://arxiv.org/pdf/gr-qc/0607104v2","http://dx.doi.org/10.1007/s10714-005-0197-6","This submission has been withdrawn by arXiv administrators due to
  inappropriate text reuse from external sources","Gen.Rel.Grav.37:2093-2104,2005","10.1007/s10714-005-0197-6","gr-qc","gr-qc"
"145","gr-qc/0607109v3","2006-07-25 09:17:47","2007-08-22 21:08:45","Energy Momentum of Marder Universe in Teleparallel gravity","  This paper has been removed by arXiv administrators because of overlap with
gr-qc/0408043 and 0706.3245.
  This paper has excessive overlap with the following papers also written by
the authors or their collaborators: gr-qc/0607110, gr-qc/0607126,
gr-qc/0608024, gr-qc/0603044, gr-qc/0505078, gr-qc/0502042, gr-qc/0502031,
gr-qc/0606028, gr-qc/0508018, gr-qc/0511095, and others.
","Sezgin Aygun|Husnu Baysal|Ismail Tarhan","","http://arxiv.org/abs/gr-qc/0607109v3","http://arxiv.org/pdf/gr-qc/0607109v3","http://dx.doi.org/10.1007/s10773-007-9375-5","This submission has been withdrawn by arXiv administrators due to
  inappropriate text reuse from external sources","Int.J.Mod.Phys.46:2607,2007","10.1007/s10773-007-9375-5","gr-qc","gr-qc"
"146","gr-qc/0607110v3","2006-07-25 12:21:33","2007-08-22 21:11:12","Energy Momentum Localization for Bianchi I-III-V-VI0 Universe in
  Teleparallel Gravity","  This paper has been removed by arXiv administrators because it plagiarizes
gr-qc/0010068.
  This paper has excessive overlap with the following papers also written by
the authors or their collaborators: gr-qc/0607109, gr-qc/0607126,
gr-qc/0607103, gr-qc/0607115, gr-qc/0607119, gr-qc/0608024, gr-qc/0601133,
gr-qc/0603044, gr-qc/0607089, gr-qc/0505078, and others.
","Sezgin Aygun|Melis Aygun|Ismail Tarhan","","http://arxiv.org/abs/gr-qc/0607110v3","http://arxiv.org/pdf/gr-qc/0607110v3","","This submission has been withdrawn by arXiv administrators due to
  inappropriate text reuse from external sources","","","gr-qc","gr-qc"
"147","gr-qc/0607115v2","2006-07-26 11:22:32","2007-08-22 21:13:36","Energy Momentum Complexes For Bianchi Type II-VIII-IX Universes","  This paper has been removed by arXiv administrators because it plagiarizes
gr-qc/0303009, hep-th/0405047, gr-qc/0412120, gr-qc/0602107, hep-th/0311050,
and gr-qc/0508005.
  This paper has excessive overlap with the following papers also written by
the authors or their collaborators: gr-qc/0607119, gr-qc/0607103,
gr-qc/0607126, gr-qc/0606080, gr-qc/0607102, gr-qc/0510123, gr-qc/0607110, and
others.
","Sezgin Aygun|Melis Aygun|Ismail Tarhan","","http://arxiv.org/abs/gr-qc/0607115v2","http://arxiv.org/pdf/gr-qc/0607115v2","","This submission has been withdrawn by arXiv administrators due to
  inappropriate text reuse from external sources","","","gr-qc","gr-qc"
"148","gr-qc/0607116v4","2006-07-26 11:48:52","2007-08-22 17:58:18","Energy Associated with Bianchi Type V10 Universe in Teleparallel Gravity","  This paper has been removed by arXiv administrators because it plagiarizes
gr-qc/0011087, gr-qc/0205028, gr-qc/0404108, gr-qc/0212018, and gr-qc/0011027.
","Mustafa Salti","","http://arxiv.org/abs/gr-qc/0607116v4","http://arxiv.org/pdf/gr-qc/0607116v4","","This submission has been withdrawn by arXiv administrators due to
  inappropriate text reuse from external sources","","","gr-qc","gr-qc"
"149","gr-qc/0607117v2","2006-07-26 11:51:52","2007-08-22 18:05:08","Four-Momentum Associated with the Vaidya Black Holes in Teleparallel
  Gravity","  This paper has been removed by arXiv administrators because it plagiarizes
hep-th/0307162, gr-qc/9601044, gr-qc/0510040, gr-qc/0205028, gr-qc/0212018, and
gr-qc/0605145.
","Murat Korunur|Mustafa Salti|Oktay Aydogdu|Irfan Acikgoz","","http://arxiv.org/abs/gr-qc/0607117v2","http://arxiv.org/pdf/gr-qc/0607117v2","","This submission has been withdrawn by arXiv administrators due to
  inappropriate text reuse from external sources","","","gr-qc","gr-qc"
"150","gr-qc/0607119v2","2006-07-26 13:33:22","2007-08-22 21:16:27","Energy And Momentum Associated With Bianchi Type Universes","  This paper has been removed by arXiv administrators because it plagiarizes
gr-qc/0303009, hep-th/0405047, gr-qc/0602107, hep-th/0311050, and others.
  This paper has excessive overlap with the following papers also written by
the authors or their collaborators: gr-qc/0607115, gr-qc/0607103,
gr-qc/0606080, gr-qc/0607126, gr-qc/0607102, gr-qc/0510123, gr-qc/0607110, and
others.
","Sezgin Aygun|Melis Aygun|Ismail Tarhan","","http://arxiv.org/abs/gr-qc/0607119v2","http://arxiv.org/pdf/gr-qc/0607119v2","","This submission has been withdrawn by arXiv administrators due to
  inappropriate text reuse from external sources","","","gr-qc","gr-qc"
"151","gr-qc/0607126v3","2006-07-27 10:25:22","2007-08-22 21:19:13","The Energy Momentum Problem in Teleparallel Gravity For Bianchi Type
  II-VIII-IX Universes","  This paper has excessive overlap with the following papers also written by
the authors or their collaborators: gr-qc/0607110, gr-qc/0607115,
gr-qc/0607119, gr-qc/0607109, gr-qc/0607103, gr-qc/0607089, gr-qc/0602012,
gr-qc/0608024, and others.
","Sezgin Aygun|Melis Aygun|Ismail Tarhan","","http://arxiv.org/abs/gr-qc/0607126v3","http://arxiv.org/pdf/gr-qc/0607126v3","","This submission has been withdrawn by arXiv administrators due to
  inappropriate text reuse from external sources","","","gr-qc","gr-qc"
"152","gr-qc/0608014v2","2006-08-03 08:06:45","2007-07-26 19:38:57","Relative Energy of the Taub Cosmological Model in Teleparallel Gravity","  This paper has been removed by arXiv administrators because it plagiarizes
gr-qc/0303034, ""The energy of the universe in teleparallel gravity,"" by T.
Vargas; and Gen relativ. Grav. 37(6) 1151-1152 (2005), ""The taub solution
again,"" by Robert T. Jantzen.
","Murat Korunur","","http://arxiv.org/abs/gr-qc/0608014v2","http://arxiv.org/pdf/gr-qc/0608014v2","","This submission has been withdrawn by arXiv administrators due to
  inappropriate text reuse from external sources","","","gr-qc","gr-qc"
"153","gr-qc/0608024v2","2006-08-04 11:24:30","2007-08-22 21:23:05","The Energy of Marder Space-Time in Moller Prescription","  This paper has been removed by arXiv administrators because it plagiarizes
gr-qc/0212018 and gr-qc/0408043.
  This paper has excessive overlap with the following papers also written by
the authors or their collaborators: gr-qc/0505078, gr-qc/0607109,
gr-qc/0511095, gr-qc/0607110, gr-qc/0603044, gr-qc/0603027, gr-qc/0607126,
gr-qc/0607116, gr-qc/0502060, gr-qc/0601070, gr-qc/0508018, gr-qc/0606028,
gr-qc/0603063, and others.
","Sezgin Aygun|Husnu Baysal|Ismail Tarhan","","http://arxiv.org/abs/gr-qc/0608024v2","http://arxiv.org/pdf/gr-qc/0608024v2","","This submission has been withdrawn by arXiv administrators due to
  inappropriate text reuse from external sources","","","gr-qc","gr-qc"
"154","math/0608202v2","2006-08-08 19:32:03","2006-08-11 15:12:05","Partial connection for $p$-torsion line bundles in charateristic $p>0$","  An arXiv administrator withdrew this article because it is a duplicate of
math/0608203.
","Helene Esnault","","http://arxiv.org/abs/math/0608202v2","http://arxiv.org/pdf/math/0608202v2","","An arXiv administrator withdrew this article because it is a
  duplicate of math/0608203","","","math.AG","math.AG"
"155","gr-qc/0608050v3","2006-08-09 20:01:34","2007-07-26 19:42:14","Topological Black Holes and Momentum Four Vector","  This paper has been removed by arXiv administrators because it plagiarizes
gr-qc/0303034, ""The energy of the universe in teleparallel gravity,"" by T.
Vargas; and hep-th/0512022, ""Topological black holes dressed with a conformally
coupled scalar field and electric charge,"" by C. Martinez, J. P. Staforelli and
R. Troncoso.
","Nurettin Pirinccioglu|Figen Binbay|Irfan Acikgoz|Oktay Aydogdu","","http://arxiv.org/abs/gr-qc/0608050v3","http://arxiv.org/pdf/gr-qc/0608050v3","http://dx.doi.org/10.1142/S0217732307022281","This submission has been withdrawn by arXiv administrators due to
  inappropriate text reuse from external sources","Mod.Phys.Lett.A22:1745-1757,2007","10.1142/S0217732307022281","gr-qc","gr-qc"
"156","math/0608355v3","2006-08-14 16:00:36","2007-03-06 11:09:59","Evaluation of Functional Integrals by means of series and the method of
  Borel transform","  In this paper I give an evaluation of a functional integral by means of a
series in functional derivatives, first of all we propose a differential
equation of first order and solve it by iterative methods, to obtain a series
for the indefinite integral in one dimension, after that we extend this concept
to infinite dimensional spaces, we introduce the functional derivative and
propose a functional differential equation for the functional integral which we
solve by iteration obtaining a series that includes the n-th order functional
derivative dn, this series can be improved by using the Euler transform for
alternating series.
  Also in the second part we study an evaluation of the Path integral using
Saddle-point methods, the corrections to the WKB approach that yield to a
divergent series are evaluate by means of a Borel transform, to accelerate its
convergence we use the Abel-Euler algorithm for power series.
","Jose Javier Garcia Moreta","","http://arxiv.org/abs/math/0608355v3","http://arxiv.org/pdf/math/0608355v3","","This submission has been withdrawn by arXiv administrators because of
  fraudulently claimed institutional affiliation and status","","","math.GM","math.GM|45.xx, 46.xx"
"157","gr-qc/0608111v3","2006-08-25 15:27:35","2007-08-22 21:26:01","The Colliding Plane Wave and Energy-Momentum Problems in General
  Relativity and Teleparallel Gravity","  This paper has been removed by arXiv administrators because it plagiarizes
gr-qc/0603075, 0706.3245, gr-qc/0403097, gr-qc/0404108, gr-qc/0303034,
hep-th/0206052, and others.
  This paper has excessive overlap with the following papers also written by
the authors or their collaborators: 0704.0525 and others.
","Sezgin Aygun|Ismail Tarhan|Husnu Baysal|Melis Aygun","","http://arxiv.org/abs/gr-qc/0608111v3","http://arxiv.org/pdf/gr-qc/0608111v3","","This submission has been withdrawn by arXiv administrators due to
  inappropriate text reuse from external sources","","","gr-qc","gr-qc"
"158","astro-ph/0609443v2","2006-09-15 16:16:43","2017-12-05 20:17:48","The Role of Primordial Kicks on Black Hole Merger Rates","  Primordial stars are likely to be very massive >30 Msun, form in isolation,
and will likely leave black holes as remnants in the centers of their host dark
matter halos. We expect primordial stars to form in halos in the mass range
10^6-10^10 Msun. Some of these early black holes, formed at redshifts z>10,
could be the seed black hole for a significant fraction of the supermassive
black holes found in galaxies in the local universe. If the black hole
descendants of the primordial stars exist, their mergers with nearby
supermassive black holes may be a prime candidate for long wavelength
gravitational wave detectors. We simulate formation and evolution of dark
matter halos in LambdaCDM universe. We seed high-redshift dark matter halos
with early black holes, and explore the merger history of the host halos and
the implications of black hole's kick velocities arising from their
coalescence. The central concentration of low mass early black holes in present
day galaxies is reduced if they experience even moderate kicks of tens of km/s.
Even such modest kicks allow the black holes to leave their parent halo, which
consequently leads to dynamical friction being less effective on the low mass
black holes that were ejected, compared to those still embedded in their parent
halos. Therefore, merger rates with central supermassive black holes in the
largest halos may be reduced by more than an order of magnitude. Using
analytical and illustrative cosmological N-body simulations, we quantify the
role of kicks on the merger rates of black holes formed from massive metal free
stars with supermassive black holes in present day galaxies.
","Miroslav Micic|Tom Abel|Steinn Sigurdsson","","http://arxiv.org/abs/astro-ph/0609443v2","http://arxiv.org/pdf/astro-ph/0609443v2","","12 pages, 9 figures, accepted by MNRAS. This submission has been
  withdrawn by arXiv administrators as a duplicate of arXiv:astro-ph/0512123","","","astro-ph","astro-ph"
"159","physics/0609168v2","2006-09-20 12:07:20","2006-11-27 09:55:18","The Application Hosting Environment: Lightweight Middleware for
  Grid-Based Computational Science","  Grid computing is distributed computing performed transparently across
multiple administrative domains. Grid middleware, which is meant to enable
access to grid resources, is currently widely seen as being too heavyweight
and, in consequence, unwieldy for general scientific use. Its heavyweight
nature, especially on the client-side, has severely restricted the uptake of
grid technology by computational scientists. In this paper, we describe the
Application Hosting Environment (AHE) which we have developed to address some
of these problems. The AHE is a lightweight, easily deployable environment
designed to allow the scientist to quickly and easily run legacy applications
on distributed grid resources. It provides a higher level abstraction of a grid
than is offered by existing grid middleware schemes such as the Globus Toolkit.
As a result the computational scientist does not need to know the details of
any particular underlying grid middleware and is isolated from any changes to
it on the distributed resources. The functionality provided by the AHE is
`application-centric': applications are exposed as web services with a
well-defined standards-compliant interface. This allows the computational
scientist to start and manage application instances on a grid in a transparent
manner, thus greatly simplifying the user experience. We describe how a range
of computational science codes have been hosted within the AHE and how the
design of the AHE allows us to implement complex workflows for deployment on
grid infrastructure.
","P. V. Coveney|R. S. Saksena|S. J. Zasada|M. McKeown|S. Pickles","","http://arxiv.org/abs/physics/0609168v2","http://arxiv.org/pdf/physics/0609168v2","","","","","physics.comp-ph","physics.comp-ph|physics.gen-ph"
"160","astro-ph/0609591v1","2006-09-20 21:20:16","2006-09-20 21:20:16","Report of the Dark Energy Task Force","  Dark energy appears to be the dominant component of the physical Universe,
yet there is no persuasive theoretical explanation for its existence or
magnitude. The acceleration of the Universe is, along with dark matter, the
observed phenomenon that most directly demonstrates that our theories of
fundamental particles and gravity are either incorrect or incomplete. Most
experts believe that nothing short of a revolution in our understanding of
fundamental physics will be required to achieve a full understanding of the
cosmic acceleration. For these reasons, the nature of dark energy ranks among
the very most compelling of all outstanding problems in physical science. These
circumstances demand an ambitious observational program to determine the dark
energy properties as well as possible.
","Andreas Albrecht|Gary Bernstein|Robert Cahn|Wendy L. Freedman|Jacqueline Hewitt|Wayne Hu|John Huth|Marc Kamionkowski|Edward W. Kolb|Lloyd Knox|John C. Mather|Suzanne Staggs|Nicholas B. Suntzeff","","http://arxiv.org/abs/astro-ph/0609591v1","http://arxiv.org/pdf/astro-ph/0609591v1","","The Dark Energy Task Force (DETF) was established by the Astronomy
  and Astrophysics Advisory Committee (AAAC) and the High Energy Physics
  Advisory Panel (HEPAP) as a joint sub-committee to advise the Department of
  Energy, the National Aeronautics and Space Administration, and the National
  Science Foundation on future dark energy research","","","astro-ph","astro-ph"
"161","gr-qc/0609101v2","2006-09-22 13:40:36","2007-07-26 18:12:51","The Lukash Plane-Wave Attractor and Relative Energy","  This paper has been removed by arXiv administrators because it plagiarizes
gr-qc/0411070, ""Structure and stability of the Lukash plane-wave spacetime,"" by
John D. Barrow and Christos G. Tsagas; and gr-qc/0205028, ""Gravitational Energy
of Kerr and Kerr Anti-de Sitter Space-times in the Teleparallel Geometry,"" by
J. F. da Rocha-neto and K. H. Castello-Branco.
","Murat Korunur|Mustafa Salti|Oktay Aydogdu","","http://arxiv.org/abs/gr-qc/0609101v2","http://arxiv.org/pdf/gr-qc/0609101v2","http://dx.doi.org/10.1142/S0217732307021755","This submission has been withdrawn by arXiv administrators due to
  inappropriate text reuse from external sources","Mod.Phys.Lett.A22:1601-1609,2007","10.1142/S0217732307021755","gr-qc","gr-qc"
"162","math/0610062v4","2006-10-02 19:36:33","2007-03-06 11:08:40","An explicit formula for Pi(x) in the form of a sum over the Non-trivial
  zeros of the Riemann Zeta function","  In this paper we discuss a method to express the Prime counting function as a
""sum"" over Non-trivial zeros of Riemann Zeta function, using techniques from
Analytic Number Theory, also we apply our results to the sum over primes of any
function Sum(p<x) f(p)
","Jose Javier Garcia Moreta","","http://arxiv.org/abs/math/0610062v4","http://arxiv.org/pdf/math/0610062v4","","This submission has been withdrawn by arXiv administrators because of
  fraudulently claimed institutional affiliation and status","","","math.GM","math.GM|11.Mxx, 11Z05"
"163","q-bio/0610036v2","2006-10-19 06:43:57","2006-12-19 18:35:17","The di-flavoenzyme reductase directly activates oxygen for the
  metabolism of diverse drug molecules by liver microsomal Cytochrome P450s","  This submission has been withdrawn by arXiv administration due to complaints
of misuse of institutional affiliation.
","Kelath Murali Manoj","","http://arxiv.org/abs/q-bio/0610036v2","http://arxiv.org/pdf/q-bio/0610036v2","","This submission has been withdrawn by arXiv administration","","","q-bio.BM","q-bio.BM|q-bio.SC"
"164","math/0610948v2","2006-10-30 21:46:06","2007-03-06 11:07:56","Quantization of Arbitrary Hamiltonians","  In this paper we discuss a method to apply Quantization rules for arbitrary
Hamiltonians that are not necessarily Polynomials in variable p, so we have H
of the form H(x,p)=F(x,p)+g(x) the method uses the results of ""Fractional
Calculus"" for derivatives and integrals.
","Jose Javier Garcia Moreta","","http://arxiv.org/abs/math/0610948v2","http://arxiv.org/pdf/math/0610948v2","","This submission has been withdrawn by arXiv administrators because of
  fraudulently claimed institutional affiliation and status","","","math.GM","math.GM|03.65.Ca, 03.65.Db, 02.60.Nm"
"165","gr-qc/0611014v2","2006-11-02 15:35:43","2007-07-26 18:17:15","Brane-World Black Holes and Energy-Momentum Vector","  This paper has been removed by arXiv administrators because it plagiarizes
hep-th/0308158, ""Braneworld cosmological models with anisotropy,"" by A. Campos,
R. Maartens, D. Matravers and C.F. Sopuerta; and gr-qc/0205028, ""Gravitational
Energy of Kerr and Kerr Anti-de Sitter Space-times in the Teleparallel
Geometry,"" by J. F. da Rocha-neto and K. H. Castello-Branco.
","Mustafa Salti|Oktay Aydogdu|Murat Korunur","","http://arxiv.org/abs/gr-qc/0611014v2","http://arxiv.org/pdf/gr-qc/0611014v2","http://dx.doi.org/10.1088/1126-6708/2006/12/078","This submission has been withdrawn by arXiv administrators due to
  inappropriate text reuse from external sources","JHEP 0612:078,2006","10.1088/1126-6708/2006/12/078","gr-qc","gr-qc"
"166","cs/0611102v1","2006-11-21 12:13:26","2006-11-21 12:13:26","Extending the Trusted Path in Client-Server Interaction","  We present a method to secure the complete path between a server and the
local human user at a network node. This is useful for scenarios like internet
banking, electronic signatures, or online voting. Protection of input
authenticity and output integrity and authenticity is accomplished by a
combination of traditional and novel technologies, e.g., SSL, ActiveX, and
DirectX. Our approach does not require administrative privileges to deploy and
is hence suitable for consumer applications. Results are based on the
implementation of a proof-of-concept application for the Windows platform.
","Hanno Langweg|Tommy Kristiansen","","http://arxiv.org/abs/cs/0611102v1","http://arxiv.org/pdf/cs/0611102v1","","8 pages, 3 figures","","","cs.CR","cs.CR|D.2.0; D.4.4; D.4.6"
"167","gr-qc/0612016v2","2006-12-02 22:07:11","2007-08-22 18:23:49","An ASSF and the Teleparallelisim","  This paper has been removed by arXiv administrators because it plagiarizes
gr-qc/0203084, gr-qc/0607138, gr-qc/0011087, gr-qc/0102070, gr-qc/0607138,
gr-qc/0109017, gr-qc/0212018, and gr-qc/9409039.
","Murat Korunur|Mustafa Salti|Oktay Aydogdu","Dicle Univ.|METU|METU","http://arxiv.org/abs/gr-qc/0612016v2","http://arxiv.org/pdf/gr-qc/0612016v2","http://dx.doi.org/10.1140/epjc/s10052-006-0195-1","This submission has been withdrawn by arXiv administrators due to
  inappropriate text reuse from external sources","Eur.Phys.J.C50:101-107,2007","10.1140/epjc/s10052-006-0195-1","gr-qc","gr-qc"
"168","cs/0612110v1","2006-12-21 19:36:38","2006-12-21 19:36:38","Architecture for Modular Data Centers","  Several factors are driving high-scale deployments of large data centers
built upon commodity components. These commodity clusters are far cheaper than
mainframe systems of the past but they bring serious heat and power density
issues. Also the high failure rate of the individual components drives
significant administrative costs. This proposal outlines an architecture for
data center design based upon 20'x8'x8' modules that substantially changes how
these systems are acquired, administered, and then later recycled.
","James Hamilton","","http://arxiv.org/abs/cs/0612110v1","http://arxiv.org/pdf/cs/0612110v1","","This article is published under a Creative Commons License Agreement
  (http://creativecommons.org/licenses/by/2.5/.) You may copy, distribute,
  display, and perform the work, make derivative works and make commercial use
  of the work, but, you must attribute the work to the author and CIDR 2007.
  3rd Biennial Conference on Innovative Data Systems Research (CIDR) January
  710, 2007, Asilomar, California, USA","","","cs.DB","cs.DB"
"169","physics/0701121v1","2007-01-10 13:48:51","2007-01-10 13:48:51","A simple guide to determine elastic properties of films on substrate
  from nanoindentation experiments","  This submission has been removed by arXiv administration because it was
submitted in violation of copyright by HAL.
","Sandrine Bec|Andre Tonck|Jean-Luc Loubet","LTDS|LTDS|LTDS","http://arxiv.org/abs/physics/0701121v1","http://arxiv.org/pdf/physics/0701121v1","","","","","physics.class-ph","physics.class-ph"
"170","physics/0701119v1","2007-01-10 13:54:04","2007-01-10 13:54:04","Nanoindentation and nanofriction on DLC films","  This submission has been removed by arXiv administration because it was
submitted in violation of copyright by HAL.
","Sandrine Bec|Andre Tonck|Julien Fontaine","LTDS|LTDS|LTDS","http://arxiv.org/abs/physics/0701119v1","http://arxiv.org/pdf/physics/0701119v1","","","","","physics.class-ph","physics.class-ph"
"171","astro-ph/0701566v1","2007-01-19 14:52:39","2007-01-19 14:52:39","Are there Radio-quiet Solar Flares?","  This submission has been withdrawn by arXiv administrators because it is a
duplicate of astro-ph/0701570.
","Arnold O. Benz|Roman Brajsa|Jasmina Magdalenic","ETH Zurich|Hvar Observatory|Hvar Observatory","http://arxiv.org/abs/astro-ph/0701566v1","http://arxiv.org/pdf/astro-ph/0701566v1","","","","","astro-ph","astro-ph"
"172","nucl-ex/0701073v1","2007-01-29 16:32:37","2007-01-29 16:32:37","Near-barrier Fusion and Transfer/Breakup induced by Weakly Bound and
  Exotic Halo Nuclei","  This submission has been withdrawn by arXiv administrators because it is a
duplicate of nucl-th/0610004.
","C. Beck","IPHC Strasbourg","http://arxiv.org/abs/nucl-ex/0701073v1","http://arxiv.org/pdf/nucl-ex/0701073v1","","8 pages, 3 eps figures, to be published in Nucl. Phys. A","Nucl. Phys. A787, 251c (2007)","","nucl-ex","nucl-ex"
"173","gr-qc/0702047v2","2007-02-08 09:39:43","2007-08-22 18:33:02","Energy and Momentum of Bell-Szekeres Space-time in Moller Prescription","  This paper has been removed by arXiv administrators because it plagiarizes
gr-qc/0602105, gr-qc/0307010, and gr-qc/9802050.
","Sezgin Aygun","","http://arxiv.org/abs/gr-qc/0702047v2","http://arxiv.org/pdf/gr-qc/0702047v2","","This submission has been withdrawn by arXiv administrators due to
  inappropriate text reuse from external sources","Acta Phys.Polon.B38:73-80,2007","","gr-qc","gr-qc"
"174","physics/0702118v1","2007-02-14 18:46:11","2007-02-14 18:46:11","Science Citation Index data: Two additional reasons against its use for
  administrative purposes","  First, for decades the use of anonymity in reviews for science funding
proposals and for evaluating manuscripts for publication has been gradually
corrupting American science, encouraging and rewarding the dark elements of
human nature. Unethical reviewers, secure and unaccountable through anonymity,
all too often make untrue and/or pejorative statements to eliminate their
professional competitors. Survival in this corrupt environment has led to a
consensus-only mentality. Consequently, important scientific contradictions, if
they can be published at all, are selectively ignored in many instances out of
fear of anonymous retaliation. Science Citation Index data in such a corrupt
environment may be of little administrative value, except for possible use in
documenting scientific fraud. Second, as knowledge of the administrative use of
Science Citation Index data spreads, scientists will adapt and will shift to
research on popular subjects to elicit greater numbers of citations, rather
than to take the paths less trodden where important scientific discoveries may
lay waiting.
","J. Marvin Herndon","","http://arxiv.org/abs/physics/0702118v1","http://arxiv.org/pdf/physics/0702118v1","","Accepted for publication in Current Science","","","physics.gen-ph","physics.gen-ph"
"175","math/0703015v1","2007-03-01 08:00:04","2007-03-01 08:00:04","Neural Network and Segmented Labour Market","  In France, for administrative reasons, unemployed workers may actually be
involved in occasional work while remaining identified as unemployed (and
receiving the corresponding benefit). This is due to the fact that the
unemployed are deemed to be seeking full-time jobs and non-fixed term contracts
of employment. This situation may be analysed as evidence of a special type of
secondary segment of the labour market in a context of massive unemployment.
The authors consider the effects of this situation both on the duration of
unemployment and its recurrence may be usefully investigated.
","Patrice Gaubert|Marie Cottrell","MATISSE|MATISSE, SAMOS","http://arxiv.org/abs/math/0703015v1","http://arxiv.org/pdf/math/0703015v1","","Extended version of the ACSEG 98 conference (Louvain-la-neuve)","European Journal of Economics and Social Systems Vol 14, number 1
  (1999) 19-40","","math.ST","math.ST|stat.TH"
"176","cs/0703082v1","2007-03-15 13:41:11","2007-03-15 13:41:11","Remarks on the O(N) Implementation of the Fast Marching Method","  The fast marching algorithm computes an approximate solution to the eikonal
equation in O(N log N) time, where the factor log N is due to the
administration of a priority queue. Recently, Yatziv, Bartesaghi and Sapiro
have suggested to use an untidy priority queue, reducing the overall complexity
to O(N) at the price of a small error in the computed solution. In this paper,
we give an explicit estimate of the error introduced, which is based on a
discrete comparison principle. This estimates implies in particular that the
choice of an accuracy level that is independent of the speed function F results
in the complexity bound O(Fmax /Fmin N). A numerical experiment illustrates
this robustness problem for large ratios Fmax /Fmin .
","Christian Rasch|Thomas Satzger","","http://arxiv.org/abs/cs/0703082v1","http://arxiv.org/pdf/cs/0703082v1","","7 pages, 2 figures","","","cs.NA","cs.NA"
"177","cs/0703113v1","2007-03-23 04:25:36","2007-03-23 04:25:36","Automatic Selection of Bitmap Join Indexes in Data Warehouses","  The queries defined on data warehouses are complex and use several join
operations that induce an expensive computational cost. This cost becomes even
more prohibitive when queries access very large volumes of data. To improve
response time, data warehouse administrators generally use indexing techniques
such as star join indexes or bitmap join indexes. This task is nevertheless
complex and fastidious. Our solution lies in the field of data warehouse
auto-administration. In this framework, we propose an automatic index selection
strategy. We exploit a data mining technique ; more precisely frequent itemset
mining, in order to determine a set of candidate indexes from a given workload.
Then, we propose several cost models allowing to create an index configuration
composed by the indexes providing the best profit. These models evaluate the
cost of accessing data using bitmap join indexes, and the cost of updating and
storing these indexes.
","Kamel Aouiche|Jerome Darmont|Omar Boussaid|Fadila Bentayeb","","http://arxiv.org/abs/cs/0703113v1","http://arxiv.org/pdf/cs/0703113v1","","","","","cs.DB","cs.DB"
"178","0704.0525v2","2007-04-04 08:55:31","2007-08-22 19:02:06","On the Energy-Momentum Problem in Static Einstein Universe","  This paper has been removed by arXiv administrators because it plagiarizes
gr-qc/0410004, gr-qc/0603075, and others.
  This paper also has excessive overlap with the following papers also written
by the authors or their collaborators: gr-qc/0608111, and others.
","Sezgin Aygun|Ismail Tarhan|Husnu Baysal","","http://arxiv.org/abs/0704.0525v2","http://arxiv.org/pdf/0704.0525v2","http://dx.doi.org/10.1088/0256-307X/24/2/015","This submission has been withdrawn by arXiv administrators due to
  inappropriate text reuse from external sources","Chin.Phys.Lett.24:355-358,2007","10.1088/0256-307X/24/2/015","gr-qc","gr-qc"
"179","0704.2186v1","2007-04-17 15:48:45","2007-04-17 15:48:45","Metal and molecule cooling in simulations of structure formation","  This submission has been withdrawn by arXiv administrators because it is a
duplicate of 0704.2182.
","U. Maio|K. Dolag|B. Ciardi|L. Tornatore","","http://arxiv.org/abs/0704.2186v1","http://arxiv.org/pdf/0704.2186v1","http://dx.doi.org/10.1111/j.1365-2966.2007.12016.x","","Mon.Not.Roy.Astron.Soc.379:963-973,2007","10.1111/j.1365-2966.2007.12016.x","astro-ph","astro-ph"
"180","0704.2782v3","2007-04-20 20:03:07","2008-03-17 21:16:53","Implications of ""peak oil"" for atmospheric CO2 and climate","  Unconstrained CO2 emission from fossil fuel burning has been the dominant
cause of observed anthropogenic global warming. The amounts of ""proven"" and
potential fossil fuel reserves are uncertain and debated. Regardless of the
true values, society has flexibility in the degree to which it chooses to
exploit these reserves, especially unconventional fossil fuels and those
located in extreme or pristine environments. If conventional oil production
peaks within the next few decades, it may have a large effect on future
atmospheric CO2 and climate change, depending upon subsequent energy choices.
Assuming that proven oil and gas reserves do not greatly exceed estimates of
the Energy Information Administration, and recent trends are toward lower
estimates, we show that it is feasible to keep atmospheric CO2 from exceeding
about 450 ppm by 2100, provided that emissions from coal, unconventional fossil
fuels, and land use are constrained. Coal-fired power plants without
sequestration must be phased out before mid-century to achieve this CO2 limit.
It is also important to ""stretch"" conventional oil reserves via energy
conservation and efficiency, thus averting strong pressures to extract liquid
fuels from coal or unconventional fossil fuels while clean technologies are
being developed for the era ""beyond fossil fuels"". We argue that a rising price
on carbon emissions is needed to discourage conversion of the vast fossil
resources into usable reserves, and to keep CO2 beneath the 450 ppm ceiling.
","P. A. Kharecha|J. E. Hansen","NASA GISS and Columbia Univ. Earth Institute|NASA GISS and Columbia Univ. Earth Institute","http://arxiv.org/abs/0704.2782v3","http://arxiv.org/pdf/0704.2782v3","http://dx.doi.org/10.1029/2007GB003142","(22 pages, 7 figures; final version accepted by Global Biogeochemical
  Cycles)","Publ. in Global Biogeochem. Cycles, 22, GB3012 (2008)","10.1029/2007GB003142","physics.ao-ph","physics.ao-ph"
"181","0704.3520v1","2007-04-26 11:47:35","2007-04-26 11:47:35","Vers l'auto-administration des entrepots de donnees","  With the wide development of databases in general and data warehouses in
particular, it is important to reduce the tasks that a database administrator
must perform manually. The idea of using data mining techniques to extract
useful knowledge for administration from the data themselves has existed for
some years. However, little research has been achieved. The aim of this study
is to search for a way of extracting useful knowledge from stored data to
automatically apply performance optimization techniques, and more particularly
indexing techniques. We have designed a tool that extracts frequent itemsets
from a given workload to compute an index configuration that helps optimizing
data access time. The experiments we performed showed that the index
configurations generated by our tool allowed performance gains of 15% to 25% on
a test database and a test data warehouse.
","Kamel Aouiche|Jerome Darmont","ERIC|ERIC","http://arxiv.org/abs/0704.3520v1","http://arxiv.org/pdf/0704.3520v1","","Version courte de 4 pages","XXXV\`emes Journ\'ees de Statistique, Session sp\'eciale
  Entreposage et Fouille de Donn\'ees, Lyon (02/06/2003) 105-108","","cs.DB","cs.DB"
"182","0705.0620v1","2007-05-04 19:54:25","2007-05-04 19:54:25","Generating Unexpected Spin Echoes in Dipolar Solids with Pi Pulses","  This submission has been withdrawn by arXiv administrators because it is a
duplicate of 0705.0667.
","Dale Li|A. E. Dementyev|Yanqun Dong|R. G. Ramos|S. E. Barrett","","http://arxiv.org/abs/0705.0620v1","http://arxiv.org/pdf/0705.0620v1","","","","","quant-ph","quant-ph"
"183","0705.2930v2","2007-05-21 07:52:02","2007-08-22 18:38:39","Magnetized Quark and Strange Quark Matter in the Spherical Symmetric
  Space-Time Admitting Conformal Motion","  This paper has been removed by arXiv administrators because it plagiarizes
astro-ph/0611537, astro-ph/0506256, astro-ph/0203033, astro-ph/0311128,
gr-qc/0505144, astro-ph/0611460, and astro-ph/0610840.
","Can Aktas|Ihsan Yilmaz","","http://arxiv.org/abs/0705.2930v2","http://arxiv.org/pdf/0705.2930v2","http://dx.doi.org/10.1007/s10714-007-0426-2","This submission has been withdrawn by arXiv administrators due to
  inappropriate text reuse from external sources","Gen.Rel.Grav.39:849-862,2007","10.1007/s10714-007-0426-2","gr-qc","gr-qc"
"184","0705.4202v3","2007-05-29 12:01:16","2007-12-13 18:53:10","Higher Dimensional Strange Quark Matter Coupled to the String Cloud with
  Electromagnetic Field Admitting One Parameter Group of Conformal Motion","  This paper has been withdrawn by arXiv administrators because of an
inappropriate amount of overlap with hep-th/0505013.
","Anirudh Pradhan|G. S. Khadekar|M. K. Mishra|S. Kumbhare","","http://arxiv.org/abs/0705.4202v3","http://arxiv.org/pdf/0705.4202v3","http://dx.doi.org/10.1088/0256-307X/24/10/083","This paper has been withdrawn by arXiv administrators","Chin.Phys.Lett.24:3013-3016,2007","10.1088/0256-307X/24/10/083","gr-qc","gr-qc"
"185","0706.0306v1","2007-06-03 19:37:41","2007-06-03 19:37:41","Submission of content to a digital object repository using a
  configurable workflow system","  The prototype of a workflow system for the submission of content to a digital
object repository is here presented. It is based entirely on open-source
standard components and features a service-oriented architecture. The front-end
consists of Java Business Process Management (jBPM), Java Server Faces (JSF),
and Java Server Pages (JSP). A Fedora Repository and a mySQL data base
management system serve as a back-end. The communication between front-end and
back-end uses a SOAP minimal binding stub. We describe the design principles
and the construction of the prototype and discuss the possibilities and
limitations of work ow creation by administrators. The code of the prototype is
open-source and can be retrieved in the project escipub at
http://sourceforge.net
","Andreas Hense|Johannes Mueller","","http://arxiv.org/abs/0706.0306v1","http://arxiv.org/pdf/0706.0306v1","","","","","cs.DL","cs.DL"
"186","0706.1130v1","2007-06-08 08:23:14","2007-06-08 08:23:14","A Communication Model for Adaptive Service Provisioning in Hybrid
  Wireless Networks","  Mobile entities with wireless links are able to form a mobile ad-hoc network.
Such an infrastructureless network does not have to be administrated. However,
self-organizing principles have to be applied to deal with upcoming problems,
e.g. information dissemination. These kinds of problems are not easy to tackle,
requiring complex algorithms. Moreover, the usefulness of pure ad-hoc networks
is arguably limited. Hence, enthusiasm for mobile ad-hoc networks, which could
eliminate the need for any fixed infrastructure, has been damped. The goal is
to overcome the limitations of pure ad-hoc networks by augmenting them with
instant Internet access, e.g. via integration of UMTS respectively GSM links.
However, this raises multiple questions at the technical as well as the
organizational level. Motivated by characteristics of small-world networks that
describe an efficient network even without central or organized design, this
paper proposes to combine mobile ad-hoc networks and infrastructured networks
to form hybrid wireless networks. One main objective is to investigate how this
approach can reduce the costs of a permanent backbone link and providing in the
same way the benefits of useful information from Internet connectivity or
service providers. For the purpose of bridging between the different types of
networks, an adequate middleware service is the focus of our investigation.
This paper shows our first steps forward to this middleware by introducing the
Injection Communication paradigm as principal concept.
","Matthias R. Brust|Steffen Rothkugel","","http://arxiv.org/abs/0706.1130v1","http://arxiv.org/pdf/0706.1130v1","","WSEAS Transactions on Circuits and Systems 2004","","","cs.NI","cs.NI|cs.AR|cs.CY|cs.HC"
"187","0706.1295v1","2007-06-09 09:03:50","2007-06-09 09:03:50","Reaction Time of a Group of Physics Students","  The reaction time of a group of students majoring in Physics is reported
here. Strong co-relation between fatigue, reaction time and performance have
been seen and may be useful for academicians and administrators responsible of
working out time-tables, course structures, students counsellings etc.
","Charu Saxena|Rini Kaur|P. Arun","","http://arxiv.org/abs/0706.1295v1","http://arxiv.org/pdf/0706.1295v1","http://dx.doi.org/10.1088/0031-9120/43/3/010","10 pages, 4 figures","","10.1088/0031-9120/43/3/010","physics.ed-ph","physics.ed-ph"
"188","0707.1776v2","2007-07-12 12:03:45","2007-08-22 19:07:32","M©ªller Energy-Momentum Complex in General Relativity for Higher
  Dimensional Universes","  This paper has been removed by arXiv administrators because it plagiarizes
gr-qc/0508005 and others.
","M. Aygun|S. Aygun|I. Yilmaz|H. Baysal|I. Tarhan","","http://arxiv.org/abs/0707.1776v2","http://arxiv.org/pdf/0707.1776v2","http://dx.doi.org/10.1088/0256-307X/24/7/010","This submission has been withdrawn by arXiv administrators due to
  inappropriate text reuse from external sources","Chin.Phys.Lett.24:1821-1824,2007","10.1088/0256-307X/24/7/010","gr-qc","gr-qc"
"189","0707.2931v1","2007-07-19 16:15:51","2007-07-19 16:15:51","Fog and Dew Collection Projects in Croatia","  The present paper discusses the fog and dew water collection in Croatia.
Zavizan, the highest meteorological station in Croatia(1594m) is chosen for
collecting of fog water with a standard fog collector (SFC). The highest daily
collection rate was 27.8 L / m2. The highest daily collection rate in days
without rain was 19.1 l/m2. Dew is also a noticeable source of water,
especially during the drier summer season. Dew condensers in Croatia have been
installed on the Adriatic coast (Zadar) and islands Vis and Bisevo. We report
and discuss the data collected since 2003. In the small Bisevo island, a
special roof has been designed to improve the formation and collection of dew
on a house. Data from April 2005 will be presented and discussed.
","M. Mileta|D. Beysens|Vadim Nikolayev|I. Milimouk|O. Clus|M. Muselli","PMMH|PMMH, SBT|PMMH, SBT|PMMH|PMMH|PMMH","http://arxiv.org/abs/0707.2931v1","http://arxiv.org/pdf/0707.2931v1","","accessible sur
  http://balwois.mpl.ird.fr/balwois/administration/full_paper/ffp-587.pdf","Proc. International Conference on ""Water Observation and
  Information System for Decision Support"" (BALWOIS 2006), Ohrid, Republic of
  Macedonia - 23, 26 May 2006 (2006) 1","","physics.flu-dyn","physics.flu-dyn"
"190","0708.0187v1","2007-08-01 16:09:39","2007-08-01 16:09:39","Wide therapeutic time window for nimesulide neuroprotection in a model
  of transient focal cerebral ischemia in the rat","  Results from several studies indicate that cyclooxygenase-2 (COX-2) is
involved in ischemic brain injury. The purpose of this study was to evaluate
the neuroprotective effects of the selective COX-2 inhibitor nimesulide on
cerebral infarction and neurological deficits in a standardized model of
transient focal cerebral ischemia in rats. Three doses of nimesulide (3, 6 and
12 mg/kg; i.p.) or vehicle were administered immediately after stroke and
additional doses were given at 6, 12, 24, 36 and 48 h after ischemia. In other
set of experiments, the effect of nimesulide was studied in a situation in
which its first administration was delayed for 3-24 h after ischemia. Total,
cortical and subcortical infarct volumes and functional outcome (assessed by
neurological deficit score and rotarod performance) were determined 3 days
after ischemia. The effect of nimesulide on prostaglandin E(2) (PGE(2)) levels
in the injured brain was also investigated. Nimesulide dose-dependently reduced
infarct volume and improved functional recovery when compared to vehicle. Of
interest is the finding that neuroprotection conferred by nimesulide (reduction
of infarct size and neurological deficits and improvement of rotarod
performance) was also observed when treatment was delayed until 24 h after
ischemia. Further, administration of nimesulide in a delayed treatment paradigm
completely abolished PGE(2) accumulation in the postischemic brain, suggesting
that COX-2 inhibition is a promising therapeutic strategy for cerebral ischemia
to target the late-occurring inflammatory events which amplify initial damage.
","E. Candelario-Jalil|A. Gonzalez-Falcon|M. Garcia-Cabrera|O. S. Leon|B. L. Fiebich","","http://arxiv.org/abs/0708.0187v1","http://arxiv.org/pdf/0708.0187v1","","","Brain Research 1007(1-2): 98-108 (2004)","","q-bio.NC","q-bio.NC|q-bio.TO"
"191","0708.0564v1","2007-08-03 18:51:15","2007-08-03 18:51:15","Neuroprotective efficacy of nimesulide against hippocampal neuronal
  damage following transient forebrain ischemia","  Cyclooxygenase-2 is involved in the inflammatory component of the ischemic
cascade, playing an important role in the delayed progression of the brain
damage. The present study evaluated the pharmacological effects of the
selective cyclooxygenase-2 inhibitor nimesulide on delayed neuronal death of
hippocampal CA1 neurons following transient global cerebral ischemia in
gerbils. Administration of therapeutically relevant doses of nimesulide (3, 6
and 12 mg/kg; i.p.) 30 min before ischemia and at 6, 12, 24, 48 and 72 h after
ischemia significantly (P<0.01) reduced hippocampal neuronal damage. Treatment
with a single dose of nimesulide given 30 min before ischemia also resulted in
a significant increase in the number of healthy neurons in the hippocampal CA1
sector 7 days after ischemia. Of interest is the finding that nimesulide
rescued CA1 pyramidal neurons from ischemic death even when treatment was
delayed until 24 h after ischemia (34+/-9% protection). Neuroprotective effect
of nimesulide is still evident 30 days after the ischemic episode, providing
the first experimental evidence that cyclooxygenase-2 inhibitors confer a
long-lasting neuroprotection. Oral administration of nimesulide was also able
to significantly reduce brain damage, suggesting that protective effects are
independent of the route of administration. The present study confirms the
ability of cyclooxygenase-2 inhibitors to reduce brain damage induced by
cerebral ischemia and indicates that nimesulide can provide protection when
administered for up to 24 h post-ischemia.
","E. Candelario-Jalil|D. Alvarez|A. Gonzalez-Falcon|M. Garcia-Cabrera|G. Martinez-Sanchez|N. Merino|A. Giuliani|O. S. Leon","","http://arxiv.org/abs/0708.0564v1","http://arxiv.org/pdf/0708.0564v1","","","European Journal of Pharmacology 453(2-3): 189-195 (2002)","","q-bio.TO","q-bio.TO"
"192","0708.1781v1","2007-08-13 22:10:46","2007-08-13 22:10:46","Effects of pyruvate administration on infarct volume and neurological
  deficits following permanent focal cerebral ischemia in rats","  Recent experimental evidences indicate that pyruvate, the final metabolite of
glycolysis, has a remarkable protective effect against different types of brain
injury. The purpose of this study was to assess the neuroprotective effect and
the neurological outcome after pyruvate administration in a model of ischemic
stroke induced by permanent middle cerebral artery occlusion (pMCAO) in rats.
Three doses of pyruvate (250, 500 and 1000 mg/kg, i.p.) or vehicle were
administered intraperitoneally 30 min after pMCAO. In other set of experiments,
pyruvate was given either before, immediately after ischemia or in a long-term
administration paradigm. Functional outcome, mortality and infarct volume were
determined 24 h after stroke. Even when the lowest doses of pyruvate reduced
mortality and neurological deficits, no concomitant reduction in infarct volume
was observed. The highest dose of pyruvate increased cortical infarction by 27%
when administered 30 min after pMCAO. In addition, when pyruvate was given
before pMCAO, a significant increase in neurological deficits was noticed.
Surprisingly, on the contrary of what was found in the case of transient global
ischemia, present findings do not support a great neuroprotective role for
pyruvate in permanent focal cerebral ischemia, suggesting two distinct
mechanisms involved in the effects of this glycolytic metabolite in the
ischemic brain.
","A. Gonzalez-Falcon|E. Candelario-Jalil|M. Garcia-Cabrera|O. S. Leon","","http://arxiv.org/abs/0708.1781v1","http://arxiv.org/pdf/0708.1781v1","","","Brain Research 990(1-2): 1-7 (2003)","","q-bio.TO","q-bio.TO"
"193","0708.1971v1","2007-08-14 22:30:36","2007-08-14 22:30:36","Nimesulide limits kainate-induced oxidative damage in the rat
  hippocampus","  Kainate induces a marked expression of cyclooxygenase-2 after its systemic
administration. Because cyclooxygenase-2 activity is associated to the
production of reactive oxygen species, we investigated the effects of
nimesulide, a selective cyclooxygenase-2 inhibitor, on kainate-induced in vivo
oxidative damage in the rat hippocampus. A clinically relevant dose of
nimesulide (6 mg/kg, i.p.) was administered three times following kainate
application (9 mg/kg, i.p.). After 24 h of kainate administration, the drastic
decrease in hippocampal glutathione content and the significant increase in
lipid peroxidation were attenuated in nimesulide-treated rats, suggesting that
the induction of cyclooxygenase-2 is involved in kainate-mediated free radicals
formation.
","E. Candelario-Jalil|H. H. Ajamieh|S. Sam|G. Martinez|O. S. Leon","","http://arxiv.org/abs/0708.1971v1","http://arxiv.org/pdf/0708.1971v1","","","European Journal of Pharmacology 390(3): 295-298 (2000)","","q-bio.TO","q-bio.TO"
"194","0708.2061v1","2007-08-15 15:31:13","2007-08-15 15:31:13","Selective vulnerability to kainate-induced oxidative damage in different
  rat brain regions","  Some markers of oxidative injury were measured in different rat brain areas
(hippocampus, cerebral cortex, striatum, hypothalamus, amygdala/piriform cortex
and cerebellum) after the systemic administration of an excitotoxic dose of
kainic acid (KA, 9 mg kg(-1) i.p.) at two different sampling times (24 and 48
h). Kainic acid was able to lower markedly (P < 0.05) the glutathione (GSH)
levels in hippocampus, cerebellum and amygdala/piriform cortex (maximal
reduction at 24 h). In a similar way, lipid peroxidation, as assessed by
malonaldehyde and 4-hydroxyalkenal levels, significantly increased (P < 0.05)
in hippocampus, cerebellum and amygdala/piriform cortex mainly at 24 h after
KA. In addition, hippocampal superoxide dismutase (SOD) activity decreased
significantly (P < 0.05) with respect to basal levels by 24 h after KA
application. On the other hand, brain areas such as hypothalamus, striatum and
cerebral cortex seem to be less susceptible to KA excitotoxicity. According to
these findings, the pattern of oxidative injury induced by systemically
administered KA seems to be highly region-specific. Further, our results have
shown that a lower antioxidant status (GSH and SOD) seems not to play an
important role in the selective vulnerability of certain brain regions because
it correlates poorly with increases in markers of oxidative damage.
","E. Candelario-Jalil|S. M. Al-Dalain|R. Castillo|G. Martinez|O. S. Fernandez","","http://arxiv.org/abs/0708.2061v1","http://arxiv.org/pdf/0708.2061v1","","","Journal of Applied Toxicology 21(5): 403-407 (2001)","","q-bio.TO","q-bio.TO"
"195","0708.2266v1","2007-08-16 18:44:53","2007-08-16 18:44:53","The study of a new gerrymandering methodology","  This paper is to obtain a simple dividing-diagram of the congressional
districts, where the only limit is that each district should contain the same
population if possibly. In order to solve this problem, we introduce three
different standards of the ""simple"" shape. The first standard is that the final
shape of the congressional districts should be of a simplest figure and we
apply a modified ""shortest split line algorithm"" where the factor of the same
population is considered only. The second standard is that the gerrymandering
should ensure the integrity of the current administrative area as the
convenience for management. Thus we combine the factor of the administrative
area with the first standard, and generate an improved model resulting in the
new diagram in which the perimeters of the districts are along the boundaries
of some current counties. Moreover, the gerrymandering should consider the
geographic features.The third standard is introduced to describe this
situation. Finally, it can be proved that the difference between the supporting
ratio of a certain party in each district and the average supporting ratio of
that particular party in the whole state obeys the Chi-square distribution
approximately. Consequently, we can obtain an archetypal formula to check
whether the gerrymandering we propose is fair.
","Pan Kai|Tan Yue|Jiang Sheng","","http://arxiv.org/abs/0708.2266v1","http://arxiv.org/pdf/0708.2266v1","","23 pages,15 figures, 2007 American mathematical modeling contest
  ""Meritorious Winner""","","","cs.CY","cs.CY"
"196","0708.3166v1","2007-08-23 12:25:50","2007-08-23 12:25:50","Web Server Benchmark Application WiiBench using Erlang/OTP R11 and
  Fedora-Core Linux 5.0","  As the web grows and the amount of traffics on the web server increase,
problems related to performance begin to appear. Some of the problems, such as
the number of users that can access the server simultaneously, the number of
requests that can be handled by the server per second (requests per second) to
bandwidth consumption and hardware utilization like memories and CPU. To give
better quality of service (\textbf{\textit{QoS}}), web hosting providers and
also the system administrators and network administrators who manage the server
need a benchmark application to measure the capabilities of their servers.
Later, the application intends to work under Linux/Unix -- like platforms and
built using Erlang/OTP R11 as a concurrent oriented language under Fedora Core
Linux 5.0. \textbf{\textit{WiiBench}} is divided into two main parts, the
controller section and the launcher section. Controller is the core of the
application. It has several duties, such as read the benchmark scenario file,
configure the program based on the scenario, initialize the launcher section,
gather the benchmark results from local and remote Erlang node where the
launcher runs and write them in a log file (later the log file will be used to
generate a report page for the sysadmin). Controller also has function as a
timer which act as timing for user inters arrival to the server. Launcher
generates a number of users based on the scenario, initialize them and start
the benchmark by sending requests to the web server. The clients also gather
the benchmark result and send them to the controller.
","A. B. Mutiara|T. A. Sabastian","","http://arxiv.org/abs/0708.3166v1","http://arxiv.org/pdf/0708.3166v1","","","","","cs.NI","cs.NI"
"197","0711.0528v1","2007-11-04 16:39:47","2007-11-04 16:39:47","Web-based Interface in Public Cluster","  A web-based interface dedicated for cluster computer which is publicly
accessible for free is introduced. The interface plays an important role to
enable secure public access, while providing user-friendly computational
environment for end-users and easy maintainance for administrators as well. The
whole architecture which integrates both aspects of hardware and software is
briefly explained. It is argued that the public cluster is globally a unique
approach, and could be a new kind of e-learning system especially for parallel
programming communities.
","Z. Akbar|L. T. Handoko","","http://arxiv.org/abs/0711.0528v1","http://arxiv.org/pdf/0711.0528v1","","9 pages, Proceeding of the 9th International Conference on
  Information Integration and Web-based Applications and Services 2007","","","cs.DC","cs.DC|cs.CY"
"198","0711.1484v3","2007-11-09 16:18:41","2007-12-13 18:28:39","Geometry and Topology in Hamiltonian Dynamics and Statistical Mechanics","  This submission has been withdrawn by arXiv administrators because it is
incomplete and thus violates arXiv policy.
","Marco Pettini","","http://arxiv.org/abs/0711.1484v3","http://arxiv.org/pdf/0711.1484v3","","This submission has been withdrawn","","","cond-mat.stat-mech","cond-mat.stat-mech"
"199","0711.2096v1","2007-11-13 23:54:39","2007-11-13 23:54:39","Cyclooxygenase Inhibition Limits Blood-Brain Barrier Disruption
  following Intracerebral Injection of Tumor Necrosis Factor-alpha in the Rat","  Increased permeability of the blood-brain barrier (BBB) is important in
neurological disorders. Neuroinflammation is associated with increased BBB
breakdown and brain injury. Tumor necrosis factor-a (TNF-a) is involved in BBB
injury and edema formation through a mechanism involving matrix
metalloproteinase (MMP) upregulation. There is emerging evidence indicating
that cyclooxygenase (COX) inhibition limits BBB disruption following ischemic
stroke and bacterial meningitis, but the mechanisms involved are not known. We
used intracerebral injection of TNF-a to study the effect of COX inhibition on
TNF-a-induced BBB breakdown, MMP expression/activity and oxidative stress. BBB
disruption was evaluated by the uptake of 14C-sucrose into the brain and by
magnetic resonance imaging (MRI) utilizing Gd-DTPA as a paramagnetic contrast
agent. Using selective inhibitors of each COX isoform, we found that COX-1
activity is more important than COX-2 in BBB opening. TNF-a induced a
significant upregulation of gelatinase B (MMP-9), stromelysin-1 (MMP-3) and
COX-2. In addition, TNF-a significantly depleted glutathione as compared to
saline. Indomethacin (10 mg/kg; i.p.), an inhibitor of COX-1 and COX-2, reduced
BBB damage at 24 h. Indomethacin significantly attenuated MMP-9 and MMP-3
expression and activation, and prevented the loss of endogenous radical
scavenging capacity following intracerebral injection of TNF-a. Our results
show for the first time that BBB disruption during neuroinflammation can be
significantly reduced by administration of COX inhibitors. Modulation of COX in
brain injury by COX inhibitors or agents modulating prostaglandin E2
formation/signaling may be useful in clinical settings associated with BBB
disruption.
","Eduardo Candelario-Jalil|Saeid Taheri|Yi Yang|Rohit Sood|Mark Grossetete|Eduardo Y. Estrada|Bernd L. Fiebich|Gary A. Rosenberg","","http://arxiv.org/abs/0711.2096v1","http://arxiv.org/pdf/0711.2096v1","","","Journal of Pharmacology and Experimental Therapeutics 323(2):
  488-498 (2007)","","q-bio.TO","q-bio.TO|q-bio.BM"
"200","0712.2168v1","2007-12-13 15:14:01","2007-12-13 15:14:01","Study of conditions of use of E-services accessible to visually disabled
  persons","  The aim of this paper is to determine the expectations that French-speaking
disabled persons have for electronic administrative sites (utility). At the
same time, it is a matter of identifying the difficulties of use that the
manipulation of these E-services poses concretely for blind people (usability)
and of evaluating the psychosocial impacts on the way of life of these people
with specific needs. We show that the lack of numerical accessibility is likely
to accentuate the social exclusion of which these people are victim by
establishing a numerical glass ceiling.
","Marc-Eric Bobiller-Chaumon|Michel Dubois|Francoise Sandoz-Guermond","GRePS|LIP - PC2S|LIESP","http://arxiv.org/abs/0712.2168v1","http://arxiv.org/pdf/0712.2168v1","","4 pages visible \`a http://ceur-ws.org/Vol-285","Dans CEUR Workshop Proceedings - DEGAS'07 : Workshop of Design &
  Evaluation of e-Government Applications and services, Rio de Janeiro :
  Br\'esil (2006)","","cs.HC","cs.HC"
"201","0712.2773v2","2007-12-17 18:42:15","2008-11-05 20:53:51","Middleware-based Database Replication: The Gaps between Theory and
  Practice","  The need for high availability and performance in data management systems has
been fueling a long running interest in database replication from both academia
and industry. However, academic groups often attack replication problems in
isolation, overlooking the need for completeness in their solutions, while
commercial teams take a holistic approach that often misses opportunities for
fundamental innovation. This has created over time a gap between academic
research and industrial practice.
  This paper aims to characterize the gap along three axes: performance,
availability, and administration. We build on our own experience developing and
deploying replication systems in commercial and academic settings, as well as
on a large body of prior related work. We sift through representative examples
from the last decade of open-source, academic, and commercial database
replication systems and combine this material with case studies from real
systems deployed at Fortune 500 customers. We propose two agendas, one for
academic research and one for industrial R&D, which we believe can bridge the
gap within 5-10 years. This way, we hope to both motivate and help researchers
in making the theory and practice of middleware-based database replication more
relevant to each other.
","Emmanuel Cecchet|George Candea|Anastasia Ailamaki","","http://arxiv.org/abs/0712.2773v2","http://arxiv.org/pdf/0712.2773v2","","14 pages. Appears in Proc. ACM SIGMOD International Conference on
  Management of Data, Vancouver, Canada, June 2008","","","cs.DB","cs.DB|cs.DC|cs.PF|H.2; C.2.4; C.4; D.4.5"
"202","0802.3582v1","2008-02-25 09:57:31","2008-02-25 09:57:31","Neural Networks and Database Systems","  Object-oriented database systems proved very valuable at handling and
administrating complex objects. In the following guidelines for embedding
neural networks into such systems are presented. It is our goal to treat
networks as normal data in the database system. From the logical point of view,
a neural network is a complex data value and can be stored as a normal data
object. It is generally accepted that rule-based reasoning will play an
important role in future database applications. The knowledge base consists of
facts and rules, which are both stored and handled by the underlying database
system. Neural networks can be seen as representation of intensional knowledge
of intelligent database systems. So they are part of a rule based knowledge
pool and can be used like conventional rules. The user has a unified view about
his knowledge base regardless of the origin of the unique rules.
","Erich Schikuta","","http://arxiv.org/abs/0802.3582v1","http://arxiv.org/pdf/0802.3582v1","","19 pages, Festschrift Informationssysteme, in honor of G. Vinek","pp. 133-152, 2007, publisher Austrian Computer Society","","cs.DB","cs.DB|cs.NE|H.2.1"
"203","0802.4191v1","2008-02-28 12:36:41","2008-02-28 12:36:41","HyperSmooth : calcul et visualisation de cartes de potentiel
  interactives","  The HyperCarte research group wishes to offer a new cartographic tool for
spatial analysis of social data, using the potential smoothing method. The
purpose of this method is to view the spreading of phenomena's in a continuous
way, at a macroscopic scale, basing on data sampled on administrative areas. We
aim to offer an interactive tool, accessible via the Web, but guarantying the
confidentiality of data. The major difficulty is induced by the high complexity
of the calculus, working on a great amount of data. We present our solution to
such a technical challenge, and our perspectives of enhancements.
","Christine Plumejeaud|Jean-Marc Vincent|Claude Grasland|Jerome Gensel|Helene Mathian|Serge Guelton|Joel Boulier","INRIA Rhone-Alpes / LIG Laboratoire d'Informatique de Grenoble|INRIA Rhone-Alpes / LIG laboratoire d'Informatique de Grenoble|GC, RIATE|LSR - IMAG|GC|INRIA Rhone-Alpes / LIG laboratoire d'Informatique de Grenoble|GC","http://arxiv.org/abs/0802.4191v1","http://arxiv.org/pdf/0802.4191v1","","","Dans SAGEO 2007, Rencontres internationales G\'eomatique et
  territoire. CdRom. - SAGEO 2007, Rencontres internationales G\'eomatique et
  territoire, France (2007)","","stat.AP","stat.AP|cs.HC"
"204","0803.0011v1","2008-02-29 21:39:29","2008-02-29 21:39:29","Qtier-Rapor: Managing Spreadsheet Systems & Improving Corporate
  Performance, Compliance and Governance","  Much of what EuSpRIG discusses is concerned with the integrity of individual
spreadsheets. In businesses, interlocking spreadsheets are regularly used to
fill functional gaps in core administrative systems. The growth and deployment
of such integrated spreadsheet SYSTEMS raises the scale of issues to a whole
new level. The correct management of spreadsheet systems is necessary to ensure
that the business achieves its goals of improved performance and good corporate
governance, within the constraints of legislative compliance - poor management
will deliver the opposite. This paper is an anatomy of the real-life issues of
the commercial use of spreadsheets in business, and demonstrates how
Qtier-Rapor has been used to instil best practice in the use of integrated
commercial spreadsheet systems.
","Keith Bishop","","http://arxiv.org/abs/0803.0011v1","http://arxiv.org/pdf/0803.0011v1","","12 Pages, 6 Colour Figures","Proc. European Spreadsheet Risks Int. Grp. (EuSpRIG) 2006 33-44
  ISBN:1-905617-08-9","","cs.OH","cs.OH|J.1; H.4.1; K.6.4; D.2.5; D.2.9; K.8.1"
"205","0803.2973v2","2008-03-20 11:59:27","2008-05-16 10:42:09","Rule Generalisation in Intrusion Detection Systems using Snort","  Intrusion Detection Systems (ids)provide an important layer of security for
computer systems and networks, and are becoming more and more necessary as
reliance on Internet services increases and systems with sensitive data are
more commonly open to Internet access. An ids responsibility is to detect
suspicious or unacceptable system and network activity and to alert a systems
administrator to this activity. The majority of ids use a set of signatures
that define what suspicious traffic is, and Snort is one popular and actively
developing open-source ids that uses such a set of signatures known as Snort
rules. Our aim is to identify a way in which Snort could be developed further
by generalising rules to identify novel attacks. In particular, we attempted to
relax and vary the conditions and parameters of current Snort rules, using a
similar approach to classic rule learning operators such as generalisation and
specialisation. We demonstrate the effectiveness of our approach through
experiments with standard datasets and show that we are able to detect
previously undeleted variants of various attacks. We conclude by discussing the
general effectiveness and appropriateness of generalisation in Snort based ids
rule processing.
","Uwe Aickelin|Jamie Twycross|Thomas Hesketh-Roberts","","http://arxiv.org/abs/0803.2973v2","http://arxiv.org/pdf/0803.2973v2","http://dx.doi.org/10.1504/IJESDF.2007.013596,","","International Journal of Electronic Security and Digital
  Forensics, 1 (1), pp 101-116, 2007","10.1504/IJESDF.2007.013596,","cs.NE","cs.NE|cs.CR"
"206","0804.0074v1","2008-04-01 06:01:53","2008-04-01 06:01:53","Private Handshakes","  Private handshaking allows pairs of users to determine which (secret) groups
they are both a member of. Group membership is kept secret to everybody else.
Private handshaking is a more private form of secret handshaking, because it
does not allow the group administrator to trace users. We extend the original
definition of a handshaking protocol to allow and test for membership of
multiple groups simultaneously. We present simple and efficient protocols for
both the single group and multiple group membership case.
  Private handshaking is a useful tool for mutual authentication, demanded by
many pervasive applications (including RFID) for privacy. Our implementations
are efficient enough to support such usually resource constrained scenarios.
","Jaap-Henk Hoepman","","http://arxiv.org/abs/0804.0074v1","http://arxiv.org/pdf/0804.0074v1","","","n F. Stajano, editor, 4th Eur. Symp. on Security and Privacy in Ad
  hoc and Sensor Networks, LNCS 4572, pages 31-42, Cambridge, UK, June 2-3 2007","","cs.CR","cs.CR"
"207","0804.3244v1","2008-04-21 07:58:24","2008-04-21 07:58:24","A Conversation with Monroe Sirken","  Born January 11, 1921 in New York City, Monroe Sirken grew up in a suburb of
Pasadena, California. He earned B.A. and M.A. degrees in sociology at UCLA in
1946 and 1947, and a Ph.D. in 1950 in sociology with a minor in mathematics at
the University of Washington in 1950 where Professor Z. W. Birnbaum was his
mentor and thesis advisor. As a Post-Doctoral Fellow of the Social Science
Research Council, Monroe spent 1950--1951 at the Statistics Laboratory,
University of California at Berkeley and the Office of the Assistant Director
for Research, U.S. Bureau of the Census in Suitland, Maryland. Monroe visited
the Census Bureau at a time of great change in the use of sampling and survey
methods, and decided to remain. He began his government career there in 1951 as
a mathematical statistician, and moved to the National Office of Vital
Statistics (NOVS) in 1953 where he was an actuarial mathematician and a
mathematical statistician. He has held a variety of research and administrative
positions at the National Center for Health Statistics (NCHS) and he was the
Associate Director, Research and Methodology and the Director, Office of
Research and Methodology until 1996 when he became a senior research scientist,
the title he currently holds. Aside from administrative responsibilities,
Monroe's major professional interests have been conducting and fostering survey
and statistical research responsive to the needs of federal statistics. His
interest in the design of rare and sensitive population surveys led to the
development of network sampling which improves precision by linking multiple
selection units to the same observation units. His interest in fostering
research on the cognitive aspects of survey methods led to the establishment of
permanent questionnaire design research laboratories, first at NCHS and later
at other federal statistical agencies here and abroad.
","Barry I. Graubard|Paul S. Levy|Gordon B. Willis","","http://arxiv.org/abs/0804.3244v1","http://arxiv.org/pdf/0804.3244v1","http://dx.doi.org/10.1214/07-STS245","Published in at http://dx.doi.org/10.1214/07-STS245 the Statistical
  Science (http://www.imstat.org/sts/) by the Institute of Mathematical
  Statistics (http://www.imstat.org)","Statistical Science 2007, Vol. 22, No. 4, 637-650","10.1214/07-STS245","stat.ME","stat.ME"
"208","0805.0849v1","2008-05-07 07:13:32","2008-05-07 07:13:32","SANA - Network Protection through artificial Immunity","  Current network protection systems use a collection of intelligent components
- e.g. classifiers or rule-based firewall systems to detect intrusions and
anomalies and to secure a network against viruses, worms, or trojans. However,
these network systems rely on individuality and support an architecture with
less collaborative work of the protection components. They give less
administration support for maintenance, but offer a large number of individual
single points of failures - an ideal situation for network attacks to succeed.
In this work, we discuss the required features, the performance, and the
problems of a distributed protection system called SANA. It consists of a
cooperative architecture, it is motivated by the human immune system, where the
components correspond to artificial immune cells that are connected for their
collaborative work. SANA promises a better protection against intruders than
common known protection systems through an adaptive self-management while
keeping the resources efficiently by an intelligent reduction of redundant
tasks. We introduce a library of several novel and common used protection
components and evaluate the performance of SANA by a proof-of-concept
implementation.
","Michael Hilker|Christoph Schommer","","http://arxiv.org/abs/0805.0849v1","http://arxiv.org/pdf/0805.0849v1","","5 pages","Proceedings of the 2nd International Workshop on Theory of
  Computer Viruses (TCV 2007), May 2007, Nancy, France","","cs.CR","cs.CR|cs.MA|C.2.0"
"209","0805.1787v1","2008-05-13 06:51:35","2008-05-13 06:51:35","A Network Protection Framework through Artificial Immunity","  Current network protection systems use a collection of intelligent components
- e.g. classifiers or rule-based firewall systems to detect intrusions and
anomalies and to secure a network against viruses, worms, or trojans. However,
these network systems rely on individuality and support an architecture with
less collaborative work of the protection components. They give less
administration support for maintenance, but offer a large number of individual
single points of failures - an ideal situation for network attacks to succeed.
In this work, we discuss the required features, the performance, and the
problems of a distributed protection system called {\it SANA}. It consists of a
cooperative architecture, it is motivated by the human immune system, where the
components correspond to artificial immune cells that are connected for their
collaborative work. SANA promises a better protection against intruders than
common known protection systems through an adaptive self-management while
keeping the resources efficiently by an intelligent reduction of redundancies.
We introduce a library of several novel and common used protection components
and evaluate the performance of SANA by a proof-of-concept implementation.
","Michael Hilker|Christoph Schommer","","http://arxiv.org/abs/0805.1787v1","http://arxiv.org/pdf/0805.1787v1","","24 pages, 2 figures","","","cs.MA","cs.MA|cs.CR|C.2.0"
"210","0806.0020v2","2008-05-30 20:54:03","2008-06-12 20:05:42","Algebraic Graph Theory (a short course for postgraduate students and
  researchers)","  This submission has been withdrawn by arXiv administration.
","A. E. Brouwer|W. H. Haemers","","http://arxiv.org/abs/0806.0020v2","http://arxiv.org/pdf/0806.0020v2","","This submission has been withdrawn by arXiv administration as it has
  been copied from A.E. Brouwer and W.H. Haemers; available online at
  http://www.win.tue.nl/~aeb/preprints/ipm.tex.gz","","","math.GM","math.GM"
"211","0806.0986v2","2008-06-05 17:09:46","2008-07-01 15:28:32","Remarks On A Nicolas Inequality","  This submission has been withdrawn by arXiv admins because it contains
inappropriate, fictitious text in the acknowledgments and the final reference.
","B. A. Kupershmidt","","http://arxiv.org/abs/0806.0986v2","http://arxiv.org/pdf/0806.0986v2","","This submission has been withdrawn by arXiv administration","","","math-ph","math-ph|math.MP"
"212","0806.1170v3","2008-06-06 14:35:03","2008-07-22 16:15:39","The 2006-2008 Oil Bubble and Beyond","  We present an analysis of oil prices in US$ and in other major currencies
that diagnoses unsustainable faster-than-exponential behavior. This supports
the hypothesis that the recent oil price run-up has been amplified by
speculative behavior of the type found during a bubble-like expansion. We also
attempt to unravel the information hidden in the oil supply-demand data
reported by two leading agencies, the US Energy Information Administration
(EIA) and the International Energy Agency (IEA). We suggest that the found
increasing discrepancy between the EIA and IEA figures provides a measure of
the estimation errors. Rather than a clear transition to a supply restricted
regime, we interpret the discrepancy between the IEA and EIA as a signature of
uncertainty, and there is no better fuel than uncertainty to promote
speculation!
","D. Sornette|R. Woodard|W. -X. Zhou","ETH Zurich|ETH Zurich|ECUST, China","http://arxiv.org/abs/0806.1170v3","http://arxiv.org/pdf/0806.1170v3","http://dx.doi.org/10.1016/j.physa.2009.01.011","4 pages; 4 figures, discussion of the oil supply-demand view point
  and uncertainties","Physica A 388, 1571-1576 (2009)","10.1016/j.physa.2009.01.011","q-fin.GN","q-fin.GN|physics.data-an|physics.soc-ph"
"213","0806.1610v1","2008-06-10 09:34:30","2008-06-10 09:34:30","SPAM over Internet Telephony and how to deal with it","  In our modern society telephony has developed to an omnipresent service.
People are available at anytime and anywhere. Furthermore the Internet has
emerged to an important communication medium. These facts and the raising
availability of broadband internet access has led to the fusion of these two
services. Voice over IP or short VoIP is the keyword, that describes this
combination. The advantages of VoIP in comparison to classic telephony are
location independence, simplification of transport networks, ability to
establish multimedia communications and the low costs. Nevertheless one can
easily see, that combining two technologies, always brings up new challenges
and problems that have to be solved. It is undeniable that one of the most
annoying facet of the Internet nowadays is email spam. According to different
sources email spam is considered to be 80 to 90 percent of the email traffic
produced. The threat of so called voice spam or Spam over Internet Telephony
(SPIT) is even more fatal, for the annoyance and disturbance factor is much
higher. As instance an email that hits the inbox at 4 p.m. is useless but will
not disturb the user much. In contrast a ringing phone at 4 p.m. will lead to a
much higher disturbance. From the providers point of view both email spam and
voice spam produce unwanted traffic and loss of trust of customers into the
service. In order to mitigate this threat different approaches from different
parties have been developed. This paper focuses on state of the art anti voice
spam solutions, analyses them and reveals their weak points. In the end a SPIT
producing benchmark tool will be introduced, that attacks the presented anti
voice spam solutions. With this tool it is possible for an administrator of a
VoIP network to test how vulnerable his system is.
","Andreas U. Schmidt|Nicolai Kuntze|Rachid El Khayari","","http://arxiv.org/abs/0806.1610v1","http://arxiv.org/pdf/0806.1610v1","","7th annual Conference Information Security South Africa (ISSA 2008)
  University of Johannesburg, South Africa, 7 -9 July 2008","","","cs.CR","cs.CR|cs.HC"
"214","0806.4085v2","2008-06-25 12:36:59","2011-05-26 15:56:54","A review of redshift and its interpretation in cosmology and
  astrophysics","  This article has been withdrawn by arXiv administrators due to excessive
unattributed and verbatim text overlap with the pre-existing Wikipedia article
on redshift
","R. Gray|J. Dunning-Davies","","http://arxiv.org/abs/0806.4085v2","http://arxiv.org/pdf/0806.4085v2","","withdrawn by arXiv administrators due to excessive unattributed and
  verbatim text overlap with the pre-existing Wikipedia article on redshift","","","physics.gen-ph","physics.gen-ph"
"215","0807.1943v1","2008-07-12 00:14:45","2008-07-12 00:14:45","Failure of antibiotic treatment in microbial populations","  The tolerance of bacterial populations to biocidal or antibiotic treatment
has been well documented in both biofilm and planktonic settings. However,
there is still very little known about the mechanisms that produce this
tolerance. Evidence that small, non-mutant subpopulations of bacteria are not
affected by antibiotic challenge has been accumulating and provides an
attractive explanation for the failure of typical dosing protocols. Although a
dosing challenge can kill all the susceptible bacteria, the remaining persister
cells can serve as a source of population regrowth. We give a robust condition
for the failure of a periodic dosing protocol for a general chemostat model,
which supports the mathematical conclusions and simulations of an earlier, more
specialized batch model. Our condition implies that the treatment protocol
fails globally, in the sense that a mixed bacterial population will ultimately
persist above a level that is independent of the initial composition of the
population. We also give a sufficient condition for treatment success, at least
for initial population compositions near the steady state of interest,
corresponding to bacterial washout. Finally, we investigate how the speed at
which the bacteria are wiped out depends on the duration of administration of
the antibiotic. We find that this dependence is not necessarily monotone,
implying that optimal dosing does not necessarily correspond to continuous
administration of the antibiotic. Thus, genuine periodic protocols can be more
advantageous in treating a wide variety of bacterial infections.
","Patrick De Leenheer|Nick Cogan","","http://arxiv.org/abs/0807.1943v1","http://arxiv.org/pdf/0807.1943v1","","11 pages, 6 figures","","","q-bio.PE","q-bio.PE|q-bio.OT"
"216","0808.0781v1","2008-08-06 06:35:49","2008-08-06 06:35:49","The Early Statistical Years: 1947--1967 A Conversation with Howard
  Raiffa","  Howard Raiffa earned his bachelor's degree in mathematics, his master's
degree in statistics and his Ph.D. in mathematics at the University of
Michigan. Since 1957, Raiffa has been a member of the faculty at Harvard
University, where he is now the Frank P. Ramsey Chair in Managerial Economics
(Emeritus) in the Graduate School of Business Administration and the Kennedy
School of Government. A pioneer in the creation of the field known as decision
analysis, his research interests span statistical decision theory, game theory,
behavioral decision theory, risk analysis and negotiation analysis. Raiffa has
supervised more than 90 doctoral dissertations and written 11 books. His new
book is Negotiation Analysis: The Science and Art of Collaborative Decision
Making. Another book, Smart Choices, co-authored with his former doctoral
students John Hammond and Ralph Keeney, was the CPR (formerly known as the
Center for Public Resources) Institute for Dispute Resolution Book of the Year
in 1998. Raiffa helped to create the International Institute for Applied
Systems Analysis and he later became its first Director, serving in that
capacity from 1972 to 1975. His many honors and awards include the
Distinguished Contribution Award from the Society of Risk Analysis; the Frank
P. Ramsey Medal for outstanding contributions to the field of decision analysis
from the Operations Research Society of America; and the Melamed Prize from the
University of Chicago Business School for The Art and Science of Negotiation.
He earned a Gold Medal from the International Association for Conflict
Management and a Lifetime Achievement Award from the CPR Institute for Dispute
Resolution. He holds honorary doctor's degrees from Carnegie Mellon University,
the University of Michigan, Northwestern University, Ben Gurion University of
the Negev and Harvard University. The latter was awarded in 2002.
","Stephen E. Fienberg","","http://arxiv.org/abs/0808.0781v1","http://arxiv.org/pdf/0808.0781v1","http://dx.doi.org/10.1214/088342307000000104","Published in at http://dx.doi.org/10.1214/088342307000000104 the
  Statistical Science (http://www.imstat.org/sts/) by the Institute of
  Mathematical Statistics (http://www.imstat.org)","Statistical Science 2008, Vol. 23, No. 1, 136-149","10.1214/088342307000000104","stat.ME","stat.ME"
"217","0808.1684v1","2008-08-12 15:19:56","2008-08-12 15:19:56","Parkinson's Law Quantified: Three Investigations on Bureaucratic
  Inefficiency","  We formulate three famous, descriptive essays of C.N. Parkinson on
bureaucratic inefficiency in a quantifiable and dynamical socio-physical
framework. In the first model we show how the use of recent opinion formation
models for small groups can be used to understand Parkinson's observation that
decision making bodies such as cabinets or boards become highly inefficient
once their size exceeds a critical 'Coefficient of Inefficiency', typically
around 20. A second observation of Parkinson - which is sometimes referred to
as Parkinson's Law - is that the growth of bureaucratic or administrative
bodies usually goes hand in hand with a drastic decrease of its overall
efficiency. In our second model we view a bureaucratic body as a system of a
flow of workers, which enter, become promoted to various internal levels within
the system over time, and leave the system after having served for a certain
time. Promotion usually is associated with an increase of subordinates. Within
the proposed model it becomes possible to work out the phase diagram under
which conditions bureaucratic growth can be confined. In our last model we
assign individual efficiency curves to workers throughout their life in
administration, and compute the optimum time to send them to old age pension,
in order to ensure a maximum of efficiency within the body - in Parkinson's
words we compute the 'Pension Point'.
","Peter Klimek|Rudolf Hanel|Stefan Thurner","","http://arxiv.org/abs/0808.1684v1","http://arxiv.org/pdf/0808.1684v1","http://dx.doi.org/10.1088/1742-5468/2009/03/P03008","15 pages, 5 figures","","10.1088/1742-5468/2009/03/P03008","physics.soc-ph","physics.soc-ph"
"218","0808.2202v2","2008-08-15 21:05:42","2009-09-10 10:02:49","Laws of Population Growth","  An important issue in the study of cities is defining a metropolitan area, as
different definitions affect the statistical distribution of urban activity. A
commonly employed method of defining a metropolitan area is the Metropolitan
Statistical Areas (MSA), based on rules attempting to capture the notion of
city as a functional economic region, and is constructed using experience. The
MSA is time-consuming and is typically constructed only for a subset (few
hundreds) of the most highly populated cities. Here, we introduce a new method
to designate metropolitan areas, denoted the ""City Clustering Algorithm"" (CCA).
The CCA is based on spatial distributions of the population at a fine
geographic scale, defining a city beyond the scope of its administrative
boundaries. We use the CCA to examine Gibrat's law of proportional growth,
postulating that the mean and standard deviation of the growth rate of cities
are constant, independent of city size. We find that the mean growth rate of a
cluster utilizing the CCA exhibits deviations from Gibrat's law, and that the
standard deviation decreases as a power-law with respect to the city size. The
CCA allows for the study of the underlying process leading to these deviations,
shown to arise from the existence of long-range spatial correlations in the
population growth. These results have socio-political implications, such as
those pertaining to the location of new economic development in cities of
varied size.
","Hernan D. Rozenfeld|Diego Rybski|Jose S. Andrade Jr.|Michael Batty|H. Eugene Stanley|Hernan A. Makse","","http://arxiv.org/abs/0808.2202v2","http://arxiv.org/pdf/0808.2202v2","http://dx.doi.org/10.1073/pnas.0807435105","30 pages, 8 figures","PNAS 105, 18702--18707 (2008)","10.1073/pnas.0807435105","physics.soc-ph","physics.soc-ph|cond-mat.stat-mech"
"219","0809.0335v1","2008-09-02 00:55:53","2008-09-02 00:55:53","Synergetic Application, Equations on Rule of Law and Two-Party Mechanism","  Based on the synergetic equations and its application, we propose the
equations on the rule of law. From these equations we may prove mathematically
that a society of the rule of law cannot lack any aspect for three types of the
legislation, the administration and the judicature. Otherwise, we propose an
equation of corruption, and discuss quantitatively some threshold values for a
system into corruption. Moreover, from synergetics we obtain the Lorenz model,
which may be a visualized two-party mechanism as a type of stable structure in
democracy. A developed direction of society should be the combination from
macroscopic to microscopic order, from an actual capable handling to an ideal
pursuance.
","Yi-Fang Chang","","http://arxiv.org/abs/0809.0335v1","http://arxiv.org/pdf/0809.0335v1","","6 pages","","","physics.gen-ph","physics.gen-ph|physics.soc-ph"
"220","0809.0723v1","2008-09-03 23:53:29","2008-09-03 23:53:29","A Simple Mechanism for Focused Web-harvesting","  The focused web-harvesting is deployed to realize an automated and
comprehensive index databases as an alternative way for virtual topical data
integration. The web-harvesting has been implemented and extended by not only
specifying the targeted URLs, but also predefining human-edited harvesting
parameters to improve the speed and accuracy. The harvesting parameter set
comprises three main components. First, the depth-scale of being harvested
final pages containing desired information counted from the first page at the
targeted URLs. Secondly, the focus-point number to determine the exact box
containing relevant information. Lastly, the combination of keywords to
recognize encountered hyperlinks of relevant images or full-texts embedded in
those final pages. All parameters are accessible and fully customizable for
each target by the administrators of participating institutions over an
integrated web interface. A real implementation to the Indonesian Scientific
Index which covers all scientific information across Indonesia is also briefly
introduced.
","Z. Akbar|L. T. Handoko","","http://arxiv.org/abs/0809.0723v1","http://arxiv.org/pdf/0809.0723v1","","6 pages, 4 figures, Proceeding of the International Conference on
  Advanced Computational Intelligence and Its Applications 2008","","","cs.IR","cs.IR|cs.CY"
"221","0809.1965v1","2008-09-11 12:01:34","2008-09-11 12:01:34","Dynamic index selection in data warehouses","  Analytical queries defined on data warehouses are complex and use several
join operations that are very costly, especially when run on very large data
volumes. To improve response times, data warehouse administrators casually use
indexing techniques. This task is nevertheless complex and fastidious. In this
paper, we present an automatic, dynamic index selection method for data
warehouses that is based on incremental frequent itemset mining from a given
query workload. The main advantage of this approach is that it helps update the
set of selected indexes when workload evolves instead of recreating it from
scratch. Preliminary experimental results illustrate the efficiency of this
approach, both in terms of performance enhancement and overhead.
","Stephane Azefack|Kamel Aouiche|Jerome Darmont","ERIC|ERIC|ERIC","http://arxiv.org/abs/0809.1965v1","http://arxiv.org/pdf/0809.1965v1","","","4th International Conference on Innovations in Information
  Technology (Innovations 07), Dubai : \'Emirats arabes unis (2006)","","cs.DB","cs.DB"
"222","0809.2306v2","2008-09-13 00:36:34","2008-09-17 18:49:36","The Model Of Paths For Generalized Kac-Moody Algebras","  This submission has been withdrawn by arXiv administrators because of
inappropriate authorship claims.
","Zhixiang Wu","","http://arxiv.org/abs/0809.2306v2","http://arxiv.org/pdf/0809.2306v2","","This submission has been withdrawn by arXiv administrators because of
  inappropriate authorship claims","","","math.QA","math.QA|math.RT|17B67, 17B10"
"223","0809.2322v1","2008-09-13 08:56:25","2008-09-13 08:56:25","An Energy-Aware On-Demand Routing Protocol for Ad-Hoc Wireless Networks","  An ad-hoc wireless network is a collection of nodes that come together to
dynamically create a network, with no fixed infrastructure or centralized
administration. An ad-hoc network is characterized by energy constrained nodes,
bandwidth constrained links and dynamic topology. With the growing use of
wireless networks (including ad-hoc networks) for real-time applications, such
as voice, video, and real-time data, the need for Quality of Service (QoS)
guarantees in terms of delay, bandwidth, and packet loss is becoming
increasingly important. Providing QoS in ad-hoc networks is a challenging task
because of dynamic nature of network topology and imprecise state information.
Hence, it is important to have a dynamic routing protocol with fast re-routing
capability, which also provides stable route during the life-time of the flows.
  In this thesis, we have proposed a novel, energy aware, stable routing
protocol named, Stability-based QoS-capable Ad-hoc On-demand Distance Vector
(SQ-AODV), which is an enhancement of the well-known Ad-hoc On-demand Distance
Vector (AODV) routing protocol for ad-hoc wireless networks. SQ-AODV utilizes a
cross-layer design approach in which information about the residual energy of a
node is used for route selection and maintenance. An important feature of
SQ-AODV protocol is that it uses only local information and requires no
additional communication or co-operation between the network nodes. SQ-AODV
possesses a make-before-break re-routing capability that enables near-zero
packet drops and is compatible with the basic AODV data formats and operation,
making it easy to adopt in ad-hoc networks.
","Mallapur Veerayya","","http://arxiv.org/abs/0809.2322v1","http://arxiv.org/pdf/0809.2322v1","","65 pages, Master's Thesis, Department of Elecrical Engineering, IIT
  Bombay","","","cs.NI","cs.NI"
"224","0809.2687v1","2008-09-16 11:44:39","2008-09-16 11:44:39","Frequent itemsets mining for database auto-administration","  With the wide development of databases in general and data warehouses in
particular, it is important to reduce the tasks that a database administrator
must perform manually. The aim of auto-administrative systems is to
administrate and adapt themselves automatically without loss (or even with a
gain) in performance. The idea of using data mining techniques to extract
useful knowledge for administration from the data themselves has existed for
some years. However, little research has been achieved. This idea nevertheless
remains a very promising approach, notably in the field of data warehousing,
where queries are very heterogeneous and cannot be interpreted easily. The aim
of this study is to search for a way of extracting useful knowledge from stored
data themselves to automatically apply performance optimization techniques, and
more particularly indexing techniques. We have designed a tool that extracts
frequent itemsets from a given workload to compute an index configuration that
helps optimizing data access time. The experiments we performed showed that the
index configurations generated by our tool allowed performance gains of 15% to
25% on a test database and a test data warehouse.
","Kamel Aouiche|Jerome Darmont|Le Gruenwald","ERIC|ERIC|","http://arxiv.org/abs/0809.2687v1","http://arxiv.org/pdf/0809.2687v1","","in 7th International Database Engineering and Application Symposium
  (IDEAS 03), Hong-Kong : Chine (2003)","","","cs.DB","cs.DB"
"225","0809.3762v4","2008-09-22 18:39:48","2012-10-08 19:16:56","Climate Science: Is it currently designed to answer questions?","  For a variety of inter-related cultural, organizational, and political
reasons, progress in climate science and the actual solution of scientific
problems in this field have moved at a much slower rate than would normally be
possible. Not all these factors are unique to climate science, but the heavy
influence of politics has served to amplify the role of the other factors. Such
factors as the change in the scientific paradigm from a dialectic opposition
between theory and observation to an emphasis on simulation and observational
programs, the inordinate growth of administration in universities and the
consequent increase in importance of grant overhead, and the hierarchical
nature of formal scientific organizations are cosidered. This paper will deal
with the origin of the cultural changes and with specific examples of the
operation and interaction of these factors. In particular, we will show how
political bodies act to control scientific institutions, how scientists adjust
both data and even theory to accommodate politically correct positions, and how
opposition to these positions is disposed of.
","Richard S. Lindzen","","http://arxiv.org/abs/0809.3762v4","http://arxiv.org/pdf/0809.3762v4","","36 pages, no figures. v2: footnotes 16, 19, 20 added, footnote 17
  changed, typos corrected. v3: description of John Holdren corrected, expanded
  discussion of I=PAT formula, typos corrected. v4: The reference to Deming
  (2005) added in v3 stated that a 1995 email in question was from Jonathan
  Overpeck. In fact, Deming had left the sender of the email unnamed. The
  revision v4 now omits the identification of Overpeck. However, the revision
  v4 now includes a more recent and verifiable reference to a 2005 email","","","physics.soc-ph","physics.soc-ph|physics.hist-ph"
"226","0810.4310v2","2008-10-23 15:56:52","2008-11-10 15:39:16","Unitarity of SL(2)-conformal blocks in genus zero","  This submission has been withdrawn by arXiv administrators due to an
unresolved conflict between the authors. This article was submitted without
consent of E. Looijenga.
","E. Looijenga|A. Varchenko","","http://arxiv.org/abs/0810.4310v2","http://arxiv.org/pdf/0810.4310v2","http://dx.doi.org/10.1016/j.geomphys.2009.02.003","This submission has been withdrawn by arXiv administrators due to an
  unresolved conflict between the authors. This article was submitted without
  consent of E. Looijenga","","10.1016/j.geomphys.2009.02.003","math.QA","math.QA|math.AG"
"227","0810.5351v1","2008-10-29 20:22:59","2008-10-29 20:22:59","An Activity-Based Model for Separation of Duty","  This paper offers several contributions for separation of duty (SoD)
administration in role-based access control (RBAC) systems. We first introduce
a new formal framework, based on business perspective, where SoD constraints
are analyzed introducing the activity concept. This notion helps organizations
define SoD constraints in terms of business requirements and reduces management
complexity in large-scale RBAC systems. The model enables the definition of a
wide taxonomy of conflict types. In particular, object-based SoD is introduced
using the SoD domain concept, namely the set of data in which transaction
conflicts may occur. Together with the formalization of the above properties,
in this paper we also show the effectiveness of our proposal: we have applied
the model to a large, existing organization; results highlight the benefits of
adopting the proposed model in terms of reduced administration cost.
","Alessandro Colantonio|Roberto Di Pietro|Alberto Ocello","","http://arxiv.org/abs/0810.5351v1","http://arxiv.org/pdf/0810.5351v1","","","","","cs.CR","cs.CR|D.4.6; K.6.5"
"228","0812.1218v1","2008-12-05 20:49:47","2008-12-05 20:49:47","The Impact of New Technologies in Public Financial Management and
  Performance: Agenda for Public Financial Management Reformance in the Context
  of Global Best Practices","  Information and Communication Technologies (ICT) has practically penetrated
into all spheres of life. Therefore a closer look at the impact of ICT in
public financial management and performance is highly justified. Public finance
is defined as a field of economics concerned with paying for collective or
governmental activities, and with the administration and design of those
activities. Activities will be viewed as services or more precisely as public
services. We believe that there is need to consider performance from the
perspective of effective performance and the perceived performance. In fact the
real or effective performance might not correspond to the perceived
performance. A service can be considered from the perspective of the
decision-maker, who in our case could be a government or a collectivity. ICT
can be employed in the three phases that concern the decision-maker: design,
implementation and evaluation. The beneficiaries of a service can employ ICT in
any of the three phases - awareness, exploitation and assessment - for
guarantying a high level of efficiency. Each phase in the environment of a
service will be presented as well as illustrations of how ICT can be employed
in order to improve the end-result of each one of them. We believe that a high
efficiency of each phase will produce a high global efficiency. It should be
noted however that the effectiveness of any system is highly dependent on the
human engagement in the system. Therefore, the impact of ICT in public
financial management will be felt only if the decision-makers and the end-users
of the services engage themselves in the success of the system. Instead of
giving a catalog of services, the focus has been on the model (or methodology)
to adopt in designing services for which ICT could enhance the implementation.
","Amos David","LORIA","http://arxiv.org/abs/0812.1218v1","http://arxiv.org/pdf/0812.1218v1","","","6th Nigeria Development Forum Retreat (2008)","","cs.OH","cs.OH"
"229","0812.4983v1","2008-12-30 01:34:29","2008-12-30 01:34:29","Bootstrapping Key Pre-Distribution: Secure, Scalable and User-Friendly
  Initialization of Sensor Nodes","  To establish secure (point-to-point and/or broadcast) communication channels
among the nodes of a wireless sensor network is a fundamental task. To this
end, a plethora of (socalled) key pre-distribution schemes have been proposed
in the past. All these schemes, however, rely on shared secret(s), which are
assumed to be somehow pre-loaded onto the sensor nodes. In this paper, we
propose a novel method for secure initialization of sensor nodes based on a
visual out-of-band channel. Using the proposed method, the administrator of a
sensor network can distribute keys onto the sensor nodes, necessary to
bootstrap key pre-distribution. Our secure initialization method requires only
a little extra cost, is efficient and scalable with respect to the number of
sensor nodes. Moreover, based on a usability study that we conducted, the
method turns out to be quite user-friendly and easy to use by naive human
users.
","Nitesh Saxena|Md. Borhan Uddin","","http://arxiv.org/abs/0812.4983v1","http://arxiv.org/pdf/0812.4983v1","","10 Pages","","","cs.CR","cs.CR"
"230","0901.3758v1","2009-01-23 19:36:38","2009-01-23 19:36:38","Carbon Nanotubes in Biology and Medicine: in vitro and in vivo
  Detection, Imaging and Drug Delivery","  Carbon nanotubes exhibit many unique intrinsic physical and chemical
properties and have been intensively explored for biological and biomedical
applications. In this review, we summarize the main results of our and other
groups in this field and clarify that surface functionalization is critical to
the behaviors of carbon nanotubes in biological systems. Ultra-sensitive
detection of biological species with carbon nanotubes can be realized after
surface passivation to inhibit the non-specific binding of bio-molecules on the
hydrophobic nanotube surface. Electrical nanosensors based on nanotubes provide
a label-free approach to biological detections. Surface enhanced Raman
spectroscopy of CNT opens up a method of protein microarray with down to 1 fM
detection sensitivity. In vitro and in vivo toxicity studies reveal that well
water soluble and serum stable nanotubes are biocompatible, non-toxic and
potentially useful for biomedical applications. In vivo biodistributions vary
with the functionalization and possibly also sizes of nanotubes, with a
tendency of accumulation in the reticuloendothelial systems, including the
liver and spleen, after intravenous administration. If well functionalized,
nanotubes may be excreted mainly through the biliary pathway in feces. Carbon
nanotube-based drug delivery has shown promises in various in vitro and in vivo
experiments including delivery pf small interfering RNA, paclitaxel and
doxorubicin. Moreover, SWNTs with various interesting intrinsic optical
properties have been used as novel photoluminance, Raman and photoacoustic
contrast agents for imaging of cells and animals. Further multidisciplinary
explorations in this field are promising and may bring new opportunities to the
realm of biomedicine.
","Zhuang Liu|Scott Tabakman|Kevin Welsher|Hongjie Dai","","http://arxiv.org/abs/0901.3758v1","http://arxiv.org/pdf/0901.3758v1","","To appear in Nano Research,
  http://www.springerlink.com/content/m6t5871n55841736/?p=53184a51bcfd44239b9091ee7e1ef676","","","cond-mat.mtrl-sci","cond-mat.mtrl-sci"
"231","0902.0048v1","2009-01-31 05:59:03","2009-01-31 05:59:03","Phospholipid-Dextran with a Single Coupling Point: a Useful Amphiphile
  for Functionalization of Nanomaterials","  Nanomaterials hold much promise for biological applications, but they require
appropriate functionalization to provide biocompatibility in biological
environments. For non-covalent functionalization with biocompatible polymers,
the polymer must also remain attached to the nanomaterial after removal of its
excess to mimic the high dilution conditions of administration in vivo.
Reported here are the synthesis and utilization singly-substituted conjugates
of dextran and a phospholipid (Dextran-DSPE) as stable coatings for
nanomaterials. Suspensions of single walled carbon nanotubes were found not
only to be stable to phosphate buffered saline (PBS), serum, and a variety of
pHs after excess polymer removal, but also provide brighter photoluminescence
than carbon nanotubes suspended by poly(ethylene glycol)-DSPE. In addition,
both gold nanoparticles (AuNPs) and gold nanorods (AuNRs) were found to
maintain their dispersion and characteristic optical absorbance after transfer
into Dextran-DSPE, and were obtained in much better yield than similar
suspensions with PEG-phospholipid and commonly used thiol-PEG. These
suspensions were also stable to PBS, serum, and a variety of pHs after removal
of excess polymer. Dextran-DSPE thus shows great promise as a general
surfactant material for the functionalization of a variety of nanomaterials,
which could facilitate future biological applications.
","Andrew P. Goodwin|Scott M. Tabakman|Kevin Welsher|Sarah P. Sherlock|Giuseppe Prencipe|Hongjie Dai","","http://arxiv.org/abs/0902.0048v1","http://arxiv.org/pdf/0902.0048v1","http://dx.doi.org/10.1021/ja807307e","","J. Am. Chem. Soc., 2009, 131 -1-, pp 289","10.1021/ja807307e","cond-mat.mtrl-sci","cond-mat.mtrl-sci"
"232","0903.0507v2","2009-03-03 11:57:42","2009-08-24 20:15:21","Using administrative data to improve the estimation of immigration to
  local areas in England","  International migration is now a significant driver of population change
across Europe but the methods available to estimate its true impact upon
sub-national areas remain inconsistent, constrained by inadequate systems of
measurement and data capture. In the absence of a population register for
England, official statistics on immigration and emigration are derived from a
combination of survey and census sources. This paper demonstrates how
administrative data systems such as those which capture registrations of recent
migrants with a local doctor, National Insurance Number registrations by
workers from abroad and the registration of foreign students for higher
education, can provide data to better understand patterns and trends in
international migration. The paper proposes a model for the estimation of
immigration at a local level, integrating existing national estimates from the
Office for National Statistics with data from these administrative sources. The
model attempts to circumvent conceptual differences between datasets through
the use of proportional distributions rather than absolute migrant counts in
the estimation process. The model methodology and the results it produces
provide alternative estimates of immigration for consideration by the Office
for National Statistics as it develops its own programme of improvement to
sub-national migration statistics.
","Peter Boden|Phil Rees","","http://arxiv.org/abs/0903.0507v2","http://arxiv.org/pdf/0903.0507v2","","","","","stat.AP","stat.AP"
"233","0903.2064v1","2009-03-11 21:35:25","2009-03-11 21:35:25","Einstein's unpublished opening lecture for his course on relativity
  theory in Argentina, 1925","  In 1922 the University of Buenos Aires (UBA) Council approved a motion to
send an invitation to Albert Einstein to visit Argentina and give a course of
lectures on his theory of relativity. The motion was proposed by Jorge Duclout
(1856-1927), who had been educated at the Eidgenossische Technische Hochschule,
Zurich (ETH). This proposal was the culmination of a series of initiatives of
various Argentine intellectuals interested in the theory of relativity. In a
very short time Dr. Mauricio Nirenstein (1877-1935), then the university's
administrative secretary, fulfilled all the requirements for the university's
invitation to be endorsed and delivered to the sage in Berlin. The visit took
place three years later, in March-April 1925.
  The Argentine press received Einstein with great interest and respect; his
early exchanges covered a wide range of topics, including international
politics and Jewish matters. Naturally, the journalists were more eager to hear
from the eminent pacifist than from the incomprehensible physicist. However,
after his initial openness with the press, the situation changed and Einstein
restricted his public discourse to topics on theoretical physics, avoiding some
controversial political, religious, or philosophical matters that he had freely
touched upon in earlier interviews.. [abridged].
","Alejandro Gangui|Eduardo L. Ortiz","","http://arxiv.org/abs/0903.2064v1","http://arxiv.org/pdf/0903.2064v1","http://dx.doi.org/10.1017/S0269889708001853","Published version available at
  http://www.universoeinstein.com.ar/einsteinargentina.htm","Sci.Context 21:435-450,2008","10.1017/S0269889708001853","physics.hist-ph","physics.hist-ph|astro-ph.CO|gr-qc|physics.soc-ph"
"234","0903.4261v1","2009-03-25 08:43:19","2009-03-25 08:43:19","On-Line Tests","  This paper presents an interactive implementation which makes the link
between a human operator and a system of a administration of a relational
databases MySQL. This application conceived as a multimedia presentations is
illustrative for the way in which the transfer and the remaking of the
information between the human operator, the module of data processing and the
database which stores the informations can be solved (with help of the PHP
language and the web use).
","Florentina Anica Pintea","","http://arxiv.org/abs/0903.4261v1","http://arxiv.org/pdf/0903.4261v1","","8 pages, exposed on 4th International Conference ""Actualities and
  Perspectives on Hardware and Software"" - APHS2007, Timisoara, Romania","Ann. Univ. Tibiscus Comp. Sci. Series V (2007), 77-84","","cs.HC","cs.HC"
"235","0904.3715v1","2009-04-23 14:56:47","2009-04-23 14:56:47","Mesh","  Whether you just want to take a peek of a remote computer status, or you want
to install the latest version of a software on several workstations, you can do
all of this from your computer. The networks are growing, the time spent
administering the workstations increases and the number of repetitive tasks is
going sky high. But here comes MESH to take that load off your shoulders. And
because of SMS commands you can take this ""command center"" wherever you will
go. Just connect a GSM phone to the computer (using a cable, IrDA or Bluetooth)
and lock/restart/shutdown computers from your LAN with the push of a cell phone
button. You can even create your own SMS commands. This is MESH - the network
administrator's Swiss knife
","Radu Vultur","","http://arxiv.org/abs/0904.3715v1","http://arxiv.org/pdf/0904.3715v1","","6 pages,exposed on 1st ""European Conference on Computer Sciences &
  Applications"" - XA2006, Timisoara, Romania","Ann. Univ. Tibiscus Comp. Sci. Series IV (2006), 261-266","","cs.OH","cs.OH"
"236","0905.1362v1","2009-05-09 01:37:47","2009-05-09 01:37:47","Reliable Process for Security Policy Deployment","  We focus in this paper on the problem of configuring and managing network
security devices, such as Firewalls, Virtual Private Network (VPN) tunnels, and
Intrusion Detection Systems (IDSs). Our proposal is the following. First, we
formally specify the security requirements of a given system by using an
expressive access control model. As a result, we obtain an abstract security
policy, which is free of ambiguities, redundancies or unnecessary details.
Second, we deploy such an abstract policy through a set of automatic
compilations into the security devices of the system. This proposed deployment
process not only simplifies the security administrator's job, but also
guarantees a resulting configuration free of anomalies and/or inconsistencies.
","Stere Preda|Nora Cuppens-Boulahia|Frederic Cuppens|Joaquin Garcia-Alfaro|Laurent Toutain","","http://arxiv.org/abs/0905.1362v1","http://arxiv.org/pdf/0905.1362v1","","12 pages","Proc. 2007 International Conference on Security and Cryptography
  (Secrypt 2007), Barcelona, Spain, July 2007","","cs.CR","cs.CR|cs.SE"
"237","0905.3910v1","2009-05-24 16:00:19","2009-05-24 16:00:19","Special functions and pathways for problems in astrophysics: An essay in
  honor of A.M. Mathai","  The paper provides a review of A.M. Mathai's applications of the theory of
special functions, particularly generalized hypergeometric functions, to
problems in stellar physics and formation of structure in the Universe and to
questions related to reaction, diffusion, and reaction-diffusion models. The
essay also highlights Mathai's recent work on entropic, distributional, and
differential pathways to basic concepts in statistical mechanics, making use of
his earlier research results in information and statistical distribution
theory. The results presented in the essay cover a period of time in Mathai's
research from 1982 to 2008 and are all related to the thematic area of the
gravitationally stabilized solar fusion reactor and fractional
reaction-diffusion, taking into account concepts of non-extensive statistical
mechanics. The time period referred to above coincides also with Mathai's
exceptional contributions to the establishment and operation of the Centre for
Mathematical Sciences, India, as well as the holding of the United Nations
(UN)/European Space Agency (ESA)/National Aeronautics and Space Administration
(NASA) of the United States/ Japanese Aerospace Exploration Agency (JAXA)
Workshops on basic space science and the International Heliophysical Year 2007,
around the world. Professor Mathai's contributions to the latter, since 1991,
are a testimony for his social conscience applied to international scientific
activity.
","H. J. Haubold","","http://arxiv.org/abs/0905.3910v1","http://arxiv.org/pdf/0905.3910v1","","21 pages, LaTeX","Proceedings of the 8th Annual Conf. SSFA, Vol. 8, 2007(2009), pp.
  3-23","","astro-ph.SR","astro-ph.SR|astro-ph.CO|math.CA"
"238","0906.0065v2","2009-05-30 06:42:55","2009-07-26 23:00:45","Managing Distributed MARF with SNMP","  The scope of this project's work focuses on the research and prototyping of
the extension of the Distributed MARF such that its services can be managed
through the most popular management protocol familiarly, SNMP. The rationale
behind SNMP vs. MARF's proprietary management protocols, is that can be
integrated with the use of common network service and device management, so the
administrators can manage MARF nodes via a already familiar protocol, as well
as monitor their performance, gather statistics, set desired configuration,
etc. perhaps using the same management tools they've been using for other
network devices and application servers.
","Serguei A. Mokhov|Lee Wei Huynh|Jian Li","Concordia University, Montreal, Canada|Concordia University, Montreal, Canada|Concordia University, Montreal, Canada","http://arxiv.org/abs/0906.0065v2","http://arxiv.org/pdf/0906.0065v2","","39 pages, 16 figures, TOC, index. A large portion of this report has
  been published at PDPTA'08. This 2007 report is a successor of the original
  DMARF work documented at arXiv:0905.2459 ; v2 adds missing .ind file for the
  index","Proceedings of PDPTA'08 (2008), Volume 2, pp. 948-954","","cs.DC","cs.DC|cs.CV|C.2.4; I.5; I.2.6; D.2.10; D.2.11; D.2.5; D.2.2; I.2.7"
"239","0906.3843v1","2009-06-21 04:06:42","2009-06-21 04:06:42","Threshold Verification Technique for Network Intrusion Detection System","  Internet has played a vital role in this modern world, the possibilities and
opportunities offered are limitless. Despite all the hype, Internet services
are liable to intrusion attack that could tamper the confidentiality and
integrity of important information. An attack started with gathering the
information of the attack target, this gathering of information activity can be
done as either fast or slow attack. The defensive measure network administrator
can take to overcome this liability is by introducing Intrusion Detection
Systems (IDSs) in their network. IDS have the capabilities to analyze the
network traffic and recognize incoming and on-going intrusion. Unfortunately
the combination of both modules in real time network traffic slowed down the
detection process. In real time network, early detection of fast attack can
prevent any further attack and reduce the unauthorized access on the targeted
machine. The suitable set of feature selection and the correct threshold value,
add an extra advantage for IDS to detect anomalies in the network. Therefore
this paper discusses a new technique for selecting static threshold value from
a minimum standard features in detecting fast attack from the victim
perspective. In order to increase the confidence of the threshold value the
result is verified using Statistical Process Control (SPC). The implementation
of this approach shows that the threshold selected is suitable for identifying
the fast attack in real time.
","M. A. Faizal|M. Mohd Zaki|S. Shahrin|Y. Robiah|S. Siti Rahayu|B. Nazrulazhar","","http://arxiv.org/abs/0906.3843v1","http://arxiv.org/pdf/0906.3843v1","","8 Pages, International Journal of Computer Science and Information
  Security","IJCSIS Vol.2, No.1, June 2009","","cs.CR","cs.CR|cs.NI"
"240","0906.5028v1","2009-06-27 02:31:20","2009-06-27 02:31:20","Isoprenaline increases Excursive Restitution Slope in the Conscious
  Rabbit with Ischaemic Heart Failure","  Background: An increased QT/RR slope is hypothesized to be predictive of
sudden cardiac death after myocardial infarction. Previous studies have shown
that beta-adrenergic stimulation increases QT/RR slope, but the effects of
beta-adrenergic stimulation on QT/RR slope in heart failure are unknown.
Methods: New Zealand White rabbits underwent coronary ligation (n=15) or sham
surgery (n=11), and implantation of a pediatric pacemaker lead in the right
ventricle for chronic ECG recording. Eight weeks after surgery, unsedated
rabbits were given intravenous administrations of 0.25 to 2.0 ml of 1
micromol/l isoprenaline, while peak QRS to QRS (RR) and Q to T peak (QT)
intervals were measured. Results: Ligated rabbits (n=6) had lower LVEF than
sham rabbits (n=7, p<.0001), but similar baseline RR (269 +/- 15 vs 292 +/- 23
ms, p=.07), QT (104 +/- 17 vs 91 +/- 9 ms, p=.1) and minimum RR (204 +/- 11 vs
208 +/- 6 ms, p=.4) intervals induced by isoprenaline (0.79 +/- 0.18 vs 0.73
+/- 0.14 ml, p=.6). Hysteresis in QT vs TQ interval plots displayed biphasic
restitution and regions of negative slope. The slope of the positive slope
region was >1 in ligated rabbits (1.27 +/- 0.66) and <1 in sham rabbits (0.35
+/- 0.14, p=.004). Absolute value of the negative slope was greater in ligated
rabbits (-0.81 +/- 0.52 vs -0.35 +/- 0.14, p=.04). Conclusion: Ischaemic heart
failure produces steeper restitution slopes during beta-adrenergically induced
QT/TQ hysteresis. This could underlie the propensity of failing hearts to
arrhythmias.
","Tomofumi Kimotsuki|Noriko Niwa|Martin N. Hicks|Michael Dunne|Stuart M. Cobbe|Mari A. Watanabe","Saint Louis University|Washington Univ. St. Louis|Glasgow University|Glasgow University|Glasgow University|Saint Louis University","http://arxiv.org/abs/0906.5028v1","http://arxiv.org/pdf/0906.5028v1","","27 pages","","","q-bio.TO","q-bio.TO"
"241","0906.5060v1","2009-06-27 10:10:18","2009-06-27 10:10:18","Incidence Handling and Response System","  A computer network can be attacked in a number of ways. The security-related
threats have become not only numerous but also diverse and they may also come
in the form of blended attacks. It becomes difficult for any security system to
block all types of attacks. This gives rise to the need of an incidence
handling capability which is necessary for rapidly detecting incidents,
minimizing loss and destruction, mitigating the weaknesses that were exploited
and restoring the computing services. Incidence response has always been an
important aspect of information security but it is often overlooked by security
administrators. in this paper, we propose an automated system which will handle
the security threats and make the computer network capable enough to withstand
any kind of attack. we also present the state-of-the-art technology in
computer, network and software which is required to build such a system.
","Prof. Dhananjay R. Kalbande|Dr. G. T. Thampi|Mr. Manish Singh","","http://arxiv.org/abs/0906.5060v1","http://arxiv.org/pdf/0906.5060v1","","8 Pages, International Journal of Computer Science and Information
  Security (IJCSIS)","IJCSIS June 2009 Issue, Vol. 2, No. 1","","cs.CR","cs.CR"
"242","0907.3183v2","2009-07-18 06:37:37","2011-10-22 05:23:52","Why Did My Query Slow Down?","  Many enterprise environments have databases running on network-attached
server-storage infrastructure (referred to as Storage Area Networks or SANs).
Both the database and the SAN are complex systems that need their own separate
administrative teams. This paper puts forth the vision of an innovative
management framework to simplify administrative tasks that require an in-depth
understanding of both the database and the SAN. As a concrete instance, we
consider the task of diagnosing the slowdown in performance of a database query
that is executed multiple times (e.g., in a periodic report-generation
setting). This task is very challenging because the space of possible causes
includes problems specific to the database, problems specific to the SAN, and
problems that arise due to interactions between the two systems. In addition,
the monitoring data available from these systems can be noisy.
  We describe the design of DIADS which is an integrated diagnosis tool for
database and SAN administrators. DIADS generates and uses a powerful
abstraction called Annotated Plan Graphs (APGs) that ties together the
execution path of queries in the database and the SAN. Using an innovative
workflow that combines domain-specific knowledge with machine-learning
techniques, DIADS was applied successfully to diagnose query slowdowns caused
by complex combinations of events across a PostgreSQL database and a production
SAN.
","Nedyalko Borisov|Shivnath Babu|Sandeep Uttamchandani|Ramani Routray|Aameek Singh","","http://arxiv.org/abs/0907.3183v2","http://arxiv.org/pdf/0907.3183v2","","A conference version of this work was published as: Why Did My Query
  Slow Down, By Nedyalko Borisov, Sandeep Uttamchandani, Ramani Routray, and
  Aameek Singh, In the Proc. of the 4th Biennial Conference on Innovative Data
  Systems Research, Asilomar, CA, USA, Jan 4-7, 2009","","","cs.DB","cs.DB"
"243","0907.3819v1","2009-07-22 11:45:02","2009-07-22 11:45:02","Self-adaptive web intrusion detection system","  The evolution of the web server contents and the emergence of new kinds of
intrusions make necessary the adaptation of the intrusion detection systems
(IDS). Nowadays, the adaptation of the IDS requires manual -- tedious and
unreactive -- actions from system administrators. In this paper, we present a
self-adaptive intrusion detection system which relies on a set of local
model-based diagnosers. The redundancy of diagnoses is exploited, online, by a
meta-diagnoser to check the consistency of computed partial diagnoses, and to
trigger the adaptation of defective diagnoser models (or signatures) in case of
inconsistency. This system is applied to the intrusion detection from a stream
of HTTP requests. Our results show that our system 1) detects intrusion
occurrences sensitively and precisely, 2) accurately self-adapts diagnoser
model, thus improving its detection accuracy.
","Thomas Guyet|Rene Quiniou|Wei Wang|Marie-Odile Cordier","Agrocampus Ouest, INRIA - IRISA|INRIA - IRISA|NTNU|INRIA - IRISA","http://arxiv.org/abs/0907.3819v1","http://arxiv.org/pdf/0907.3819v1","","","","","cs.NI","cs.NI|cs.AI|cs.MA"
"244","0908.0548v1","2009-08-04 21:12:21","2009-08-04 21:12:21","IPv6 and IPv4 Threat reviews with Automatic Tunneling and Configuration
  Tunneling Considerations Transitional Model:A Case Study for University of
  Mysore Network","  The actual transition from IPv4 to IPv6 requires network administrators to
become aware of the next generation protocol and the associated risk
problems.Due to the scale and complexity of current internet architecture how
to protect from the existing investment and reduce the negative influence to
users and service providers during the transition from IPv4 to IPv6 is a very
important future topic for the advanced version of an internet
architecture.This paper summarizes and compares the IPv6 transition mechanism
methods like Dual Stack,Tunneling issues like IPv6 Automatic tunneling and
manually configured tunneling considerations, the IPv6 transition
scenarios,IPv6 transition security problems,highlights IPv6 and IPv4 threat
review with automatic tunneling and configuration tunneling considerations.In
this paper we have proposed a transitional threat model for automatic tunneling
and a configuration tunneling that could be followed by the University of
Mysore(UoM),to estimate automatic tunneling and a manually configured tunneling
threat review issues.Furthermore,there are different tunneling mechanisms such
as IPv6 over IPv4 GRE Tunnel,Tunnel broker,Automatic IPv4 Compatible Tunnel and
Automatic 6 to 4 Tunnel and also outlines many of the common known threats
against IPv6 and then it compares and contrast how these threats are similar
ones,might affect an IPv6 network.
","Hanumanthappa J.|Manjaiah D. H","","http://arxiv.org/abs/0908.0548v1","http://arxiv.org/pdf/0908.0548v1","","12 Pages IEEE Format, International Journal of Computer Science and
  Information Security, IJCSIS 2009, ISSN 1947 5500, Impact Factor 0.423","International Journal of Computer Science and Information
  Security, IJCSIS July 2009, Vol. 3, No. 1, USA","","cs.NI","cs.NI|cs.CR"
"245","0908.0627v1","2009-08-05 09:29:22","2009-08-05 09:29:22","The Future of Nuclear Energy: Facts and Fiction Chapter I: Nuclear
  Fission Energy Today","  Nuclear fission energy is considered to be somewhere between the holy grail,
required to solve all energy worries of the human industrialized civilization,
and a fast path directly to hell. Discussions about future energy sources and
the possible contribution from nuclear energy are often dominated by variations
of fundamentalists and often irrational approaches. As a result, very little is
known by the general public and even by decision makers about the contribution
of nuclear energy today, about uranium supplies, uranium resources and current
and future technological challenges and limitations.
  This analysis about nuclear energy and its contribution for tomorrow tries to
shed light on the nuclear reality and its limitations in the near and long term
future. The report, presented in four chapters, is based essentially on the
data provided in the documents from the IAEA (International Atomic Energy
Administration) and the NEA (the Nuclear Energy Agency from the OECD countries,
the WNA (World Nuclear Association) and the IEA (International Energy Agency).
","Michael Dittmar","","http://arxiv.org/abs/0908.0627v1","http://arxiv.org/pdf/0908.0627v1","","16 pages 0 figures","","","physics.soc-ph","physics.soc-ph|physics.pop-ph"
"246","0908.0986v1","2009-08-07 05:59:25","2009-08-07 05:59:25","Ethernet Networks: Current Trends and Tools","  Ethernet topology discovery has gained increasing interest in the recent
years. This trend is motivated mostly by increasing number of carrier Ethernet
networks as well as the size of these networks, and consequently the increasing
sales of these networks. To manage these networks efficiently, detailed and
accurate knowledge of their topology is needed. Knowledge of a network's
entities and the physical connections between them can be useful in various
prospective. Administrators can use topology information for network planning
and fault detecting. Topology information can also be used during protocol and
routing algorithm development, for performance prediction and as a basis for
accurate network simulations. From a network security perspective, threat
detection, network monitoring, network access control and forensic
investigations can benefit from accurate network topology information. In this
paper, we analyze market trends and investigate current tools available for
both research and commercial purposes.
","Abdulqader M. El-Sayed","","http://arxiv.org/abs/0908.0986v1","http://arxiv.org/pdf/0908.0986v1","","Proceedings of IEEE International Conference on Advanced Information
  Networking and Applications, 8 pages","","","cs.NI","cs.NI"
"247","0908.2300v1","2009-08-17 07:56:30","2009-08-17 07:56:30","Latent Markov model for longitudinal binary data: An application to the
  performance evaluation of nursing homes","  Performance evaluation of nursing homes is usually accomplished by the
repeated administration of questionnaires aimed at measuring the health status
of the patients during their period of residence in the nursing home. We
illustrate how a latent Markov model with covariates may effectively be used
for the analysis of data collected in this way. This model relies on a not
directly observable Markov process, whose states represent different levels of
the health status. For the maximum likelihood estimation of the model we apply
an EM algorithm implemented by means of certain recursions taken from the
literature on hidden Markov chains. Of particular interest is the estimation of
the effect of each nursing home on the probability of transition between the
latent states. We show how the estimates of these effects may be used to
construct a set of scores which allows us to rank these facilities in terms of
their efficacy in taking care of the health conditions of their patients. The
method is used within an application based on data concerning a set of nursing
homes located in the Region of Umbria, Italy, which were followed for the
period 2003--2005.
","Francesco Bartolucci|Monia Lupparelli|Giorgio E. Montanari","","http://arxiv.org/abs/0908.2300v1","http://arxiv.org/pdf/0908.2300v1","http://dx.doi.org/10.1214/08-AOAS230","Published in at http://dx.doi.org/10.1214/08-AOAS230 the Annals of
  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of
  Mathematical Statistics (http://www.imstat.org)","Annals of Applied Statistics 2009, Vol. 3, No. 2, 611-636","10.1214/08-AOAS230","stat.AP","stat.AP"
"248","0908.3930v1","2009-08-27 02:34:11","2009-08-27 02:34:11","SocialFilter: Collaborative Spam Mitigation using Social Networks","  Spam mitigation can be broadly classified into two main approaches: a)
centralized security infrastructures that rely on a limited number of trusted
monitors to detect and report malicious traffic; and b) highly distributed
systems that leverage the experiences of multiple nodes within distinct trust
domains. The first approach offers limited threat coverage and slow response
times, and it is often proprietary. The second approach is not widely adopted,
partly due to the lack of guarantees regarding the trustworthiness of nodes
that comprise the system.
  Our proposal, SocialFilter, aims to achieve the trustworthiness of
centralized security services and the wide coverage, responsiveness and
inexpensiveness of large-scale collaborative spam mitigation. We propose a
large-scale distributed system that enables clients with no email
classification functionality to query the network on the behavior of a host. A
SocialFilter node builds trust for its peers by auditing their behavioral
reports and by leveraging the social network of SocialFilter administrators.
The node combines the confidence its peers have in their own reports and the
trust it places on its peers to derive the likelihood that a host is spamming.
  The simulation-based evaluation of our approach indicates its potential under
a real-world deployment: during a simulated spam campaign, SocialFilternodes
characterized 92% of spam bot connections with confidence greater than 50%,
while yielding no false positives
","Michael Sirivianos|Xiaowei Yang|Kyungbaek Kim","","http://arxiv.org/abs/0908.3930v1","http://arxiv.org/pdf/0908.3930v1","","10 pages","","","cs.CR","cs.CR|cs.DC"
"249","0909.0093v1","2009-09-01 05:32:29","2009-09-01 05:32:29","Energy Efficient Location Aided Routing Protocol for Wireless MANETs","  A Mobile Ad-Hoc Network (MANET) is a collection of wireless mobile nodes
forming a temporary network without using any centralized access point,
infrastructure, or centralized administration. In this paper we introduce an
Energy Efficient Location Aided Routing (EELAR) Protocol for MANETs that is
based on the Location Aided Routing (LAR). EELAR makes significant reduction in
the energy consumption of the mobile nodes batteries by limiting the area of
discovering a new route to a smaller zone. Thus, control packets overhead is
significantly reduced. In EELAR a reference wireless base station is used and
the network's circular area centered at the base station is divided into six
equal sub-areas. At route discovery instead of flooding control packets to the
whole network area, they are flooded to only the sub-area of the destination
mobile node. The base station stores locations of the mobile nodes in a
position table. To show the efficiency of the proposed protocol we present
simulations using NS-2. Simulation results show that EELAR protocol makes an
improvement in control packet overhead and delivery ratio compared to AODV,
LAR, and DSR protocols.
","Mohammad A. Mikki","","http://arxiv.org/abs/0909.0093v1","http://arxiv.org/pdf/0909.0093v1","","9 Pages IEEE format, International Journal of Computer Science and
  Information Security, IJCSIS 2009, ISSN 1947 5500, Impact factor 0.423,
  http://sites.google.com/site/ijcsis/","International Journal of Computer Science and Information
  Security, IJCSIS, Vol. 4, No. 1 & 2, August 2009, USA","","cs.NI","cs.NI"
"250","0909.1402v1","2009-09-08 06:28:48","2009-09-08 06:28:48","Impact of Rushing attack on Multicast in Mobile Ad Hoc Network","  A mobile ad hoc network (MANETs) is a self-organizing system of mobile nodes
that communicate with each other via wireless links with no fixed
infrastructure or centralized administration such as base station or access
points. Nodes in a MANETs operate both as host as well as routers to forward
packets for each other in a multihop fashion. For many applications in wireless
networks, multicasting is an important and frequent communication service. By
multicasting, since a single message can be delivered to multiple receivers
simultaneously. It greatly reduces the transmission cost when sending the same
packet to multiple recipients.
  The security issue of MANETs in group communications is even more challenging
because of involvement of multiple senders and multiple receivers. At that time
of multicasting, mobile ad hoc network are unprotected by the attacks of
malicious nodes because of vulnerabilities of routing protocols. Some of the
attacks are Rushing attack, Blackhole attack, Sybil attack, Neighbor attack and
Jellyfish attack.
  This paper is based on Rushing attack. In Rushing attack, the attacker
exploits the duplicate suppression mechanism by quickly forwarding route
discovery packets in order to gain access to the forwarding group and this will
affect the Average Attack Success Rate.
  In this paper, the goal is to measure the impact of Rushing attack and their
node positions which affect the performance metrics of Average Attack Success
Rate with respect to three scenarios: near sender, near receiver and anywhere
within the network. The performance of the Attack Success Rate with respect to
above three scenarios is also compared.
","V. Palanisamy|P. Annadurai","","http://arxiv.org/abs/0909.1402v1","http://arxiv.org/pdf/0909.1402v1","","7 Pages IEEE format, International Journal of Computer Science and
  Information Security, IJCSIS 2009, ISSN 1947 5500, Impact Factor 0.423,
  http://sites.google.com/site/ijcsis/","International Journal of Computer Science and Information
  Security, IJCSIS, Vol. 4, No. 1 & 2, August 2009, USA","","cs.CR","cs.CR|cs.NI"
"251","0909.1421v1","2009-09-08 08:22:34","2009-09-08 08:22:34","The Future of Nuclear Energy: Facts and Fiction Chapter III: How
  (un)reliable are the Red Book Uranium Resource Data?","  For more than 40 years, the Nuclear Energy Agency of the OECD countries and
the International Atomic Energy Administration of the United Nations have
published a biannual document with the title ""Uranium Resources, Production and
Demand"". This book, known as the ""Red Book"", summarizes data about the actual
and near future nuclear energy situation and presents the accumulated worldwide
knowledge about the existing and expected uranium resources. These data are
widely believed to provide an accurate and solid basis for future decisions
about nuclear energy. Unfortunately, as it is demonstrated in this paper, they
do not.
  The conventional worldwide uranium resources are estimated by the authors of
the Red Book as 5.5 million tons. Out of these, 3.3 million tons are assigned
to the reasonable assured category and 2.2 million tons are associated with the
not yet discovered but assumed to exist inferred resources. Our analysis shows
that neither the 3.3 million tons of ""assured"" resources nor the 2.2 million
tons of inferred resources are justified by the Red Book data and that the
actual known exploitable resources are probably much smaller.
  Despite many shortcomings of the uranium resource data, some interesting and
valuable information can be extracted from the Red Book. Perhaps most
importantly, the Red Book resource data can be used to test the
""economic-geological hypothesis"", which claims for example that a doubling of
uranium price will increase the amount exploitable uranium resources by an even
larger factor. The relations between the uranium resources claimed for the
different resource categories and their associated cost estimates are found to
be in clear contradiction with this hypothesis.
","Michael Dittmar","","http://arxiv.org/abs/0909.1421v1","http://arxiv.org/pdf/0909.1421v1","","19 pages, 8 tables","","","physics.soc-ph","physics.soc-ph|physics.pop-ph"
"252","0909.1779v1","2009-09-09 18:10:20","2009-09-09 18:10:20","The Case for RodentStore, an Adaptive, Declarative Storage System","  Recent excitement in the database community surrounding new
applications?analytic, scientific, graph, geospatial, etc.?has led to an
explosion in research on database storage systems. New storage systems are
vital to the database community, as they are at the heart of making database
systems perform well in new application domains. Unfortunately, each such
system also represents a substantial engineering effort including a great deal
of duplication of mechanisms for features such as transactions and caching. In
this paper, we make the case for RodentStore, an adaptive and declarative
storage system providing a high-level interface for describing the physical
representation of data. Specifically, RodentStore uses a declarative storage
algebra whereby administrators (or database design tools) specify how a logical
schema should be grouped into collections of rows, columns, and/or arrays, and
the order in which those groups should be laid out on disk. We describe the key
operators and types of our algebra, outline the general architecture of
RodentStore, which interprets algebraic expressions to generate a physical
representation of the data, and describe the interface between RodentStore and
other parts of a database system, such as the query optimizer and executor. We
provide a case study of the potential use of RodentStore in representing dense
geospatial data collected from a mobile sensor network, showing the ease with
which different storage layouts can be expressed using some of our algebraic
constructs and the potential performance gains that a RodentStore-built storage
system can offer.
","Philippe Cudre-Mauroux|Eugene Wu|Sam Madden","MIT|MIT|MIT","http://arxiv.org/abs/0909.1779v1","http://arxiv.org/pdf/0909.1779v1","","CIDR 2009","","","cs.DB","cs.DB"
"253","0910.5685v1","2009-10-29 17:30:19","2009-10-29 17:30:19","A Radio System for Avoiding Illuminating Aircraft with a Laser Beam","  When scientific experiments require transmission of powerful laser or radio
beams through the atmosphere the Federal Aviation Administration (FAA) requires
that precautions be taken to avoid inadvertent illumination of aircraft. Here
we describe a highly reliable system for detecting aircraft entering the
vicinity of a laser beam by making use of the Air Traffic Control (ATC)
transponders required on most aircraft. This system uses two antennas, both
aligned with the laser beam. One antenna has a broad beam and the other has a
narrow beam. The ratio of the transponder power received in the narrow beam to
that received in the broad beam gives a measure of the angular distance of the
aircraft from the axis that is independent of the range or the transmitter
power. This ratio is easily measured and can be used to shutter the laser when
the aircraft is too close to the beam. Prototype systems operating on
astronomical telescopes have produced good results.
","W. A. Coles|T. W. Murphy Jr.|J. F. Melser|J. K. Tu|G. A. White|K. H. Kassabian|K. Bales|B. B. Baumgartner","","http://arxiv.org/abs/0910.5685v1","http://arxiv.org/pdf/0910.5685v1","http://dx.doi.org/10.1086/664067","16 pages; 8 figures (3 in color); submitted to PASP","PASP 124, 42-50, (2012)","10.1086/664067","astro-ph.IM","astro-ph.IM"
"254","0911.2733v1","2009-11-13 23:53:27","2009-11-13 23:53:27","Academic performance & student engagement in level 1 physics
  undergraduates","  At the beginning of academic year 2007-08, staff in the Department of Physics
& Astronomy at the University of Glasgow started to implement a number of
substantial changes to the administration of the level 1 physics undergraduate
class. The main aims were to improve the academic performance and progression
statistics. With this in mind, a comprehensive system of learning support was
introduced, the main remit being the provision of an improved personal contact
and academic monitoring and support strategy for all students at level 1. The
effects of low engagement with compulsory continuous assessment components had
already been observed to have a significant effect for students sitting in the
middle of the grade curve. Analysis of data from the 2007-08 class showed that
even some nominally high-achieving students achieved lowered grades due to the
effects of low engagement. Nonetheless, academic and other support measures put
in place during 2007-08 played a part in raising the passrate for the level 1
physics class by approximately 8% as well as raising the progression rate by
approximately 10%.
","M. M. Casey|S. McVitie","","http://arxiv.org/abs/0911.2733v1","http://arxiv.org/pdf/0911.2733v1","http://dx.doi.org/10.1088/0143-0807/30/5/022","12 pages, 3 figures","Eur. J. Phys. 30 (2009) 1153-1161","10.1088/0143-0807/30/5/022","physics.ed-ph","physics.ed-ph"
"255","0912.0549v1","2009-12-03 01:31:36","2009-12-03 01:31:36","Modular Workflow Engine for Distributed Services using Lightweight Java
  Clients","  In this article we introduce the concept and the first implementation of a
lightweight client-server-framework as middleware for distributed computing. On
the client side an installation without administrative rights or privileged
ports can turn any computer into a worker node. Only a Java runtime environment
and the JAR files comprising the workflow client are needed. To connect all
clients to the engine one open server port is sufficient. The engine submits
data to the clients and orchestrates their work by workflow descriptions from a
central database. Clients request new task descriptions periodically, thus the
system is robust against network failures. In the basic set-up, data up- and
downloads are handled via HTTP communication with the server. The performance
of the modular system could additionally be improved using dedicated file
servers or distributed network file systems.
  We demonstrate the design features of the proposed engine in real-world
applications from mechanical engineering. We have used this system on a compute
cluster in design-of-experiment studies, parameter optimisations and robustness
validations of finite element structures.
","R. -M. Vetter|W. Lennartz|J. -V. Peetz","","http://arxiv.org/abs/0912.0549v1","http://arxiv.org/pdf/0912.0549v1","","14 pages, 8 figures","","","cs.SE","cs.SE|cs.CE"
"256","0912.2293v1","2009-12-11 17:41:30","2009-12-11 17:41:30","Detection and Prevention of New and Unknown Malware using Honeypots","  Security has become ubiquitous in every domain today as newly emerging
malware pose an ever-increasing perilous threat to systems. Consequently,
honeypots are fast emerging as an indispensible forensic tool for the analysis
of malicious network traffic. Honeypots can be considered to be traps for
hackers and intruders and are generally deployed complimentary to Intrusion
Detection Systems (IDS) and Intrusion Prevention Systems (IPS) in a network.
They help system administrators perform a rigorous analysis of external and
internal attacks on their networks. They are also used by security firms and
research labs to capture the latest variants of malware. However, honeypots
would serve a slightly different purpose in our proposed system. We intend to
use honeypots for generating and broadcasting instant cures for new and unknown
malware in a network. The cures which will be in the form of on-the-fly
anti-malware signatures would spread in a fashion that is similar to the way
malware spreads across networks. The most striking advantage of implementing
this technology is that an effective initial control can be exercised on
malware. Proposed system would be capable of providing cures for new fatal
viruses which have not yet been discovered by prime security firms of the
world.
","Shishir Kumar|Durgesh Pant","","http://arxiv.org/abs/0912.2293v1","http://arxiv.org/pdf/0912.2293v1","","","IJCSE Volume 1 Issue 2 2009 56-61","","cs.NI","cs.NI|cs.CR"
"257","0912.4613v2","2009-12-23 11:35:13","2010-04-03 04:28:16","Chain Routing: A new routing framework for the Internet based on
  complete orders","  A new framework to perform routing at the Autonomous System level is proposed
in this paper. This mechanism, called Chain Routing, uses complete orders as
its main topological unit. Since complete orders are acyclic digraphs that
possess a known topology, it is possible to define an acyclic structure to
route packets between a group of Autonomous Systems. The adoption of complete
orders also allows easy identification and avoidance of persistent route
oscillations, eliminates the possibility of developing transient loops in
paths, and provides a structure that facilitates the implementation of traffic
engineering. Moreover, by combining Chain Routing with other mechanisms that
implement complete orders in time, we suggest that it is possible to design a
new routing protocol which could be more reliable and stable than BGP's current
implementation. Although Chain Routing will require an increase of the message
overhead and greater coordination between network administrators, the rewards
in stability and resilience should more than compensate for this effort.
","P. David Arjona-Villicana|Costas C. Constantinou|Alexander S. Stepanenko","","http://arxiv.org/abs/0912.4613v2","http://arxiv.org/pdf/0912.4613v2","","Submitted to Computer Networks","IET Communications 5(16), 2011","","cs.NI","cs.NI"
"258","1001.0943v1","2010-01-06 17:57:14","2010-01-06 17:57:14","The structure of borders in a small world","  Geographic borders are not only essential for the effective functioning of
government, the distribution of administrative responsibilities and the
allocation of public resources, they also influence the interregional flow of
information, cross-border trade operations, the diffusion of innovation and
technology, and the spatial spread of infectious diseases. However, as growing
interactions and mobility across long distances, cultural, and political
borders continue to amplify the small world effect and effectively decrease the
relative importance of local interactions, it is difficult to assess the
location and structure of effective borders that may play the most significant
role in mobility-driven processes. The paradigm of spatially coherent
communities may no longer be a plausible one, and it is unclear what structures
emerge from the interplay of interactions and activities across spatial scales.
Here we analyse a multi-scale proxy network for human mobility that
incorporates travel across a few to a few thousand kilometres. We determine an
effective system of geographically continuous borders implicitly encoded in
multi-scale mobility patterns. We find that effective large scale boundaries
define spatially coherent subdivisions and only partially coincide with
administrative borders. We find that spatial coherence is partially lost if
only long range traffic is taken into account and show that prevalent models
for multi-scale mobility networks cannot account for the observed patterns.
These results will allow for new types of quantitative, comparative analyses of
multi-scale interaction networks in general and may provide insight into a
multitude of spatiotemporal phenomena generated by human activity.
","C. Thiemann|F. Theis|D. Grady|R. Brune|D. Brockmann","","http://arxiv.org/abs/1001.0943v1","http://arxiv.org/pdf/1001.0943v1","http://dx.doi.org/10.1371/journal.pone.0015422","9 pages","","10.1371/journal.pone.0015422","physics.soc-ph","physics.soc-ph|cond-mat.stat-mech"
"259","1001.2258v2","2010-01-13 18:39:25","2011-09-07 15:29:00","Internal Location Based System For Mobile Devices Using Passive RFID And
  Wireless Technology","  This article has been withdrawn by arXiv administrators due to plagiarized
content from arXiv:1009.3448.
","A. D. Potgantwar|Vijay M. Wadhai","","http://arxiv.org/abs/1001.2258v2","http://arxiv.org/pdf/1001.2258v2","","This article has been withdrawn by arXiv administrators due to
  plagiarized content from arXiv:1009.3448","International Journal of Computer Science and Information
  Security, IJCSIS, Vol. 6, No. 3, pp. 153-159, December 2009, USA","","cs.CY","cs.CY"
"260","1001.3481v2","2010-01-20 07:35:21","2010-02-03 16:35:49","Resolution scalability improvement for JPEG2000 standard color image","  Removed by arXiv administration. This article was plagiarised from
http://www.dmi.unict.it/~battiato/download/NSIP_2003_VQ.pdf and other
locations.
","U. Vijayasankar|S. Prasadh.|A. Arul Lawrence Selvakumar","","http://arxiv.org/abs/1001.3481v2","http://arxiv.org/pdf/1001.3481v2","","Removed by arXiv administration. This article was plagiarised from
  http://www.dmi.unict.it/~battiato/download/NSIP_2003_VQ.pdf and other
  locations","","","cs.GR","cs.GR"
"261","1001.3483v2","2010-01-20 07:37:51","2010-02-03 16:32:58","BGP Converges to stable solution in Interdomain routing","  Withdrawn by arXiv administration. This article was plagiarised directly from
http://ieeexplore.ieee.org/xpl/freeabs_all.jsp?arnumber=5061961 , which
appeared in the conference INFOCOM 2009.
","G. Mohammed Nazer|A. Arul Lawrence Selvakumar","","http://arxiv.org/abs/1001.3483v2","http://arxiv.org/pdf/1001.3483v2","","Withdrawn by arXiv administration. This article was plagiarised
  directly from http://ieeexplore.ieee.org/xpl/freeabs_all.jsp?arnumber=5061961
  , which appeared in the conference INFOCOM 2009","","","cs.NI","cs.NI"
"262","1001.3816v2","2010-01-21 14:37:36","2010-01-22 03:19:25","The P versus NP Problem","  Removed by arXiv administration.
  This article was plagiarized directly from Stephen Cook's description of the
problem for the Clay Mathematics Institute. See
http://gauss.claymath.org:8888/millennium/P_vs_NP/pvsnp.pdf for the original
text.
","Rakesh Dube","","http://arxiv.org/abs/1001.3816v2","http://arxiv.org/pdf/1001.3816v2","","Removed by arXiv administration due to plagiarism from Stephen Cook's
  description of the problem for the Clay Mathematics Institute. See
  http://gauss.claymath.org:8888/millennium/P_vs_NP/pvsnp.pdf for the original
  text","","","cs.CC","cs.CC"
"263","1001.4981v1","2010-01-27 16:17:13","2010-01-27 16:17:13","Cluster Identification and Characterization of Physical Fields","  The description of complex configuration is a difficult issue. We present a
powerful technique for cluster identification and characterization. The scheme
is designed to treat with and analyze the experimental and/or simulation data
from various methods. Main steps are as follows. We first divide the space
using face or volume elements from discrete points. Then, combine the elements
with the same and/or similar properties to construct clusters with special
physical characterizations. In the algorithm, we adopt administrative structure
of hierarchy-tree for spatial bodies such as points, lines, faces, blocks, and
clusters. Two fast search algorithms with the complexity are realized. The
establishing of the hierarchy-tree and the fast searching of spatial bodies are
general, which are independent of spatial dimensions. Therefore, it is easy to
extend the skill to other fields. As a verification and validation, we treated
with and analyzed some two-dimensional and three-dimensional random data.
","Guangcai Zhang|Aiguo Xu|Guo Lu|Zeyao Mo","","http://arxiv.org/abs/1001.4981v1","http://arxiv.org/pdf/1001.4981v1","","PDF file, Science in China G (in press)","Sci China Phys Mech Astron September (2010) Vol. 53 No. 9
  1610-1618","","cond-mat.stat-mech","cond-mat.stat-mech|cs.CG"
"264","1002.2829v1","2010-02-15 08:30:48","2010-02-15 08:30:48","Dynamic Cognitive Process Application of Blooms Taxonomy for Complex
  Software Design in the Cognitive Domain","  Software design in Software Engineering is a critical and dynamic cognitive
process. Accurate and flawless system design will lead to fast coding and early
completion of a software project. Blooms taxonomy classifies cognitive domain
into six dynamic levels such as Knowledge at base level to Comprehension,
Application, Analysis, Synthesis and Evaluation at the highest level in the
order of increasing complexity. A case study indicated in this paper is a gira
system, which is a gprs based Intranet Remote Administration which monitors and
controls the intranet from a mobile device. This paper investigates from this
case study that the System Design stage in Software Engineering uses all the
six levels of Blooms Taxonomy. The application of the highest levels of Blooms
Taxonomy such as Synthesis and Evaluation in the design of gira indicates that
Software Design in Software Development Life Cycle is a complex and critical
cognitive process.
","NR Shashi Kumar|TP Pushpavathi|R Selvarani","","http://arxiv.org/abs/1002.2829v1","http://arxiv.org/pdf/1002.2829v1","","13 pages, 6 figures","Awarded as the Best paper, International Conference, Team Tech,
  IISc., pp 101, 2008","","cs.SE","cs.SE"
"265","1002.3015v1","2010-02-16 06:53:21","2010-02-16 06:53:21","GPRS Based Intranet Remote Administration GIRA","  In a world of increasing mobility, there is a growing need for people to
communicate with each other and have timely access to information regardless of
the location of the individuals or the information. With the advent of moblle
technology, the way of communication has changed. The gira system is basically
a mobile phone technology service. In this paper we discuss about a novel local
area network control system called gprs based Intranet Remote Administration
gira. This system finds application in a mobile handset. With this system, a
network administrator will have an effective remote control over the network.
gira system is developed using gprs, gcf Generic Connection Framework of j2me,
sockets and rmi technologies
","Shashi Kumar N. R.|R. Selvarani|Pushpavathi T. P","","http://arxiv.org/abs/1002.3015v1","http://arxiv.org/pdf/1002.3015v1","","4 pages, 2 figures","Journal of Research and Industry, Volume 1, pp 36-39, 2008","","cs.SE","cs.SE"
"266","1002.3992v1","2010-02-21 18:59:25","2010-02-21 18:59:25","The government of state's power bodies by means of the Internet","  The electronic government involves developing the informational society,
which refers to an economy and a society in which the access, acquisition,
memorizing, taking, transmitting, spreading and using the knowledge accede to a
decisive role. The informational society involves changes in the domains of
administration (e-Government), business (electronic commerce and e-business),
education (long distance education), culture (multimedia centers and virtual
libraries), mass- media (TV, video advertising panels), and in the labor manner
(tele-work and virtual commuting).The e-government refers to the interaction
between the Government, Parliament and other public institutions with the
citizens by the electronic means.
","L. Bercea|G. Nemtoi|C. Ungureanu","","http://arxiv.org/abs/1002.3992v1","http://arxiv.org/pdf/1002.3992v1","","","Journal of Computing, Volume 2, Issue 2, February 2010,
  https://sites.google.com/site/journalofcomputing/","","cs.CY","cs.CY"
"267","1003.0955v1","2010-03-04 06:51:51","2010-03-04 06:51:51","Precise Request Tracing and Performance Debugging for Multi-tier
  Services of Black Boxes","  As more and more multi-tier services are developed from commercial components
or heterogeneous middleware without the source code available, both developers
and administrators need a precise request tracing tool to help understand and
debug performance problems of large concurrent services of black boxes.
Previous work fails to resolve this issue in several ways: they either accept
the imprecision of probabilistic correlation methods, or rely on knowledge of
protocols to isolate requests in pursuit of tracing accuracy. This paper
introduces a tool named PreciseTracer to help debug performance problems of
multi-tier services of black boxes. Our contributions are two-fold: first, we
propose a precise request tracing algorithm for multi-tier services of black
boxes, which only uses application-independent knowledge; secondly, we present
a component activity graph abstraction to represent causal paths of requests
and facilitate end-to-end performance debugging. The low overhead and tolerance
of noise make PreciseTracer a promising tracing tool for using on production
systems.
","Zhihong Zhang|Jianfeng Zhan|Yong Li|Lei Wang|Dan Meng|Bo Sang","","http://arxiv.org/abs/1003.0955v1","http://arxiv.org/pdf/1003.0955v1","http://dx.doi.org/10.1109/DSN.2009.5270321","","Proceeding of IEEE/IFIP 39th Dependable System and Network (DSN
  2009)","10.1109/DSN.2009.5270321","cs.DC","cs.DC|cs.PF"
"268","1003.1053v1","2010-03-04 14:09:22","2010-03-04 14:09:22","Mathematical Modelling of Allergy and Specific Immunotherapy:
  Th1-Th2-Treg Interactions","  Regulatory T cells (Treg) have recently been identified as playing a central
role in allergy and during allergen-specific immunotherapy. We have extended
our previous mathematical model describing the nonlinear dynamics of Th1-Th2
regulation by including Treg cells and their major cytokines. We hypothesize
that immunotherapy mainly acts on the T cell level and that the decisive
process can be regarded as a dynamical phenomenon. The model consists of
nonlinear differential equations which describe the proliferation and mutual
suppression of different T cell subsets. The old version of the model was based
upon the Th1-Th2 paradigm and is successful in describing the ""Th1-Th2 switch""
which was considered the decisive event during specific immunotherapy. In
recent years, however, the Th1-Th2 paradigm has been questioned and therefore,
we have investigated a modified model in order to account for the influence of
a regulatory T cell type. We examined the extended model by means of numerical
simulations and analytical methods. As the modified model is more complex, we
had to develop new methods to portray its characteristics. The concept of
stable manifolds of fixed points of a stroboscobic map turned out to be
especially important. We found that when including regulatory T cells, our
model can describe the events in allergen-specific immunotherapy more
accurately. Our results suggest that the decisive effect of immunotherapy, the
increased proliferation of Treg and suppression of Th2 cells, crucially depends
on the administration of high dose injections right before the maintenance
phase sets in. Empirical protocols could therefore be improved by optimizing
this step of therapy.
","Fridolin Gross|Gerhard Metzner|Ulrich Behn","","http://arxiv.org/abs/1003.1053v1","http://arxiv.org/pdf/1003.1053v1","","","","","q-bio.CB","q-bio.CB|q-bio.PE"
"269","1003.3322v1","2010-03-17 08:44:11","2010-03-17 08:44:11","Mobile Codes Localization in Ad hoc Networks: a Comparative Study of
  Centralized and Distributed Approaches","  This paper presents a new approach in the management of mobile ad hoc
networks. Our alternative, based on mobile agent technology, allows the design
of mobile centralized server in ad hoc network, where it is not obvious to
think about a centralized management, due to the absence of any administration
or fixed infrastructure in these networks. The aim of this centralized approach
is to provide permanent availability of services in ad hoc networks which are
characterized by a distributed management. In order to evaluate the performance
of the proposed approach, we apply it to solve the problem of mobile code
localization in ad hoc networks. A comparative study, based upon a simulation,
of centralized and distributed localization protocols in terms of messages
number exchanged and response time shows that the centralized approach in a
distributed form is more interesting than a totally centralized approach.
","Youcef Zafoune|Aicha Mokhtari|Rushed kanawati","USTHB University, Algeria and|USTHB University, Algeria and|Institute Galilee, Paris13 University, Paris, French","http://arxiv.org/abs/1003.3322v1","http://arxiv.org/pdf/1003.3322v1","http://dx.doi.org/10.5121/ijcnc.2010.2213","14 Pages, IJCNC Journal 2010","International Journal of Computer Networks & Communications 2.2
  (2010) 164-177","10.5121/ijcnc.2010.2213","cs.NI","cs.NI"
"270","1003.3565v1","2010-03-18 12:00:59","2010-03-18 12:00:59","An Overview of Mobile Ad Hoc Networks for the Existing Protocols and
  Applications","  Mobile Ad Hoc Network (MANET) is a collection of two or more devices or nodes
or terminals with wireless communications and networking capability that
communicate with each other without the aid of any centralized administrator
also the wireless nodes that can dynamically form a network to exchange
information without using any existing fixed network infrastructure. And it's
an autonomous system in which mobile hosts connected by wireless links are free
to be dynamically and some time act as routers at the same time, and we discuss
in this paper the distinct characteristics of traditional wired networks,
including network configuration may change at any time, there is no direction
or limit the movement and so on, and thus needed a new optional path Agreement
(Routing Protocol) to identify nodes for these actions communicate with each
other path, An ideal choice way the agreement should not only be able to find
the right path, and the Ad Hoc Network must be able to adapt to changing
network of this type at any time. and we talk in details in this paper all the
information of Mobile Ad Hoc Network which include the History of ad hoc,
wireless ad hoc, wireless mobile approaches and types of mobile ad Hoc
networks, and then we present more than 13 types of the routing Ad Hoc Networks
protocols have been proposed. In this paper, the more representative of routing
protocols, analysis of individual characteristics and advantages and
disadvantages to collate and compare, and present the all applications or the
Possible Service of Ad Hoc Networks.
","Saleh Ali K. Al-Omari|Putra Sumari","Universiti Sains Malaysia, Malaysia|Universiti Sains Malaysia, Malaysia","http://arxiv.org/abs/1003.3565v1","http://arxiv.org/pdf/1003.3565v1","http://dx.doi.org/10.5121/jgraphhoc.2010.2107","24 Pages, JGraph-Hoc Journal","International journal on applications of graph theory in wireless
  ad hoc networks and sensor networks 2.1 (2010) 87-110","10.5121/jgraphhoc.2010.2107","cs.NI","cs.NI"
"271","1003.4445v2","2010-03-23 15:28:09","2017-01-12 19:02:25","Fundamental parameters of RR Lyrae stars from multicolour photometry and
  Kurucz atmospheric models. I. Theory and practical implementation","  A photometric calibration of Kurucz static model atmospheres is used to
obtain parameters of RR Lyrae stars: variation of stellar angular radius
$\vartheta$, effective temperature $T_{\rm e}$, and gravity $g_{\rm e}$ as a
function of phase, interstellar reddening $E(B-V)$ toward the star, and
atmospheric metallicity $M$. Photometric and hydrodynamic conditions are given
to find the phases of pulsation when the quasi-static atmosphere approximation
(QSAA) can be applied. The QSAA is generalized to a non-uniformly moving
spherical atmosphere, and the distance $d$, mass ${\cal M}$, and atmospheric
motion are derived from the laws of mass and momentum conservation. To
demonstrate the efficiency of the method, the $UBV(RI)_C$ photometry of SU Dra
was used to derive the following parameters: $[M]=-1.60\pm .10$ dex,
$E(B-V)=0.015\pm .010$, $d=663\pm 67$ pc, ${\cal M}=(0.68\pm .03){\cal
M}_\odot$, equilibrium luminosity, $L_{\rm eq}=45.9\pm 9.3L_\odot$, $T_{\rm
eq}=6813\pm 20$ K.
","Szabolcs Barcza","","http://arxiv.org/abs/1003.4445v2","http://arxiv.org/pdf/1003.4445v2","","This submission has been withdrawn by arXiv administrators because it
  is a duplicate of arXiv:1007.3441, which is the published version","","","astro-ph.SR","astro-ph.SR"
"272","1003.4745v2","2010-03-24 20:21:31","2018-06-15 17:10:12","Beam test calibration of the balloon-borne imaging calorimeter for the
  CREAM experiment","  CREAM (Cosmic Ray Energetics And Mass) is a multi-flight balloon mission
designed to collect direct data on the elemental composition and individual
energy spectra of cosmic rays. Two instrument suites have been built to be
flown alternately on a yearly base. The tungsten/Sci-Fi imaging calorimeter for
the second flight, scheduled for December 2005, was calibrated with electron
and proton beams at CERN. A calibration procedure based on the study of the
longitudinal shower profile is described and preliminary results of the beam
test are presented.
","P. S. Marrocchesi|H. S. Ahn|M. G. Bagliesi|A. Basti|G. Bigongiari|A. Castellina|M. A. Ciocci|A. Di Virgilio|T. Lomtatze|O. Ganel|K. C. Kim|M. H. Lee|F. Ligabue|L. Lutz|P. Maestro|A. Malinine|M. Meucci|V. Millucci|F. Morsani|E. S. Seo|R. Sina|J. Wu|J. Wu|Y. S. Yoon|R. Zei|S. Y. Zinn","","http://arxiv.org/abs/1003.4745v2","http://arxiv.org/pdf/1003.4745v2","","This submission has been withdrawn by arXiv administrators because it
  is a duplicate of arXiv:physics/0507174","","","astro-ph.HE","astro-ph.HE|astro-ph.IM"
"273","1003.5631v1","2010-03-29 18:09:06","2010-03-29 18:09:06","A Mobile Message Scheduling and Delivery System using m-Learning
  framework","  Wireless data communications in form of Short Message Service (SMS) and
Wireless Access Protocols (WAP) browsers have gained global popularity, yet,
not much has been done to extend the usage of these devices in electronic
learning (e-learning) and information sharing. This project explores the
extension of e learning into wireless/ handheld (W/H) computing devices with
the help of a mobile learning (m-learning) framework. This framework provides
the requirements to develop m-learning application that can be used to share
academic and administrative information among people within the university
campus. A prototype application has been developed to demonstrate the important
functionality of the proposed system in simulated environment. This system is
supposed to work both in bulk SMS and interactive SMS delivery mode. Here we
have combined both Short Message Service (SMS) and Wireless Access Protocols
(WAP) browsers. SMS is used for Short and in time information delivery and WAP
is used for detailed information delivery like course content, training
material, interactive evolution tests etc. The push model is used for sending
personalized multicasting messages to a group of mobile users with a common
profile thereby improving the effectiveness and usefulness of the cntent
delivered. Again pull mechanism can be applied for sending information as SMS
when requested by end user in interactive SMS delivery mode. The main strength
of the system is that, the actual SMS delivery application can be hosted on a
mobile device, which can operate even when the device is on move.
","Moumita Majumder|Sumit Dhar","","http://arxiv.org/abs/1003.5631v1","http://arxiv.org/pdf/1003.5631v1","","","Journal of Telecommunications,Volume 1, Issue 2, pp30-34, March
  2010","","cs.OH","cs.OH"
"274","1004.0604v1","2010-04-05 09:52:05","2010-04-05 09:52:05","On Generation of Firewall Log Status Reporter (SRr) Using Perl","  Computer System Administration and Network Administration are few such areas
where Practical Extraction Reporting Language (Perl) has robust utilization
these days apart from Bioinformatics. The key role of a System/Network
Administrator is to monitor log files. Log file are updated every day. To scan
the summary of large log files and to quickly determine if there is anything
wrong with the server or network we develop a Firewall Log Status Reporter
(SRr). SRr helps to generate the reports based on the parameters of interest.
SRr provides the facility to admin to generate the individual firewall report
or all reports in one go. By scrutinizing the results of the reports admin can
trace how many times a particular request has been made from which source to
which destination and can track the errors easily. Perl scripts can be seen as
the UNIX script replacement in future arena and SRr is one development with the
same hope that we can believe in. SRr is a generalized and customizable utility
completely written in Perl and may be used for text mining and data mining
application in Bioinformatics research and development too.
","Sugam Sharma|Hari Cohly|Tzusheng Pei","Iowa State University, USA|Jackson State University, USA|Jackson State University, USA","http://arxiv.org/abs/1004.0604v1","http://arxiv.org/pdf/1004.0604v1","","10Pages","International Journal of Network Security & Its Applications 1.2
  (2009) 90-99","","cs.SE","cs.SE"
"275","1004.0772v1","2010-04-06 04:49:49","2010-04-06 04:49:49","Security properties in an open peer-to-peer network","  This paper proposes to address new requirements of confidentiality, integrity
and availability properties fitting to peer-to-peer domains of resources. The
enforcement of security properties in an open peer-topeer network remains an
open problem as the literature have mainly proposed contribution on
availability of resources and anonymity of users. That paper proposes a novel
architecture that eases the administration of a peer-to-peer network. It
considers a network of safe peer-to-peer clients in the sense that it is a
commune client software that is shared by all the participants to cope with the
sharing of various resources associated with different security requirements.
However, our proposal deals with possible malicious peers that attempt to
compromise the requested security properties. Despite the safety of an open
peer-to-peer network cannot be formally guaranteed, since a end user has
privileges on the target host, our solution provides several advanced security
enforcement. First, it enables to formally define the requested security
properties of the various shared resources. Second, it evaluates the trust and
the reputation of the requesting peer by sending challenges that test the
fairness of its peer-to-peer security policy. Moreover, it proposes an advanced
Mandatory Access Control that enforces the required peer-to-peer security
properties through an automatic projection of the requested properties onto
SELinux policies. Thus, the SELinux system of the requesting peer is
automatically configured with respect to the required peer-to-peer security
properties.
","Jean-Francois Lalande|David Rodriguez|Christian Toinard","Ensi de Bourges, France|Ensi de Bourges, France|Ensi de Bourges, France","http://arxiv.org/abs/1004.0772v1","http://arxiv.org/pdf/1004.0772v1","","17Pages","International Journal of Network Security & Its Applications 1.3
  (2009) 73-89","","cs.CR","cs.CR|cs.NI"
"276","1004.4588v1","2010-04-26 18:10:20","2010-04-26 18:10:20","Investigation on QoS of Campus-wide WiFi Networks","  WiFi is widely implemented in campus wide including administrative, teaching
and student's accommodation. Wireless communications are associated with
interconnect devices which includes cellular networks, infrared, bluetooth and
WiFi enabled devices. It involves mobility and freedom of assessing information
anytime and anywhere. A study on WiFi networks in a campus environment is
presented in this paper. The aim of the research was to investigate the
connectivity problems to WiFi networks. The study includes WiFi performance
analysis as well as network auditing. Channel overlapping and saturation
condition were some of the problems encountered. Different types of software
were used for analyzing the results.
","Norrozila Sulaiman|Che Yahaya Yaakub","","http://arxiv.org/abs/1004.4588v1","http://arxiv.org/pdf/1004.4588v1","","Norrozila Sulaiman and Che Yahaya Yaakub, ""Investigation on QoS of
  Campus-wide WiFi Networks"", Journal of Telecommunications, Volume 2, Issue 1,
  p12-16, April 2010","Journal of Telecommunications, Volume 2, Issue 1, p12-16, April
  2010","","cs.NI","cs.NI"
"277","1005.0212v1","2010-05-03 07:43:22","2010-05-03 07:43:22","Construction graphique d'entrepots et de magasins de donnees","  Nowadays, decisional systems have became a significant research topic in
databases. Data warehouses and data marts are the main elements of such
systems. This paper presents our decisional support system. We present
graphical interfaces which help the administrator to build data warehouses and
data marts. We present a data warehouse building interface based on an
object-oriented conceptual model. This model allows the warehouse data
historisation at three levels: attribute, class and environment. Also, we
present a data mart building interface which allows warehouse data to be
reorganised through a multidimensional object-oriented model.
","Frederic Bret|Olivier Teste","IRIT|IRIT","http://arxiv.org/abs/1005.0212v1","http://arxiv.org/pdf/1005.0212v1","","","Congr\`es INFormatique des ORganisations et Syst\`emes
  d'Information et de D\'ecision - INFORSID'99, La Garde : France (1999)","","cs.DB","cs.DB"
"278","1005.0280v6","2010-05-03 13:08:23","2011-09-26 19:47:16","Superconductivity as a consequence of an ordering of the electron gas
  zero-point oscillations","  This paper has been administratively withdrawn by arXiv, duplicate of
arXiv:1008.2691.
","Boris V. Vasiliev","","http://arxiv.org/abs/1005.0280v6","http://arxiv.org/pdf/1005.0280v6","http://dx.doi.org/10.1016/j.physc.2010.10.010","This paper has been administratively withdrawn by arXiv, duplicate of
  arXiv:1008.2691","","10.1016/j.physc.2010.10.010","physics.gen-ph","physics.gen-ph"
"279","1005.0972v1","2010-05-06 10:47:03","2010-05-06 10:47:03","Adaptive Tuning Algorithm for Performance tuning of Database Management
  System","  Performance tuning of Database Management Systems(DBMS) is both complex and
challenging as it involves identifying and altering several key performance
tuning parameters. The quality of tuning and the extent of performance
enhancement achieved greatly depends on the skill and experience of the
Database Administrator (DBA). As neural networks have the ability to adapt to
dynamically changing inputs and also their ability to learn makes them ideal
candidates for employing them for tuning purpose. In this paper, a novel tuning
algorithm based on neural network estimated tuning parameters is presented. The
key performance indicators are proactively monitored and fed as input to the
Neural Network and the trained network estimates the suitable size of the
buffer cache, shared pool and redo log buffer size. The tuner alters these
tuning parameters using the estimated values using a rate change computing
algorithm. The preliminary results show that the proposed method is effective
in improving the query response time for a variety of workload types. .
","S. F. Rodd|U. P. Kulkarni","","http://arxiv.org/abs/1005.0972v1","http://arxiv.org/pdf/1005.0972v1","","IEEE Publication format, International Journal of Computer Science
  and Information Security, IJCSIS, Vol. 8 No. 1, April 2010, USA. ISSN 1947
  5500, http://sites.google.com/site/ijcsis/","","","cs.DB","cs.DB"
"280","1005.3569v1","2010-05-19 23:25:32","2010-05-19 23:25:32","The 1998 November 14 Occultation of GSC 0622-00345 by Saturn. I.
  Techniques for Ground-Based Stellar Occultations","  On 1998 November 14, Saturn and its rings occulted the star GSC 0622-00345.
We observed atmospheric immersion with NSFCAM at the National Aeronautics and
Space Administration's Infrared Telescope Facility on Mauna Kea, Hawaii.
Immersion occurred at 55.5\circ S planetocentric latitude. A 2.3 {\mu}m,
methane-band filter suppressed reflected sunlight. Atmospheric emersion and
ring data were not successfully obtained. We describe our observation,
light-curve production, and timing techniques, including improvements in
aperture positioning, removal of telluric scintillation effects, and timing.
Many of these techniques are known within the occultation community, but have
not been described in the reviewed literature. We present a light curve whose
signal-to-noise ratio per scale height is 267, among the best ground-based
signals yet achieved, despite a disadvantage of up to 8 mag in the stellar flux
compared to prior work.
","Joseph Harrington|Richard G. French","","http://arxiv.org/abs/1005.3569v1","http://arxiv.org/pdf/1005.3569v1","http://dx.doi.org/10.1088/0004-637X/716/1/398","LaTeX/emulateapj, 6 pages, 3 figures. Online items: The FITS-format
  light curve and the IDL code for the timing model are available from ApJ or
  the lead author","The Astrophysical Journal, 716:398-403, 2010 June 10","10.1088/0004-637X/716/1/398","astro-ph.EP","astro-ph.EP"
"281","1005.5271v1","2010-05-28 11:55:04","2010-05-28 11:55:04","A Restful Approach for Managing Citizen profiles Using A Semantic
  Support","  Several steps are missing in the current high-speed race towards the holistic
support of citizen needs in the domain of eGovernment. This paper is focused on
how to provide support for the citizen profile. This profile, in a wide sense,
includes personal information as well documents in possession of the citizen.
This also involves the provision of those mechanisms required to publish,
access and submit the convenient information to a Public Administration in due
curse of a transactional services provided with the last one. Main features of
the system are related to interoperability and possibilities for its inclusion
in a cost effective manner in already developed platforms. To make that
possible, this approach will take full advantage of semantic technologies and
the RESTful paradigm to design the entire system. The paper presents the
overall system with some notes on the deployment of the solution for its
further reuse in similar contexts.
","Luis Alvarez Sabucedo|Luis Anido Rifon","","http://arxiv.org/abs/1005.5271v1","http://arxiv.org/pdf/1005.5271v1","http://dx.doi.org/10.5121/ijdms.2010.2201","18 Pages, IJDMS","International Journal of Database Management Systems 2.2 (2010)
  1-18","10.5121/ijdms.2010.2201","cs.IR","cs.IR"
"282","1005.5606v1","2010-05-31 07:08:34","2010-05-31 07:08:34","Implementation of a Cloud Data Server (CDS) for Providing Secure Service
  in E-Business","  Cloud Data Servers is the novel approach for providing secure service to
e-business .Millions of users are surfing the Cloud for various purposes,
therefore they need highly safe and persistent services. Usually hackers target
particular Operating Systems or a Particular Controller. Inspiteof several
ongoing researches Conventional Web Servers and its Intrusion Detection System
might not be able to detect such attacks. So we implement a Cloud Data Server
with Session Controller Architecture using Redundancy and Disconnected Data
Access Mechanism. In this paper, we generate the hash code using MD5 algorithm.
With the help of which we can circumvent even the attacks, which are undefined
by traditional Systems .we implement Cloud Data Sever using Java and Hash Code
backup Management using My SQL. Here we Implement AES Algorithm for providing
more Security for the hash Code. The CDS using the Virtual Controller controls
and monitors the Connections and modifications of the page so as to prevent
malicious users from hacking the website. In the proposed approach an activity
analyzer takes care of intimating the administrator about possible intrusions
and the counter measures required to tackle them. The efficiency ratio of our
approach is 98.21% compared with similar approaches.
","D. Kesavaraja|R. Balasubramanian|D. Sasireka","Sivanthi Aditanar College of Engineering, India|Manonmaniam Sundaranar University, India|PSN College of Engineering and Technology, India","http://arxiv.org/abs/1005.5606v1","http://arxiv.org/pdf/1005.5606v1","http://dx.doi.org/10.5121/ijdms.2010.2203","12 Pages, IJDMS 2010","International Journal of Database Management Systems 2.2 (2010)
  44-55","10.5121/ijdms.2010.2203","cs.DC","cs.DC"
"283","1006.1563v1","2010-06-08 14:12:08","2010-06-08 14:12:08","ToLeRating UR-STD","  A new emerging paradigm of Uncertain Risk of Suspicion, Threat and Danger,
observed across the field of information security, is described. Based on this
paradigm a novel approach to anomaly detection is presented. Our approach is
based on a simple yet powerful analogy from the innate part of the human immune
system, the Toll-Like Receptors. We argue that such receptors incorporated as
part of an anomaly detector enhance the detector's ability to distinguish
normal and anomalous behaviour. In addition we propose that Toll-Like Receptors
enable the classification of detected anomalies based on the types of attacks
that perpetrate the anomalous behaviour. Classification of such type is either
missing in existing literature or is not fit for the purpose of reducing the
burden of an administrator of an intrusion detection system. For our model to
work, we propose the creation of a taxonomy of the digital Acytota, based on
which our receptors are created.
","Jan Feyereisl|Uwe Aickelin","","http://arxiv.org/abs/1006.1563v1","http://arxiv.org/pdf/1006.1563v1","","7 pages, 4 figures, 1 table, 2nd International Conference on Emerging
  Security Information, Systems and Technologies,","Proceedings of the 2nd International Conference on Emerging
  Security Information, Systems and Technologies, Cap Esterel, France, p
  287-293, 2008","","cs.AI","cs.AI|cs.CR|cs.NE"
"284","1006.1916v1","2010-06-09 22:21:00","2010-06-09 22:21:00","Building Computer Network Attacks","  In this work we start walking the path to a new perspective for viewing
cyberwarfare scenarios, by introducing conceptual tools (a formal model) to
evaluate the costs of an attack, to describe the theater of operations,
targets, missions, actions, plans and assets involved in cyberwarfare attacks.
We also describe two applications of this model: autonomous planning leading to
automated penetration tests, and attack simulations, allowing a system
administrator to evaluate the vulnerabilities of his network.
","Ariel Futoransky|Luciano Notarfrancesco|Gerardo Richarte|Carlos Sarraute","CoreLabs, Core Security Technologies|CoreLabs, Core Security Technologies|CoreLabs, Core Security Technologies|CoreLabs, Core Security Technologies","http://arxiv.org/abs/1006.1916v1","http://arxiv.org/pdf/1006.1916v1","","CoreLabs Technical Report","","","cs.CR","cs.CR|cs.AI"
"285","1006.3180v1","2010-06-16 10:46:04","2010-06-16 10:46:04","H2O: An Autonomic, Resource-Aware Distributed Database System","  This paper presents the design of an autonomic, resource-aware distributed
database which enables data to be backed up and shared without complex manual
administration. The database, H2O, is designed to make use of unused resources
on workstation machines. Creating and maintaining highly-available, replicated
database systems can be difficult for untrained users, and costly for IT
departments. H2O reduces the need for manual administration by autonomically
replicating data and load-balancing across machines in an enterprise.
Provisioning hardware to run a database system can be unnecessarily costly as
most organizations already possess large quantities of idle resources in
workstation machines. H2O is designed to utilize this unused capacity by using
resource availability information to place data and plan queries over
workstation machines that are already being used for other tasks. This paper
discusses the requirements for such a system and presents the design and
implementation of H2O.
","Angus Macdonald|Alan Dearle|Graham Kirby","","http://arxiv.org/abs/1006.3180v1","http://arxiv.org/pdf/1006.3180v1","","Presented at SICSA PhD Conference 2010 (http://www.sicsaconf.org/)","","","cs.DB","cs.DB|cs.DC"
"286","1006.4652v1","2010-06-23 22:07:30","2010-06-23 22:07:30","Organizational structure and communication networks in a university
  environment","  The ``six degrees of separation"" between any two individuals on Earth has
become emblematic of the 'small world' theme, even though the information
conveyed via a chain of human encounters decays very rapidly with increasing
chain length, and diffusion of information via this process may be very
inefficient in large human organizations. The information flow on a
communication network in a large organization, the University of Oslo, has been
studied by analyzing e-mail records. The records allow for quantification of
communication intensity across organizational levels and between organizational
units (referred to as ``modules""). We find that the number of e-mails messages
within modules scales with module size to the power of $1.29\pm .06$, and the
frequency of communication between individuals decays exponentially with the
number of links required upwards in the organizational hierarchy before they
are connected. Our data also indicates that the number of messages sent by
administrative units is proportional to the number of individuals at lower
levels in the administrative hierarchy, and the ``divergence of information""
within modules is associated with this linear relationship. The observed
scaling is consistent with a hierarchical system in which individuals far apart
in the organization interact little with each other and receive a
disproportionate number of messages from higher levels in the administrative
hierarchy.
","Joachim Mathiesen|Bj©ªrn Jamtveit|Kim Sneppen","","http://arxiv.org/abs/1006.4652v1","http://arxiv.org/pdf/1006.4652v1","http://dx.doi.org/10.1103/PhysRevE.82.016104","9 pages, 3 figures","Phys. Rev. E 82, 016104 (2010)","10.1103/PhysRevE.82.016104","physics.soc-ph","physics.soc-ph|cond-mat.stat-mech"
"287","1006.5804v1","2010-06-30 08:39:25","2010-06-30 08:39:25","Observation-Driven Configuration of Complex Software Systems","  The ever-increasing complexity of software systems makes them hard to
comprehend, predict and tune due to emergent properties and non-deterministic
behaviour. Complexity arises from the size of software systems and the wide
variety of possible operating environments: the increasing choice of platforms
and communication policies leads to ever more complex performance
characteristics. In addition, software systems exhibit different behaviour
under different workloads. Many software systems are designed to be
configurable so that policies can be chosen to meet the needs of various
stakeholders. For complex software systems it can be difficult to accurately
predict the effects of a change and to know which configuration is most
appropriate. This thesis demonstrates that it is useful to run automated
experiments that measure a selection of system configurations. Experiments can
find configurations that meet the stakeholders' needs, find interesting
behavioural characteristics, and help produce predictive models of the system's
behaviour. The design and use of ACT (Automated Configuration Tool) for running
such experiments is described, in combination a number of search strategies for
deciding on the configurations to measure. Design Of Experiments (DOE) is
discussed, with emphasis on Taguchi Methods. These statistical methods have
been used extensively in manufacturing, but have not previously been used for
configuring software systems. The novel contribution here is an industrial case
study, applying the combination of ACT and Taguchi Methods to DC-Directory, a
product from Data Connection Ltd (DCL). The case study investigated the
applicability of Taguchi Methods for configuring complex software systems.
Taguchi Methods were found to be useful for modelling and configuring DC-
Directory, making them a valuable addition to the techniques available to
system administrators and developers.
","Aled Sage","","http://arxiv.org/abs/1006.5804v1","http://arxiv.org/pdf/1006.5804v1","","PhD Thesis, University of St Andrews, 2003. Supervisor: Graham Kirby","","","cs.SE","cs.SE"
"288","1007.0908v2","2010-07-06 14:54:35","2010-10-15 19:19:23","Modeling Super-spreading Events for Infectious Diseases: Case Study SARS","  Super-spreading events for infectious diseases occur when some infected
individuals infect more than the average number of secondary cases. Several
super-spreading individuals have been identified for the 2003 outbreak of
severe acute respiratory syndrome (SARS). We develop a model for
super-spreading events of infectious diseases, which is based on the outbreak
of SARS. Using this model we describe two methods for estimating the parameters
of the model, which we demonstrate with the small-scale SARS outbreak at the
Amoy Gardens, Hong Kong, and the large-scale outbreak in the entire Hong Kong
Special Administrative Region. One method is based on parameters calculated for
the classical susceptible - infected - removed (SIR) disease model. The second
is based on parameter estimates found in the literature. Using the parameters
calculated for the SIR model, our model predicts an outcome similar to that for
the SIR model. On the other hand, using parameter estimates from SARS
literature our model predicts a much more serious epidemic.
","Thembinkosi Mkhatshwa|Anna Mummert","","http://arxiv.org/abs/1007.0908v2","http://arxiv.org/pdf/1007.0908v2","","8 pages, 1 figure, 7 tables","","","q-bio.PE","q-bio.PE|92B"
"289","1007.1266v1","2010-07-08 00:19:13","2010-07-08 00:19:13","Network Anomaly Detection: Flow-based or Packet-based Approach?","  One of the most critical tasks for network administrator is to ensure system
uptime and availability. For the network security, anomaly detection systems,
along with firewalls and intrusion prevention systems are the must-have tools.
So far in the field of network anomaly detection, people are working on two
different approaches. One is flow-based; usually rely on network elements to
make so-called flow information available for analysis. The second approach is
packet-based; which directly analyzes the data packet information for the
detection of anomalies. This paper describes the main differences between the
two approaches through an in-depth analysis. We try to answer the question of
when and why an approach is better than the other. The answer is critical for
network administrators to make their choices in deploying a defending system,
securing the network and ensuring business continuity.
","Huy Nguyen|Deokjai Choi","","http://arxiv.org/abs/1007.1266v1","http://arxiv.org/pdf/1007.1266v1","","Published on Chonnam National University Networking Journal, Gwangju,
  Korea June 2008","","","cs.NI","cs.NI|C.2.3"
"290","1007.1383v1","2010-07-08 14:06:26","2010-07-08 14:06:26","A Model of the Number of Antibiotic Resistant Bacteria in Rivers","  The large reservoir of antibiotic resistant bacteria in raw and treated water
supplies is a matter of public health concern. Currently, the National
Antimicrobial Resistance Monitoring Systems, a collaborative effort of the
Centers for Disease Control, the US Department of Agriculture, and the US Food
and Drug Administration, does not monitor antimicrobial resistance in surface
waters. Given the serious nature of antibiotic resistance in clinical settings,
and the likelihood that antibiotic resistant bacteria can be transmitted to
humans from large environmental reservoirs via drinking water, explanations for
the distribution of antibiotic resistant bacteria and tools for studying this
distribution must be found. Here we focus on mathematical modeling of
cultivable bacteria in a river, which will be used to study the distribution of
antibiotic resistant bacteria in the environment. We consider both antibiotic
resistant and non-antibiotic resistant bacteria in the model, and, taking into
account the strong correlation between land use and antibiotic resistant
bacteria in rivers, we include a function for the influx of bacteria into the
river from the shore. We simulate the model for two different time scales and
show that if there is too many bacteria from the land entering the river, the
river entirely fills with antibiotic resistant bacteria, while less frequent
influxes allows time for the bacteria to lose the antibiotic resistant gene.
This mathematically verifies that reduction in antibiotic use near the banks of
rivers, will reduce the counts of antibiotic resistant bacteria in rivers.
","Bonita Lawrence|Anna Mummert|Charles Somerville","","http://arxiv.org/abs/1007.1383v1","http://arxiv.org/pdf/1007.1383v1","","18 pages, 5 figures, 2 tables","","","q-bio.PE","q-bio.PE|92B05"
"291","1007.4053v1","2010-07-23 06:50:52","2010-07-23 06:50:52","AstroGrid-D: Grid Technology for Astronomical Science","  We present status and results of AstroGrid-D, a joint effort of
astrophysicists and computer scientists to employ grid technology for
scientific applications. AstroGrid-D provides access to a network of
distributed machines with a set of commands as well as software interfaces. It
allows simple use of computer and storage facilities and to schedule or monitor
compute tasks and data management. It is based on the Globus Toolkit middleware
(GT4). Chapter 1 describes the context which led to the demand for advanced
software solutions in Astrophysics, and we state the goals of the project. We
then present characteristic astrophysical applications that have been
implemented on AstroGrid-D in chapter 2. We describe simulations of different
complexity, compute-intensive calculations running on multiple sites, and
advanced applications for specific scientific purposes, such as a connection to
robotic telescopes. We can show from these examples how grid execution improves
e.g. the scientific workflow. Chapter 3 explains the software tools and
services that we adapted or newly developed. Section 3.1 is focused on the
administrative aspects of the infrastructure, to manage users and monitor
activity. Section 3.2 characterises the central components of our architecture:
The AstroGrid-D information service to collect and store metadata, a file
management system, the data management system, and a job manager for automatic
submission of compute tasks. We summarise the successfully established
infrastructure in chapter 4, concluding with our future plans to establish
AstroGrid-D as a platform of modern e-Astronomy.
","Harry Enke|Matthias Steinmetz|Hans-Martin Adorf|Alexander Beck-Ratzka|Frank Breitling|Thomas Bruesemeister|Arthur Carlson|Torsten Ensslin|Mikael Hoegqvist|Iliya Nickelt|Thomas Radke|Alexander Reinefeld|Angelika Reiser|Tobias Scholl|Rainer Spurzem|Juergen Steinacker|Wolfgang Voges|Joachim Wambsganss|Steve White","AIP|AIP|MPA|AEI|AIP|ZAH|MPE|MPA|ZIB|AIP|AEI|ZIB|TUM|TUM|ZAH, NAOC|ZAH|MPE|ZAH|AIP","http://arxiv.org/abs/1007.4053v1","http://arxiv.org/pdf/1007.4053v1","http://dx.doi.org/10.1016/j.newast.2010.07.005","14 pages, 12 figures Subjects: data analysis, image processing,
  robotic telescopes, simulations, grid. Accepted for publication in New
  Astronomy","New Astron.16:79-93,2011","10.1016/j.newast.2010.07.005","cs.DC","cs.DC|astro-ph.IM|cs.DB|cs.NI"
"292","1007.4057v1","2010-07-23 07:37:32","2010-07-23 07:37:32","Precise, Scalable and Online Request Tracing for Multi-tier Services of
  Black Boxes","  As more and more multi-tier services are developed from commercial
off-the-shelf components or heterogeneous middleware without source code
available, both developers and administrators need a request tracing tool to
(1) exactly know how a user request of interest travels through services of
black boxes; (2) obtain macro-level user request behavior information of
services without the necessity of inundating within massive logs. Previous
research efforts either accept imprecision of probabilistic correlation methods
or present precise but unscalable tracing approaches that have to collect and
analyze large amount of logs; Besides, previous precise request tracing
approaches of black boxes fail to propose macro-level abstractions that enables
debugging performance-in-the-large, and hence users have to manually interpret
massive logs. This paper introduces a precise, scalable and online request
tracing tool, named PreciseTracer, for multi-tier services of black boxes. Our
contributions are four-fold: first, we propose a precise request tracing
algorithm for multi-tier services of black boxes, which only uses
application-independent knowledge; second, we respectively present micro-level
and macro-level abstractions: component activity graphs and dominated causal
path patterns to represent causal paths of each individual request and
repeatedly executed causal paths that account for significant fractions; third,
we present two mechanisms: tracing on demand and sampling to significantly
increase system scalability; fourth, we design and implement an online request
tracing tool. PreciseTracer's fast response, low overhead and scalability make
it a promising tracing tool for large-scale production systems.
","Bo Sang|Jianfeng Zhan|Zhihong Zhang|Lei Wang|Dongyan Xu|Yabing Huang|Dan Meng","","http://arxiv.org/abs/1007.4057v1","http://arxiv.org/pdf/1007.4057v1","","15 pages. 21 figures. This is an extended work of our paper: Z.
  Zhang, J. Zhan, Y. Li, L. Wang and D. Meng, Precise request tracing and
  performance debugging for multi-tier services of black boxes, The 39th
  IEEE/IFIP International Conference on Dependable Systems & Networks (DSN
  '09), 2009","","","cs.DC","cs.DC"
"293","1008.0616v2","2010-08-03 18:30:57","2013-04-25 20:15:52","Formaleuros, Formalbitcoins, and Virtual Monies","  Formalist positions towards money are considered from a perspective of formal
methods in computing. The Formaleuro (FEUR) as a dimension for monetary
quantities is proposed as well as the Formalbitcoin (FBTC) which represents an
item ready for circulation in a model of informational money.
  An attempt is made to understand the concept of money from scratch. In order
to provide a definition of money the need is felt to make use of a tailored
theory of definition. To that end a theory of imaginative definitions is
presented and its implications for definitions of money are sketched.
  It is argued that a theory of money may be dependent on the role of its
holder. A survey of some roles is given, with the so-called subordinate
administrative role (SAR) in a central position.
  The concepts of virtual memory and virtual machine are taken as the point of
departure for a definition of the notion of virtual money. It is argued that
from the perspective of a component (division) of a large organization (ORG)
its local financial system (LFS) provides a virtual money vm(LFS, ORG) which
may well fail to meet the most common general and acknowledged moneyness
criteria. Inverse moneyness preference is coined as phrase to assert the
tendency of top-management of ORG to make its virtual money deviate from these
criteria.
","Jan A. Bergstra","","http://arxiv.org/abs/1008.0616v2","http://arxiv.org/pdf/1008.0616v2","","98 pages. Revision of arXiv.org/abs/1008.0616v1. The revision
  involves bringing the paper up to date with the appearance of Bitcoin, a
  significant update of the section on Islamic finance and interest
  prohibition, deletion of fragments of minor importance, an update of
  references, moving the ad hoc theory of definitions to an appendix, and a
  small change of the title","","","cs.CY","cs.CY"
"294","1008.2767v1","2010-08-16 20:14:11","2010-08-16 20:14:11","A Light-Weight Communication Library for Distributed Computing","  We present MPWide, a platform independent communication library for
performing message passing between computers. Our library allows coupling of
several local MPI applications through a long distance network and is
specifically optimized for such communications. The implementation is
deliberately kept light-weight, platform independent and the library can be
installed and used without administrative privileges. The only requirements are
a C++ compiler and at least one open port to a wide area network on each site.
In this paper we present the library, describe the user interface, present
performance tests and apply MPWide in a large scale cosmological N-body
simulation on a network of two computers, one in Amsterdam and the other in
Tokyo.
","Derek Groen|Steven Rieder|Paola Grosso|Cees de Laat|Simon Portegies Zwart","Leiden|Leiden|Amsterdam|Amsterdam|Leiden","http://arxiv.org/abs/1008.2767v1","http://arxiv.org/pdf/1008.2767v1","http://dx.doi.org/10.1088/1749-4699/3/1/015002","17 pages, 10 figures, published in Computational Science & Discovery","Derek Groen et al 2010 Comput. Sci. Disc. 3 015002","10.1088/1749-4699/3/1/015002","cs.DC","cs.DC|68M14 (primary), 68M20, 85-08, 85A40 (secondary)|C.2.4; C.2.5"
"295","1009.0368v1","2010-09-02 10:00:51","2010-09-02 10:00:51","Discovering potential user browsing behaviors using custom-built apriori
  algorithm","  Most of the organizations put information on the web because they want it to
be seen by the world. Their goal is to have visitors come to the site, feel
comfortable and stay a while and try to know completely about the running
organization. As educational system increasingly requires data mining, the
opportunity arises to mine the resulting large amounts of student information
for hidden useful information (patterns like rule, clustering, and
classification, etc). The education domain offers ground for many interesting
and challenging data mining applications like astronomy, chemistry,
engineering, climate studies, geology, oceanography, ecology, physics, biology,
health sciences and computer science. Collecting the interesting patterns using
the required interestingness measures, which help us in discovering the
sophisticated patterns that are ultimately used for developing the site. We
study the application of data mining to educational log data collected from
Guru Nanak Institute of Technology, Ibrahimpatnam, India. We have proposed a
custom-built apriori algorithm to find the effective pattern analysis. Finally,
analyzing web logs for usage and access trends can not only provide important
information to web site developers and administrators, but also help in
creating adaptive web sites.
","Sandeep Singh Rawat|Lakshmi Rajamani","","http://arxiv.org/abs/1009.0368v1","http://arxiv.org/pdf/1009.0368v1","","10 pages","","","cs.DB","cs.DB"
"296","1009.2018v1","2010-09-10 14:07:10","2010-09-10 14:07:10","INSA scientific activities in the space astronomy area","  Support to Astronomy operations is an important and long-lived activity
within INSA. Probably the best known (and traditional) INSA activities are
those related with real-time spacecraft operations: Ground station maintenance
and operation (Ground station engineers and operators); spacecraft and payload
real-time operation (spacecraft and instruments controllers); computing
infrastructure maintenance (operators, analysts) and general site services.In
this paper, we'll show a different perspective, probably not so well-known,
presenting some INSA recent activities at the European Space Astronomy Centre
(ESAC) and NASA Madrid Deep Space Communication Complex (MDSCC) directly
related to scientific operations. Basic lines of activity involved include:
Operations support for science operations; system and software support for real
time systems; technical administration and IT support; R \& D activities,
radioastronomy (at MDSCC and ESAC) and scientific research projects. This paper
is structured as follows: first, INSA activities in two ESA cornerstone
astrophysics missions, XMM-Newton and Herschel, will be outlined. Then, our
activities related to Science infrastructure services, represented by the
Virtual Observatory (VO) framework and the Science Archives development
facilities are briefly shown. Radio Astronomy activities will be described
afterwards, and finally, a few research topics in which INSA scientists are
involved will be also described.
","Ricardo Perez Martinez|Miguel Sanchez Portal","","http://arxiv.org/abs/1009.2018v1","http://arxiv.org/pdf/1009.2018v1","http://dx.doi.org/10.1007/978-3-642-11250-8_160","6 pages. Highlights of Spanish Astrophysics V Proceedings of the VIII
  Scientific Meeting of the Spanish AstronomicalSociety (SEA) held in
  Santander, 7-11 July, 2008","Highlights of Spanish Astrophysics V, 2010, 339-341","10.1007/978-3-642-11250-8_160","astro-ph.IM","astro-ph.IM"
"297","1009.2368v1","2010-09-13 12:39:02","2010-09-13 12:39:02","Network evolution and QOS provisioning for integrated
  femtocell/macrocell networks","  Integrated femtocell/macrocell networks, comprising a conventional cellular
network overlaid with femtocells, offer an economically appealing way to
improve coverage, quality of service, and access network capacity. The key
element to successful femtocells/macrocell integration lies in its
self-organizing capability. Provisioning of quality of service is the main
technical challenge of the femtocell/macrocell integrated networks, while the
main administrative challenge is the choice of the proper evolutionary path
from the existing macrocellular networks to the integrated network. In this
article, we introduce three integrated network architectures which, while
increasing the access capacity, they also reduce the deployment and operational
costs. Then, we discuss a number of technical issues, which are key to making
such integration a reality, and we offer possible approaches to their solution.
These issues include efficient frequency and interference management, quality
of service provisioning of the xDSL-based backhaul networks, and intelligent
handover control.
","Mostafa Zaman Chowdhury|Yeong Min Jang|Zygmunt J. Haas","","http://arxiv.org/abs/1009.2368v1","http://arxiv.org/pdf/1009.2368v1","","16 pages, 10 figures, Published in International Journal of Wireless
  & Mobile Networks (IJWMN)","International Journal of Wireless & Mobile Networks (IJWMN),
  Vol.2, No.3, August 2010","","cs.NI","cs.NI|cs.MM"
"298","1009.6045v1","2010-09-30 06:22:41","2010-09-30 06:22:41","A proposed ""osi based"" network troubles identification model","  The OSI model, developed by ISO in 1984, attempts to summarize complicated
network cases on layers. Moreover, network troubles are expressed by taking the
model into account. However, there has been no standardization for network
troubles up to now. Network troubles have only been expressed by the name of
the related layer. In this paper, it is pointed out that possible troubles on
the related layer vary and possible troubles on each layer are categorized for
functional network administration and they are standardized in an eligible way.
The proposed model for network trouble shooting was developed considering the
OSI model.
","Murat Kayri|<U+0130>smail Kayri","","http://arxiv.org/abs/1009.6045v1","http://arxiv.org/pdf/1009.6045v1","http://dx.doi.org/10.5121/ijngn.2010.2302","7 pages","International Journal of Next-Generation Networks (IJNGN) Vol.2,
  No.3, September 2010","10.5121/ijngn.2010.2302","cs.NI","cs.NI"
"299","1010.0325v2","2010-10-02 12:04:31","2010-10-21 19:49:02","Routing Protocols for Cognitive Radio Networks: A Survey","  This article has been withdrawn by arXiv administrators because it
plagiarises http://www2.ece.ohio-state.edu/~ekici/papers/crnroutingsurvey.pdf
","S. M. Kamruzzaman|Dong Geun Jeong","","http://arxiv.org/abs/1010.0325v2","http://arxiv.org/pdf/1010.0325v2","","This article has been withdrawn by arXiv administrators because it
  plagiarises http://www2.ece.ohio-state.edu/~ekici/papers/crnroutingsurvey.pdf","Journal of Information Industrial Engineering, Vol. 16, pp.
  153-169, Aug. 2010","","cs.NI","cs.NI"
"300","1010.1613v1","2010-10-08 07:21:07","2010-10-08 07:21:07","Nonparametric inference procedure for percentiles of the random effects
  distribution in meta-analysis","  To investigate whether treating cancer patients with
erythropoiesis-stimulating agents (ESAs) would increase the mortality risk,
Bennett et al. [Journal of the American Medical Association 299 (2008)
914--924] conducted a meta-analysis with the data from 52 phase III trials
comparing ESAs with placebo or standard of care. With a standard parametric
random effects modeling approach, the study concluded that ESA administration
was significantly associated with increased average mortality risk. In this
article we present a simple nonparametric inference procedure for the
distribution of the random effects. We re-analyzed the ESA mortality data with
the new method. Our results about the center of the random effects distribution
were markedly different from those reported by Bennett et al. Moreover, our
procedure, which estimates the distribution of the random effects, as opposed
to just a simple population average, suggests that the ESA may be beneficial to
mortality for approximately a quarter of the study populations. This new
meta-analysis technique can be implemented with study-level summary statistics.
In contrast to existing methods for parametric random effects models, the
validity of our proposal does not require the number of studies involved to be
large. From the results of an extensive numerical study, we find that the new
procedure performs well even with moderate individual study sample sizes.
","Rui Wang|Lu Tian|Tianxi Cai|L. J. Wei","","http://arxiv.org/abs/1010.1613v1","http://arxiv.org/pdf/1010.1613v1","http://dx.doi.org/10.1214/09-AOAS280","Published in at http://dx.doi.org/10.1214/09-AOAS280 the Annals of
  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of
  Mathematical Statistics (http://www.imstat.org)","Annals of Applied Statistics 2010, Vol. 4, No. 1, 520-532","10.1214/09-AOAS280","stat.AP","stat.AP"
"301","1010.2548v2","2010-10-13 00:15:15","2010-10-21 19:01:25","Thermodynamics of unparticle-enhanced black holes","  This article has been withdrawn by arXiv administrators because it
plagiarises arXiv:1006.4556 (published in Phys. Lett. B 693, 129 (2010)).
","Yun Soo Myung","","http://arxiv.org/abs/1010.2548v2","http://arxiv.org/pdf/1010.2548v2","","This article has been withdrawn by arXiv administrators","","","gr-qc","gr-qc|hep-ph"
"302","1010.4986v1","2010-10-24 18:24:44","2010-10-24 18:24:44","A Testbed Implementation for Securing OLSR in Mobile Ad hoc Networks","  Contemporary personal computing devices are increasingly required to be
portable and mobile enabling user's wireless access, to wired network
infrastructures and services. This approach to mobile computing and
communication is only appropriate in situations where a coherent infrastructure
is available. There are many situations where these requirements are not
fulfilled such as; developing nations, rural areas, natural disasters, and
military conflicts to name but a few. A practical solution is to use mobile
devices interconnected via a wireless medium to form a network, known as a
Mobile Ad-hoc Network (MANET), and provide the services normally found in wired
networks. Security in MANETs is an issue of paramount importance due to the
wireless nature of the communication links. Additionally due to the lack of
central administration security issues are different from conventional
networks. For the purposes of this article we have used the ""WMN test-bed"" to
enable secure routing in MANETs. The use of cryptography is an efficient proven
way of securing data in communications, but some cryptographic algorithms are
not as efficient as others and require more processing power, which is
detrimental to MANETs. In this article we have assessed different cryptographic
approaches to securing the OLSR (Optimised Link State Routing) protocol to
provide a basis for research. We conclude the paper with a series of
performance evaluation results regarding different cryptographic and hashing
schemes. Our findings clearly show that the most efficient combination of
algorithms used for authentication and encryption are SHA-1 and AES
respectively. Using this combination over their counterparts will lead to a
considerable reduction in processing time and delay on the network, creating an
efficient transaction moving towards satisfying resource constraints and
security requirements.
","Emmanouil A. Panaousis|George Drew|Grant P. Millar|Tipu A. Ramrekha|Christos Politis","","http://arxiv.org/abs/1010.4986v1","http://arxiv.org/pdf/1010.4986v1","","20 pages, 13 figures","E.A. Panaousis, G. Drew, G. Millar, T.A. Ramrekha, C. Politis, ""A
  Test-bed Implementation for Securing OLSR in Mobile Ad-hoc Networks,""
  International Journal of Network Security & Its Applications, Vol. 2, No. 4,
  October, 2010","","cs.NI","cs.NI"
"303","1011.0539v1","2010-11-02 08:38:50","2010-11-02 08:38:50","Performance Analysis of Contention Window Cheating Misbehaviors in
  Mobile Ad Hoc Networks","  Mobile Ad Hoc Network (MANET) is a collection of nodes that can be rapidly
deployed as a multi-hop network without the aid of any centralized
administration. Misbehavior is challenged by bandwidth and energy efficient
medium access control and fair share of throughput. Node misbehavior plays an
important role in MANET. In this survey, few of the contention window
misbehavior is reviewed and compared. The contention window cheating either
minimizes the active communication of the network or reduces bandwidth
utilization of a particular node. The classification presented is in no case
unique but summarizes the chief characteristics of many published proposals for
contention window cheating. After getting insight into the different contention
window misbehavior, few of the enhancements that can be done to improve the
existing contention window are suggested. The purpose of this paper is to
facilitate the research efforts in combining the existing solutions to offer
more efficient methods to reduce contention window cheating mechanisms.
","R. Kalaiarasi|Getsy S. Sara|S. Neelavathy Pari|D. Sridharan","","http://arxiv.org/abs/1011.0539v1","http://arxiv.org/pdf/1011.0539v1","","12 pages,4 figures, 1 table","International journal of computer science & information Technology
  (IJCSIT) Vol.2, No.5, October 2010","","cs.NI","cs.NI"
"304","1011.1160v1","2010-11-04 13:52:00","2010-11-04 13:52:00","A Conversation with James Hannan","  Jim Hannan is a professor who has lived an interesting life and one whose
fundamental research in repeated games was not fully appreciated until late in
his career. During his service as a meteorologist in the Army in World War II,
Jim played poker and made weather forecasts. It is curious that his later
research included strategies for repeated play that apply to selecting the best
forecaster. James Hannan was born in Holyoke, Massachusetts on September 14,
1922. He attended St. Jerome's High School and in January 1943 received the
Ph.B. from St. Michael's College in Colchester, Vermont. Jim enlisted in the US
Army Air Force to train and serve as a meteorologist. This took him to army
airbases in China by the close of the war. Following discharge from the army,
Jim studied mathematics at Harvard and graduated with the M.S. in June 1947. To
prepare for doctoral work in statistics at the University of North Carolina
that fall, Jim went to the University of Michigan in the summer of 1947. The
routine admissions' physical revealed a spot on the lung and the possibility of
tuberculosis. This caused Jim to stay at Ann Arbor through the fall of 1947 and
then at a Veterans Administration Hospital in Framingham, Massachusetts to have
his condition followed more closely. He was discharged from the hospital in the
spring and started his study at Chapel Hill in the fall of 1948. There he began
research in compound decision theory under Herbert Robbins. Feeling the need
for teaching experience, Jim left Chapel Hill after two years and short of
thesis to take a three year appointment as an instructor at Catholic University
in Washington, DC. When told that renewal was not coming, Jim felt pressure to
finish his degree.
","Dennis Gilliland|R. V. Ramamoorthi","","http://arxiv.org/abs/1011.1160v1","http://arxiv.org/pdf/1011.1160v1","http://dx.doi.org/10.1214/09-STS283","Published in at http://dx.doi.org/10.1214/09-STS283 the Statistical
  Science (http://www.imstat.org/sts/) by the Institute of Mathematical
  Statistics (http://www.imstat.org)","Statistical Science 2010, Vol. 25, No. 1, 126-144","10.1214/09-STS283","stat.OT","stat.OT"
"305","1011.2105v1","2010-11-07 02:25:01","2010-11-07 02:25:01","Towards Greener and Safer Mines","  Miniaturised sensors and networking are technical proven concepts. Both the
technologies are proven and various components e.g., sensors, controls, etc.
are commercially available. Technology scene in above areas presents enormous
possibilities for developing innovative applications for real life situations.
Mining operations in many countries have lot of scope for improving
environmental and safety measures. Efforts have been made to develop a system
to efficiently monitor a particular environment by deploying a wireless sensor
network using commercially available components. Wireless Sensor Network has
been integrated with telecom network through a gateway using a suitable
topology which can be selected at the application layer. The developed system
demonstrates a way to connect wireless sensor network to external network which
enables the distant administrator to access real time data and act expediently
from long-distance to improve the environmental situation or prevent a
disaster. Potentially, it can be used to avoid the awful situations leading to
terrible environment in underground mines. Keywords: Wireless sensor network,
Mine safety, Environment monitoring and telecom.
","Dhruv Srivastava","","http://arxiv.org/abs/1011.2105v1","http://arxiv.org/pdf/1011.2105v1","","7 pages,9 figures","","","cs.OH","cs.OH"
"306","1011.5746v2","2010-11-26 10:21:14","2011-09-07 15:19:03","Intutionistic Fuzzy Ideals in ¥Ã-semiring","  This article has been withdrawn by arXiv administrators due to plagiarized
content from arXiv:1010.2469.
","Nayyar Mehmood|I. H Qureshi","","http://arxiv.org/abs/1011.5746v2","http://arxiv.org/pdf/1011.5746v2","","This article has been withdrawn by arXiv administrators due to
  plagiarized content from arXiv:1010.2469","","","math.LO","math.LO"
"307","1012.2720v1","2010-12-13 13:10:12","2010-12-13 13:10:12","Managing Delegation in Access Control Models","  In the field of access control, delegation is an important aspect that is
considered as a part of the administration mechanism. Thus, a complete access
control must provide a flexible administration model to manage delegation.
Unfortunately, to our best knowledge, there is no complete model for describing
all delegation requirements for role-based access control. Therefore, proposed
models are often extended to consider new delegation characteristics, which is
a complex task to manage and necessitate the redefinition of these models. In
this paper we describe a new delegation approach for extended role-based access
control models. We show that our approach is flexible and is sufficient to
manage all delegation requirements.
","Meriam Ben Ghorbel-Talbi|Frederic Cuppens|Nora Cuppens-Boulahia|Adel Bouhoula","","http://arxiv.org/abs/1012.2720v1","http://arxiv.org/pdf/1012.2720v1","http://dx.doi.org/10.1109/ADCOM.2007.105","","Proceedings of the 15th International Conference on Advanced
  Computing and Communication (ADCOM'07), 2007","10.1109/ADCOM.2007.105","cs.CR","cs.CR"
"308","1012.5590v1","2010-12-27 10:53:25","2010-12-27 10:53:25","Automated Symbolic Analysis of ARBAC-Policies (Extended Version)","  One of the most widespread framework for the management of access-control
policies is Administrative Role Based Access Control (ARBAC). Several automated
analysis techniques have been proposed to help maintaining desirable security
properties of ARBAC policies. One limitation of many available techniques is
that the sets of users and roles are bounded. In this paper, we propose a
symbolic framework to overcome this difficulty. We design an automated security
analysis technique, parametric in the number of users and roles, by adapting
recent methods for model checking infinite state systems that use first-order
logic and state-of-the-art theorem proving techniques. Preliminary experiments
with a prototype implementations seem to confirm the scalability of our
technique.
","A. Armando|S. Ranise","","http://arxiv.org/abs/1012.5590v1","http://arxiv.org/pdf/1012.5590v1","","Long version of the paper entitled ""Automated Symbolic Analysis of
  ARBAC Policies"" published in Proc. of 6th Int. Workshop on Security and Trust
  Management (co-located with EUROPKI'10, CRITIS'10, and ESORICS'10), Athens,
  Sept. 23-24 (2010). Also, to appear in LNCS","","","cs.CR","cs.CR|cs.LO"
"309","1101.1252v2","2011-01-06 16:51:56","2011-01-11 13:59:17","Scientific data searching, sharing and retrieval","  In the recent years, there has been significant advancement in the areas of
scientific data management and retrieval techniques, especially in terms of
standards and protocols for archiving data. Oak Ridge National Laboratory
Distributed Data Archive Center for biogeochemical dynamics is making efforts
in building advanced toolsets for these purposes. Mercury is a web-based
metadata harvesting, data discovery and access system, built for researchers to
search for, share and obtain biogeochemical data. Originally developed for
single National Aeronautics and Space Administration (NASA) project, Mercury
now used over fourteen different projects across three US federal agencies.
Mercury renders various capabilities including metadata management, indexing,
searching, data sharing, and also software reusability.
","Ranjeet Devarakonda|Giri Palanisamy","","http://arxiv.org/abs/1101.1252v2","http://arxiv.org/pdf/1101.1252v2","","","","","cs.IT","cs.IT|math.IT"
"310","1101.2341v1","2011-01-12 12:38:36","2011-01-12 12:38:36","The VLT dealing with the Atmosphere, a Night Operation point of view","  The Science Operation Department is composed of Astronomers, Telescope
Instruments Operators (TIO) and Data Handling Administrators (DHAs). Their main
goal is to produce top-quality astronomical data by operating a suite of 9
telescopes, 14 Instruments and related systems, supporting the execution of
Visitor Mode Observations or executing Service Mode Observations. Astronomers
and TIOs have to deal with atmospheric parameters like seeing, coherence time,
isoplanatic angle, precipitable water vapor, etc. in order to take in real time
the best decisions on the best program to be executed according to the current
conditions. We describe the tools available in the control room, provided by
the environmental monitoring and forecast system.
","Julio Navarrete","","http://arxiv.org/abs/1101.2341v1","http://arxiv.org/pdf/1101.2341v1","","Contribution in conference ""Comprehensive characterization of
  astronomical sites"", held October 4-10, 2010, in Kislovodsk, Russia, 11
  pages, 15 figures","","","astro-ph.IM","astro-ph.IM"
"311","1101.5763v1","2011-01-30 11:33:51","2011-01-30 11:33:51","A New Semantic Web Approach for Constructing, Searching and Modifying
  Ontology Dynamically","  Semantic web is the next generation web, which concerns the meaning of web
documents It has the immense power to pull out the most relevant information
from the web pages, which is also meaningful to any user, using software
agents. In today's world, agent communication is not possible if concerned
ontology is changed a little. We have pointed out this very problem and
developed an Ontology Purification System to help agent communication. In our
system you can send queries and view the search results. If it can't meet the
criteria then it finds out the mismatched elements. Modification is done within
a second and you can see the difference. That's why we emphasis on the word
dynamic. When Administrator is updating the system, at the same time that
updation is visible to the user.
","Debajyoti Mukhopadhyay|Chandrima Chakrabarti|Sounak Chakravorty","","http://arxiv.org/abs/1101.5763v1","http://arxiv.org/pdf/1101.5763v1","","6 pages, 14 figures","","","cs.IR","cs.IR"
"312","1102.0112v1","2011-02-01 10:07:48","2011-02-01 10:07:48","Obtaining Traffic Information by Urban Air Quality Inspection","  The level of air quality in urban centres is affected by emission of several
pollutants, mainly coming from the vehicles flowing in their road networks.
This is a well known phenomenon that influences the quality of life of people.
Despite the deep concern of researchers and technicians, we are far from a
total understanding of this phenomenon. On the contrary, the availability of
reliable forecasting models would constitute an important tool for
administrators in order of assessing suitable actions concerning the
transportation policies, public as well private. Referring to the situation of
the running fleet and the measured pollutant concentrations concerning the
Italian town of Palermo, a data-deduced traffic model is here derived, its
truthfulness being justified by a fuzzyfication of the phenomenon. A first
validation of the model is supplied by utilising the emissions characteristics
and the pollutant concentrations referring to a two years period of time. This
work could represent a first attempt in defining a new approach to the problem
of the pollution of the urban contexts, in order of providing administrators
with a reliable and easier tool.
","P. Ferrante|D. Lo Bosco|S. Nicolosi|G. Scaccianoce|M. Traverso|G. Rizzo","","http://arxiv.org/abs/1102.0112v1","http://arxiv.org/pdf/1102.0112v1","","","","","physics.flu-dyn","physics.flu-dyn|physics.ao-ph"
"313","1102.0583v1","2011-02-02 23:04:49","2011-02-02 23:04:49","Thin Client Web-Based Campus Information Systems for Fiji National
  University","  Fiji National University is encountering many difficulties with its current
administrative systems. These difficulties include accessibility, scalability,
performance, flexibility and integration. We propose a new campus information
system, FNU-CIS to addresses these difficulties. FNU-CIS has the potential to
provide wide range of the services for students and staffs at the university.
In order to assist in the design and implementation of proposed FNU-CIS, we
present an overview, software architecture and prototype implementation of our
proposed system. We discuss the key properties of our system, compare it with
other similar systems available and outline our future plans for research in
FNU-CIS implementation.
","Bimal Aklesh Kumar","","http://arxiv.org/abs/1102.0583v1","http://arxiv.org/pdf/1102.0583v1","http://dx.doi.org/10.5121/ijsea.2011.2102","14 pages","International Journal of Software Engineering & Applications
  (IJSEA), Vol.2, No.1, January 2011","10.5121/ijsea.2011.2102","cs.SE","cs.SE"
"314","1102.1588v2","2011-02-08 12:35:26","2011-02-22 16:17:37","VST processing facility: first astronomical applications","  VST--Tube is a new software package designed to process optical astronomical
images. It is an automated pipeline to go from the raw exposures to fully
calibrated co-added images, and to extract catalogs with aperture and PSF
photometry. A set of tools allow the data administration and the quality check
of the intermediate and final products. VST-Tube comes with a Graphical User
Interface to facilitate the interaction between data and user. We outline here
the VST--Tube architecture and show some applications enlightening some of the
characteristics of the pipeline.
","A. Grado|M. Capaccioli|L. Limatola|F. Getman","","http://arxiv.org/abs/1102.1588v2","http://arxiv.org/pdf/1102.1588v2","","Presented to the 54th Congress SAIt, 4-7 May 2010, Naples, Italy","","","astro-ph.IM","astro-ph.IM"
"315","1102.2131v2","2011-02-10 14:39:49","2011-02-17 07:33:00","Analytical Study of Object Components for Distributed and Ubiquitous
  Computing Environment","  The Distributed object computing is a paradigm that allows objects to be
distributed across a heterogeneous network, and allows each of the components
to interoperate as a unified whole. A new generation of distributed
applications, such as telemedicine and e-commerce applications, are being
deployed in heterogeneous and ubiquitous computing environments. The objective
of this paper is to explore an applicability of a component based services in
ubiquitous computational environment. While the fundamental structure of
various distributed object components is similar, there are differences that
can profoundly impact an application developer or the administrator of a
distributed simulation exercise and to implement in Ubiquitous Computing
Environment.
","Usha Batra|Deepak Dahiya|Sachin Bhardwaj","","http://arxiv.org/abs/1102.2131v2","http://arxiv.org/pdf/1102.2131v2","","This paper has been withdrawn by the authors","WSEAS TRANSACTIONS on INFORMATION SCIENCE & APPLICATIONS, Issue 6,
  Volume 5, June 2008, ISSN: 1790-0832","","cs.DC","cs.DC"
"316","1102.2339v1","2011-02-11 12:53:14","2011-02-11 12:53:14","A decompilation of the pi-calculus and its application to termination","  We study the correspondence between a concurrent lambda-calculus in
administrative, continuation passing style and a pi-calculus and we derive a
termination result for the latter.
","Roberto Amadio","PPS","http://arxiv.org/abs/1102.2339v1","http://arxiv.org/pdf/1102.2339v1","","","","","cs.PL","cs.PL"
"317","1102.2581v1","2011-02-13 11:33:39","2011-02-13 11:33:39","Effective Administration of a Large Undergraduate Physics Class: From
  Enrolment to Assessment Feedback","  At the beginning of academic year 2007-08, staff in the School of Physics &
Astronomy at the University of Glasgow started to implement a number of
substantial changes to the first year physics class. The main aims were to
improve the academic performance and progression statistics for the class, with
early identification of ""at-risk"" students being one priority. The introduction
of novel electronic and computer-based data processing strategies ensured both
a quick turnaround time in processing student records and also a much-improved
confidence in the accuracy of assessment feedback returned to students. These
techniques were a contributory factor in helping raise the pass-rate by ~10%
and the direct progression rate by ~13% by the end of 2008-09. The
effectiveness of these processes have proved adequate for coping with the
unexpected and dramatic 50% increase in class size 2009-10 and are now in the
process of being rolled out to other classes within the School
","M. M. Casey|M. W. Kille","","http://arxiv.org/abs/1102.2581v1","http://arxiv.org/pdf/1102.2581v1","","12 pages, 4 figures, titlepage information provided in both English
  and Spanish","Journal of Science Education / Revista de Educaci\'on en Ciencias
  12 (2), 2011","","physics.ed-ph","physics.ed-ph"
"318","1102.3793v1","2011-02-18 09:41:30","2011-02-18 09:41:30","Theoretical Framework and Empirical Modeling for Time Required to
  Vaccinate a Population in an Epidemic","  The paper describes a method to understand time required to vaccinate against
viruses in total as well as subpopulations. As a demonstration, a model based
estimate for time required to vaccinate H1N1 in India, given its administrative
difficulties is provided. We have proved novel theorems for the time functions
defined in the paper. Such results are useful in planning for future epidemics.
The number of days required to vaccinate entire high risk population in three
subpopulations (villages, tehsils and towns) are noted to be 84, 89 and 88
respectively. There exists state wise disparities in the health infrastructure
and capacities to deliver vaccines and hence national estimates need to be
re-evaluated based on individual performances in the states.
","Arni S. R. Srinivasa Rao|Thomas Kurien","","http://arxiv.org/abs/1102.3793v1","http://arxiv.org/pdf/1102.3793v1","","14 pages, 1 Table, 5 Figures (A preliminary draft)","Handbook of Statistics (2017), Volume 37","","q-bio.QM","q-bio.QM|physics.soc-ph|92D30, 60E05, 26D07"
"319","1103.0658v1","2011-03-03 11:43:11","2011-03-03 11:43:11","Performance Comparisons of Routing Protocols in Mobile Ad Hoc Networks","  Mobile Ad hoc Network (MANET) is a collection of wireless mobile nodes that
dynamically form a network temporarily without any support of central
administration. Moreover, Every node in MANET moves arbitrarily making the
multi-hop network topology to change randomly at unpredictable times. There are
several familiar routing protocols like DSDV, AODV, DSR, etc...which have been
proposed for providing communication among all the nodes in the network. This
paper presents a performance comparison of proactive and reactive protocols
DSDV, AODV and DSR based on metrics such as throughput, packet delivery ratio
and average end-to-end delay by using the NS-2 simulator.
","P. Manickam|T. Guru Baskar|M. Girija|Dr. D. Manimegalai","","http://arxiv.org/abs/1103.0658v1","http://arxiv.org/pdf/1103.0658v1","http://dx.doi.org/10.5121/ijwmn.2011.3109","9 Pages,10 Figures, 3 Tables","International Journal of Wireless & Mobile Networks (IJWMN) Vol.
  3, No. 1, February 2011","10.5121/ijwmn.2011.3109","cs.NI","cs.NI"
"320","1103.0759v1","2011-03-03 19:09:47","2011-03-03 19:09:47","Scheduler Vulnerabilities and Attacks in Cloud Computing","  In hardware virtualization a hypervisor provides multiple Virtual Machines
(VMs) on a single physical system, each executing a separate operating system
instance. The hypervisor schedules execution of these VMs much as the scheduler
in an operating system does, balancing factors such as fairness and I/O
performance. As in an operating system, the scheduler may be vulnerable to
malicious behavior on the part of users seeking to deny service to others or
maximize their own resource usage.
  Recently, publically available cloud computing services such as Amazon EC2
have used virtualization to provide customers with virtual machines running on
the provider's hardware, typically charging by wall clock time rather than
resources consumed. Under this business model, manipulation of the scheduler
may allow theft of service at the expense of other customers, rather than
merely reallocating resources within the same administrative domain.
  We describe a flaw in the Xen scheduler allowing virtual machines to consume
almost all CPU time, in preference to other users, and demonstrate kernel-based
and user-space versions of the attack. We show results demonstrating the
vulnerability in the lab, consuming as much as 98% of CPU time regardless of
fair share, as well as on Amazon EC2, where Xen modifications protect other
users but still allow theft of service. In case of EC2, following the
responsible disclosure model, we have reported this vulnerability to Amazon;
they have since implemented a fix that we have tested and verified (See
Appendix B). We provide a novel analysis of the necessary conditions for such
attacks, and describe scheduler modifications to eliminate the vulnerability.
  We present experimental results demonstrating the effectiveness of these
defenses while imposing negligible overhead.
","Fangfei Zhou|Manish Goel|Peter Desnoyers|Ravi Sundaram","","http://arxiv.org/abs/1103.0759v1","http://arxiv.org/pdf/1103.0759v1","","","","","cs.DC","cs.DC"
"321","1103.2467v1","2011-03-12 18:01:34","2011-03-12 18:01:34","Commuter networks and community detection: a method for planning sub
  regional areas","  A major issue for policy makers and planners is the definition of the ""ideal""
regional partition, i.e. the delimitation of sub-regional domains showing a
sufficient level of homogeneity with respect to some specific territorial
features. In Sardinia, the second major island in the Mediterranean sea,
politicians and analysts have been involved in a 50 year process of
identification of the correct pattern for the province, an intermediate
administrative body in between the Regional and the municipal administration.
In this paper, we compare some intermediate body partitions of Sardinia with
the patterns of the communities of workers and students, by applying grouping
methodologies based on the characterization of Sardinian commuters' system as a
complex weighted network. We adopt an algorithm based on the maximization of
the weighted modularity of this network to detect productive basins composed by
municipalities showing a certain degree of cohesiveness in terms of commuter
flows. The results obtained lead to conclude that new provinces in Sardinia
seem to have been designed -even unconsciously- as labour basins of
municipalities with similar commuting behaviour.
","Andrea De Montis|Simone Caschili|Alessandro Chessa","","http://arxiv.org/abs/1103.2467v1","http://arxiv.org/pdf/1103.2467v1","","19 pages","","","physics.soc-ph","physics.soc-ph|cs.SI"
"322","1104.1200v1","2011-04-06 21:43:05","2011-04-06 21:43:05","Modularity maximization and tree clustering: Novel ways to determine
  effective geographic borders","  Territorial subdivisions and geographic borders are essential for
understanding phenomena in sociology, political science, history, and
economics. They influence the interregional flow of information and
cross-border trade and affect the diffusion of innovation and technology.
However, most existing administrative borders were determined by a variety of
historic and political circumstances along with some degree of arbitrariness.
Societies have changed drastically, and it is doubtful that currently existing
borders reflect the most logical divisions. Fortunately, at this point in
history we are in a position to actually measure some aspects of the geographic
structure of society through human mobility. Large-scale transportation systems
such as trains and airlines provide data about the number of people traveling
between geographic locations, and many promising human mobility proxies are
being discovered, such as cell phones, bank notes, and various online social
networks. In this chapter we apply two optimization techniques to a human
mobility proxy (bank note circulation) to investigate the effective geographic
borders that emerge from a direct analysis of human mobility.
","Daniel Grady|Rafael Brune|Christian Thiemann|Fabian Theis|Dirk Brockmann","","http://arxiv.org/abs/1104.1200v1","http://arxiv.org/pdf/1104.1200v1","","","","","physics.soc-ph","physics.soc-ph|cs.SI"
"323","1104.2434v1","2011-04-13 10:24:30","2011-04-13 10:24:30","A Conversation with George G. Roussas","  George G. Roussas was born in the city of Marmara in central Greece, on June
29, 1933. He received a B.A. with high honors in Mathematics from the
University of Athens in 1956, and a Ph.D. in Statistics from the University of
California, Berkeley, in 1964. In 1964--1966, he served as Assistant Professor
of Mathematics at the California State University, San Jose, and he was a
faculty member of the Department of Statistics at the University of Wisconsin,
Madison, in 1966--1976, starting as an Assistant Professor in 1966, becoming a
Professor in 1972. He was a Professor of Applied Mathematics and Director of
the Laboratory of Applied Mathematics at the University of Patras, Greece, in
1972--1984. He was elected Dean of the School of Physical and Mathematical
Sciences at the University of Patras in 1978, and Chancellor of the university
in 1981. He served for about three years as Vice President-Academic Affairs of
the then new University of Crete, Greece, in 1981--1985. In 1984, he was a
Visiting Professor in the Intercollege Division of Statistics at the University
of California, Davis, and he was appointed Professor, Associate Dean and Chair
of the Graduate Group in Statistics in the same university in 1985; he served
in the two administrative capacities in 1985--1999. He is an elected member of
the International Statistical Institute since 1974, a Fellow of the Royal
Statistical Society since 1975, a Fellow of the Institute of Mathematical
Statistics since 1983, and a Fellow of the American Statistical Association
since 1986. He served as a member of the Council of the Hellenic Mathematical
Society, and as President of the Balkan Union of Mathematicians.
","Debasis Bhattacharya|Francisco J. Samaniego","","http://arxiv.org/abs/1104.2434v1","http://arxiv.org/pdf/1104.2434v1","http://dx.doi.org/10.1214/09-STS299A","Published in at http://dx.doi.org/10.1214/09-STS299A the Statistical
  Science (http://www.imstat.org/sts/) by the Institute of Mathematical
  Statistics (http://www.imstat.org)","Statistical Science 2010, Vol. 25, No. 4, 566-587","10.1214/09-STS299A","stat.OT","stat.OT"
"324","1104.2475v1","2011-04-13 12:56:07","2011-04-13 12:56:07","Practical Quantum Cryptography: the Q-KeyMaker","  In the next years the data transmission connections will constitute one of
the principal tools of communication among cities, enterprises and public
administration. With their enhanced connectivity, the systems and nets of
information are now exposed to an increased vulnerability and new safety
problems are emerging. Insofar Quantum Key Distribution (QKD) has matured to
real world applications and can enhance the safety of the communication nets.
In this paper we present the QKD network designed and implemented by Selex-SI
and we give an overview of the obtained results.
","Fabio A. Bovino|Maurizio Giardina","","http://arxiv.org/abs/1104.2475v1","http://arxiv.org/pdf/1104.2475v1","","Proceedings of the Third International Symposium on Applied Sciences
  in Biomedical and Communication Technologies (ISABEL 2010), Rome, Italy,
  November 2010","","","quant-ph","quant-ph"
"325","1105.0983v2","2011-05-05 06:16:25","2012-11-19 19:43:04","Bochner-Weitzenboeck formula and Li-Yau estimates on Finsler manifolds","  We prove the Bochner-Weitzenboeck formula for the (nonlinear) Laplacian on
general Finsler manifolds and derive Li-Yau type gradient estimates as well as
parabolic Harnack inequalities. Moreover, we deduce Bakry-Emery gradient
estimates. All these estimates depend on lower bounds for the weighted flag
Ricci tensor.
","Shin-ichi Ohta|Karl-Theodor Sturm","","http://arxiv.org/abs/1105.0983v2","http://arxiv.org/pdf/1105.0983v2","","Administratively withdrawn: duplicate of 1105.0983","","","math.AP","math.AP"
"326","1106.3369v1","2011-06-16 23:05:50","2011-06-16 23:05:50","Evaluation of Fiji National University Campus Information Systems","  Fiji National University (FNU) has been encountering many difficulties with
its current campus administrative systems. These difficulties include
accessibility, scalability, performance, flexibility and integration. In order
to address these difficulties, we developed a thin client web based campus
information system. The newly designed system allows the students, academic and
administration staff of the university to handle their day to day affairs with
the university online. In this paper we describe three types of evaluation
carried out to determine the suitability of newly developed system for FNU
environment. User interface evaluation was carried out to assess user interface
on a set of usability principles, usability evaluation to see the ease at which
users can use the system and finally performance evaluation to verify and
validate user response time required to complete various tasks. The result of
each of these evaluations were analysed and the system was rectified as part of
iterative design process.
","Bimal Aklesh Kumar","","http://arxiv.org/abs/1106.3369v1","http://arxiv.org/pdf/1106.3369v1","","vol. 2 no. 3 2011","","","cs.HC","cs.HC"
"327","1106.3739v8","2011-06-19 13:00:56","2011-09-05 17:09:07","Reducing Interpolation on Multi-Dimensional Grid to Quantizing Grid's
  Data-Base as a Recursion","  In his article ""Powerlist: A Structure for Parallel Recursion"" Jayadev Misra
wrote:
  ""Many data parallel algorithms - Fast Fourier Transform, Batcher's sorting
schemes and prefix sum - exhibit recursive structure. We propose a data
structure, powerlist, that permits succinct descriptions of such algorithms,
highlighting the roles of both parallelism and recursion. Simple algebraic
properties of this data structure can be exploited to derive properties of
these algorithms and establish equivalence of different algorithms that solve
the same problem.""
  The quote above illustrates a widely shared assumption about recursion
implementations: either they are done in purely structural terms or they cannot
be done at all.
  Multi-dimensional interpolation on a grid is one of hosts of semi-recursive
schemes that, while often referred to as recursive and routinely described in
vaguely recursive terms, cannot be implemented as a recursion in their
structural entirety.
  This article describes a computer-implemented scheme for isolating the
recursive core of interpolation on a multi-grid, an arrangement that both stems
from and provides a structural framework to a number of multi-dimensional
interpolation optimization techniques that, once implemented, provide gains in
multi-dimensional interpolation speed that, compared to some known benchmarks,
measure in multiple orders of magnitude.
  Categories and Subject Descriptors: Multi-dimensional Programming; Concurrent
Programming; Recursion
  General terms: Parallel Processing, Prioritized Processing, Interpolation,
Recursion, Multi-Cube
","Roman Gitlin","","http://arxiv.org/abs/1106.3739v8","http://arxiv.org/pdf/1106.3739v8","","This article has been withdrawn by arXiv administrators as containing
  excessive overlap with arXiv:1106.3314","","","cs.CG","cs.CG"
"328","1106.3842v1","2011-06-20 08:53:20","2011-06-20 08:53:20","Night-sky brightness monitoring in Hong Kong - a city-wide light
  pollution assessment","  Results of the first comprehensive light pollution survey in Hong Kong are
presented. The night-sky brightness was measured and monitored around the city
using a portable light sensing device called the Sky Quality Meter over a
15-month period beginning in March 2008. A total of 1,957 data sets were taken
at 199 distinct locations, including urban and rural sites covering all 18
Administrative Districts of Hong Kong. The survey shows that the environmental
light pollution problem in Hong Kong is severe - the urban night-skies (sky
brightness at 15.0 mag per arcsec square) are on average ~100 times brighter
than at the darkest rural sites (20.1 mag per arcsec square), indicating that
the high lighting densities in the densely populated residential and commercial
areas lead to light pollution. In the worst polluted urban location studied,
the night-sky at 13.2 mag per arcsec square can be over 500 times brighter than
the darkest sites in Hong Kong. The observed night-sky brightness is found to
be affected by human factors such as land utilization and population density of
the observation sites, together with meteorological and/or environmental
factors. Moreover, earlier night-skies (at 9:30pm local time) are generally
brighter than later time (at 11:30pm), which can be attributed to some public
and commercial lightings being turned off later at night. On the other hand, no
concrete relationship between the observed sky brightness and air pollutant
concentrations could be established with the limited survey sampling. Results
from this survey will serve as an important database for the public to assess
whether new rules and regulations are necessary to control the use of outdoor
lightings in Hong Kong.
","Chun Shing Jason Pun|Chu Wing So","","http://arxiv.org/abs/1106.3842v1","http://arxiv.org/pdf/1106.3842v1","http://dx.doi.org/10.1007/s10661-011-2136-1","33 pages, 13 figures, Environmental Monitoring and Assessment, in
  press","Environmental Monitoring and Assessment 184 (2012) 2537-2557","10.1007/s10661-011-2136-1","astro-ph.IM","astro-ph.IM"
"329","1107.5899v1","2011-07-29 08:56:00","2011-07-29 08:56:00","Hierarchical Bayesian estimation of inequality measures with
  nonrectangular censored survey data with an application to wealth
  distribution of French households","  We consider the estimation of wealth inequality measures with their
confidence interval, based on survey data with interval censoring. We rely on a
Bayesian hierarchical model. It consists of a model where, due to survey
sampling and unit nonresponse, the summaries of the wealth distribution of
households are observed with error; a mixture of multivariate models for the
wealth components where groups correspond to portfolios of assets; and a prior
on the parameters. A Gibbs sampler is used for numerical purposes to do the
inference. We apply this strategy to the French 2004 Wealth Survey. In order to
alleviate the nonresponse, the amounts were systematically collected in the
form of brackets. Matched administrative data on the liability of the
respondents for wealth tax and response to overview questions are used to
better localize the wealth components. It implies nonrectangular
multidimensional censoring. The variance of the error term in the model for the
population inequality measures is obtained using linearization and taking into
account the complex sampling design and the various weight adjustments.
","Eric Gautier","","http://arxiv.org/abs/1107.5899v1","http://arxiv.org/pdf/1107.5899v1","http://dx.doi.org/10.1214/10-AOAS443","Published in at http://dx.doi.org/10.1214/10-AOAS443 the Annals of
  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of
  Mathematical Statistics (http://www.imstat.org)","Annals of Applied Statistics 2011, Vol. 5, No. 2B, 1632-1656","10.1214/10-AOAS443","stat.AP","stat.AP"
"330","1108.1344v1","2011-08-05 14:54:25","2011-08-05 14:54:25","Architecture Of A Identity Based Firewall System","  Classic firewall systems are built to filter traffic based on IP addresses,
source and destination ports and protocol types. The modern networks have grown
to a level where the possibility for users' mobility is a must. In such
networks, modern firewalls may introduce such complexity where administration
can become very frustrating since it needs the intervention of a firewall
administrator. The solution for this problem is an identity based firewall
system. In this paper we will present a new design of a firewall system that
uses the user's identity to filter the traffic. In the design phase we will
define key points which have to be satisfied as a crucial milestone for the
functioning of the whole Identity based firewall system.
","Nenad Stojanovski|Marjan Gusev","","http://arxiv.org/abs/1108.1344v1","http://arxiv.org/pdf/1108.1344v1","http://dx.doi.org/10.5121/ijnsa","9 pages, 3 figures, 1 table; (ISSN: 0975- 2307); International
  Journal of Network Security & Its Applications July 2011, Volume 3, Number 4","","10.5121/ijnsa","cs.CR","cs.CR"
"331","1108.2177v1","2011-08-10 13:29:08","2011-08-10 13:29:08","Bayesian Models and Methods in Public Policy and Government Settings","  Starting with the neo-Bayesian revival of the 1950s, many statisticians
argued that it was inappropriate to use Bayesian methods, and in particular
subjective Bayesian methods in governmental and public policy settings because
of their reliance upon prior distributions. But the Bayesian framework often
provides the primary way to respond to questions raised in these settings and
the numbers and diversity of Bayesian applications have grown dramatically in
recent years. Through a series of examples, both historical and recent, we
argue that Bayesian approaches with formal and informal assessments of priors
AND likelihood functions are well accepted and should become the norm in public
settings. Our examples include census-taking and small area estimation, US
election night forecasting, studies reported to the US Food and Drug
Administration, assessing global climate change, and measuring potential
declines in disability among the elderly.
","Stephen E. Fienberg","","http://arxiv.org/abs/1108.2177v1","http://arxiv.org/pdf/1108.2177v1","http://dx.doi.org/10.1214/10-STS331","Published in at http://dx.doi.org/10.1214/10-STS331 the Statistical
  Science (http://www.imstat.org/sts/) by the Institute of Mathematical
  Statistics (http://www.imstat.org)","Statistical Science 2011, Vol. 26, No. 2, 212-226","10.1214/10-STS331","stat.ME","stat.ME"
"332","1108.5746v1","2011-08-29 20:33:45","2011-08-29 20:33:45","On drug transport after intravenous administration","  A mathematical model of a drug transport after rapid injection is given. It
takes into account three processes: - drug plasma protein binding in central
compartment - transport processes between the central compartment and the
peripheral compartment - elimination of a drug from the central compartment. .
","Slawomir Piekarski|Miroslaw Rewekant","","http://arxiv.org/abs/1108.5746v1","http://arxiv.org/pdf/1108.5746v1","","5 pages","","","q-bio.TO","q-bio.TO|q-bio.QM|92C45"
"333","1109.0473v1","2011-09-02 15:15:09","2011-09-02 15:15:09","A Multi-Wavelength Analysis of Active Regions and Sunspots by Comparison
  of Automated Detection Algorithms","  Since the Solar Dynamics Observatory (SDO) began recording ~ 1 TB of data per
day, there has been an increased need to automatically extract features and
events for further analysis. Here we compare the overall detection performance,
correlations between extracted properties, and usability for feature tracking
of four solar feature-detection algorithms: the Solar Monitor Active Region
Tracker (SMART) detects active regions in line-of-sight magnetograms; the
Automated Solar Activity Prediction code (ASAP) detects sunspots and pores in
white-light continuum images; the Sunspot Tracking And Recognition Algorithm
(STARA) detects sunspots in white-light continuum images; the Spatial
Possibilistic Clustering Algorithm (SPoCA) automatically segments solar EUV
images into active regions (AR), coronal holes (CH) and quiet Sun (QS). One
month of data from the SOHO/MDI and SOHO/EIT instruments during 12 May - 23
June 2003 is analysed. The overall detection performance of each algorithm is
benchmarked against National Oceanic and Atmospheric Administration (NOAA) and
Solar Influences Data Analysis Centre (SIDC) catalogues using various feature
properties such as total sunspot area, which shows good agreement, and the
number of features detected, which shows poor agreement. Principal Component
Analysis indicates a clear distinction between photospheric properties, which
are highly correlated to the first component and account for 52.86% of
variability in the data set, and coronal properties, which are moderately
correlated to both the first and second principal components. Finally, case
studies of NOAA 10377 and 10365 are conducted to determine algorithm stability
for tracking the evolution of individual features. We find that magnetic flux
and total sunspot area are the best indicators of active-region emergence.
","Cis Verbeeck|Paul A. Higgins|Tufan Colak|Fraser T. Watson|Veronique Delouille|Benjamin Mampaey|Rami Qahwaji","","http://arxiv.org/abs/1109.0473v1","http://arxiv.org/pdf/1109.0473v1","","30 pages, 18 figures. Accepted for publication in Solar Physics
  topical issue : Solar Image Processing in the Petabyte Era","","","astro-ph.SR","astro-ph.SR"
"334","1109.0680v1","2011-09-04 07:07:04","2011-09-04 07:07:04","Beyond the Boundaries of Open, Closed and Pirate Archives: Lessons from
  a Hybrid Approach","  The creation of open archives i.e. archives where access is regulated by open
licensing models (content, source, data), should be seen as part of a broader
socio-economic phenomenon that finds legal expression in specific
organizational and technical formats.This paper examines the origins and main
characteristics of the open archives phenomenon. We investigate the extent to
which different models of production of economic or social value can be
expressed in different forms of licensing in the context of open archives.
Through this process, we assess the extent to which the digital archive is
moving towards providing access that is deeper (meaning, that offers more
access rights) and wider (in the sense that most of the information given is in
open content licensing) or face a gradual stratification and polarization of
the content. Such stratification entails the emergence of two types of content:
content to which access is extremely limited and content to which access
remains completely open. This differentiation between classes of content is the
result of multiple factors: from purely legislative, administrative and
contractual restrictions (e.g. data protection and confidentiality
restrictions) to information economics (e.g. peer production) or social
(minimum universal access).
  We claim that with respect to the access management model, most of the
current archiving processes include elements of openness. Usually, this is the
result of economic necessity expressed in licensing instruments or
organisational arrangements. The viability and the socio-economic importance of
the digital archives also contributes to the use of open archiving practices.
In such a context, although pure forms of open digital archives may remain an
ideal, the reality of hybrid open digital archives is a necessity.
","Prodromos Tsiavos|Petros Stefaneas","London School of Economics, UK|National Technical University of Athens, Greece","http://arxiv.org/abs/1109.0680v1","http://arxiv.org/pdf/1109.0680v1","","","","","cs.DL","cs.DL"
"335","1109.2282v1","2011-09-11 05:23:39","2011-09-11 05:23:39","Efficiency of Biometric integration with Salt Value at an Enterprise
  Level and Data Centres","  This chapter is going to deal with enhancing the efficiency of Biometric by
integrating it with Salt Value (randomly generated value of varying length).
Normally at an enterprise level or data centres, the servers are maintained
with complex passwords and they are known only to the system administrators.
Even after applying lot of securities at an expert level, the hackers are able
to penetrate through the network and break the passwords easily. Here how the
biometric can play a vital role and that too with the inclusion of Salt value
can prevent the hacker from stealing the confidential data's of an
organization.
","Bhargav. Balakrishnan","","http://arxiv.org/abs/1109.2282v1","http://arxiv.org/pdf/1109.2282v1","","26 Pages 9 Figures Intech Open access publishers","","","cs.CR","cs.CR"
"336","1109.6597v2","2011-09-29 17:15:31","2012-05-17 19:57:47","Where is Synergy Indicated in the Norwegian Innovation System?
  Triple-Helix Relations among Technology, Organization, and Geography","  Using information theory and data for all (0.5 million) Norwegian firms, the
national and regional innovation systems are decomposed into three subdynamics:
(i) economic wealth generation, (ii) technological novelty production, and
(iii) government interventions and administrative control. The mutual
information in three dimensions can then be used as an indicator of potential
synergy, that is, reduction of uncertainty. We aggregate the data at the NUTS3
level for 19 counties, the NUTS2 level for seven regions, and the single NUTS1
level for the nation. Measured as in-between group reduction of uncertainty,
11.7 % of the synergy was found at the regional level, whereas only another
2.7% was added by aggregation at the national level. Using this triple-helix
indicator, the counties along the west coast are indicated as more
knowledge-based than the metropolitan area of Oslo or the geographical
environment of the Technical University in Trondheim. Foreign direct investment
seems to have larger knowledge spill-overs in Norway (oil, gas, offshore,
chemistry, and marine) than the institutional knowledge infrastructure in
established universities. The northern part of the country, which receives
large government subsidies, shows a deviant pattern.
","¨ªivind Strand|Loet Leydesdorff","","http://arxiv.org/abs/1109.6597v2","http://arxiv.org/pdf/1109.6597v2","","Technological Forecasting & Social Change (forthcoming)","","","physics.soc-ph","physics.soc-ph"
"337","1109.6880v1","2011-09-30 16:24:41","2011-09-30 16:24:41","Explanation-Based Auditing","  To comply with emerging privacy laws and regulations, it has become common
for applications like electronic health records systems (EHRs) to collect
access logs, which record each time a user (e.g., a hospital employee) accesses
a piece of sensitive data (e.g., a patient record). Using the access log, it is
easy to answer simple queries (e.g., Who accessed Alice's medical record?), but
this often does not provide enough information. In addition to learning who
accessed their medical records, patients will likely want to understand why
each access occurred. In this paper, we introduce the problem of generating
explanations for individual records in an access log. The problem is motivated
by user-centric auditing applications, and it also provides a novel approach to
misuse detection. We develop a framework for modeling explanations which is
based on a fundamental observation: For certain classes of databases, including
EHRs, the reason for most data accesses can be inferred from data stored
elsewhere in the database. For example, if Alice has an appointment with Dr.
Dave, this information is stored in the database, and it explains why Dr. Dave
looked at Alice's record. Large numbers of data accesses can be explained using
general forms called explanation templates. Rather than requiring an
administrator to manually specify explanation templates, we propose a set of
algorithms for automatically discovering frequent templates from the database
(i.e., those that explain a large number of accesses). We also propose
techniques for inferring collaborative user groups, which can be used to
enhance the quality of the discovered explanations. Finally, we have evaluated
our proposed techniques using an access log and data from the University of
Michigan Health System. Our results demonstrate that in practice we can provide
explanations for over 94% of data accesses in the log.
","Daniel Fabbri|Kristen LeFevre","","http://arxiv.org/abs/1109.6880v1","http://arxiv.org/pdf/1109.6880v1","","VLDB2012","Proceedings of the VLDB Endowment (PVLDB), Vol. 5, No. 1, pp. 1-12
  (2011)","","cs.DB","cs.DB"
"338","1110.0486v1","2011-10-03 20:26:30","2011-10-03 20:26:30","Vision to reality: From Robert R. Wilson's frontier to Leon M.
  Lederman's Fermilab","  This paper examines the roles of vision and leadership in creating and
directing Fermi National Accelerator Laboratory from the late 1960s through the
1980s. The story divides into two administrations having different problems and
accomplishments, that of Robert R. Wilson (1967-1978), which saw the
transformation from cornfield to frontier physics facility, and that of Leon
Max Lederman (1979-1989), in which the laboratory evolved into one of the
world's major high-energy facilities. Lederman's pragmatic vision of a
user-based experimental community helped him to convert the pioneering facility
that Wilson had built frugally into a laboratory with a stable scientific,
cultural, and funding environment.
","Lillian H. Hoddeson|Adrienne W. Kolb","Illinois U., Urbana|Fermilab","http://arxiv.org/abs/1110.0486v1","http://arxiv.org/pdf/1110.0486v1","http://dx.doi.org/10.1007/s000160300003","","Phys.Perspect.5:67-86,2004","10.1007/s000160300003","physics.hist-ph","physics.hist-ph"
"339","1110.0667v1","2011-10-04 12:47:15","2011-10-04 12:47:15","Storage Area Network Implementation on an Educational Institute Network
  Computer Networking and Communication","  the storage infrastructure is the foundation on which information relies and
therefore must support a company's business objectives and business model. In
this environment, simply deploying more and faster storage devices is not
enough; a new kind of infrastructure is needed, one that provides more enhanced
network availability, data accessibility, and system manageability than is
provided by today's infrastructure. The SAN meets this challenge. The SAN
liberates the storage device, so it is not on a particular server bus, and
attaches it directly to the network. In other words, storage is externalized
and functionally distributed across the organization. The SAN also enables the
centralizing of storage devices and the clustering of servers, which makes for
easier and less expensive administration. So the idea is to create an
intelligent SAN infrastructure that stretches to meet increased demands, allows
highly available and heterogeneous access to expanding information.
","Safarini Osama","","http://arxiv.org/abs/1110.0667v1","http://arxiv.org/pdf/1110.0667v1","","5 Pages; ISSN: 2221-0741","World of Computer Science and Information Technology Journal
  (WCSIT), Vol. 1 No. 7, 292-296, 2011","","cs.NI","cs.NI"
"340","1110.1567v3","2011-10-06 15:44:00","2013-02-05 15:27:49","A Modified GHG Intensity Indicator: Toward a Sustainable Global Economy
  based on a Carbon Border Tax and Emissions Trading","  It will be difficult to gain the agreement of all the actors on any proposal
for climate change management, if universality and fairness are not considered.
In this work, a universal measure of emissions to be applied at the
international level is proposed, based on a modification of the Greenhouse Gas
Intensity (GHG-INT) measure. It is hoped that the generality and low
administrative cost of this measure, which we call the Modified Greenhouse Gas
Intensity measure (MGHG-INT), will eliminate any need to classify nations. The
core of the MGHG-INT is what we call the IHDI-adjusted Gross Domestic Product
(IDHIGDP), based on the Inequality-adjusted Human Development Index (IHDI). The
IDHIGDP makes it possible to propose universal measures, such as MGHG-INT. We
also propose a carbon border tax applicable at national borders, based on
MGHG-INT and IDHIGDP. This carbon tax is supported by a proposed global
Emissions Trading System (ETS). The proposed carbon tax is analyzed in a
short-term scenario, where it is shown that it can result in significant
reduction in global emissions while keeping the economy growing at a positive
rate. In addition to annual GHG emissions, cumulative GHG emissions over two
decades are considered with almost the same results.
","Reza Farrahi Moghaddam|Fereydoun Farrahi Moghaddam|Mohamed Cheriet","","http://arxiv.org/abs/1110.1567v3","http://arxiv.org/pdf/1110.1567v3","http://dx.doi.org/10.1016/j.enpol.2013.02.012","Accepted for publication in Energy Policy (Elsevier)","Energy Policy, vol 57, pp. 363-380, 2013","10.1016/j.enpol.2013.02.012","q-fin.GN","q-fin.GN"
"341","1110.1556v2","2011-10-07 14:55:23","2011-10-15 17:09:11","Jewish Problems","  This is a special collection of problems that were given to select applicants
during oral entrance exams to the math department of Moscow State University.
These problems were designed to prevent Jews and other undesirables from
getting a passing grade. Among problems that were used by the department to
blackball unwanted candidate students, these problems are distinguished by
having a simple solution that is difficult to find. Using problems with a
simple solution protected the administration from extra complaints and appeals.
This collection therefore has mathematical as well as historical value.
","Tanya Khovanova|Alexey Radul","","http://arxiv.org/abs/1110.1556v2","http://arxiv.org/pdf/1110.1556v2","","21 pages, 14 figures","published as ""KIller Problems"" in The American Matheamtical
  Monthly Vol. 119, No. 10 (2012), pp815-82","","math.HO","math.HO"
"342","1110.2777v1","2011-10-12 19:47:51","2011-10-12 19:47:51","Bubble oscillations and motion under vibration","  Bubbles under vibration can behave in unusual ways, e.g., moving downward
against the force of buoyancy. While the bubble downward motion due to the
Bjerknes force is well known at acoustic frequencies close to the bubble
resonant frequency, these experiments demonstrate that these effects can be
observed at relatively low frequencies as well. Experiments were performed in a
thin, quasi-two-dimensional rectangular acrylic box partially filled with
20-cSt PDMS silicone oil with overlying ambient air. The apparatus was
subjected to sinusoidal axial vibration that produced breakup of the gas-liquid
free surface, producing liquid jets into the air, droplets pinching off from
these jets, gas cavities in the liquid from impacts of these droplets, and
bubble transport below the interface. Vibration conditions for the attached
videos are 280 Hz frequency, 15 g acceleration, and 94 micron peak-to-peak
displacement. Behaviors shown in the videos include the following. 1. Free
surface breakup into jets and droplets, and formation of bubbles under the free
surface. 2. Bubbles thus generated moving downward in the cell. 3. Bubbles
attracted to the first bubble deep in the cell and eventually merging to form a
large bubble at the base of the cell. 4. Bubble cluster at the base of the cell
merging to form a larger bubble, which stabilizes at a levitated location below
the free surface and acts to damp out the surface breakup. 5. The levitated
bubble interface and its breakup are similar to the free surface breakup into
jets and droplets, but the jets in the bubble are facing downward. Sandia
National Laboratories is a multi-program laboratory managed and operated by
Sandia Corporation, a wholly owned subsidiary of Lockheed Martin Corporation,
for the U.S. Department of Energy's National Nuclear Security Administration
under contract DE-AC04-94AL85000.
","Tim O'Hern|Bion Shelden|John Torczynski","Sandia National Laboratories|Sandia National Laboratories|Sandia National Laboratories","http://arxiv.org/abs/1110.2777v1","http://arxiv.org/pdf/1110.2777v1","http://dx.doi.org/10.1063/1.4747165","Attached videos are low and high resolution versions of the video
  shown at the APS DFD Gallery of Fluid Motion 2011","","10.1063/1.4747165","physics.flu-dyn","physics.flu-dyn"
"343","1110.2849v1","2011-10-13 07:13:11","2011-10-13 07:13:11","ARBAC Policy for a Large Multi-National Bank","  Administrative role-based access control (ARBAC) is the first comprehensive
administrative model proposed for role-based access control (RBAC). ARBAC has
several features for designing highly expressive policies, but current work has
not highlighted the utility of these expressive policies. In this report, we
present a case study of designing an ARBAC policy for a bank comprising 18
branches. Using this case study we provide an assessment about the features of
ARBAC that are likely to be used in realistic policies.
","Karthick Jayaraman|Vijay Ganesh|Mahesh Tripunitara|Martin C Rinard|Steve J. Chapin","","http://arxiv.org/abs/1110.2849v1","http://arxiv.org/pdf/1110.2849v1","","","","","cs.CR","cs.CR"
"344","1110.3031v1","2011-10-13 19:10:13","2011-10-13 19:10:13","The LTP Experiment on LISA Pathfinder: Operational Definition of TT
  Gauge in Space","  The European Space Agency (ESA) and the National Aeronautics and Space
Administration (NASA) are planning the Laser Interferometer Space Antenna
(LISA) mission in order to detect GW.
  The need of accurate testing of free-fall and knowledge of noise in a space
environment similar to LISA's is considered mandatory a pre-phase for the
project. Therefore the LISA Pathfinder mission has been designed by ESA to fly
the LISA Technology Package (LTP), aiming at testing free-fall by measuring the
residual acceleration between two test-bodies in the dynamical scheme we
address as ""drag-free"". The spectral map of the residual acceleration as
function of frequency will convey information on the local noise level, thus
producing a picture of the environmental working conditions for LISA itself.
  The thesis contains abundant material on the problem of compensating static
gravity, the development of a theory of orthogonalization of reference and
cross-talk for the LTP experiment. The construction of the laser detection
procedure starting from GR and differential geometry arguments is carried on.
Effort was put in pointing out the physical motivations for the choices made in
several other papers by the author and colleagues. In this perspective the
thesis is meant as a summary tool for the LTP collaboration.
  In the second part of the thesis we summarize our contributions for a
measurement of G onboard LTP and review on possible tests of fundamental
physics the mission might embody.
  A wide part of the thesis is now part of the LTP Operation Master Plan,
describing the real science and operations onboard LISA Pathfinder. This thesis
was defended on September 26th, 2006 at the University of Como, Italy.
","Michele Armano","","http://arxiv.org/abs/1110.3031v1","http://arxiv.org/pdf/1110.3031v1","","PhD thesis. About 240 pages","","","physics.space-ph","physics.space-ph|gr-qc|physics.data-an|physics.ins-det|83C35, 93C20, 93C80, 65T99|J.2"
"345","1110.3994v2","2011-10-18 14:26:00","2011-11-28 20:31:07","Voronoi Cell Patterns: theoretical model and applications","  We use a simple fragmentation model to describe the statistical behavior of
the Voronoi cell patterns generated by a set of points in 1D and in 2D. In
particular, we are interested in the distribution of sizes of these Voronoi
cells. Our model is completely defined by two probability distributions in 1D
and again in 2D, the probability to add a new point inside an existing cell and
the probability that this new point is at a particular position relative to the
preexisting point inside this cell. In 1D the first distribution depends on a
single parameter while the second distribution is defined through a
fragmentation kernel; in 2D both distributions depend on a single parameter.
The fragmentation kernel and the control parameters are closely related to the
physical properties of the specific system under study. We use our model to
describe the Voronoi cell patterns of several systems. Specifically, we study
the island nucleation with irreversible attachment, the 1D car parking problem,
the formation of second-level administrative divisions, and the pattern formed
by the Paris M\'etro stations.
","Diego Luis Gonzalez Cabrera|T. L. Einstein","","http://arxiv.org/abs/1110.3994v2","http://arxiv.org/pdf/1110.3994v2","http://dx.doi.org/10.1103/PhysRevE.84.051135","12 pages, 9 figures","Phys. Rev. E 84, 051135 [10 pp ] (2011)","10.1103/PhysRevE.84.051135","cond-mat.stat-mech","cond-mat.stat-mech|physics.soc-ph"
"346","1111.0920v1","2011-11-03 17:20:26","2011-11-03 17:20:26","Extracting spatial information from networks with low-order eigenvectors","  We consider the problem of inferring meaningful spatial information in
networks from incomplete information on the connection intensity between the
nodes of the network. We consider two spatially distributed networks: a
population migration flow network within the US, and a network of mobile phone
calls between cities in Belgium. For both networks we use the eigenvectors of
the Laplacian matrix constructed from the link intensities to obtain
informative visualizations and capture natural geographical subdivisions. We
observe that some low order eigenvectors localize very well and seem to reveal
small geographically cohesive regions that match remarkably well with political
and administrative boundaries. We discuss possible explanations for this
observation by describing diffusion maps and localized eigenfunctions. In
addition, we discuss a possible connection with the weighted graph cut problem,
and provide numerical evidence supporting the idea that lower order
eigenvectors point out local cuts in the network. However, we do not provide a
formal and rigorous justification for our observations.
","Mihai Cucuringu|Vincent D. Blondel|Paul Van Dooren","","http://arxiv.org/abs/1111.0920v1","http://arxiv.org/pdf/1111.0920v1","","15 pages","","","cs.SI","cs.SI|physics.soc-ph|15A18, 92-08, 91C20, 90B18"
"347","1111.1772v1","2011-11-07 23:56:49","2011-11-07 23:56:49","Information Security Plan for Flight Simulator Applications","  The Department of Defense has a need for an identity management system that
uses two factor authentications to ensure that only the correct individuals get
access to their top secret flight simulator program. Currently the Department
of Defense does not have a web interface sign in system. We will be creating a
system that will allow them to access their programs, back office and
administrator functions remotely. A security plan outlining our security
architecture will be delivered prior to the final code roll out. The plan will
include responses to encryption used and the security architecture applied in
the final documentation. The code will be delivered in phases to work out any
issues that may occur during the implementation
","Jason Slaughter|Syed Shawon M. Rahman","","http://arxiv.org/abs/1111.1772v1","http://arxiv.org/pdf/1111.1772v1","http://dx.doi.org/10.5121/ijcsit.2011.3301","15 pages; International Journal of Computer Science & Information
  Technology (IJCSIT), Vol. 3, No 3, June 2011","","10.5121/ijcsit.2011.3301","cs.CR","cs.CR"
"348","1111.2092v1","2011-11-09 03:22:16","2011-11-09 03:22:16","Pushing Your Point of View: Behavioral Measures of Manipulation in
  Wikipedia","  As a major source for information on virtually any topic, Wikipedia serves an
important role in public dissemination and consumption of knowledge. As a
result, it presents tremendous potential for people to promulgate their own
points of view; such efforts may be more subtle than typical vandalism. In this
paper, we introduce new behavioral metrics to quantify the level of controversy
associated with a particular user: a Controversy Score (C-Score) based on the
amount of attention the user focuses on controversial pages, and a Clustered
Controversy Score (CC-Score) that also takes into account topical clustering.
We show that both these measures are useful for identifying people who try to
""push"" their points of view, by showing that they are good predictors of which
editors get blocked. The metrics can be used to triage potential POV pushers.
We apply this idea to a dataset of users who requested promotion to
administrator status and easily identify some editors who significantly changed
their behavior upon becoming administrators. At the same time, such behavior is
not rampant. Those who are promoted to administrator status tend to have more
stable behavior than comparable groups of prolific editors. This suggests that
the Adminship process works well, and that the Wikipedia community is not
overwhelmed by users who become administrators to promote their own points of
view.
","Sanmay Das|Allen Lavoie|Malik Magdon-Ismail","","http://arxiv.org/abs/1111.2092v1","http://arxiv.org/pdf/1111.2092v1","","","","","cs.SI","cs.SI|cs.LG"
"349","1111.2542v1","2011-11-10 18:07:13","2011-11-10 18:07:13","Three Tier Encryption Algorithm For Secure File Transfer","  This encryption algorithm is mainly designed for having a secure file
transfer in the low privilege servers and as well as in a secured environment
too. This methodology will be implemented in the data center and other
important data transaction sectors of the organisation where the encoding
process of the software will be done by the database administrator or system
administrators and his trusted clients will have decoding process of the
software. This software will not be circulated to the unauthorised customers.
","Bhargav Balakrishnan","","http://arxiv.org/abs/1111.2542v1","http://arxiv.org/pdf/1111.2542v1","","ICKD 2010","","","cs.CR","cs.CR"
"350","1111.2933v1","2011-11-12 14:52:12","2011-11-12 14:52:12","A Simple Network Management Architecture for Supporting Network
  Administrator and QoS Requirements","  In this paper, a simple network management architecture for supporting both
QoS requirements and organization network management policies is purposed. By
grouping the traffic flows according to the QoS requirements or certain network
management policies, the network resources are effectively controlled. The
purposed architecture is easy to deploy; the gateway is the only equipment that
needs installation, leaving the rest of the system untouched. The architecture
has not significantly degraded the overall system utilization when applying it
to the outgoing bound of the gateway. The architecture can also be implemented
on the wireless LAN at the access point because the architecture is designed in
such the way that it is independent to both the lower and upper protocol
layers.
","Anan Phonphoem|Aphirak Jansang","","http://arxiv.org/abs/1111.2933v1","http://arxiv.org/pdf/1111.2933v1","","10 pages, 11 figures, The 15th international conference on Computer
  communication (ICCC 2002), Mumbai, India. August 11-14, 2002","","","cs.NI","cs.NI"
"351","1111.5639v1","2011-11-23 22:28:38","2011-11-23 22:28:38","A New Technique to Backup and Restore DBMS using XML and .NET
  Technologies","  In this paper, we proposed a new technique for backing up and restoring
different Database Management Systems (DBMS). The technique is enabling to
backup and restore a part of or the whole database using a unified interface
using ASP.NET and XML technologies. It presents a Web Solution allowing the
administrators to do their jobs from everywhere, locally or remotely. To show
the importance of our solution, we have taken two case studies, oracle 11g and
SQL Server 2008.
","Seifedine Kadry|Mohamad Smaili|Hussam Kassem|Hassan Hayek","","http://arxiv.org/abs/1111.5639v1","http://arxiv.org/pdf/1111.5639v1","","","International Journal on Computer Science and Engineering Vol. 02,
  No. 04, 2010, 1092-1102","","cs.DB","cs.DB"
"352","1111.5737v1","2011-11-24 12:09:40","2011-11-24 12:09:40","Agile and Pro-Active Public Administration as a Collaborative Networked
  Organization","  In highly competitive, globalized economies and societies of always-on-line
people intensively using the Internet and mobile phones, public administrations
have to adapt to new challenges. Enterprises and citizens expect public
administrations to be agile and pro-active to foster development. A way to
achieve agility and pro-activity is application of a model of Collaborative
Network Organizations in its two forms: Virtual Organizations (VO) and Virtual
Organization Breeding Environments (VOBE). In the paper, advantages are shown
of public administration playing a role of a Virtual Organization customer on
the one hand, and a Virtual Organization member on the other hand. It is also
shown how public administration playing a role of a Virtual Organization
Breeding Environment may improve its agility and promote advanced technologies
and management methods among local organizations. It is argued in the paper
that public administration should provide a Virtual Organization Breeding
Environment as a part of public services.
","Wojciech Cellary|Willy Picard","","http://arxiv.org/abs/1111.5737v1","http://arxiv.org/pdf/1111.5737v1","http://dx.doi.org/10.1145/1930321.1930324","6 pages, 2 figures","","10.1145/1930321.1930324","cs.CY","cs.CY|J.1; K.4.3"
"353","1111.6401v1","2011-11-28 10:52:28","2011-11-28 10:52:28","Graph based E-Government web service composition","  Nowadays, e-government has emerged as a government policy to improve the
quality and efficiency of public administrations. By exploiting the potential
of new information and communication technologies, government agencies are
providing a wide spectrum of online services. These services are composed of
several web services that comply with well defined processes. One of the big
challenges is the need to optimize the composition of the elementary web
services. In this paper, we present a solution for optimizing the computation
effort in web service composition. Our method is based on Graph Theory. We
model the semantic relationship between the involved web services through a
directed graph. Then, we compute all shortest paths using for the first time,
an extended version of the Floyd-Warshall algorithm.
","Hajar Elmaghraoui|Imane Zaoui|Dalila Chiadmi|Laila Benhlima","","http://arxiv.org/abs/1111.6401v1","http://arxiv.org/pdf/1111.6401v1","","","","","cs.AI","cs.AI"
"354","1111.6793v1","2011-11-29 12:31:25","2011-11-29 12:31:25","Query Driven Visualization","  The request driven way of deriving data in Astro-WISE is extended to a query
driven way of visualization. This allows scientists to focus on the science
they want to perform, because all administration of their data is automated.
This can be done over an abstraction layer that enhances control and
flexibility for the scientist.
","Hugo Buddelmeijer|Edwin A. Valentijn","","http://arxiv.org/abs/1111.6793v1","http://arxiv.org/pdf/1111.6793v1","","4 pages, Procedings ADASS XXI, ASP Conference Series","Astronomical Data Analysis Software and Systems XXI, volume 461,
  year 2012, page 521","","astro-ph.IM","astro-ph.IM|cs.SE"
"355","1111.6917v1","2011-11-29 17:54:07","2011-11-29 17:54:07","Spreadsheet on Cloud -- Framework for Learning and Health Management
  System","  Cloud Computing has caused a paradigm shift in the world of computing.
Several use case scenarios have been floating around the programming world in
relation to this. Applications such as Spreadsheets have the capability to use
the Cloud framework to create complex web based applications. In our effort to
do the same, we have proposed a Spreadsheet on the cloud as the framework for
building new web applications, which will be useful in various scenarios,
specifically a School administration system and governance scenarios, such as
Health and Administration. This paper is a manifestation of this work, and
contains some use cases and architectures which can be used to realize these
scenarios in the most efficient manner.
","K. S. Preeti|Vijit Singh|Sushant Bhatia|Ekansh Preet Singh|Manu Sheel Gupta","","http://arxiv.org/abs/1111.6917v1","http://arxiv.org/pdf/1111.6917v1","","13 Pages, 8 Colour Figures; Proc. European Spreadsheet Risks Int.
  Grp. (EuSpRIG) 2011 ISBN 978-0-9566256-9-4","","","cs.SE","cs.SE"
"356","1112.1212v2","2011-12-06 10:04:36","2012-02-20 12:34:00","Quantum election scheme based on anonymous quantum key distribution","  An unconditionally secure authority-certified anonymous quantum key
distribution scheme using conjugate coding is presented, base on which we
construct a quantum election scheme without the help of entanglement state. We
show that this election scheme ensures the completeness, soundness, privacy,
eligibility, unreusability, fairness and verifiability of a large-scale
election in which the administrator and counter are semi-honest. This election
scheme can work even if there exist loss and errors in quantum channels. In
addition, any irregularity in this scheme is sensible.
","Rui-Rui Zhou|Li Yang","","http://arxiv.org/abs/1112.1212v2","http://arxiv.org/pdf/1112.1212v2","http://dx.doi.org/10.1088/1674-1056/21/8/080301","15 pages, 3 tables","","10.1088/1674-1056/21/8/080301","quant-ph","quant-ph"
"357","1112.5703v1","2011-12-24 05:42:32","2011-12-24 05:42:32","A Comprehensive Performance Analysis of Proactive, Reactive and Hybrid
  MANETs Routing Protocols","  A mobile Ad-hoc network (MANET) is a dynamic multi hop wireless network
established by a group of nodes in which there is no central administration.
Due to mobility of nodes and dynamic network topology, the routing is one of
the most important challenges in ad-hoc networks. Several routing algorithms
for MANETs have been proposed by the researchers which have been classified
into various categories, however, the most prominent categories are proactive,
reactive and hybrid. The performance comparison of routing protocols for MANETs
has been presented by other researcher also, however, none of these works
considers proactive, reactive and hybrid protocols together. In this paper, the
performance of proactive (DSDV), reactive (DSR and AODV) and hybrid (ZRP)
routing protocols has been compared. The performance differentials are analyzed
on the basis of throughput, average delay, routing overhead and number of
packets dropped with a variation of number of nodes, pause time and mobility.
","Kavita Pandey|Abhishek Swaroop","","http://arxiv.org/abs/1112.5703v1","http://arxiv.org/pdf/1112.5703v1","","","IJCSI Volume 8, Issue 6, November 2011","","cs.NI","cs.NI"
"358","1201.0234v1","2011-12-31 05:36:46","2011-12-31 05:36:46","A Statistical Approach Towards Robust Progress Estimation","  The need for accurate SQL progress estimation in the context of decision
support administration has led to a number of techniques proposed for this
task. Unfortunately, no single one of these progress estimators behaves
robustly across the variety of SQL queries encountered in practice, meaning
that each technique performs poorly for a significant fraction of queries. This
paper proposes a novel estimator selection framework that uses a statistical
model to characterize the sets of conditions under which certain estimators
outperform others, leading to a significant increase in estimation robustness.
The generality of this framework also enables us to add a number of novel
""special purpose"" estimators which increase accuracy further. Most importantly,
the resulting model generalizes well to queries very different from the ones
used to train it. We validate our findings using a large number of industrial
real-life and benchmark workloads.
","Arnd Christian Konig|Bolin Ding|Surajit Chaudhuri|Vivek Narasayya","","http://arxiv.org/abs/1201.0234v1","http://arxiv.org/pdf/1201.0234v1","","VLDB2012","Proceedings of the VLDB Endowment (PVLDB), Vol. 5, No. 4, pp.
  382-393 (2011)","","cs.DB","cs.DB"
"359","1201.1081v1","2012-01-05 08:33:59","2012-01-05 08:33:59","Secure SQL Server - Enabling Secure Access to Remote Relational Data","  The Secure SQL Server - SecSS, is a technology primarily developed to enable
self-service governance of states, as described in (Paulin 2012). Self-service
governance is a novel model of governance that rejects service-based public
administration and instead proposes that governed subjects manage their legal
relations in a self-service manner, based on ad-hoc determination of
eligibilities. In this article we describe the prototype SecSS and its
evaluation in a complex governmental scenario.
","Alois Paulin","","http://arxiv.org/abs/1201.1081v1","http://arxiv.org/pdf/1201.1081v1","","","","","cs.CY","cs.CY"
"360","1201.1134v2","2012-01-05 11:47:11","2012-09-06 13:49:36","Formal security analysis of registration protocols for interactive
  systems: a methodology and a case of study","  In this work we present and formally analyze CHAT-SRP (CHAos based
Tickets-Secure Registration Protocol), a protocol to provide interactive and
collaborative platforms with a cryptographically robust solution to classical
security issues. Namely, we focus on the secrecy and authenticity properties
while keeping a high usability. In this sense, users are forced to blindly
trust the system administrators and developers. Moreover, as far as we know,
the use of formal methodologies for the verification of security properties of
communication protocols isn't yet a common practice. We propose here a
methodology to fill this gap, i.e., to analyse both the security of the
proposed protocol and the pertinence of the underlying premises. In this
concern, we propose the definition and formal evaluation of a protocol for the
distribution of digital identities. Once distributed, these identities can be
used to verify integrity and source of information. We base our security
analysis on tools for automatic verification of security protocols widely
accepted by the scientific community, and on the principles they are based
upon. In addition, it is assumed perfect cryptographic primitives in order to
focus the analysis on the exchange of protocol messages. The main property of
our protocol is the incorporation of tickets, created using digests of chaos
based nonces (numbers used only once) and users' personal data. Combined with a
multichannel authentication scheme with some previous knowledge, these tickets
provide security during the whole protocol by univocally linking each
registering user with a single request. [..]
","Jesus Diaz|David Arroyo|Francisco B. Rodriguez","","http://arxiv.org/abs/1201.1134v2","http://arxiv.org/pdf/1201.1134v2","","32 pages, 7 figures, 8 listings, 1 table","","","cs.CR","cs.CR"
"361","1201.3907v1","2012-01-18 20:44:35","2012-01-18 20:44:35","The Call-by-need Lambda Calculus, Revisited","  The existing call-by-need lambda calculi describe lazy evaluation via
equational logics. A programmer can use these logics to safely ascertain
whether one term is behaviorally equivalent to another or to determine the
value of a lazy program. However, neither of the existing calculi models
evaluation in a way that matches lazy implementations.
  Both calculi suffer from the same two problems. First, the calculi never
discard function calls, even after they are completely resolved. Second, the
calculi include re-association axioms even though these axioms are merely
administrative steps with no counterpart in any implementation.
  In this paper, we present an alternative axiomatization of lazy evaluation
using a single axiom. It eliminates both the function call retention problem
and the extraneous re-association axioms. Our axiom uses a grammar of contexts
to describe the exact notion of a needed computation. Like its predecessors,
our new calculus satisfies consistency and standardization properties and is
thus suitable for reasoning about behavioral equivalence. In addition, we
establish a correspondence between our semantics and Launchbury's natural
semantics.
","Stephen Chang|Matthias Felleisen","","http://arxiv.org/abs/1201.3907v1","http://arxiv.org/pdf/1201.3907v1","","ESOP 2012","","","cs.PL","cs.PL"
"362","1201.4555v1","2012-01-22 12:41:18","2012-01-22 12:41:18","Implementation of Portion Approach in Distributed Firewall Application
  for Network Security Framework","  The stimulate of this research seeks collaboration of firewalls which, could
reach to the capability of distributed points of security policy; the front-end
entity may much interact by the invaders so the separation between this entity
and back-end entity to make the secure domain protection is necessary;
collaborative security entity has the various task in the organization and
there is a certain security policy to apply in; the entities like DPFF have to
be protected from outsiders. Firewalls are utilized typically to be the main
layer of security in the network framework. The research is presented the
particular segment of the proposed framework that DPFF based on the developed
iptable firewall to be the layers of defense, which is protected front and
backend of the framework with a dynamic security and policy update to control
the framework's safeguard through proposed portion approach algorithm that
utilize to reduce the traffic and efficiency in detection and policy update
mechanism. The policy update mechanism for DPFF is given the way of its
employment. The complete framework signifies a distributed firewall, where the
administrator configures the policy rules set, which could be separately or
else from administration nodes' side.
","Harleen Kaur|Omid MahdiEbadati E.|M. Afshar Alm","","http://arxiv.org/abs/1201.4555v1","http://arxiv.org/pdf/1201.4555v1","","11 pages, 8 figurs, 4 tables, IJCSI; ISSN (Online): 1694-0814","IJCSI International Journal of Computer Science Issues, Vol. 8,
  Issue 6, No 2, November 2011 IJCSI, International Journal of Computer Science
  Issues, Vol. 8, Issue 6, No 2, November 2011, 207-217","","cs.NI","cs.NI|cs.CR"
"363","1202.1664v1","2012-02-08 11:38:47","2012-02-08 11:38:47","Trust Based Scheme for QoS Assurance in Mobile Ad-Hoc Networks","  A mobile ad-hoc network (MANET) is a peer-to-peer wireless network where
nodes can communicate with each other without the use of infrastructure such as
access points or base stations. These networks are self-configuring, capable of
self-directed operation and hastily deployable. Nodes cooperate to provide
connectivity, operates without centralized administration. Nodes are itinerant,
topology can be very dynamic and nodes must be able to relay traffic since
communicating nodes might be out of range. The dynamic nature of MANET makes
network open to attacks and unreliability. Routing is always the most
significant part for any networks. Each node should not only work for itself,
but should be cooperative with other nodes. Node misbehaviour due to selfish or
malicious intention could significantly degrade the performance of MANET. The
Qos parameters like PDR, throughput and delay are affected directly due to such
misbehaving nodes. We focus on trust management framework, which is intended to
cope with misbehaviour problem of node and increase the performance of MANETs.
A trust-based system can be used to track this misbehaving of nodes, spot them
and isolate them from routing and provide reliability. In this paper a Trust
Based Reliable AODV [TBRAODV] protocol is presented which implements a trust
value for each node. For every node trust value is calculated and based trust
value nodes are allowed to participate in routing or else identified to become
a misbehaving node. This enhances reliability in AODV routing and results in
increase of PDR, decrease in delay and throughput is maintained. This work is
implemented and simulated on NS-2. Based on simulation results, the proposed
protocol provides more consistent and reliable data transfer compared with
general AODV, if there are misbehaving nodes in the MANET
","Sridhar Subramanian|Baskaran Ramachandran","","http://arxiv.org/abs/1202.1664v1","http://arxiv.org/pdf/1202.1664v1","http://dx.doi.org/10.5121/ijnsa.2012.4108","","","10.5121/ijnsa.2012.4108","cs.NI","cs.NI"
"364","1202.1865v1","2012-02-09 00:55:26","2012-02-09 00:55:26","Principle of Virtual Use Method in Common Gateway Interface Program on
  the DACS Scheme","  In the world of the Internet, Web Servers such as Apache and Internet
Information Server (IIS) were developed to exchange information among client
computers having different Operation System. They have only the function of
displaying static information such as HTML files and image files into the Web
Browser. However, when the information is updated, the administrator updates it
by manual operation. In some cases, because it is necessary to update several
places about the same information, the work load becomes high than it is assume
and update error and update omission may occur. These problems were solved by
use of a Common Gateway Interface (CGI) program such as a bulletin board system
and a Blog system. However, these programs opened to Internet have often no
user authentication mechanism and no access control mechanism. That is, they
have the problem that user can access it freely only by getting the URL and
inputting it to a Web Browser. Therefore, in this paper, we show a method to
add the user authentication and access control mechanism for them. It is called
virtual use method of CGI and is realized in the case of introducing the
Destination Addressing Control System (DACS) Scheme, which is a kind of Policy
Based Network Management Scheme (PBNM). As the result, this kind of the CGI
program can be used in the organization with the above two functions.
","Kazuya Odagiri|Shogo Shimizu|Naohiro Ishii|Makoto Takizawa","","http://arxiv.org/abs/1202.1865v1","http://arxiv.org/pdf/1202.1865v1","","15 pages","International Journal of Computer Networks & Communications,
  Vol.4, No.1, pp.147-161, January 2012","","cs.NI","cs.NI"
"365","1202.4508v1","2012-02-21 01:41:49","2012-02-21 01:41:49","Processes, Roles and Their Interactions","  Taking an interaction network oriented perspective in informatics raises the
challenge to describe deterministic finite systems which take part in networks
of nondeterministic interactions. The traditional approach to describe
processes as stepwise executable activities which are not based on the
ordinarily nondeterministic interaction shows strong centralization tendencies.
As suggested in this article, viewing processes and their interactions as
complementary can circumvent these centralization tendencies.
  The description of both, processes and their interactions is based on the
same building blocks, namely finite input output automata (or transducers).
Processes are viewed as finite systems that take part in multiple, ordinarily
nondeterministic interactions. The interactions between processes are described
as protocols.
  The effects of communication between processes as well as the necessary
coordination of different interactions within a processes are both based on the
restriction of the transition relation of product automata. The channel based
outer coupling represents the causal relation between the output and the input
of different systems. The coordination condition based inner coupling
represents the causal relation between the input and output of a single system.
  All steps are illustrated with the example of a network of resource
administration processes which is supposed to provide requesting user processes
exclusive access to a single resource.
","Johannes Reich","","http://arxiv.org/abs/1202.4508v1","http://arxiv.org/pdf/1202.4508v1","http://dx.doi.org/10.4204/EPTCS.78.3","In Proceedings IWIGP 2012, arXiv:1202.4229","EPTCS 78, 2012, pp. 24-38","10.4204/EPTCS.78.3","cs.DC","cs.DC"
"366","1202.4910v3","2012-02-22 14:01:57","2014-11-06 22:24:26","Distributed Private Heavy Hitters","  In this paper, we give efficient algorithms and lower bounds for solving the
heavy hitters problem while preserving differential privacy in the fully
distributed local model. In this model, there are n parties, each of which
possesses a single element from a universe of size N. The heavy hitters problem
is to find the identity of the most common element shared amongst the n
parties. In the local model, there is no trusted database administrator, and so
the algorithm must interact with each of the $n$ parties separately, using a
differentially private protocol. We give tight information-theoretic upper and
lower bounds on the accuracy to which this problem can be solved in the local
model (giving a separation between the local model and the more common
centralized model of privacy), as well as computationally efficient algorithms
even in the case where the data universe N may be exponentially large.
","Justin Hsu|Sanjeev Khanna|Aaron Roth","","http://arxiv.org/abs/1202.4910v3","http://arxiv.org/pdf/1202.4910v3","http://dx.doi.org/10.1007/978-3-642-31594-7_39","","","10.1007/978-3-642-31594-7_39","cs.DS","cs.DS|cs.CR|cs.DB"
"367","1202.5722v1","2012-02-26 04:36:54","2012-02-26 04:36:54","S3A: Secure System Simplex Architecture for Enhanced Security of
  Cyber-Physical Systems","  Until recently, cyber-physical systems, especially those with safety-critical
properties that manage critical infrastructure (e.g. power generation plants,
water treatment facilities, etc.) were considered to be invulnerable against
software security breaches. The recently discovered 'W32.Stuxnet' worm has
drastically changed this perception by demonstrating that such systems are
susceptible to external attacks. Here we present an architecture that enhances
the security of safety-critical cyber-physical systems despite the presence of
such malware. Our architecture uses the property that control systems have
deterministic execution behavior, to detect an intrusion within 0.6 {\mu}s
while still guaranteeing the safety of the plant. We also show that even if an
attack is successful, the overall state of the physical system will still
remain safe. Even if the operating system's administrative privileges have been
compromised, our architecture will still be able to protect the physical system
from coming to harm.
","Sibin Mohan|Stanley Bak|Emiliano Betti|Heechul Yun|Lui Sha|Marco Caccamo","","http://arxiv.org/abs/1202.5722v1","http://arxiv.org/pdf/1202.5722v1","","12 pages","","","cs.CR","cs.CR|cs.SY"
"368","1203.2434v2","2012-03-12 09:29:07","2012-08-21 15:38:16","Institutional repository `eKMAIR': establishing and populating a
  research repository for the National University ""Kyiv Mohyla Academy""","  University libraries have an increasingly important role to play in
supporting open access publishing and dissemination of research outputs.1 In
particular, many libraries are playing a leading role in establishing and
managing institutional repositories. Institutional repositories are, most
often, Open Access Initiative (OAI)-compliant databases of a university or
other research institution's intellectual output, most typically research
papers, although many other forms of digital media can also be stored and
disseminated. Their main function is to provide improved access to the full
text of research articles and improve retrieval of relevant research.
  The National University ""Kyiv Mohyla Academy"" is a small-sized institution
with approximately 3,000 students and 500 academic staff. Although it is a
teaching-intensive university, developing research and knowledge-transfer
capacity is a strategic priority and four research institutes have been
established, with further research activity going on in the academic schools
and research centres.
","Tetiana Yaroshenko","","http://arxiv.org/abs/1203.2434v2","http://arxiv.org/pdf/1203.2434v2","","This paper has been administratively withdrawn because it was
  submitted under false pretenses by someone other than Tetiana Yaroshenko and
  she did not write any of the text","Naukovi zapiski UKMA, vol.12 N.3 p.13-21","","cs.OH","cs.OH"
"369","1203.2824v2","2012-03-13 14:39:15","2012-03-14 00:59:03","Identifying Tipping Points in a Decision-Theoretic Model of Network
  Security","  Although system administrators are frequently urged to protect the machines
in their network, the fact remains that the decision to protect is far from
universal. To better understand this decision, we formulate a
decision-theoretic model of a system administrator responsible for a network of
size n against an attacker attempting to penetrate the network and infect the
machines with a virus or similar exploit. By analyzing the model we are able to
demonstrate the cost sensitivity of smaller networks as well as identify
tipping points that can lead the administrator to switch away from the decision
to protect.
","C. F. Larry Heimann|Alan Nochenson","","http://arxiv.org/abs/1203.2824v2","http://arxiv.org/pdf/1203.2824v2","","10 pages, 4 figures. Pre-print","","","cs.CR","cs.CR"
"370","1203.3997v1","2012-03-18 21:19:40","2012-03-18 21:19:40","CloudGenius: Decision Support for Web Server Cloud Migration","  Cloud computing is the latest computing paradigm that delivers hardware and
software resources as virtualized services in which users are free from the
burden of worrying about the low-level system administration details. Migrating
Web applications to Cloud services and integrating Cloud services into existing
computing infrastructures is non-trivial. It leads to new challenges that often
require innovation of paradigms and practices at all levels: technical,
cultural, legal, regulatory, and social. The key problem in mapping Web
applications to virtualized Cloud services is selecting the best and compatible
mix of software images (e.g., Web server image) and infrastructure services to
ensure that Quality of Service (QoS) targets of an application are achieved.
The fact that, when selecting Cloud services, engineers must consider
heterogeneous sets of criteria and complex dependencies between infrastructure
services and software images, which are impossible to resolve manually, is a
critical issue. To overcome these challenges, we present a framework (called
CloudGenius) which automates the decision-making process based on a model and
factors specifically for Web server migration to the Cloud. CloudGenius
leverages a well known multi-criteria decision making technique, called
Analytic Hierarchy Process, to automate the selection process based on a model,
factors, and QoS parameters related to an application. An example application
demonstrates the applicability of the theoretical CloudGenius approach.
Moreover, we present an implementation of CloudGenius that has been validated
through experiments.
","Michael Menzel|Rajiv Ranjan","","http://arxiv.org/abs/1203.3997v1","http://arxiv.org/pdf/1203.3997v1","","10 pages, Proceedings of the 21st International Conference on World
  Wide Web","","","cs.DC","cs.DC|cs.SE|D.2.2; H.4.2"
"371","1203.5259v1","2012-03-23 14:39:07","2012-03-23 14:39:07","Autonomic Model for Self-Configuring C#.NET Applications","  With the advances in computational technologies over the last decade, large
organizations have been investing in Information Technology to automate their
internal processes to cut costs and efficiently support their business
projects. However, this comes to a price. Business requirements always change.
Likewise, IT systems constantly evolves as developers make new versions of
them, which require endless administrative manual work to customize and
configure them, especially if they are being used in different contexts, by
different types of users, and for different requirements. Autonomic computing
was conceived to provide an answer to these ever-changing requirements.
Essentially, autonomic systems are self-configuring, self-healing,
self-optimizing, and self-protecting; hence, they can automate all complex IT
processes without human intervention. This paper proposes an autonomic model
based on Venn diagram and set theory for self-configuring C#.NET applications,
namely the self-customization of their GUI, event-handlers, and security
permissions. The proposed model does not require altering the source-code of
the original application; rather, it uses an XML-based customization file to
turn on and off the internal attributes of the application. Experiments
conducted on the proposed model, showed a successful automatic customization
for C# applications and an effective self-adaption based on dynamic business
requirements. As future work, other programming languages such as Java and C++
are to be supported, in addition to other operating systems such as Linux and
Mac so as to provide a standard platform-independent autonomic self-configuring
model.
","Youssef Bassil|Paul Semaan","","http://arxiv.org/abs/1203.5259v1","http://arxiv.org/pdf/1203.5259v1","","LACSC - Lebanese Association for Computational Sciences -
  http://www.lacsc.org","International Journal of Research Studies in Computing, Vol.1,
  No.1, pp.21-34, April 2012","","cs.OH","cs.OH"
"372","1203.5748v1","2012-03-26 18:07:54","2012-03-26 18:07:54","Self-Healing by Means of Runtime Execution Profiling","  A self-healing application brings itself into a stable state after a failure
put the software into an unstable state. For such self-healing software
application, finding fix for a previously unseen fault is a grand challenge.
Asking the user to provide fixes for every fault is bad for productivity,
especially when the users are non-savvy in technical aspect of computing. If
failure scenarios come into existence, the user wants the runtime environment
to handle those situations autonomically. This paper presents a new technique
of finding self-healing actions by matching a fault scenario to already
established fault models. By profiling and capturing runtime parameters and
execution pathWays, stable execution models are established and later are used
to match with an unstable execution scenario. Experimentation and results are
presented that showed that even with additional overheads; this technique can
prove beneficial for autonomically healing faults and reliving system
administrators from mundane troubleshooting situations.
","Mohammad Muztaba Fuad|Debzani Deb|Jinsuk Baek","","http://arxiv.org/abs/1203.5748v1","http://arxiv.org/pdf/1203.5748v1","http://dx.doi.org/10.1109/ICCITechn.2011.6164784","Proceedings of 14th International Conference on Computer and
  Information Technology (ICCIT 2011) 22-24 December, 2011, Dhaka, Bangladesh","","10.1109/ICCITechn.2011.6164784","cs.SE","cs.SE"
"373","1203.6405v1","2012-03-29 00:07:00","2012-03-29 00:07:00","Concurrency Control for Adaptive Indexing","  Adaptive indexing initializes and optimizes indexes incrementally, as a side
effect of query processing. The goal is to achieve the benefits of indexes
while hiding or minimizing the costs of index creation. However,
index-optimizing side effects seem to turn read-only queries into update
transactions that might, for example, create lock contention. This paper
studies concurrency control in the context of adaptive indexing. We show that
the design and implementation of adaptive indexing rigorously separates index
structures from index contents; this relaxes the constraints and requirements
during adaptive indexing compared to those of traditional index updates. Our
design adapts to the fact that an adaptive index is refined continuously, and
exploits any concurrency opportunities in a dynamic way. A detailed
experimental analysis demonstrates that (a) adaptive indexing maintains its
adaptive properties even when running concurrent queries, (b) adaptive indexing
can exploit the opportunity for parallelism due to concurrent queries, (c) the
number of concurrency conflicts and any concurrency administration overheads
follow an adaptive behavior, decreasing as the workload evolves and adapting to
the workload needs.
","Goetz Graefe|Felix Halim|Stratos Idreos|Harumi Kuno|Stefan Manegold","","http://arxiv.org/abs/1203.6405v1","http://arxiv.org/pdf/1203.6405v1","","VLDB2012","Proceedings of the VLDB Endowment (PVLDB), Vol. 5, No. 7, pp.
  656-667 (2012)","","cs.DB","cs.DB"
"374","1204.0195v1","2012-04-01 10:59:37","2012-04-01 10:59:37","Management Language Specifications For Digital Ecosystems","  This paper defines the specifications of a management language intended to
automate the control and administration of various service components connected
to a digital ecosystem. It is called EML short for Ecosystem Management
Language and it is based on proprietary syntax and notation and contains a set
of managerial commands issued by the system's administrator via a command
console. Additionally, EML is shipped with a collection of self-adaptation
procedures called SAP. Their purpose is to provide self-adaptation properties
to the ecosystem allowing it to self-optimize itself based on the state of its
execution environment. On top of that, there exists the EMU short for Ecosystem
Management Unit which interprets, validates, parses, and executes EML commands
and SAP procedures. Future research can improve upon EML so much so that it can
be extended to support a larger set of commands in addition to a larger set of
SAP procedures.
","Youssef Bassil","","http://arxiv.org/abs/1204.0195v1","http://arxiv.org/pdf/1204.0195v1","","LACSC - Lebanese Association for Computational Sciences,
  http://www.lacsc.org/; Journal of Global Research in Computer Science, Vol.
  3, No. 1, January 2012","","","cs.SE","cs.SE"
"375","1204.2204v3","2012-04-10 16:06:46","2012-06-18 05:29:55","Programming Cloud Resource Orchestration Framework: Operations and
  Research Challenges","  The emergence of cloud computing over the past five years is potentially one
of the breakthrough advances in the history of computing. It delivers hardware
and software resources as virtualization-enabled services and in which
administrators are free from the burden of worrying about the low level
implementation or system administration details. Although cloud computing
offers considerable opportunities for the users (e.g. application developers,
governments, new startups, administrators, consultants, scientists, business
analyst, etc.) such as no up-front investment, lowering operating cost, and
infinite scalability, it has many unique research challenges that need to be
carefully addressed in the future. In this paper, we present a survey on key
cloud computing concepts, resource abstractions, and programming operations for
orchestrating resources and associated research challenges, wherever
applicable.
","Rajiv Ranjan|Boualem Benatallah","","http://arxiv.org/abs/1204.2204v3","http://arxiv.org/pdf/1204.2204v3","","19 pages","","","cs.DC","cs.DC"
"376","1204.3993v1","2012-04-18 07:03:04","2012-04-18 07:03:04","A hierarchical latent class model for predicting disability small area
  counts from survey data","  This article considers the estimation of the number of severely disabled
people using data from the Italian survey on Health Conditions and Appeal to
Medicare. Disability is indirectly measured using a set of categorical items,
which survey a set of functions concerning the ability of a person to
accomplish everyday tasks. Latent Class Models can be employed to classify the
population according to different levels of a latent variable connected with
disability. The survey, however, is designed to provide reliable estimates at
the level of Administrative Regions (NUTS2 level), while local authorities are
interested in quantifying the amount of population that belongs to each latent
class at a sub-regional level. Therefore, small area estimation techniques
should be used. The challenge of the present application is that the variable
of interest is not directly observed. Adopting a full Bayesian approach, we
base small area estimation on a Latent Class model in which the probability of
belonging to each latent class changes with covariates and the influence of age
is learnt from the data using penalized splines. Deimmler-Reisch bases are
shown to improve speed and mixing of MCMC chains used to simulate posteriors.
","Enrico Fabrizi|Giorgio E. Montanari|Maria Giovanna Ranalli","","http://arxiv.org/abs/1204.3993v1","http://arxiv.org/pdf/1204.3993v1","http://dx.doi.org/10.1111/rssa.12112","","","10.1111/rssa.12112","stat.AP","stat.AP|stat.ME|62D05 (Primary) 62G08, 62H17 (Secondary)"
"377","1204.6091v1","2012-04-27 00:34:19","2012-04-27 00:34:19","A structured approach to VO reconfigurations through Policies","  One of the strength of Virtual Organisations is their ability to dynamically
and rapidly adapt in response to changing environmental conditions. Dynamic
adaptability has been studied in other system areas as well and system
management through policies has crystallized itself as a very prominent
solution in system and network administration. However, these areas are often
concerned with very low-level technical aspects. Previous work on the APPEL
policy language has been aimed at dynamically adapting system behaviour to
satisfy end-user demands and - as part of STPOWLA - APPEL was used to adapt
workflow instances at runtime. In this paper we explore how the ideas of APPEL
and STPOWLA can be extended from workflows to the wider scope of Virtual
Organisations. We will use a Travel Booking VO as example.
","Stephan Reiff-Marganiec","University of Leicester","http://arxiv.org/abs/1204.6091v1","http://arxiv.org/pdf/1204.6091v1","http://dx.doi.org/10.4204/EPTCS.83.3","In Proceedings FAVO 2011, arXiv:1204.5796","EPTCS 83, 2012, pp. 22-31","10.4204/EPTCS.83.3","cs.MA","cs.MA|cs.SE"
"378","1205.0104v1","2012-05-01 07:41:54","2012-05-01 07:41:54","Migration of data for iKnow application at EURM - a case study","  Software evolves. After many revisions and improvements software gets retired
and replaced. When replacement takes place, one needs to migrate the data from
the old database into the new database, so the new application can replace the
old application. Student administration application (SAA) currently used by
European University (EURM) has been outgrown by the university, and needs
replacement. iKnow application developed as part of the iKnow Tempus project is
scheduled to replace the existing Student Administration application at EURM.
This paper describes the problems that were encountered while migrating the
data from the old databases of SAA to the new database designed for the iKnow
application. The problems were resolved using the well-known solutions typical
for an ETL process, since data migration can be considered as a type of ETL
process. In this paper we describe the solutions for the problems that we
encountered while migrating the data.
","Marko Vu<U+010D>kovi<U+0107>|Toni Stojanovski","","http://arxiv.org/abs/1205.0104v1","http://arxiv.org/pdf/1205.0104v1","","CIIT Conference, April 2012, Bitola Macedonia","","","cs.SE","cs.SE"
"379","1205.1078v1","2012-05-04 22:29:44","2012-05-04 22:29:44","Analyses of Baby Name Popularity Distribution in U.S. for the Last 131
  Years","  We examine the complete dataset of baby name popularity collected by U.S.
Social Security Administration for the last 131 years (1880-2010). The ranked
baby name popularity can be fitted empirically by a piecewise function
consisting of Beta function for the high-ranking names and power-law function
for low-ranking names, but not power-law (Zipf's law) or Beta function by
itself.
","Wentian Li","","http://arxiv.org/abs/1205.1078v1","http://arxiv.org/pdf/1205.1078v1","http://dx.doi.org/10.1002/cplx.21409","6 figures","Complexity, 18(1):44-50 (2012)","10.1002/cplx.21409","nlin.AO","nlin.AO|stat.AP"
"380","1205.1604v1","2012-05-08 07:12:47","2012-05-08 07:12:47","ACO based routing for MANETs","  Mobile ad hoc network (MANET) is a collection of wireless mobile nodes. It
dynamically forms a temporary network without using any pre existing network
infrastructure or centralized administration i.e. with minimal prior planning.
All nodes have routing capabilities and forward data packets to other nodes in
multi-hop fashion. As the network is dynamic, the network topology continuously
experiences alterations during deployment. The biggest challenge in MANETs is
to find a path between communicating nodes. The considerations of the MANET
environment and the nature of the mobile nodes create further complications
which results in the need to develop special routing algorithms to meet these
challenges. Swarm intelligence, a bio-inspired technique, which has proven to
be very adaptable in other problem domains, has been applied to the MANET
routing problem as it forms a good fit to the problem. In ant societies the
activities of the individuals are not regulated by any explicit form of
centralized control but are the result of self-organizing dynamics driven by
local interactions and communications among a number of relatively simple
individuals. This unique characteristic has made ant societies an attractive
and inspiring model for building new algorithms and new multi-agent systems. In
this paper, we have studied and reviewed Ant Colony based routing algorithms
and its variants. Finally, a performance evaluation of the original ARA and the
EARA is carried out with respect to each other.
","Mohammad Arif|Tara Rani","","http://arxiv.org/abs/1205.1604v1","http://arxiv.org/pdf/1205.1604v1","","12 pages, 7 figures, ISSN:0975-3834 (Online); 0975-4679 (Print), 2012","","","cs.NI","cs.NI"
"381","1205.2440v1","2012-05-11 06:55:42","2012-05-11 06:55:42","Novel multifunctional 90Y-labelled albumin magnetic microspheres for
  cancer therapy","  We present in vitro and in vivo studies of yttrium-90 (90Y)-labelled human
serum albumin magnetic microspheres (HSAMMS) as multifunctional agent for
bimodal radionuclide-hyperthermia cancer therapy. The HSAMMS were produced
using a modified emulsification-heat stabilization technique and contained
10-nm magnetite nanoparticles coated with citric acid, distributed as
inhomogeneous clusters within the albumin microspheres. The average particle
size of the complete HSAMMS was 20 (mu)m, and they exhibited superparamagnetic
behavior at room temperature. The stability of the 90^Y-labelled HSAMMS was
investigated in vitro (in saline and human serum) and in vivo by analyzing
their biodistribution in normal Wistar rats. The in vitro experiments revealed
the high stability of the labelled HSAMMS in saline and human serum after 72 h.
Following the intravenous administration of the 90^Y-HSAMMS in rats, 88.81% of
the activity localizes in the lungs after 1 h, with 82.67% remaining after 72
h. These data on 90Y-HSAMMS provide good evidence for their potential use in
bimodal radionuclide-hyperthermia cancer therapy.
","S. Vranje<U+0161>-\Djuri<U+0107>|M. Radovi<U+0107>|N. Nikoli<U+0107>|D. Jankovi<U+0107>|G. F. Goya|T. E. Torres|M. P. Calatayud|I. J. Bruvera|M. R. Ibarra|V. Spasojevi<U+0107>|B. Jancar|B. Anti<U+0107>","","http://arxiv.org/abs/1205.2440v1","http://arxiv.org/pdf/1205.2440v1","http://dx.doi.org/10.1039/c2jm35593k","27 pages, 9 figures, submitted to Acta Biomateriala","J. Mater. Chem., 2012,22, 24017-24025","10.1039/c2jm35593k","q-bio.TO","q-bio.TO|cond-mat.mtrl-sci|physics.med-ph"
"382","1205.3655v5","2012-05-16 12:26:51","2012-11-19 17:34:47","P versus UP","  Admin note: withdrawn by arXiv admin because of the use of a pseudonym, in
violation of arXiv policy.
","Asia Furones","","http://arxiv.org/abs/1205.3655v5","http://arxiv.org/pdf/1205.3655v5","","Administratively withdrawn due to policy violations","","","cs.CC","cs.CC"
"383","1205.3686v1","2012-05-16 14:29:51","2012-05-16 14:29:51","Valuation and hedging of the ruin-contingent life annuity (RCLA)","  This paper analyzes a novel type of mortality contingent-claim called a
ruin-contingent life annuity (RCLA). This product fuses together a
path-dependent equity put option with a ""personal longevity"" call option. The
annuitant's (i.e. long position) payoff from a generic RCLA is \$1 of income
per year for life, akin to a defined benefit pension, but deferred until a
pre-specified financial diffusion process hits zero. We derive the PDE and
relevant boundary conditions satisfied by the RCLA value (i.e. the hedging
cost) assuming a complete market where No Arbitrage is possible. We then
describe some efficient numerical techniques and provide estimates of a typical
RCLA under a variety of realistic parameters.
  The motivation for studying the RCLA on a stand-alone basis is two-fold.
First, it is implicitly embedded in approximately \$1 trillion worth of U.S.
variable annuity (VA) policies; which have recently attracted scrutiny from
financial analysts and regulators. Second, the U.S. administration - both
Treasury and Department of Labor - have been encouraging Defined Contribution
(401k) plans to offer stand-alone longevity insurance to participants, and we
believe the RCLA would be an ideal and cost effective candidate for that job.
","Huaxiong Huang|Moshe A. Milevsky|Thomas S. Salisbury","","http://arxiv.org/abs/1205.3686v1","http://arxiv.org/pdf/1205.3686v1","","","J. Risk and Insurance 81 (2014), 367-395","","q-fin.PR","q-fin.PR"
"384","1205.4048v1","2012-05-17 21:41:08","2012-05-17 21:41:08","An Interface for the Virtual Observatory of the University of Guanajuato","  We present the first attempts to build a user-friendly interface for the
Virtual Observatory of the University of Guanajuato. The data tables will be
accessible to the public through PHP scripts and SQL database managers, such as
MySQL and PostgreSQL, all administrated through phpMyAdmin and pgMyAdmin.
Although it is not made public yet, this interface will be the basis upon which
the final front end for our VO will be built. Furthermore, we present a
preliminary version of a web front end to the publicly available stellar
population synthesis code STARLIGHT (starlight.ufsc.br) which will be made
available with our VO. This front end aims to provide an easy and flexible
access to the code itself, letting users fit their own observed spectra with
their preferred combination of physical and technical parameters, rather than
making available only the results of fitting a specific sample of spectra with
predefined parameters.
","Rene A. Ortega-Minakata|Juan P. Torres-Papaqui|Heinz Andernach|Hermenegildo Fernandez-Santos","Departamento de Astronomia, Universidad de Guanajuato|Departamento de Astronomia, Universidad de Guanajuato|Departamento de Astronomia, Universidad de Guanajuato|Maestria en Medios Interactivos, Universidad Tecnologica de la Mixteca","http://arxiv.org/abs/1205.4048v1","http://arxiv.org/pdf/1205.4048v1","","4 pages, 3 figures, 2 tables; to appear in More than Research, Volume
  3, Proceedings of the 3rd International Supercomputing Conference in Mexico,
  ISUM 2012, Guanajuato, Mexico, 14-16 March 2012","","","astro-ph.IM","astro-ph.IM|astro-ph.CO"
"385","1205.4261v1","2012-05-18 20:48:55","2012-05-18 20:48:55","Deployment of software components: Application to Wireless System","  The wide variety of wireless devices brings to design mobile applications as
a collection of interchangeable software components adapted to the deployment
environment of the software. To ensure the proper functioning of the software
assembly and make a real enforcement in case of failures, the introduction of
concepts, models and tools necessary for the administration of these components
is crucial. This article proposes a method for deploying components in wireless
systems.
","Kouninef Belkacem|Bouzerita Mohamed","","http://arxiv.org/abs/1205.4261v1","http://arxiv.org/pdf/1205.4261v1","","","ARPN Journal of Systems and Software Volume 1 No. 3, JUNE 2011
  ISSN 2222-9833","","cs.SE","cs.SE|68Nxx|D.2.10; D.2.11"
"386","1205.5720v1","2012-05-25 15:18:32","2012-05-25 15:18:32","Tie-RBAC: An application of RBAC to Social Networks","  This paper explores the application of role-based access control to social
networks, from the perspective of social network analysis. Each tie, composed
of a relation, a sender and a receiver, involves the sender's assignation of
the receiver to a role with permissions. The model is not constrained to
system-defined relations and lets users define them unilaterally. It benefits
of RBAC's advantages, such as policy neutrality, simplification of security
administration and permissions on other roles. Tie-RBAC has been implemented in
a core for building social network sites, Social Stream.
","Antonio Tapiador|Diego Carrera|Joaquin Salvachua","","http://arxiv.org/abs/1205.5720v1","http://arxiv.org/pdf/1205.5720v1","","Web 2.0 Security & Privacy 2011","","","cs.SI","cs.SI|cs.CR|K.6.5; H.3.5"
"387","1205.5874v2","2012-05-26 11:34:36","2012-11-19 16:27:13","Every set has its S-divisor","  Admin note: withdrawn by arxiv admin due to use of pseudonym against arXiv
policy.
"," Strongart","","http://arxiv.org/abs/1205.5874v2","http://arxiv.org/pdf/1205.5874v2","","Withdrawn by arXiv administration due to policy violation","","","math.GM","math.GM"
"388","1205.5960v1","2012-05-27 12:41:42","2012-05-27 12:41:42","Ontology-oriented e-gov services retrieval","  The semantic e-government is a new application field accompanying the
development of semantic web where the ontologies have become a fertile field of
investigation. This is due firstly to both the complexity and the size of
e-government systems and secondly to the importance of the issues. However,
permitting easy and personalized access to e-government services has become, at
this juncture, an arduous and not spontaneous process. Indeed, the provided
e-gov services to the user represent a critical contact point between
administrations and users. The encountered problems in the e-gov services
retrieving process are: the absence of an integrated one-stop government, the
difficulty of localizing the services' sources, the lack of mastery of search
terms and the deficiency of multilingualism of the online services. In order to
solve these problems, to facilitate access to e-gov services and to satisfy the
needs of potential users, we propose an original approach to this issue. This
approach incorporates a semantic layer as a crucial element in the retrieving
process. It consists in implementing a personalized search system that
integrates ontology of the e-gov domain in this process.
","Hassania Ouchetto|Ouail Ouchetto|Ounsa Roudies","","http://arxiv.org/abs/1205.5960v1","http://arxiv.org/pdf/1205.5960v1","","","IJCSI International Journal of Computer Science Issues, Vol. 9,
  Issue 2, No 3, March 2012 ISSN (Online): 1694-0814","","cs.CY","cs.CY"
"389","1205.6571v1","2012-05-30 08:05:37","2012-05-30 08:05:37","Robust and economical multi-sample, multi-wavelength UV/vis absorption
  and fluorescence detector for biological and chemical contamination","  We present a portable multi-channel, multi-sample UV/vis absorption and
fluorescence detection device, which has no moving parts, can operate
wirelessly and on batteries, interfaces with smart mobile phones or tablets,
and has the sensitivity of commercial instruments costing an order of magnitude
more. We use UV absorption to measure the concentration of ethylene glycol in
water solutions at all levels above those deemed unsafe by the United States
Food and Drug Administration; in addition we use fluorescence to measure the
concentration of d-glucose. Both wavelengths can be used concurrently to
increase measurement robustness and increase detection sensitivity. Our small
robust economical device can be deployed in the absence of laboratory
infrastructure, and therefore may find applications immediately following
natural disasters, and in more general deployment for much broader-based
testing of food, agricultural and household products to prevent outbreaks of
poisoning and disease.
","Peter J. Lu|Melanie M. Hoehl|James B. Macarthur|Peter A. Sims|Hongshen Ma|Alexander H. Slocum","","http://arxiv.org/abs/1205.6571v1","http://arxiv.org/pdf/1205.6571v1","","13 pages, 6 figures","","","physics.ins-det","physics.ins-det|cond-mat.mtrl-sci|physics.optics|physics.soc-ph"
"390","1206.1029v1","2012-06-02 17:49:26","2012-06-02 17:49:26","Influenza Virus Vaccine Efficacy Based On Conserved Sequence Alignment","  The rapid outbreak of bird flu challenges the outcome of effective vaccine
for the upcoming years. The recent research established different norms to
eliminate flu pandemics. This can be made possible with skilled experimental
analyses and by tracking the recent virulent strain and can be broadly
applicable with effective testing of vaccine efficacy. Every year World Health
Organization (WHO) reveals the administration of drug and vaccine to counter
arrest the spread of flu among the population. As there are recurrent failures
in priming the population, the complete eradication of the flu pandemic is
still appears to be an unresolved problem. To overcome the current scenario,
high level efforts with theoretical and practical research is required and it
can enhance the scope in this field. The recent advancements also allow the
researchers to endeavor effective vaccine to meet the emerging flu types. Only
the standardized vaccination among the population at the time of flu pandemics
will revolutionalize the current propositions against influenza virus. This
paper shows the deficiencies of vaccine fitness research as there are reported
failures and less efficacy of vaccine even after priming the population from
referred evidences and studies. It also shows simple experimental approach in
detecting the effective vaccine among the vaccines announced by WHO.
","Baby Jerald|T. R. Gopalakrishnan Nair","","http://arxiv.org/abs/1206.1029v1","http://arxiv.org/pdf/1206.1029v1","http://dx.doi.org/10.1109/ICoBE.2012.6179031","International Conference on Biomedical Engineering (ICoBE), 2012 Date
  of Conference: 27-28 Feb. 2012 Page(s): 327 - 329, 2 figures, 3 pages","","10.1109/ICoBE.2012.6179031","q-bio.OT","q-bio.OT"
"391","1206.1415v3","2012-06-07 07:55:04","2015-06-04 14:47:22","Quadrupole Excitation in Tunnel Splitting Oscillation in Nano-Particle
  $Mn_{12}$","  We analyze the interference between tunneling paths that occurs for a spin
system with special Hamiltonian both for dipole and quadrupole excitation.
Using an instanton approach, we find that as the strength of the second-order
transverse anisotropy is increased, the tunnel splitting for both excitations
are modulated, with zeros occurring periodically and the number of quenching
points for quadrupole excitation decreases. This effect results from the
interference of four tunneling paths connecting easy-axis spin orientations and
occurs in the absence of any magnetic field.
","Yousef Yousefi|Khikmat Kh. Muminov","","http://arxiv.org/abs/1206.1415v3","http://arxiv.org/pdf/1206.1415v3","http://dx.doi.org/10.1155/2012/530765","arXiv admin note: This paper has been withdrawn by arXiv
  administrators: v2 is withdrawn because it is unrelated to v1 and thus an
  inappropriate replacement. v1 is withdrawn because of excessive unattributed
  use of the work of others","","10.1155/2012/530765","cond-mat.other","cond-mat.other"
"392","1206.1653v2","2012-06-08 02:44:05","2013-06-05 16:02:19","PriSM: A Private Social Mesh for Leveraging Social Networking at
  Workplace","  In this work we describe the PriSM framework for decentralized deployment of
a federation of autonomous social networks (ASN). The individual ASNs are
centrally managed by organizations according to their institutional needs,
while cross-ASN interactions are facilitated subject to security and
confidentiality requirements specified by administrators and users of the ASNs.
Such decentralized deployment, possibly either on private or public clouds,
provides control and ownership of information/flow to individual organizations.
Lack of such complete control (if third party online social networking services
were to be used) has so far been a great barrier in taking full advantage of
the novel communication mechanisms at workplace that have however become
commonplace for personal usage with the advent of Web 2.0 platforms and online
social networks. PriSM provides a practical solution for organizations to
harness the advantages of online social networking both in
intra/inter-organizational settings without sacrificing autonomy, security and
confidentiality needs.
","Stefano Braghin|Jackson Tan|Rajesh Sharma|Anwitaman Datta","","http://arxiv.org/abs/1206.1653v2","http://arxiv.org/pdf/1206.1653v2","","","","","cs.DC","cs.DC"
"393","1206.1748v1","2012-06-08 12:31:52","2012-06-08 12:31:52","A novel approach for security issues in VoIP networks in Virtualization
  with IVR","  VoIP (Voice over Internet Protocol) is a growing technology during last
decade. It provides the audio, video streaming facility on successful
implementation in the network. However, it provides the text transport facility
over the network. Due to implementation of it the cost effective solution, it
can be developed for the intercommunication among the employees of a
prestigious organization. The proposed idea has been implemented on the audio
streaming area of the VoIP technology. In the audio streaming, the security
vulnerabilities are possible on the VoIP server during communication between
two parties. In the proposed model, first the VoIP system has been implemented
with IVR (Interactive Voice Response) as a case study and with the
implementation of the security parameters provided to the asterisk server which
works as a VoIP service provider. The asterisk server has been configured with
different security parameters like VPN server, Firewall iptable rules,
Intrusion Detection and Intrusion Prevention System. Every parameter will be
monitored by the system administrator of the VoIP server along with the MySQL
database. The system admin will get every update related to the attacks on the
server through Mail server attached to the asterisk server. The main beauty of
the proposed system is VoIP server alone is configured as a VoIP server, IVR
provider, Mail Server with IDS and IPS, VPN server, connection with database
server in a single asterisk server inside virtualization environment. The VoIP
system is implemented for a Local Area Network inside the university system
","Kinjal Shah|Satya Prakash Ghrera|Alok Thaker","","http://arxiv.org/abs/1206.1748v1","http://arxiv.org/pdf/1206.1748v1","http://dx.doi.org/10.5121/ijdps.2012.3319","20 pages, 4 figures, 3 tables, 3 snap shots","International Journal of Distributed and Parallel Systems (IJDPS),
  Vol.3, No.3, May 2012, 219-238","10.5121/ijdps.2012.3319","cs.NI","cs.NI|68U35|D.4.0; C.2.1"
"394","1206.1833v1","2012-06-08 18:19:01","2012-06-08 18:19:01","CyberChair: A Web-Based Groupware Application to Facilitate the Paper
  Reviewing Process","  In this paper we describe CyberChair, a web-based groupware application that
supports the review process for technical contributions to conferences.
CyberChair deals with most administrative tasks that are involved in the review
process, such as storing author information, abstracts, (camera-ready) papers
and reviews. It generates several overviews based on the reviews which support
the Program Committee (PC) in selecting the best papers. CyberChair points out
conflicting reviews and offers the reviewers means to easily communicate to
solve these conflicts. In his paper Identify the Champion, O. Nierstrasz
describes this review process in terms of a pattern language. CyberChair
supports PCs by using these patterns in its implementation.
","Richard van de Stadt","","http://arxiv.org/abs/1206.1833v1","http://arxiv.org/pdf/1206.1833v1","","7 pages, 7 figures, created 29 May 2000, when I still worked at the
  University of Twente, The Netherlands. The paper was submitted to a Python
  conference and accepted, but due to a misunderstanding with my employer, I
  had to withdraw the paper from the conference. The paper describes the public
  version, called CyberChair, and not the commercial version called
  CyberChairPRO","","","cs.DL","cs.DL"
"395","1206.2302v3","2012-06-11 18:07:25","2013-01-21 20:09:37","The Elements of Item Response Theory and its Framework in Analyzing
  Introductory Astronomy College Student Misconceptions. I. Galaxies","  This is the first in a series of papers that analyze college student beliefs
in realms where common astronomy misconceptions are prevalent. Data was
collected through administration of an inventory distributed at the end of an
introductory college astronomy course. In this paper, we present the basic
mathematics of item response theory (IRT), and then we use it to explore
concepts related to galaxies. We show how IRT determines the difficulty of each
galaxy topic under consideration. We find that the concept of galaxy spatial
distribution presents the greatest challenge to students of all the galaxy
topics. We also find and present the most logical sequence to teach galaxy
topics as a function of the audience's age.
","Andrej Favia|Neil F. Comins|Geoffrey L. Thorpe","","http://arxiv.org/abs/1206.2302v3","http://arxiv.org/pdf/1206.2302v3","","43 pages double-spaced, 14 figures, contains two Appendices, larger
  sample size included","","","physics.ed-ph","physics.ed-ph"
"396","1206.5298v2","2012-06-24 00:41:22","2013-06-12 10:36:53","Limited Urban Growth: London's Street Network Dynamics since the 18th
  Century","  We investigate the growth dynamics of Greater London defined by the
administrative boundary of the Greater London Authority, based on the evolution
of its street network during the last two centuries. This is done by employing
a unique dataset, consisting of the planar graph representation of nine time
slices of Greater London's road network spanning 224 years, from 1786 to 2010.
Within this time-frame, we address the concept of the metropolitan area or city
in physical terms, in that urban evolution reveals observable transitions in
the distribution of relevant geometrical properties. Given that London has a
hard boundary enforced by its long-standing green belt, we show that its street
network dynamics can be described as a fractal space-filling phenomena up to a
capacitated limit, whence its growth can be predicted with a striking level of
accuracy. This observation is confirmed by the analytical calculation of key
topological properties of the planar graph, such as the topological growth of
the network and its average connectivity. This study thus represents an example
of a strong violation of Gibrat's law. In particular, we are able to show
analytically how London evolves from a more loop-like structure, typical of
planned cities, toward a more tree-like structure, typical of self-organized
cities. These observations are relevant to the discourse on sustainable urban
planning with respect to the control of urban sprawl in many large cities,
which have developed under the conditions of spatial constraints imposed by
green belts and hard urban boundaries.
","A. Paolo Masucci|Kiril Stanilov|Michael Batty","","http://arxiv.org/abs/1206.5298v2","http://arxiv.org/pdf/1206.5298v2","http://dx.doi.org/10.1371/journal.pone.0069469","PlosOne, in publication","PLoS ONE 8(8): e69469 (2013)","10.1371/journal.pone.0069469","physics.data-an","physics.data-an|physics.soc-ph"
"397","1206.7038v2","2012-06-25 16:42:08","2012-07-05 19:09:44","Comments on ""Comments on ""Prediction of Subharmonic Oscillation in
  Switching Converters Under Different Control Strategies""""","  arXiv admin note: This submission has been removed by arXiv administrators
due to unprofessional personal attack.
","El Aroudi","","http://arxiv.org/abs/1206.7038v2","http://arxiv.org/pdf/1206.7038v2","","arXiv admin note: This submission has been removed by arXiv
  administrators due to unprofessional personal attack","","","cs.SY","cs.SY|math.DS|nlin.CD"
"398","1206.6322v1","2012-06-27 16:09:46","2012-06-27 16:09:46","A New Scale for Attribute Dependency in Large Database Systems","  Large, data centric applications are characterized by its different
attributes. In modern day, a huge majority of the large data centric
applications are based on relational model. The databases are collection of
tables and every table consists of numbers of attributes. The data is accessed
typically through SQL queries. The queries that are being executed could be
analyzed for different types of optimizations. Analysis based on different
attributes used in a set of query would guide the database administrators to
enhance the speed of query execution. A better model in this context would help
in predicting the nature of upcoming query set. An effective prediction model
would guide in different applications of database, data warehouse, data mining
etc. In this paper, a numeric scale has been proposed to enumerate the strength
of associations between independent data attributes. The proposed scale is
built based on some probabilistic analysis of the usage of the attributes in
different queries. Thus this methodology aims to predict future usage of
attributes based on the current usage.
","Soumya Sen|Anjan Dutta|Agostino Cortesi|Nabendu Chaki","","http://arxiv.org/abs/1206.6322v1","http://arxiv.org/pdf/1206.6322v1","","12 pages - paper accepted for presentation and publication in CISIM
  2012 International Confrence","","","cs.IR","cs.IR|cs.DB"
"399","1206.6859v1","2012-06-27 16:27:12","2012-06-27 16:27:12","Propagation of Delays in the National Airspace System","  The National Airspace System (NAS) is a large and complex system with
thousands of interrelated components: administration, control centers,
airports, airlines, aircraft, passengers, etc. The complexity of the NAS
creates many difficulties in management and control. One of the most pressing
problems is flight delay. Delay creates high cost to airlines, complaints from
passengers, and difficulties for airport operations. As demand on the system
increases, the delay problem becomes more and more prominent. For this reason,
it is essential for the Federal Aviation Administration to understand the
causes of delay and to find ways to reduce delay. Major contributing factors to
delay are congestion at the origin airport, weather, increasing demand, and air
traffic management (ATM) decisions such as the Ground Delay Programs (GDP).
Delay is an inherently stochastic phenomenon. Even if all known causal factors
could be accounted for, macro-level national airspace system (NAS) delays could
not be predicted with certainty from micro-level aircraft information. This
paper presents a stochastic model that uses Bayesian Networks (BNs) to model
the relationships among different components of aircraft delay and the causal
factors that affect delays. A case study on delays of departure flights from
Chicago O'Hare international airport (ORD) to Hartsfield-Jackson Atlanta
International Airport (ATL) reveals how local and system level environmental
and human-caused factors combine to affect components of delay, and how these
components contribute to the final arrival delay at the destination airport.
","Kathryn Blackmond Laskey|Ning Xu|Chun-Hung Chen","","http://arxiv.org/abs/1206.6859v1","http://arxiv.org/pdf/1206.6859v1","","Appears in Proceedings of the Twenty-Second Conference on Uncertainty
  in Artificial Intelligence (UAI2006)","","","cs.AI","cs.AI"
"400","1206.6666v1","2012-06-28 12:49:16","2012-06-28 12:49:16","Analyzing establishment nonresponse using an interpretable regression
  tree model with linked administrative data","  To gain insight into how characteristics of an establishment are associated
with nonresponse, a recursive partitioning algorithm is applied to the
Occupational Employment Statistics May 2006 survey data to build a regression
tree. The tree models an establishment's propensity to respond to the survey
given certain establishment characteristics. It provides mutually exclusive
cells based on the characteristics with homogeneous response propensities. This
makes it easy to identify interpretable associations between the characteristic
variables and an establishment's propensity to respond, something not easily
done using a logistic regression propensity model. We test the model obtained
using the May data against data from the November 2006 Occupational Employment
Statistics survey. Testing the model on a disjoint set of establishment data
with a very large sample size $(n=179,360)$ offers evidence that the regression
tree model accurately describes the association between the establishment
characteristics and the response propensity for the OES survey. The accuracy of
this modeling approach is compared to that of logistic regression through
simulation. This representation is then used along with frame-level
administrative wage data linked to sample data to investigate the possibility
of nonresponse bias. We show that without proper adjustments the nonresponse
does pose a risk of bias and is possibly nonignorable.
","Polly Phipps|Daniell Toth","","http://arxiv.org/abs/1206.6666v1","http://arxiv.org/pdf/1206.6666v1","http://dx.doi.org/10.1214/11-AOAS521","Published in at http://dx.doi.org/10.1214/11-AOAS521 the Annals of
  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of
  Mathematical Statistics (http://www.imstat.org)","Annals of Applied Statistics 2012, Vol. 6, No. 2, 772-794","10.1214/11-AOAS521","stat.AP","stat.AP"
"401","1207.0758v2","2012-07-03 17:28:54","2012-07-11 07:52:53","Surveying Solutions to Securing On-Demand Routing Protocols in MANETs","  A Mobile ad hoc Network or MANET is a wireless network of mobile devices that
has the ability to self-configure and self-organise and is characterised by an
absence of centralised administration and network infrastructure. An
appreciable number of routing protocols used in a typical MANET have left the
critical aspect of security out of consideration by assuming that all of its
constituent nodes are trustworthy and non-malicious. In this paper, we discuss
some of the major threats that such networks are vulnerable to, because of
these inherently insecure protocols. The focus is specifically on the
source-initiated and on-demand routing protocols. Further, solutions and
modifications to these protocols that have been proposed over time, enabling
them to mitigate the aforementioned threats to some extent, are also analysed.
","Nitish Balachandran","","http://arxiv.org/abs/1207.0758v2","http://arxiv.org/pdf/1207.0758v2","","7 pages, 1 table, updated refernces","","","cs.NI","cs.NI"
"402","1207.4229v1","2012-07-17 22:40:07","2012-07-17 22:40:07","European Extremely Large Telescope Site Characterization II: High
  angular resolution parameters","  This is the second article of a series devoted to European Extremely Large
Telescope (E-ELT) site characterization. In this article we present the main
properties of the parameters involved in high angular resolution observations
from the data collected in the site testing campaign of the E-ELT during the
Design Study (DS) phase. Observations were made in 2008 and 2009, in the four
sites selected to shelter the future E-ELT (characterized under the ELT-DS
contract): Aklim mountain in Morocco, Observatorio del Roque de los Muchachos
(ORM) in Spain, Mac\'on range in Argentina, and Cerro Ventarrones in Chile. The
same techniques, instruments and acquisition procedures were taken on each
site. A Multiple Aperture Scintillation Sensor (MASS) and a Differential Image
Motion Monitor (DIMM) were installed at each site. Global statistics of the
integrated seeing, the free atmosphere seeing, the boundary layer seeing and
the isoplanatic angle were studied for each site, and the results are presented
here. In order to estimate other important parameters such as the coherence
time of the wavefront and the overall parameter ""coherence \'etendue""
additional information of vertical profiles of the wind speed was needed. Data
were retrieved from the National Oceanic and Atmospheric Administration (NOAA)
archive. Ground wind speed was measured by Automatic Weather Stations (AWS).
More aspects of the turbulence parameters such as their seasonal trend, their
nightly evolution and their temporal stability were also obtained and analyzed.
","Hector Vazquez Ramio|Jean Vernin|Casiana Munoz-Tunon|Marc Sarazin|Antonia M. Varela|Herve Trinquet|Jose Miguel Delgado|Jesus J. Fuensalida|Marcos Reyes|Abdelmajid Benhida|Zouhair Benkhaldoun|Diego Garcia Lambas|Youssef Hach|M. Lazrek|Gianluca Lombardi|Julio Navarrete|Pablo Recabarren|Victor Renzi|Mohammed Sabil|Ruben Vrech","","http://arxiv.org/abs/1207.4229v1","http://arxiv.org/pdf/1207.4229v1","http://dx.doi.org/10.1086/667599","46 pages and 17 figures. Accepted to be published in PASP","","10.1086/667599","astro-ph.IM","astro-ph.IM|physics.ao-ph"
"403","1207.4291v1","2012-07-18 08:02:33","2012-07-18 08:02:33","ConnectiCity, augmented perception of the city","  As we move through cities in our daily lives, we are in a constant state of
transformation of the spaces around us. The form and essence of urban space
directly affects people's behavior, describing in their perception what is
possible or impossible, allowed or prohibited, suggested or advised against. We
are now able to fill and stratify space/time with digital information layers,
completely wrapping cities in a membrane of information and of opportunities
for interaction and communication. Mobile devices, smartphones, wearables,
digital tags, near field communication devices, location based services and
mixed/augmented reality have gone much further in this direction, turning the
world into an essentially read/write, ubiquitous publishing surface. The usage
of mobile devices and ubiquitous technologies alters the understanding of
place. In this process, the definition of (urban) landscape powerfully shifts
from a definition which is purely administrative (e.g.: the borders of the
flower bed in the middle of a roundabout) to one that is multiplied according
to all individuals which experience that location; as a lossless sum of their
perceptions; as a stratification of interpretations and activities which forms
our cognition of space and time. In our research we investigated the
possibilities to use the scenario which sees urban spaces progressively filling
with multiple layers of real-time, ubiquitous, digital information to
conceptualize, design and implement a series of usage scenarios. It is possible
to create multiple layers of narratives which traverse the city and which allow
us to read them in different ways, according to the different strategies and
methodologies enabling us to highlight how cities express points of view on the
environment, culture, economy, transports, energy and politics.
","Salvatore Iaconesi|Oriana Persico","","http://arxiv.org/abs/1207.4291v1","http://arxiv.org/pdf/1207.4291v1","","19 pages, 6 figures, presented at Information Visualization 12,
  Montpellier, France","","","cs.CY","cs.CY|cs.SI|physics.soc-ph"
"404","1207.5734v1","2012-07-24 15:56:44","2012-07-24 15:56:44","A Review of routing protocols for mobile cognitive radio ad hoc networks","  Ad hoc network is a collection of wireless mobile nodes that dynamically form
a temporary network without the use of any existing network infrastructure or
centralized administration. A cognitive radio is a radio that can change its
transmitter parameters based on interaction with the environment in which it
operates. The basic idea of cognitive radio networks is that the unlicensed
devices (cognitive radio users or secondary users) need to vacate the spectrum
band once the licensed device (primary user) is detected. Cognitive capability
and reconfigurability are the key characteristics of cognitive radio. Routing
is an important issue in Mobile Cognitive Radio Ad Hoc Networks (MCRAHNs). In
this paper, a survey of routing protocols for mobile cognitive radio ad
networks is discussed.
","S. Selvakanmani|M. Sumathi","","http://arxiv.org/abs/1207.5734v1","http://arxiv.org/pdf/1207.5734v1","","","","","cs.NI","cs.NI"
"405","1208.0259v1","2012-08-01 16:02:35","2012-08-01 16:02:35","Electronic administration in Spain: from its beginnings to the present","  This study presents the basic lines of electronic administration in Spain.
The complexity of the Spanish political-administrative system makes such a
study challenging, in view of the considerable degree of autonomy and
competences of the regional administrative bodies and local agencies with
respect to the central government, the former being more visible in the 17
regions of Spain. Nonetheless, the central government maintains a series of
legal instruments that allow a certain common framework of action to be
imposed, aside from what is put into effect through diverse programs aimed
precisely to develop common tools for the regions and municipalities of Spain.
  After an introduction that provides some necessary background, this study
describes the legislative framework in which Spain's electronic administrative
system has developed. The data included in the study refer to investment in
information and communication technologies (ICT) and the services offered by
the different Administrations on the internet; internet access by citizens,
homes, businesses, and employees, as well as the interactivity existing with
administrations by means of the internet; the origins and rise of various
political initiatives of the Central Government involving electronic
administration; and finally, the situation of civil service personnel, as
catalysts of the success of Information Society in the Public Administration
within Spain.
","Antonio Munoz-Canavate|Pedro Hipola","","http://arxiv.org/abs/1208.0259v1","http://arxiv.org/pdf/1208.0259v1","http://dx.doi.org/10.1016/j.giq.2010.05.008","","Government Information Quarterly, vol 28, 1, January 2011, pp.
  74-90","10.1016/j.giq.2010.05.008","cs.CY","cs.CY|cs.DL"
"406","1208.3557v1","2012-08-17 09:09:55","2012-08-17 09:09:55","Distributed Denial of Service Prevention Techniques","  The significance of the DDoS problem and the increased occurrence,
sophistication and strength of attacks has led to the dawn of numerous
prevention mechanisms. Each proposed prevention mechanism has some unique
advantages and disadvantages over the others. In this paper, we present a
classification of available mechanisms that are proposed in literature on
preventing Internet services from possible DDoS attacks and discuss the
strengths and weaknesses of each mechanism. This provides better understanding
of the problem and enables a security administrator to effectively equip his
arsenal with proper prevention mechanisms for fighting against DDoS threat.
","B. B. Gupta|R. C. Joshi|Manoj Misra","","http://arxiv.org/abs/1208.3557v1","http://arxiv.org/pdf/1208.3557v1","","ISSN: 1793-8198","International Journal of Computer and Electrical Engineering
  (IJCEE), vol. 2, number 2, pp. 268-276, 2010","","cs.CR","cs.CR"
"407","1208.3847v1","2012-08-19 15:46:23","2012-08-19 15:46:23","On applications of conservation laws in pharmacokinetics","  There has been certain criticism raised by A. Rescigno [2,8,9,12-15] against
the standard formulation of pharmacokinetics. In 2011 it has been suggested
that inconsistencies in pharmacokinetics should be eliminated after deriving
""pharmacokinetic parameters"" from conservation laws [3]. In the following text
a simple system of conservation laws for extra - vascular administration of a
drug is explicitly given and preliminary discussion concerning this issue is
included.
","S. Piekarski|M. Rewekant","","http://arxiv.org/abs/1208.3847v1","http://arxiv.org/pdf/1208.3847v1","","6 pages","","","q-bio.TO","q-bio.TO|92C45"
"408","1208.4169v1","2012-08-21 02:53:09","2012-08-21 02:53:09","A Storage Advisor for Hybrid-Store Databases","  With the SAP HANA database, SAP offers a high-performance in-memory
hybrid-store database. Hybrid-store databases---that is, databases supporting
row- and column-oriented data management---are getting more and more prominent.
While the columnar management offers high-performance capabilities for
analyzing large quantities of data, the row-oriented store can handle
transactional point queries as well as inserts and updates more efficiently. To
effectively take advantage of both stores at the same time the novel question
whether to store the given data row- or column-oriented arises. We tackle this
problem with a storage advisor tool that supports database administrators at
this decision. Our proposed storage advisor recommends the optimal store based
on data and query characteristics; its core is a cost model to estimate and
compare query execution times for the different stores. Besides a per-table
decision, our tool also considers to horizontally and vertically partition the
data and manage the partitions on different stores. We evaluated the storage
advisor for the use in the SAP HANA database; we show the recommendation
quality as well as the benefit of having the data in the optimal store with
respect to increased query performance.
","Philipp Rosch|Lars Dannecker|Gregor Hackenbroich|Franz Faerber","","http://arxiv.org/abs/1208.4169v1","http://arxiv.org/pdf/1208.4169v1","","VLDB2012","Proceedings of the VLDB Endowment (PVLDB), Vol. 5, No. 12, pp.
  1748-1758 (2012)","","cs.DB","cs.DB"
"409","1208.5875v1","2012-08-29 10:14:46","2012-08-29 10:14:46","Business Intelligence: A Rapidly Growing Option through Web Mining","  The World Wide Web is a popular and interactive medium to distribute
information in this scenario. The web is huge, diverse, ever changing, widely
disseminated global information service center. We are familiar with terms like
e-commerce, e-governance, e-market, e-finance, e-learning, e-banking etc. for
an organization it is new challenge to maintain direct contact with customers
because of the rapid growth in e-commerce, e-publishing and electronic service
delivery. To deal with this there is need of intelligent marketing strategies
and CRM (customer relationship management) i.e. the effective way of
integrating enterprise applications in real time. Web mining is the vast field
that helps to understand various concepts of different fields. Web usage mining
techniques are attempted to reason about different materialized issues of
Business Intelligence which include marketing expertise as domain knowledge and
are specifically designed for electronic commerce purposes. To this end, the
chapter provides an introduction to the field of Web mining and examines
existing as well as potential Web mining applications applicable for different
business function, like marketing, human resources, and fiscal administration.
Suggestions for improving information technology infrastructure are made, which
can help businesses interested in Web mining hit the ground running.
","Priyanka Rahi","Department of Computer Science H.P. University, Shimla, H.P., India","http://arxiv.org/abs/1208.5875v1","http://arxiv.org/pdf/1208.5875v1","","8 pages, 2 figure, 2 tables. arXiv admin note: text overlap with
  arXiv:cs/0405030 by other authors","","","cs.OH","cs.OH"
"410","1209.1803v1","2012-09-09 14:33:37","2012-09-09 14:33:37","Secure and Privacy-Preserving Authentication Protocols for Wireless Mesh
  Networks","  Wireless mesh networks (WMNs) have emerged as a promising concept to meet the
challenges in next-generation wireless networks such as providing flexible,
adaptive, and reconfigurable architecture while offering cost-effective
solutions to service providers. As WMNs become an increasingly popular
replacement technology for last-mile connectivity to the home networking,
community and neighborhood networking, it is imperative to design efficient and
secure communication protocols for these networks. However, several
vulnerabilities exist in currently existing protocols for WMNs. These security
loopholes can be exploited by potential attackers to launch attack on WMNs. The
absence of a central point of administration makes securing WMNs even more
challenging. The broadcast nature of transmission and the dependency on the
intermediate nodes for multi-hop communications lead to several security
vulnerabilities in WMNs. The attacks can be external as well as internal in
nature. External attacks are launched by intruders who are not authorized users
of the network. For example, an intruding node may eavesdrop on the packets and
replay those packets at a later point of time to gain access to the network
resources. On the other hand, the internal attacks are launched by the nodes
that are part of the WMN. On example of such attack is an intermediate node
dropping packets which it was supposed to forward. This chapter presents a
comprehensive discussion on the current authentication and privacy protection
schemes for WMN. In addition, it proposes a novel security protocol for node
authentication and message confidentiality and an anonymization scheme for
privacy protection of users in WMNs.
","Jaydip Sen","","http://arxiv.org/abs/1209.1803v1","http://arxiv.org/pdf/1209.1803v1","","32 pages, 10 figures. The work is an extended version of the author's
  previous works submitted in CoRR: arXiv:1107.5538v1 and arXiv:1102.1226v1","Secure and Privacy-Preserving Authentication Protocols for
  Wireless Mesh Networks - Book Chapter in Applied Cryptography and Network
  Security, Editor: Jaydip Sen, pp. 3 - 34, April 2012, published by INTECH,
  Croatia","","cs.CR","cs.CR|cs.NI"
"411","1209.4152v2","2012-09-19 05:23:40","2015-06-05 14:36:53","A note on the classification of linking pairings on 2-groups","  This paper has been withdrawn by arXiv administrators because of disputed
claims of authorship among former collaborators
","Ben Ntatin|William Glunt","","http://arxiv.org/abs/1209.4152v2","http://arxiv.org/pdf/1209.4152v2","http://dx.doi.org/10.4236/apm.2013.31002","This paper has been withdrawn by arXiv administrators because of
  disputed claims of authorship among former collaborators","Advances in Pure Mathematics 3:1 (January 2013) 6-13","10.4236/apm.2013.31002","math.GN","math.GN|math.GT|57N10, 57M27"
"412","1209.4410v1","2012-09-20 02:27:24","2012-09-20 02:27:24","Technical details on Kuranishi structure and virtual fundamental chain","  This is an expository article on the theory of Kuranishi structure and is
based on a series of pdf files we uploaded for the discussion of the google
group named `Kuranishi' (with its administrator H. Hofer). There we replied to
several questions concerning Kuranishi structure raised by K. Wehrheim. At this
stage we submit this article to the e-print arXiv, all the questions or
objections asked in that google group were answered, supplemented or confuted
by us. We first discuss the abstract theory of Kuranishi structure and virtual
fundamental chain/cycle. This part can be read independently from other parts.
We then describe the construction of Kuranishi structure on the moduli space of
pseudoholomorphic curves, including the complete analytic detail of the gluing
construction as well as the smoothness of the resulting Kuranishi structure.
The case of S^1 equivariant Kuranishi structure which appears in the study of
time independent Hamiltonian and the moduli space of Floer's equation is
included.
","Kenji Fukaya|Yong-Geun Oh|Hiroshi Ohta|Kaoru Ono","","http://arxiv.org/abs/1209.4410v1","http://arxiv.org/pdf/1209.4410v1","","257 pages. arXiv admin note: substantial text overlap with
  arXiv:1208.1340 by other authors","","","math.SG","math.SG|math.DG"
"413","1209.4751v1","2012-09-21 08:49:57","2012-09-21 08:49:57","Energy Aware, Scalable, K-Hop Based Cluster Formation In MANET","  The study of Mobile Ad-hoc Network remains attractive due to the desire to
achieve better performance and scalability. MANETs are distributed systems
consisting of mobile hosts that are connected by multi-hop wireless links. Such
systems are self organized and facilitate communication in the network without
any centralized administration. MANETs exhibit battery power constraint and
suffer scalability issues therefore cluster formation is expensive. This is due
to the large number of messages passed during the process of cluster formation.
Clustering has evolved as an imperative research domain that enhances system
performance such as throughput and delay in Mobile Ad hoc Networks (MANETs) in
the presence of both mobility and a large number of mobile terminals.In this
thesis, we present a clustering scheme that minimizes message overhead and
congestion for cluster formation and maintenance. The algorithm is devised to
be independent of the MANET Routing algorithm. Depending upon the context, the
clustering algorithm may be implemented in the routing or in higher layers. The
dynamic formation of clusters helps reduce data packet overhead, node
complexity and power consumption, The simulation has been performed in ns-2.
The simulation shows that the number of clusters formed is in proportion with
the number of nodes in MANET.
","Priyanka Chatterjee|Nikhil Agarwal","","http://arxiv.org/abs/1209.4751v1","http://arxiv.org/pdf/1209.4751v1","","","","","cs.DC","cs.DC|cs.DS|cs.NI"
"414","1209.5507v1","2012-09-25 06:27:07","2012-09-25 06:27:07","Performance comparison of various routing protocols in different
  mobility models","  Mobile Ad hoc Network (MANET) is a infrastructure less network in which two
or more devices have wireless communication which can communicate with each
other and exchange information without need of any centralized administrator.
Each node in the ad hoc network acts as a router, forwarding data packets for
other nodes. The main issue is to compare the existing routing protocol and
finding the best one. The scope of this study is to test routing performance of
three different routing protocols (AODV, OLSR and DSDV) with respect to various
mobility models using NS2 simulator. In this paper the parameters used for
comparison are packet delivery fraction (PDF), average end to end delay (AEED),
normalized routing load (NRL) and throughput.
","Neha Rani|Preeti Sharma|Pankaj Sharma","A.B.E.S. Engineering college|A.B.E.S. Engineering college|A.B.E.S. Engineering college","http://arxiv.org/abs/1209.5507v1","http://arxiv.org/pdf/1209.5507v1","","13 pages,13 figures, 1 Table","IJASUC (International journal of Ad-hoc, Sensor and Ubiquitous
  computing)Volume 3, Number 4, August 2012","","cs.NI","cs.NI"
"415","1210.1516v3","2012-10-01 16:15:34","2012-11-19 17:33:08","Goldbach's conjecture","  Admin note: withdrawn by arXiv admin because of the use of a pseudonym, in
violation of arXiv policy.
","Asia Furones","","http://arxiv.org/abs/1210.1516v3","http://arxiv.org/pdf/1210.1516v3","","Administratively withdrawn due to policy violation","","","math.GM","math.GM"
"416","1210.2196v1","2012-10-08 09:26:46","2012-10-08 09:26:46","Stability of fault plane solutions for Mw >= 4.8 in northern Italy in
  2012","  We propose a critical analysis of the moment tensor solutions of the major
seismic events that affected northern Italy in 2012. Inverting full waveforms
at regional distance using the non-linear method named INPAR, we investigate
period dependent resolution that affects in particular the solutions of shallow
events. This is mainly due to the poor resolution of Mzx and Mzy components of
the seismic tensor when inverting signals whose wavelengths significantly
exceed the source depth. As a consequence, instability affects both source
depth and fault plane solution retrieval, and spurious large Compensated Linear
Vector Dipole components arise. The inversion performed at cutoff periods
shorter than 20 s reveals in many cases different details of the rupture
process, that are not resolved inverting at longer cutoff periods. Thus we
conclude that inversion of full waveforms at cutoff period as short as possible
should be preferred.
","Enrico Brandmayr|Fabio Romanelli|Giuliano Francesco Panza","Department of Mathematics and Geosciences, University of Trieste|Department of Mathematics and Geosciences, University of Trieste|Department of Mathematics and Geosciences, University of Trieste","http://arxiv.org/abs/1210.2196v1","http://arxiv.org/pdf/1210.2196v1","","","","","physics.geo-ph","physics.geo-ph"
"417","1210.3307v1","2012-10-11 17:55:51","2012-10-11 17:55:51","Modelling an Automatic Proof Generator for Functional Dependency Rules
  Using Colored Petri Net","  Database administrators need to compute closure of functional dependencies
(FDs) for normalization of database systems and enforcing integrity rules.
Colored Petri net (CPN) is a powerful formal method for modelling and
verification of various systems. In this paper, we modelled Armstrong's axioms
for automatic proof generation of a new FD rule from initial FD rules using
CPN. For this purpose, a CPN model of Armstrong's axioms presents and initial
FDs considered in the model as initial color set. Then we search required FD in
the state space of the model via model checking. If it exists in the state
space, then a recursive ML code extracts the proof of this FD rule using
further searches in the state space of the model.
","Saeid Pashazadeh|Maryam Pashazadeh","","http://arxiv.org/abs/1210.3307v1","http://arxiv.org/pdf/1210.3307v1","http://dx.doi.org/10.5121/ijfcst.2012.2504","17 pages, 4 figures","International Journal in Foundations of Computer Science &
  Technology (IJFCST) 2 (2012) 31-47","10.5121/ijfcst.2012.2504","cs.DB","cs.DB|cs.FL|cs.SE"
"418","1210.4129v1","2012-10-11 19:01:15","2012-10-11 19:01:15","Towards international E-stat for monitoring the socio-economic
  activities across the globe","  We investigate relationship between annual electric power consumption per
capita and gross domestic production (GDP) per capita for 131 countries. We
found that the relationship can be fitted with a power-law function. We examine
the relationship for 47 prefectures in Japan. Furthermore, we investigate
values of annual electric power production reported by four international
organizations. We collected the data from U.S. Energy Information
Administration (EIA), Statistics by International Energy Agency (IEA), OECD
Factbook (Economic, Environmental and Social Statistics), and United Nations
(UN) Energy Statistics Yearbook. We found that the data structure, values, and
unit depend on the organizations. This implies that it is further necessary to
establish data standards and an organization to collect, store, and distribute
the data on socio-economic systems.
","Aki-Hiro Sato|Ken Umeno","","http://arxiv.org/abs/1210.4129v1","http://arxiv.org/pdf/1210.4129v1","","6 pages, 5 figures","","","q-fin.GN","q-fin.GN|physics.soc-ph"
"419","1210.3561v1","2012-10-12 16:13:37","2012-10-12 16:13:37","On separation of time scales in pharmacokinetics","  A lot of criticism against the standard formulation of pharmacokinetics has
been raised by several authors. It seems that the natural reaction for that
criticism is to comment it from the point of view of the theory of conservation
laws. Simple example of balance equations for the intravenous administration of
drug has been given in 2011 and the corresponding equations for extravasal
administration are in the text. In principle, the equations of that kind allow
one to describe in the self consistent manner different processes of
administration, distribution, metabolism and elimination of drugs. Moreover, it
is possible to model different pharmacokinetic parameters of the
non-compartmental pharmacokinetics and therefore to comment criticism of
Rosigno. However, for practical purposes one needs approximate methods, in
particular, those based on separation of the time scales. In this text, such
method is described and its effectiveness is discussed. Basic equations are in
the next chapter. Final remarks are at the end of the text.
","S. Piekarski|M. Rewekant","","http://arxiv.org/abs/1210.3561v1","http://arxiv.org/pdf/1210.3561v1","","","","","q-bio.QM","q-bio.QM|97M60"
"420","1210.3716v1","2012-10-13 16:16:27","2012-10-13 16:16:27","Redistribution spurs growth by using a portfolio effect on human capital","  We demonstrate by mathematical analysis and systematic computer simulations
that redistribution can lead to sustainable growth in a society. The human
capital dynamics of each agent is described by a stochastic multiplicative
process which, in the long run, leads to the destruction of individual human
capital and the extinction of the individualistic society. When agents are
linked by fully-redistributive taxation the situation might turn to individual
growth in the long run. We consider that a government collects a proportion of
income and reduces it by a fraction as costs for administration (efficiency
losses). The remaining public good is equally redistributed to all agents. We
derive conditions under which the destruction of human capital can be turned
into sustainable growth, despite the losses from the random growth process and
despite the administrative costs. Sustainable growth is induced by
redistribution. This effect could be explained by a simple portfolio-effect
which re-balances individual stochastic processes.
  The findings are verified for three different tax schemes: proportional tax,
taking proportional more from the rich, and proportionally more from the poor.
We discuss which of these tax schemes is optimal with respect to maximize
growth under a fixed rate of administrative costs, or with respect to maximize
the governmental income. This leads us to some general conclusions about
governmental decisions, the relation to public good games, and the use of
taxation in a risk taking society.
","Jan Lorenz|Fabian Paetzel|Frank Schweitzer","","http://arxiv.org/abs/1210.3716v1","http://arxiv.org/pdf/1210.3716v1","http://dx.doi.org/10.1371/journal.pone.0054904","12 pages, plus 8 Figures, plus matlab-code to run simulation and
  produce figure","","10.1371/journal.pone.0054904","q-fin.GN","q-fin.GN|physics.soc-ph"
"421","1210.5267v1","2012-10-18 21:40:33","2012-10-18 21:40:33","MultiLCIRT: An R package for multidimensional latent class item response
  models","  We illustrate a class of Item Response Theory (IRT) models for binary and
ordinal polythomous items and we describe an R package for dealing with these
models, which is named MultiLCIRT. The models at issue extend traditional IRT
models allowing for (i) multidimensionality and (ii) discreteness of latent
traits. This class of models also allows for different parameterizations for
the conditional distribution of the response variables given the latent traits,
depending on both the type of link function and the constraints imposed on the
discriminating and the difficulty item parameters. We illustrate how the
proposed class of models may be estimated by the maximum likelihood approach
via an Expectation-Maximization algorithm, which is implemented in the
MultiLCIRT package, and we discuss in detail issues related to model selection.
In order to illustrate this package, we analyze two datasets: one concerning
binary items and referred to the measurement of ability in mathematics and the
other one coming from the administration of ordinal polythomous items for the
assessment of anxiety and depression. In the first application, we illustrate
how aggregating items in homogeneous groups through a model-based hierarchical
clustering procedure which is implemented in the proposed package. In the
second application, we describe the steps to select a specific model having the
best fit in our class of IRT models.
","Francesco Bartolucci|Silvia Bacci|Michela Gnaldi","","http://arxiv.org/abs/1210.5267v1","http://arxiv.org/pdf/1210.5267v1","","36 pages, 1 figures, 4 tables. arXiv admin note: substantial text
  overlap with arXiv:1201.4667","","","stat.AP","stat.AP|stat.CO"
"422","1210.7626v1","2012-10-29 11:36:32","2012-10-29 11:36:32","Hep Cluster First Step Towards Grid Computing","  HEP Cluster is designed and implemented in Scientific Linux Cern 5.5 to grant
High Energy Physics researchers one place where they can go to undertake a
particular task or to provide a parallel processing architecture in which CPU
resources are shared across a network and all machines function as one large
supercomputer. It gives physicists a facility to access computers and data,
transparently, without having to consider location, operating system, account
administration, and other details. By using this facility researchers can
process their jobs much faster than the stand alone desktop systems. Keywords:
Cluster, Network, Storage, Parallel Computing & Gris.
","Vivek Chalotra|Anju Bhasin|Anik Gupta|Sanjeev Singh Sambyal","","http://arxiv.org/abs/1210.7626v1","http://arxiv.org/pdf/1210.7626v1","","7th JK Science Congress 2011, Jammu, India","","","cs.DC","cs.DC"
"423","1211.2280v1","2012-11-10 02:11:55","2012-11-10 02:11:55","A Novel Architecture For Network Coded Electronic Health Record Storage
  System","  The use of network coding for large scale content distribution improves
download time. This is demonstrated in this work by the use of network coded
Electronic Health Record Storage System (EHR-SS). An architecture of 4-layer to
build the EHR-SS is designed. The application integrates the data captured for
the patient from three modules namely administrative data, medical records of
consultation and reports of medical tests. The lower layer is the data
capturing layer using RFID reader. The data is captured in the lower level from
different nodes. The data is combined with some linear coefficients using
linear network coding. At the lower level the data from different tags are
combined and stored and at the level 2 coding combines the data from multiple
readers and a corresponding encoding vector is generated. This network coding
is done at the server node through small mat lab net-cod interface software.
While accessing the stored data, the user data has the data type represented in
the form of decoding vector. For storing and retrieval the primary key is the
patient id. The results obtained were observed with a reduction of download
time of about 12% for our case study set up.
","B. Venkatalakshmi|S. Shanmugavel","","http://arxiv.org/abs/1211.2280v1","http://arxiv.org/pdf/1211.2280v1","","22-pages Journal","International Journal of Advanced Computer Science and
  Applications(IJACSA), 2012","","cs.IT","cs.IT|math.IT"
"424","1211.2495v1","2012-11-12 02:13:46","2012-11-12 02:13:46","Route Planning Made Easy - An Automated System for Sri Lanka","  Commercial cities like Colombo constantly face to problem of traffic
congestion due to the large number of people visiting the city for various
reasons. Also these cities have a large number of roads with many roads
connecting any two selected locations. Finding the best path between two
locations in Colombo city is not a trivial task, due to the complexity of the
road network and other reasons such as heavy traffic, changes to the road
networks such as road closures and one-ways. This paper presents the results of
a study carried out to understand this problem and development of a system to
plan the travel way ahead of the planned day or time of the journey. This
system can compute the best route from between two locations taking multiple
factors such as traffic conditions, road closures or one-way declarations etc.,
into account. This system also has the capability to compute the best route
between any two locations on a future date based on the road conditions on that
date. The system comprises three main modules and two user interfaces one for
normal users and the other for administrators. The Administrative interface can
only be accessed via web browser running on a computer, while the other
interface can be accessed either via a web browser or a GPRS enabled mobile
phone. The system is powered mainly by the Geographic Information System (GIS)
technology and the other supporting technologies used are database management
system, ASP.Net technology and the GPRS technology. Finally the developed
system was evaluated for its functionality and user friendliness using a user
survey. The results of the survey are also presented in this paper.
","M. F. M. Firdhous|D. L. Basnayake|K. H. L. Kodithuwakku|N. K. Haththalla|N. W. Charlin|P. M. R. I. K. Bandara","","http://arxiv.org/abs/1211.2495v1","http://arxiv.org/pdf/1211.2495v1","","17 pages, 17 figures and 2 tables","Route Planning Made Easy-An Automated System for Sri Lanka, Bhumi:
  the Planning Research Journal of the University of Moratuwa, Vol. 02 No. 02,
  2010, pp. 13-24","","cs.CY","cs.CY"
"425","1211.4493v2","2012-11-19 16:53:32","2012-11-20 03:32:57","Survey on Incremental Approaches for Network Anomaly Detection","  As the communication industry has connected distant corners of the globe
using advances in network technology, intruders or attackers have also
increased attacks on networking infrastructure commensurately. System
administrators can attempt to prevent such attacks using intrusion detection
tools and systems. There are many commercially available signature-based
Intrusion Detection Systems (IDSs). However, most IDSs lack the capability to
detect novel or previously unknown attacks. A special type of IDSs, called
Anomaly Detection Systems, develop models based on normal system or network
behavior, with the goal of detecting both known and unknown attacks. Anomaly
detection systems face many problems including high rate of false alarm,
ability to work in online mode, and scalability. This paper presents a
selective survey of incremental approaches for detecting anomaly in normal
system or network traffic. The technological trends, open problems, and
challenges over anomaly detection using incremental approach are also
discussed.
","Monowar H. Bhuyan|D. K. Bhattacharyya|J. K. Kalita","","http://arxiv.org/abs/1211.4493v2","http://arxiv.org/pdf/1211.4493v2","","14 pages, 1 figure, 11 tables referred journal publication","International Journal of Communication Networks and Information
  Security (KUST), vol. 3, no. 3, pp. 226-239, 2011","","cs.CR","cs.CR|cs.NI|68T10|K.6.5"
"426","1211.4555v1","2012-11-19 20:35:01","2012-11-19 20:35:01","Distributed Control of Generation in a Transmission Grid with a High
  Penetration of Renewables","  Deviations of grid frequency from the nominal frequency are an indicator of
the global imbalance between genera- tion and load. Two types of control, a
distributed propor- tional control and a centralized integral control, are cur-
rently used to keep frequency deviations small. Although generation-load
imbalance can be very localized, both controls primarily rely on frequency
deviation as their in- put. The time scales of control require the outputs of
the centralized integral control to be communicated to distant generators every
few seconds. We reconsider this con- trol/communication architecture and
suggest a hybrid ap- proach that utilizes parameterized feedback policies that
can be implemented in a fully distributed manner because the inputs to these
policies are local observables at each generator. Using an ensemble of
forecasts of load and time-intermittent generation representative of possible
fu- ture scenarios, we perform a centralized off-line stochas- tic optimization
to select the generator-specific feedback parameters. These parameters need
only be communi- cated to generators once per control period (60 minutes in our
simulations). We show that inclusion of local power flows as feedback inputs is
crucial and reduces frequency deviations by a factor of ten. We demonstrate our
con- trol on a detailed transmission model of the Bonneville Power
Administration (BPA). Our findings suggest that a smart automatic and
distributed control, relying on ad- vanced off-line and system-wide
computations commu- nicated to controlled generators infrequently, may be a
viable control and communication architecture solution. This architecture is
suitable for a future situation when generation-load imbalances are expected to
grow because of increased penetration of time-intermittent generation.
","Krishnamurthy Dvijotham|Michael Chertkov|Scott Backhaus","","http://arxiv.org/abs/1211.4555v1","http://arxiv.org/pdf/1211.4555v1","","","","","cs.SY","cs.SY|math.OC"
"427","1211.5227v1","2012-11-22 08:33:09","2012-11-22 08:33:09","Service Composition Design Pattern for Autonomic Computing Systems using
  Association Rule based Learning and Service-Oriented Architecture","  In this paper we present a Service Injection and composition Design Pattern
for Unstructured Peer-to-Peer networks, which is designed with Aspect-oriented
design patterns, and amalgamation of the Strategy, Worker Object, and
Check-List Design Patterns used to design the Self-Adaptive Systems. It will
apply self reconfiguration planes dynamically without the interruption or
intervention of the administrator for handling service failures at the servers.
When a client requests for a complex service, Service Composition should be
done to fulfil the request. If a service is not available in the memory, it
will be injected as Aspectual Feature Module code. We used Service Oriented
Architecture (SOA) with Web Services in Java to Implement the composite Design
Pattern. As far as we know, there are no studies on composition of design
patterns for Peer-to-peer computing domain. The pattern is described using a
java-like notation for the classes and interfaces. A simple UML class and
Sequence diagrams are depicted.
","Vishnuvardhan Mannava|T. Ramesh","","http://arxiv.org/abs/1211.5227v1","http://arxiv.org/pdf/1211.5227v1","","19 pages, 7 figures, International Journal of Grid Computing &
  Applications (IJGCA). arXiv admin note: text overlap with arXiv:1208.3836","IJGCA, 3(3), 21-39 (2012)","","cs.SE","cs.SE|cs.DC|cs.LG|D.2.11; D.2.10; D.3.3; H.2.8; I.2.6"
"428","1211.5596v1","2012-11-22 08:55:25","2012-11-22 08:55:25","A Composite Design Pattern for Service Injection and Composition of Web
  Services for Peer-To-Peer Computing with Service Oriented Architecture","  In this paper we present a Service Injection and composition Design Pattern
for Unstructured Peer-to-Peer networks, which is designed with Aspect-oriented
design patterns, and amalgamation of the Strategy, Worker Object, and
Check-List Design Patterns used to design the Self-Adaptive Systems. It will
apply self reconfiguration planes dynamically without the interruption or
intervention of the administrator for handling service failures at the servers.
When a client requests for a complex service, Service Composition should be
done to fulfil the request. If a service is not available in the memory, it
will be injected as Aspectual Feature Module code. We used Service Oriented
Architecture (SOA) with Web Services in Java to Implement the composite Design
Pattern. As far as we know, there are no studies on composition of design
patterns for Peer-to-peer computing domain. The pattern is described using a
java-like notation for the classes and interfaces. A simple UML class and
Sequence diagrams are depicted.
","Vishnuvardhan Mannava|T. Ramesh","","http://arxiv.org/abs/1211.5596v1","http://arxiv.org/pdf/1211.5596v1","http://dx.doi.org/10.5121/ijwsc","15 pages, 9 figures, International Journal on Web Service Computing
  (IJWSC). arXiv admin note: substantial text overlap with arXiv:1208.3836,
  arXiv:1211.5227","International Journal on Web Service Computing 3 (3), 49-63, 2012","10.5121/ijwsc","cs.SE","cs.SE|cs.DC|cs.NI|D.2.11; D.2.10; D.3.3; C.2.4; D.1.3"
"429","1211.5629v1","2012-11-24 00:58:19","2012-11-24 00:58:19","Prototype for Extended XDB Using Wiki","  This paper describes a prototype of extended XDB. XDB is an open-source and
extensible database architecture developed by National Aeronautics and Space
Administration (NASA) to provide integration of heterogeneous and distributed
information resources for scientific and engineering applications. XDB enables
an unlimited number of desktops and distributed information sources to be
linked seamlessly and efficiently into an information grid using Data Access
and Retrieval Composition (DARC) protocol which provides a contextual search
and retrieval capability useful for lightweight web applications. This paper
shows the usage of XDB on common data management in the enterprise without
burdening users and application developers with unnecessary complexity and
formal schemas. Supported by NASA Ames Research Center through NASA Exploration
System Mission Directorate (ESMD) Higher Education grant, a project team at
Fairfield University extended this concept and developed an extended XDB
protocol and a prototype providing text-searches for Wiki. The technical
specification of the protocol was posted to Source Forge (sourceforge.net) and
a prototype providing text-searches for Wiki was developed. The prototype was
created for 16 tags of the MediaWiki dialect. As part of future works, the
prototype will be further extended to the complete Wiki markups and other
dialects of Wiki.
","Wook-Sung Yoo","","http://arxiv.org/abs/1211.5629v1","http://arxiv.org/pdf/1211.5629v1","","8 pages","International Journal of Database Management Systems (IJDMS)
  Vol.4, No.5, October 2012","","cs.DB","cs.DB|cs.SE"
"430","1211.5927v3","2012-11-26 12:10:15","2013-05-22 17:26:08","Optimal quantum communication using multiparticle partially entangled
  states","  Administratively withdrawn.
","Atul Kumar|Satyabrata Adhikari|Subhashish Banerjee|Sovik Roy","","http://arxiv.org/abs/1211.5927v3","http://arxiv.org/pdf/1211.5927v3","http://dx.doi.org/10.1103/PhysRevA.87.022307","arXiv admin note: According to
  http://pra.aps.org/pdf/PRA/v87/i5/e059904: ""This article should be considered
  withdrawn from publication. Although the paper shows that maximal slice (MS)
  states are not locally equivalent to the \chi states, it fails to reference
  and discuss the closely related material presented in the following article:
  T. Gao, F. L. Yan, and Y. C. Li, Europhys. Lett. 84, 5001 (2008). The article
  also makes extensive use of a preprint by Dr. A. Kumar, Dr. A. Kalchenko, and
  Dr. S. Ghose. Although the article acknowledges S. Ghose, Dr. Ghose and Dr.
  A. Kaltchenko are not included as authors. Although Drs. Adhikari, Banerjee,
  and Roy did not know about the preprint, Drs. Kumar, Adhikari, Banerjee, and
  Roy, together with Physical Review A, apologize to both Drs. Ghose and
  Kaltchenko, and also to Dr. Gao and his coauthors for these omissions""","","10.1103/PhysRevA.87.022307","quant-ph","quant-ph"
"431","1301.1563v1","2012-12-26 13:20:44","2012-12-26 13:20:44","Academic Ranking with Web Mining and Axiomatic Analysis","  Academic ranking is a public topic, such as for universities, colleges, or
departments, which has significant educational, administrative and social
effects. Popular ranking systems include the US News & World Report (USNWR),
the Academic Ranking of World Universities (ARWU), and others. The most popular
observables for such ranking are academic publications and their citations.
However, a rigorous, quantitative and thorough methodology has been missing for
this purpose. With modern web technology and axiomatic bibliometric analysis,
here we perform a feasibility study on Microsoft Academic Search metadata and
obtain the first-of-its-kind ranking results for American departments of
computer science. This approach can be extended for fully automatic intuitional
and college ranking based on comprehensive data on Internet.
","Kun Tang|Qiwei Jin|Xin Zou|Jiansheng Yang|Michael Vannier|Ge Wang","","http://arxiv.org/abs/1301.1563v1","http://arxiv.org/pdf/1301.1563v1","","11 pages, 3 figures, 3 tables, and 10 references","","","cs.DL","cs.DL"
"432","1301.0762v1","2013-01-04 16:23:48","2013-01-04 16:23:48","From EGEE OPerations Portal towards EGI OPerations Portal","  EGEE to EGI structure based on NGIs evolution induces a large move from the
operations that will rely on a sustainable and largely decentralized model. One
of the key evolutions for the challenge in the regionalisation relies in the
scalability and the flexibility required regarding information source types and
information handling. For 5 years, we have developed and maintained a
standard-based component that allows us to address both theses issues. This
open-source tool, named Lavoisier, has been a critical success factor for the
operations dashboard, one of the Operations Portal main features. Indeed, it
enables coherent efficient and reliable data handling which is customizable and
scalable, as Lavoisier is an extensible service designed to provide a unified
view of data collected from multiple heterogeneous data sources. Data views are
represented and accessed as XML documents through standard languages such as
XSLT, XPath. Moreover, scalability and reliability are enforced by a caching
mechanism adaptable to specific data sources and use-cases. We will namely
expose how the concept and the implementation enable clear roles separation
between plug-in developer, service configuration administrator or end-user.
Also, maintainability of the portal code has increased dramatically as the
latter is now independent from the data sources technology or from the cache
management policies. Finally, integration of data has recently been simplified
as the service administrator proceeds now through web interfaces
","Helene Cordier|Cyril L'Orphelin|Sylvain Reynaud|Olivier Lequeux|Sinika Loikkanen|Pierre Veyre","CC IN2P3|CC IN2P3|CC IN2P3|CC IN2P3|CC IN2P3|CC IN2P3","http://arxiv.org/abs/1301.0762v1","http://arxiv.org/pdf/1301.0762v1","","arXiv admin note: text overlap with arXiv:1301.0523","ISGC 2010, Taipei : Taiwan, Province Of China (2010)","","cs.SE","cs.SE|cs.DC"
"433","1301.1912v1","2013-01-09 16:22:40","2013-01-09 16:22:40","Cloud Penetration Testing","  This paper presents the results of a series of penetration tests performed on
the OpenStack Essex Cloud Management Software. Several different types of
penetration tests were performed including network protocol and command line
fuzzing, session hijacking and credential theft. Using these techniques
exploitable vulnerabilities were discovered that could enable an attacker to
gain access to restricted information contained on the OpenStack server, or to
gain full administrative privileges on the server. Key recommendations to
address these vulnerabilities are to use a secure protocol, such as HTTPS, for
communications between a cloud user and the OpenStack Horizon Dashboard, to
encrypt all files that store user or administrative login credentials, and to
correct a software bug found in the OpenStack Cinder typedelete command.
","Ralph LaBarge|Thomas McGuire","","http://arxiv.org/abs/1301.1912v1","http://arxiv.org/pdf/1301.1912v1","http://dx.doi.org/10.5121/ijccsa.2012.2604","20 pages, 16 figures; International Journal on Cloud Computing:
  Services and Architecture (IJCCSA),Vol.2, No.6, December 2012","","10.5121/ijccsa.2012.2604","cs.CR","cs.CR|cs.SE|D.4.6"
"434","1301.2405v1","2013-01-11 07:46:27","2013-01-11 07:46:27","Dating medieval English charters","  Deeds, or charters, dealing with property rights, provide a continuous
documentation which can be used by historians to study the evolution of social,
economic and political changes. This study is concerned with charters (written
in Latin) dating from the tenth through early fourteenth centuries in England.
Of these, at least one million were left undated, largely due to administrative
changes introduced by William the Conqueror in 1066. Correctly dating such
charters is of vital importance in the study of English medieval history. This
paper is concerned with computer-automated statistical methods for dating such
document collections, with the goal of reducing the considerable efforts
required to date them manually and of improving the accuracy of assigned dates.
Proposed methods are based on such data as the variation over time of word and
phrase usage, and on measures of distance between documents. The extensive (and
dated) Documents of Early England Data Set (DEEDS) maintained at the University
of Toronto was used for this purpose.
","Gelila Tilahun|Andrey Feuerverger|Michael Gervers","","http://arxiv.org/abs/1301.2405v1","http://arxiv.org/pdf/1301.2405v1","http://dx.doi.org/10.1214/12-AOAS566","Published in at http://dx.doi.org/10.1214/12-AOAS566 the Annals of
  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of
  Mathematical Statistics (http://www.imstat.org)","Annals of Applied Statistics 2012, Vol. 6, No. 4, 1615-1640","10.1214/12-AOAS566","stat.AP","stat.AP|cs.CL"
"435","1301.2520v1","2013-01-11 15:23:41","2013-01-11 15:23:41","On modelling of bioavailability of drugs in terms of conservation laws","  In the text S.Piekarski, M.Rewekant,(arXiv:1208.3847)it has been mentioned
that some information on bioavailability and bioequivalence of drugs can be
obtained from simulations based on the conservation laws. Here we shortly
discuss that possibility starting from the fundamental pharmacokinetic
parameter called AUC (Area Under the Curve). The curve is is the profile shape
of plasma drug concentration in time intervals after drug administration into
organism. Our aim here is to give some information on the subject for the
reader with no experience in pharmacokinetics.
","S. Piekarski|M. Rewekant","","http://arxiv.org/abs/1301.2520v1","http://arxiv.org/pdf/1301.2520v1","","5 pages","","","q-bio.QM","q-bio.QM|92c45"
"436","1301.5273v2","2013-01-20 02:02:30","2013-02-05 18:25:26","Using Periodicity of Nucleotide Sequences","  Withdrawn by arXiv administrators due to content entirely plagiarized from
other authors (not in arXiv).
","Rick B. Jenison","","http://arxiv.org/abs/1301.5273v2","http://arxiv.org/pdf/1301.5273v2","","arXiv admin note: entirely plagiarized from
  http://www.ncbi.nlm.nih.gov/pubmed/19261626","","","q-bio.GN","q-bio.GN|cs.CE"
"437","1301.5386v1","2013-01-23 02:35:03","2013-01-23 02:35:03","Information systems security in special and public libraries: an
  assessment of status","  Explores the use of an assessment instrument based on a model named library
information systems security assessment model (LISSAM) to assess the 155 status
in special and public libraries in Malaysia. The study aims to determine the
implementation status of technological and organizational components of the
LISSAM model. An implementation index as well as a scoring tool is presented to
assess the IS safeguarding measures in a library. Data used was based on
questionnaires distributed to a total of 50 individuals who are responsible for
the information systems (IS) or IT in the special and public libraries in
Malaysia. Findings revealed that over 95% of libraries have high level of
technological implementation but 54% were fair poorly on organizational
measures, especially on lack of security procedures, administrative tools and
awareness creation activities.
","R. Ismail|A. N. Zainab","","http://arxiv.org/abs/1301.5386v1","http://arxiv.org/pdf/1301.5386v1","","","","","cs.DL","cs.DL"
"438","1302.0029v1","2013-01-31 22:39:53","2013-01-31 22:39:53","Serverification of Molecular Modeling Applications: the Rosetta Online
  Server that Includes Everyone (ROSIE)","  The Rosetta molecular modeling software package provides experimentally
tested and rapidly evolving tools for the 3D structure prediction and
high-resolution design of proteins, nucleic acids, and a growing number of
non-natural polymers. Despite its free availability to academic users and
improving documentation, use of Rosetta has largely remained confined to
developers and their immediate collaborators due to the code's difficulty of
use, the requirement for large computational resources, and the unavailability
of servers for most of the Rosetta applications. Here, we present a unified web
framework for Rosetta applications called ROSIE (Rosetta Online Server that
Includes Everyone). ROSIE provides (a) a common user interface for Rosetta
protocols, (b) a stable application programming interface for developers to add
additional protocols, (c) a flexible back-end to allow leveraging of computer
cluster resources shared by RosettaCommons member institutions, and (d)
centralized administration by the RosettaCommons to ensure continuous
maintenance. This paper describes the ROSIE server infrastructure, a
step-by-step 'serverification' protocol for use by Rosetta developers, and the
deployment of the first nine ROSIE applications by six separate developer
teams: Docking, RNA de novo, ERRASER, Antibody, Sequence Tolerance,
Supercharge, Beta peptide design, NCBB design, and VIP redesign. As illustrated
by the number and diversity of these applications, ROSIE offers a general and
speedy paradigm for serverification of Rosetta applications that incurs
negligible cost to developers and lowers barriers to Rosetta use for the
broader biological community. ROSIE is available at
http://rosie.rosettacommons.org.
","Sergey Lyskov|Fang-Chieh Chou|Shane O Conchuir|Bryan S. Der|Kevin Drew|Daisuke Kuroda|Jianqing Xu|Brian D. Weitzner|P. Douglas Renfrew|Parin Sripakdeevong|Benjamin Borgo|James J. Havranek|Brian Kuhlman|Tanja Kortemme|Richard Bonneau|Jeffrey J. Gray|Rhiju Das","","http://arxiv.org/abs/1302.0029v1","http://arxiv.org/pdf/1302.0029v1","http://dx.doi.org/10.1371/journal.pone.0063906","","","10.1371/journal.pone.0063906","q-bio.QM","q-bio.QM|q-bio.BM"
"439","1302.2222v1","2013-02-09 11:43:52","2013-02-09 11:43:52","Ontology-Based Administration of Web Directories","  Administration of a Web directory and maintenance of its content and the
associated structure is a delicate and labor intensive task performed
exclusively by human domain experts. Subsequently there is an imminent risk of
a directory structures becoming unbalanced, uneven and difficult to use to all
except for a few users proficient with the particular Web directory and its
domain. These problems emphasize the need to establish two important issues: i)
generic and objective measures of Web directories structure quality, and ii)
mechanism for fully automated development of a Web directory's structure. In
this paper we demonstrate how to formally and fully integrate Web directories
with the Semantic Web vision. We propose a set of criteria for evaluation of a
Web directory's structure quality. Some criterion functions are based on
heuristics while others require the application of ontologies. We also suggest
an ontology-based algorithm for construction of Web directories. By using
ontologies to describe the semantics of Web resources and Web directories'
categories it is possible to define algorithms that can build or rearrange the
structure of a Web directory. Assessment procedures can provide feedback and
help steer the ontology-based construction process. The issues raised in the
article can be equally applied to new and existing Web directories.
","Marko Horvat|Gordan Gledec|Nikola Bogunovi<U+0107>","","http://arxiv.org/abs/1302.2222v1","http://arxiv.org/pdf/1302.2222v1","","22 pages, 9 figures","Lecture Notes in Computer Science, Transactions on Computational
  Collective Intelligence I., 6220, pp. 101-120 (2010)","","cs.IR","cs.IR|cs.DL"
"440","1302.2615v1","2013-02-09 11:48:39","2013-02-09 11:48:39","Assessing Semantic Quality of Web Directory Structure","  The administration of a Web directory content and associated structure is a
labor intensive task performed by human domain experts. Because of that there
always exists a realistic risk of the structure becoming unbalanced, uneven and
difficult to use to all except for a few users proficient in a particular Web
directory. These problems emphasize the importance of generic and objective
measures of Web directories structure quality. In this paper we demonstrate how
to formally merge Web directories into the Semantic Web vision. We introduce a
set of objective criterions for evaluation of a Web directory's structure
quality. Some criteria functions are based on heuristics while others require
the application of ontologies.
","Marko Horvat|Gordan Gledec|Nikola Bogunovi<U+0107>","","http://arxiv.org/abs/1302.2615v1","http://arxiv.org/pdf/1302.2615v1","http://dx.doi.org/10.1007/978-3-642-15034-0_7","12 pages, 6 figures. arXiv admin note: substantial text overlap with
  arXiv:1302.2222","Lecture Notes in Computer Science, Lecture Notes in Artificial
  Intelligence, 1, 5796, pp. 377-388 (2009)","10.1007/978-3-642-15034-0_7","cs.IR","cs.IR|cs.DL"
"441","1303.6604v1","2013-03-06 11:22:51","2013-03-06 11:22:51","Underestimating Costs in Public Works Projects: Error or Lie?","  This article presents results from the first statistically significant study
of cost escalation in transportation infrastructure projects. Based on a sample
of 258 transportation infrastructure projects worth US$90 billion and
representing different project types, geographical regions, and historical
periods, it is found with overwhelming statistical significance that the cost
estimates used to decide whether such projects should be built are highly and
systematically misleading. Underestimation cannot be explained by error and is
best explained by strategic misrepresentation, that is, lying. The policy
implications are clear: legislators, administrators, investors, media
representatives, and members of the public who value honest numbers should not
trust cost estimates and cost-benefit analyses produced by project promoters
and their analysts.
","Bent Flyvbjerg|Mette K. Skamris Holm|S©ªren L. Buhl","","http://arxiv.org/abs/1303.6604v1","http://arxiv.org/pdf/1303.6604v1","http://dx.doi.org/10.1080/01944360208976273","40 pages","Bent Flyvbjerg, Mette K. Skamris Holm, S{\o}ren L. Buhl,
  ""Underestimating Costs in Public Works Projects: Error or Lie?"" Journal of
  the American Planning Association, vol. 68, no. 3, Summer 2002, pp. 279-295","10.1080/01944360208976273","q-fin.GN","q-fin.GN"
"442","1303.2202v1","2013-03-09 12:23:37","2013-03-09 12:23:37","Information Transfer in the Agricultural Sector in Spain","  This article examines the structures of information transfer to the
agricultural (production) and agro-alimentary (transformation and
commercialization of the products) sector within Spain. An historical
perspective is provided to better illustrate the reality and complexity of
Spain with regard to the systems of agrarian extension, agricultural research,
the resources provided by Spain's central administration, and the use of
information by related enterprises.
  The Service of Agrarian Extension appeared in Spain in the 1950s, and new
political-administrative structures (agribusiness associations, cooperatives)
were founded when Spain became a democratic nation in the late 1970s and with
the arrival of electronic information, largely in the 1990s. We describe the
research and technological centers supporting innovation in the agro-alimentary
sector and the communication media dedicated to the agricultural sector. The
article illustrates that the systems of agricultural information in Spain have
been largely derived from initiatives of the Public Administration, with few
private initiatives.
","Antonio Munoz-Canavate|Pedro Hipola","","http://arxiv.org/abs/1303.2202v1","http://arxiv.org/pdf/1303.2202v1","http://dx.doi.org/10.1080/10496501003682496","","Antonio Munoz-Canavate, Pedro Hipola (2010). Information Transfer
  in the Agricultural Sector in Spain. Journal of Agricultural & Food
  Information, 11: 2, 123-142. DOI: 10.1080/10496501003682496","10.1080/10496501003682496","cs.DL","cs.DL|cs.CY"
"443","1303.2798v1","2013-03-12 07:55:36","2013-03-12 07:55:36","Memory of Quark Matter Card Game","  Scientists at the Relativistic Heavy Ion Collider (RHIC, BNL) recently
discovered, that the hottest known form of matter is not a gas, but acts like a
fluid. Furthermore, this fluid of quarks expands and flows much more perfectly
than water or any other well known fluid. This aspect of the RHIC discovery can
be introduced even to primary levels of physics education, noting that the
usual solid to liquid to gas sequence of phase transitions now are known to be
followed by a transition to a nearly perfect fluid, a liquid of quarks, at the
largest temperatures made by humans. The educational games described herein
were invented by middle school students, members of a Science Club in Hungary.
The games were invented for their entertainment, the educational applications
in teaching high energy particle and nuclear physics to laypersons are quite
unexpected but most welcomed. This manuscript describes games with a deck of
cards called Quark Matter cards, where each card represents an elementary
particle. The games include an important contribution by Angela Melocoton, an
administrator of the Guests, Users and Visitors (GUV) Center at BNL. It
describes in simple terms, how to play the Memory of Quark Matter style card
games.
","J. Csorg<U+0151>|Cs. Torok|T. Csorg<U+0151>","Berze Science Club, Gyongyos, Hungary|Berze Science Club, Gyongyos, Hungary|Wigner RCP, Budapest, Hungary","http://arxiv.org/abs/1303.2798v1","http://arxiv.org/pdf/1303.2798v1","","4 pages, 1 figure. Updated version of a handout booklet, distributed
  by the GUV Center of Brookhaven National Laboratory, that disseminated
  information about various Quark Matter Card Games","","","physics.pop-ph","physics.pop-ph|hep-ex|hep-ph|nucl-ex|physics.ed-ph"
"444","1303.4614v1","2013-03-19 14:23:24","2013-03-19 14:23:24","Handwritten and Printed Text Separation in Real Document","  The aim of the paper is to separate handwritten and printed text from a real
document embedded with noise, graphics including annotations. Relying on
run-length smoothing algorithm (RLSA), the extracted pseudo-lines and
pseudo-words are used as basic blocks for classification. To handle this, a
multi-class support vector machine (SVM) with Gaussian kernel performs a first
labelling of each pseudo-word including the study of local neighbourhood. It
then propagates the context between neighbours so that we can correct possible
labelling errors. Considering running time complexity issue, we propose linear
complexity methods where we use k-NN with constraint. When using a kd-tree, it
is almost linearly proportional to the number of pseudo-words. The performance
of our system is close to 90%, even when very small learning dataset where
samples are basically composed of complex administrative documents.
","Abdel Belaid|K. C. Santosh|Vincent Poulain D'Andecy","LORIA|LORIA|","http://arxiv.org/abs/1303.4614v1","http://arxiv.org/pdf/1303.4614v1","","Machine Vision Applications (2013)","","","cs.CV","cs.CV"
"445","1304.1520v1","2013-03-27 19:39:40","2013-03-27 19:39:40","Shootout-89: A Comparative Evaluation of Knowledge-based Systems that
  Forecast Severe Weather","  During the summer of 1989, the Forecast Systems Laboratory of the National
Oceanic and Atmospheric Administration sponsored an evaluation of artificial
intelligence-based systems that forecast severe convective storms. The
evaluation experiment, called Shootout-89, took place in Boulder, and focussed
on storms over the northeastern Colorado foothills and plains (Moninger, et
al., 1990). Six systems participated in Shootout-89. These included traditional
expert systems, an analogy-based system, and a system developed using methods
from the cognitive science/judgment analysis tradition. Each day of the
exercise, the systems generated 2 to 9 hour forecasts of the probabilities of
occurrence of: non significant weather, significant weather, and severe
weather, in each of four regions in northeastern Colorado. A verification
coordinator working at the Denver Weather Service Forecast Office gathered
ground-truth data from a network of observers. Systems were evaluated on the
basis of several measures of forecast skill, and on other metrics such as
timeliness, ease of learning, and ease of use. Systems were generally easy to
operate, however the various systems required substantially different levels of
meteorological expertise on the part of their users--reflecting the various
operational environments for which the systems had been designed. Systems
varied in their statistical behavior, but on this difficult forecast problem,
the systems generally showed a skill approximately equal to that of persistence
forecasts and climatological (historical frequency) forecasts. The two systems
that appeared best able to discriminate significant from non significant
weather events were traditional expert systems. Both of these systems required
the operator to make relatively sophisticated meteorological judgments. We are
unable, based on only one summer's worth of data, to determine the extent to
which the greater skill of the two systems was due to the content of their
knowledge bases, or to the subjective judgments of the operator. A follow-on
experiment, Shootout-91, is currently being planned. Interested potential
participants are encouraged to contact the author at the address above.
","W. R. Moninger|J. A. Flueck|C. Lusk|W. F. Roberts","","http://arxiv.org/abs/1304.1520v1","http://arxiv.org/pdf/1304.1520v1","","Appears in Proceedings of the Fifth Conference on Uncertainty in
  Artificial Intelligence (UAI1989)","","","cs.AI","cs.AI"
"446","1303.7423v2","2013-03-29 15:37:39","2013-04-03 19:14:29","The 'nanobig rods' class of gold nanorods: optimized dimensions for
  improved in vivo therapeutic and imaging efficacy","  Currently, gold nanorods can be synthesized in a wide range of sizes.
However, for intended biological applications gold nanorods with approximate
dimensions 50 nm x 15 nm are used. We investigate by computer simulation the
effect of particle dimensions on the optical and thermal properties in the
context of the specific applications of photoacoustic imaging. In addition we
discuss the influence of particle size in overcoming the following biophysical
barriers when administrated in vivo: extravasation, avoidance of uptake by
organs of the reticuloendothelial system, penetration through the interstitium,
binding capability and uptake by the target cells. Although more complex
biological influences can be introduced in future analysis, the present work
illustrates that larger gold nanorods, designated by us as ""nanobig rods"", may
perform relatively better at meeting the requirements for successful in vivo
applications compared to their smaller counterparts which are conventionally
used.
","Constantin Ungureanu|Gerben A. Koning|Ton G. van Leeuwen|Srirang Manohar","","http://arxiv.org/abs/1303.7423v2","http://arxiv.org/pdf/1303.7423v2","http://dx.doi.org/10.1088/0957-4484/24/21/215102","","","10.1088/0957-4484/24/21/215102","physics.med-ph","physics.med-ph"
"447","1304.0555v3","2013-04-02 08:07:58","2013-06-28 07:59:43","Distributed quantum election scheme","  In an electronic voting protocol, a distributed scheme can be used for
forbidding the malicious acts of the voting administrator and the counter
during the election, but it cannot prevent them from collaborating to trace the
ballots and destroy their privacy after the election. We present a distributed
anonymous quantum key distribution scheme and further construct a distributed
quantum election scheme with a voting administrator made up of more than one
part. This quantum election scheme can resist the malicious acts of the voting
administrator and the counter after the election and can work in a system with
lossy and noisy quantum channels.
","Rui-Rui Zhou|Li Yang","","http://arxiv.org/abs/1304.0555v3","http://arxiv.org/pdf/1304.0555v3","","23 pages","","","quant-ph","quant-ph|cs.CR"
"448","1304.1411v2","2013-04-04 15:54:48","2013-07-19 09:43:59","RITA: An Index-Tuning Advisor for Replicated Databases","  Given a replicated database, a divergent design tunes the indexes in each
replica differently in order to specialize it for a specific subset of the
workload. This specialization brings significant performance gains compared to
the common practice of having the same indexes in all replicas, but requires
the development of new tuning tools for database administrators. In this paper
we introduce RITA (Replication-aware Index Tuning Advisor), a novel
divergent-tuning advisor that offers several essential features not found in
existing tools: it generates robust divergent designs that allow the system to
adapt gracefully to replica failures; it computes designs that spread the load
evenly among specialized replicas, both during normal operation and when
replicas fail; it monitors the workload online in order to detect changes that
require a recomputation of the divergent design; and, it offers suggestions to
elastically reconfigure the system (by adding/removing replicas or
adding/dropping indexes) to respond to workload changes. The key technical
innovation behind RITA is showing that the problem of selecting an optimal
design can be formulated as a Binary Integer Program (BIP). The BIP has a
relatively small number of variables, which makes it feasible to solve it
efficiently using any off-the-shelf linear-optimization software. Experimental
results demonstrate that RITA computes better divergent designs compared to
existing tools, offers more features, and has fast execution times.
","Quoc Trung Tran|Ivo Jimenez|Rui Wang|Neoklis Polyzotis|Anastasia Ailamaki","","http://arxiv.org/abs/1304.1411v2","http://arxiv.org/pdf/1304.1411v2","","15 pages, 11 figures","","","cs.DB","cs.DB"
"449","1304.1836v2","2013-04-06 00:18:59","2013-04-11 19:06:01","A Simulation and Modeling of Access Points with Definition Language","  This submission has been withdrawn by arXiv administrators because it
contains fictitious content and was submitted under a pseudonym, which is
against arXiv policy.
","Tairen Sun","","http://arxiv.org/abs/1304.1836v2","http://arxiv.org/pdf/1304.1836v2","","Withdrawn by arXiv admins","","","cs.OH","cs.OH"
"450","1304.3259v1","2013-04-11 11:18:25","2013-04-11 11:18:25","Effect of Mobility and Traffic Models on the Energy Consumption in MANET
  Routing Protocols","  A Mobile Ad hoc Network (MANET) is a group of mobile nodes that can be set up
randomly and formed without the need of any existing network infrastructure or
centralized administration. In this network the mobile devices are dependent on
battery power, it is important to minimize their energy consumption. Also
storage capacity and power are severely limited. In situations such as
emergency rescue, military actions, and scientific field missions, energy
conservation plays an even more important role which is critical to the success
of the tasks performed by the network. Therefore, energy conservation should be
considered carefully when designing or evaluating ad hoc routing protocols. In
this paper we concentrated on the energy consumption issues of existing routing
protocols in MANET under various mobility models and whose connections
communicate in a particular traffic model (CBR, Exponential, and Pareto). This
paper describes a performance comparison of the AODV, DSR and DSDV routing
protocols in term of energy consumed due to packet type (routing/MAC) during
transmission and reception of control packets. The mobility models used in this
work are Random Waypoint, Manhattan Grid and Reference Point Group. Simulations
have been carried out using NS-2 and its associated tools for animation and
analysis of results.
","Said El Kafhali|Abdelkrim Haqiq","","http://arxiv.org/abs/1304.3259v1","http://arxiv.org/pdf/1304.3259v1","","8 pages. arXiv admin note: substantial text overlap with
  arXiv:1304.2035","International Journal of Soft Computing and Engineering (IJSCE),
  Volume-3, Issue-1, March 2013","","cs.NI","cs.NI"
"451","1304.4525v3","2013-04-16 17:23:32","2013-06-02 12:48:20","Overspend? Late? Failure? What the Data Say About IT Project Risk in the
  Public Sector","  Implementing large-scale information and communication technology (IT)
projects carries large risks and easily might disrupt operations, waste
taxpayers' money, and create negative publicity. Because of the high risks it
is important that government leaders manage the attendant risks. We analysed a
sample of 1,355 public sector IT projects. The sample included large-scale
projects, on average the actual expenditure was $130 million and the average
duration was 35 months. Our findings showed that the typical project had no
cost overruns and took on average 24% longer than initially expected. However,
comparing the risk distribution with the normative model of a thin-tailed
distribution, projects' actual costs should fall within -30% and +25% of the
budget in nearly 99 out of 100 projects. The data showed, however, that a
staggering 18% of all projects are outliers with cost overruns >25%. Tests
showed that the risk of outliers is even higher for standard software (24%) as
well as in certain project types, e.g., data management (41%), office
management (23%), eGovernment (21%) and management information systems (20%).
Analysis showed also that projects duration adds risk: every additional year of
project duration increases the average cost risk by 4.2 percentage points.
Lastly, we suggest four solutions that public sector organization can take: (1)
benchmark your organization to know where you are, (2) de-bias your IT project
decision-making, (3) reduce the complexities of your IT projects, and (4)
develop Masterbuilders to learn from the best in the field.
","Alexander Budzier|Bent Flyvbjerg","","http://arxiv.org/abs/1304.4525v3","http://arxiv.org/pdf/1304.4525v3","","Published in Commonwealth Secretariat (Eds.): Commonwealth Governance
  Handbook 2012/13: Democracy, development and public administration, London:
  Commonwealth Secretariat, December 2012. ISBN 978-1-908609-04-5","","","q-fin.GN","q-fin.GN"
"452","1304.6460v1","2013-04-24 01:16:04","2013-04-24 01:16:04","Are elite journals declining?","  Previous work indicates that over the past 20 years, the highest quality work
have been published in an increasingly diverse and larger group of journals. In
this paper we examine whether this diversification has also affected the
handful of elite journals that are traditionally considered to be the best. We
examine citation patterns over the past 40 years of 7 long-standing
traditionally elite journals and 6 journals that have been increasing in
importance over the past 20 years. To be among the top 5% or 1% cited papers,
papers now need about twice as many citations as they did 40 years ago. Since
the late 1980s and early 1990s elite journals have been publishing a decreasing
proportion of these top cited papers. This also applies to the two journals
that are typically considered as the top venues and often used as bibliometric
indicators of ""excellence"", Science and Nature. On the other hand, several new
and established journals are publishing an increasing proportion of most cited
papers. These changes bring new challenges and opportunities for all parties.
Journals can enact policies to increase or maintain their relative position in
the journal hierarchy. Researchers now have the option to publish in more
diverse venues knowing that their work can still reach the same audiences.
Finally, evaluators and administrators need to know that although there will
always be a certain prestige associated with publishing in ""elite"" journals,
journal hierarchies are in constant flux so inclusion of journals into this
group is not permanent.
","Vincent Lariviere|George A. Lozano|Yves Gingras","","http://arxiv.org/abs/1304.6460v1","http://arxiv.org/pdf/1304.6460v1","","12 pages, 4 figures","","","cs.DL","cs.DL"
"453","1304.7843v1","2013-04-30 03:18:12","2013-04-30 03:18:12","A Hybrid Rule Based Fuzzy-Neural Expert System For Passive Network
  Monitoring","  An enhanced approach for network monitoring is to create a network monitoring
tool that has artificial intelligence characteristics. There are a number of
approaches available. One such approach is by the use of a combination of rule
based, fuzzy logic and neural networks to create a hybrid ANFIS system. Such
system will have a dual knowledge database approach. One containing membership
function values to compare to and do deductive reasoning and another database
with rules deductively formulated by an expert (a network administrator). The
knowledge database will be updated continuously with newly acquired patterns.
In short, the system will be composed of 2 parts, learning from data sets and
fine-tuning the knowledge-base using neural network and the use of fuzzy logic
in making decision based on the rules and membership functions inside the
knowledge base. This paper will discuss the idea, steps and issues involved in
creating such a system.
","Azruddin Ahmad|Gobithasan Rudrusamy|Rahmat Budiarto|Azman Samsudin|Sureswaran Ramadass","","http://arxiv.org/abs/1304.7843v1","http://arxiv.org/pdf/1304.7843v1","","","2002 Proceedings of the Arab Conference on Information Technology
  ACIT 2002, Dhaka, Pg.746-752","","cs.AI","cs.AI|cs.NI"
"454","1305.2561v1","2013-05-12 05:52:08","2013-05-12 05:52:08","Strategic Planning for Network Data Analysis","  As network traffic monitoring software for cybersecurity, malware detection,
and other critical tasks becomes increasingly automated, the rate of alerts and
supporting data gathered, as well as the complexity of the underlying model,
regularly exceed human processing capabilities. Many of these applications
require complex models and constituent rules in order to come up with decisions
that influence the operation of entire systems. In this paper, we motivate the
novel ""strategic planning"" problem -- one of gathering data from the world and
applying the underlying model of the domain in order to come up with decisions
that will monitor the system in an automated manner. We describe our use of
automated planning methods to this problem, including the technique that we
used to solve it in a manner that would scale to the demands of a real-time,
real world scenario. We then present a PDDL model of one such application
scenario related to network administration and monitoring, followed by a
description of a novel integrated system that was built to accept generated
plans and to continue the execution process. Finally, we present evaluations of
two different automated planners and their different capabilities with our
integrated system, both on a six-month window of network data, and using a
simulator.
","Kartik Talamadupula|Octavian Udrea|Anton Riabov|Anand Ranganathan","","http://arxiv.org/abs/1305.2561v1","http://arxiv.org/pdf/1305.2561v1","","9 pages","","","cs.AI","cs.AI|97R40"
"455","1305.6862v3","2013-05-27 13:48:14","2013-10-29 14:11:57","Measuring the Knowledge-Based Economy of China in terms of Synergy among
  Technological, Organizational, and Geographic Attributes of Firms","  Using the possible synergy among geographic, size, and technological
distributions of firms in the Orbis database, we find the greatest reduction of
uncertainty at the level of the 31 provinces of China, and an additional 18.0%
at the national level. Some of the coastal provinces stand out as expected, but
the metropolitan areas of Beijing and Shanghai are (with Tianjan and Chonqing)
most pronounced at the next-lower administrative level of (339) prefectures,
since these four metropoles are administratively defined at both levels.
Focusing on high- and medium-tech manufacturing, a shift toward Beijing and
Shanghai is indicated, and the synergy is on average enhanced (as expected; but
not for all provinces). Unfortunately, the Orbis data is incomplete since it
was collected for commercial and not for administrative or governmental
purposes. However, we show a methodology that can be used by others who may
have access to higher-quality statistical data for the measurement.
","Loet Leydesdorff|Ping Zhou","","http://arxiv.org/abs/1305.6862v3","http://arxiv.org/pdf/1305.6862v3","","accepted for publication in Scientometrics (October, 2013)","","","cs.CY","cs.CY"
"456","1306.1140v1","2013-06-05 15:18:12","2013-06-05 15:18:12","Preventive Care Resource Allocation in Developing Countries: Can
  Rational Planning Techniques Help in Allocating Vaccinators in Dera Ismail
  Khan District of Pakistan?","  Preventive care service delivery faces immense challenges when it comes to
the level of coverage. Within this context, the case of child immunization is
presented by applying operations management tools. This paper explores the
application of integer programming techniques to support the Expanded Programme
of Immunization (EPI) service in the Dera Ismail Khan District of Pakistan. The
main concern here is equitable service delivery to decentralized localities
based on two criteria: (1) achieving the highest possible level of vaccination
among the target population; and 2) ensuring equality among geographically
scattered populations, especially in rural dwellings. For this purpose two
integer programming models have been applied on the basis of (1) sub-dividing
health district into localities and allocating vaccinators to visit and
vaccinate children within their administrative boundaries, and (2) within the
localized planning system allowing vaccinators to visit and vaccinate children
across the administrative boundaries subject to savings in travel time. Both
models show interesting results in terms of need satisfaction and travel-time
savings with a minimum level of deviation from equity. The solutions provide a
trade-off between alternative organizational tactics, and the argument is made
that rational planning methods applied interactively can contribute to the
delivery of an immunization service that is equitable and cost effective.
","Mohammad Ishfaq|Faran Ahmad Qadri|Nadeem Javaid","","http://arxiv.org/abs/1306.1140v1","http://arxiv.org/pdf/1306.1140v1","","","J. Basic. Appl. Sci. Res., 3(4)1021-1026, 2013","","cs.CY","cs.CY"
"457","1306.1781v3","2013-06-07 17:21:39","2013-10-20 13:48:24","The Composition of Wage Differentials between Migrants and Natives","  We consider the role of unobservables, such as differences in search
frictions, reservation wages, and productivities for the explanation of wage
differentials between migrants and natives. We disentangle these by estimating
an empirical general equilibrium search model with on-the-job search due to
Bontemps, Robin, and van den Berg (1999) on segments of the labour market
defined by occupation, age, and nationality using a large scale German
administrative dataset.
  The native-migrant wage differential is then decomposed into several parts,
and we focus especially on the component that we label ""migrant effect"", being
the difference in wage offers between natives and migrants in the same
occupation-age segment in firms of the same productivity. Counterfactual
decompositions of wage differentials allow us to identify and quantify their
drivers, thus explaining within a common framework what is often labelled the
unexplained wage gap.
","Panagiotis Nanos|Christian Schluter","","http://arxiv.org/abs/1306.1781v3","http://arxiv.org/pdf/1306.1781v3","","","","","stat.AP","stat.AP|q-fin.GN"
"458","1306.3624v1","2013-06-16 04:07:57","2013-06-16 04:07:57","Reporting an Experience on Design and Implementation of e-Health Systems
  on Azure Cloud","  Electronic Health (e-Health) technology has brought the world with
significant transformation from traditional paper-based medical practice to
Information and Communication Technologies (ICT)-based systems for automatic
management (storage, processing, and archiving) of information. Traditionally
e-Health systems have been designed to operate within stovepipes on dedicated
networks, physical computers, and locally managed software platforms that make
it susceptible to many serious limitations including: 1) lack of on-demand
scalability during critical situations; 2) high administrative overheads and
costs; and 3) in-efficient resource utilization and energy consumption due to
lack of automation. In this paper, we present an approach to migrate the ICT
systems in the e-Health sector from traditional in-house Client/Server (C/S)
architecture to the virtualised cloud computing environment. To this end, we
developed two cloud-based e-Health applications (Medical Practice Management
System and Telemedicine Practice System) for demonstrating how cloud services
can be leveraged for developing and deploying such applications. The Windows
Azure cloud computing platform is selected as an example public cloud platform
for our study. We conducted several performance evaluation experiments to
understand the Quality Service (QoS) tradeoffs of our applications under
variable workload on Azure.
","Shilin Lu|Rajiv Ranjan|Peter Strazdins","","http://arxiv.org/abs/1306.3624v1","http://arxiv.org/pdf/1306.3624v1","http://dx.doi.org/10.1007/978-3-642-41428-2_12","Submitted to third IEEE International Conference on Cloud and Green
  Computing (CGC 2013)","","10.1007/978-3-642-41428-2_12","cs.CY","cs.CY|cs.DC"
"459","1306.4044v2","2013-06-18 00:06:52","2013-06-19 22:43:15","Attack Planning in the Real World","  Assessing network security is a complex and difficult task. Attack graphs
have been proposed as a tool to help network administrators understand the
potential weaknesses of their network. However, a problem has not yet been
addressed by previous work on this subject; namely, how to actually execute and
validate the attack paths resulting from the analysis of the attack graph. In
this paper we present a complete PDDL representation of an attack model, and an
implementation that integrates a planner into a penetration testing tool. This
allows to automatically generate attack paths for penetration testing
scenarios, and to validate these attacks by executing the corresponding actions
-including exploits- against the real target network. We present an algorithm
for transforming the information present in the penetration testing tool to the
planning domain, and show how the scalability issues of attack graphs can be
solved using current planners. We include an analysis of the performance of our
solution, showing how our model scales to medium-sized networks and the number
of actions available in current penetration testing tools.
","Jorge Lucangeli Obes|Carlos Sarraute|Gerardo Richarte","Core Security Technologies|Core Security Technologies|Core Security Technologies","http://arxiv.org/abs/1306.4044v2","http://arxiv.org/pdf/1306.4044v2","","SecArt'2010 at AAAI 2010, Atlanta, USA. July 12, 2010","","","cs.CR","cs.CR|cs.AI"
"460","1306.4840v1","2013-06-20 12:04:28","2013-06-20 12:04:28","Quantitative Analysis of the Tumor/Metastasis System and its Optimal
  Therapeutic Control","  A mathematical model for time development of metastases and their
distribution in size and carrying capacity is presented. The model is used to
theoretically investigate anti-cancer therapies such as surgery and chemical
treatments (cytotoxic or anti-angiogenic), in monotherapy or in combination.
Quantification of the effect of surgery on the size distribution of metastatic
colonies is derived. For systemic therapies, emphasis is placed on the
differences between the treatment of an isolated lesion and a population of
metastases. Combination therapy is addressed, in particular the problem of the
drugs administration sequence. Theoretical optimal schedules are derived that
show the superiority of a metronomic administration scheme (defined as a
continuous administration of a given amount of drug spread during the whole
therapeutic cycle) on a classical Maximum Tolerated Dose scheme (where the dose
is given as a few concentrated administrations at the beginning of the cycle),
for the total metastatic burden in the organism.
","Sebastien Benzekry|Dominique Barbolosi|Assia Benabdallah|Florence Hubert|Philip Hahnfeldt","CCSB, INRIA Bordeaux - Sud-Ouest|CRO2|LATP|LATP|CCSB","http://arxiv.org/abs/1306.4840v1","http://arxiv.org/pdf/1306.4840v1","","","","","q-bio.TO","q-bio.TO|math.AP"
"461","1306.4842v1","2013-06-20 12:08:17","2013-06-20 12:08:17","Maximum Tolerated Dose Versus Metronomic Scheduling in the Treatment of
  Metastatic Cancers","  Although optimal control theory has been used for the theoretical study of
anti-cancerous drugs scheduling optimization, with the aim of reducing the
primary tumor volume, the effect on metastases is often ignored. Here, we use a
previously published model for metastatic development to define an optimal
control problem at the scale of the entire organism of the patient. In silico
study of the impact of different scheduling strategies for anti-angiogenic and
cytotoxic agents (either in monotherapy or in combination) is performed to
compare a low-dose, continuous, metronomic administration scheme with a more
classical maximum tolerated dose schedule. Simulation results reveal
differences between primary tumor reduction and control of metastases but
overall suggest use of the metronomic protocol.
","Sebastien Benzekry|Philip Hahnfeldt","CCSB|CCSB","http://arxiv.org/abs/1306.4842v1","http://arxiv.org/pdf/1306.4842v1","","","","","q-bio.TO","q-bio.TO|math.AP"
"462","1306.5366v1","2013-06-23 01:54:55","2013-06-23 01:54:55","National Oceanic and Atmospheric Administration Publishes Misleading
  Information on Gulf of Mexico ""Dead Zone""","  Mississippi River nutrient loads and water stratification on the
Louisiana-Texas shelf contribute to an annually recurring, short-lived hypoxic
bottom layer in areas of the northern Gulf of Mexico comprising less than 2% of
the total Gulf of Mexico bottom area. Many publications demonstrate increases
in biomass and fisheries production attributed to nutrient loading from river
plumes. Decreases in fisheries production when nutrient loads are decreased are
also well documented. However, the National Oceanic and Atmospheric
Administration (NOAA) persists in describing the area adjacent to the
Mississippi River discharge as a ""dead zone"" and predicting dire consequences
if nutrient loads are not reduced. In reality, these areas teem with aquatic
life and provide 70-80% of the Gulf of Mexico fishery production. On June 18,
2013, NOAA published a misleading figure purporting to show the ""dead zone"" in
an article predicting a possible record dead zone area for 2013
(http://www.noaanews.noaa.gov/stories2013/20130618_deadzone.html). This area is
not a region of hypoxic bottom water at all nor is it related directly to 2013
predicted hypoxia. This figure appeared as early as 2004 in a National
Aeronautics and Space Administration (NASA) article
(http://www.nasa.gov/vision/earth/environment/dead_zone.html) as a satellite
image where the red area represents turbidity and is much larger than the
short-lived areas of hypoxic bottom water documented in actual NOAA
measurements. Thus, it is misleading for NOAA to characterize the red area in
that image as a ""dead zone."" The NOAA has also published other misleading and
exaggerated descriptions of the consequences of nutrient loading.
","Michael W. Courtney|Joshua M. Courtney","","http://arxiv.org/abs/1306.5366v1","http://arxiv.org/pdf/1306.5366v1","","6 pages, 3 figures","","","q-bio.PE","q-bio.PE"
"463","1307.1684v1","2013-07-05 18:26:14","2013-07-05 18:26:14","Simulation of wireless dynamic source routing protocol with IP traffic
  flow analysis, memory efficiency and increased throughput","  Today in fast technology development in wireless mobile adhoc network there
is vast scope for research. As it is known that wireless communication for
mobile network has many application areas like routing services, security
services etc. The mobile adhoc network is the wireless network for
communication in which the mobile nodes are organized without any centralized
administrator. There are many Manet routing protocols like reactive, proactive,
hybrid etc. In this paper the reactive Manet routing protocol like DSR is
simulated for traffic analysis for 50 mobile nodes for IP traffic flows. Also
throughput is analyzed for DSR and ER-DSR protocol. And finally the memory
utilized during simulation of DSR and ER-DSR is evaluated in order to compare
both.
","Diya Naresh Vadhwani|Megha Singh|Deepak Kulhare","","http://arxiv.org/abs/1307.1684v1","http://arxiv.org/pdf/1307.1684v1","","5 pages, 11 figures, 8 tables","ACEEE International Journal on Communication (IJCom), Issue. 3,
  Vol. 4, Nov 2013","","cs.NI","cs.NI"
"464","1307.3638v1","2013-07-13 10:15:55","2013-07-13 10:15:55","A Nobel Defence Scheme Against Selfish Node Attack in MANET","  Security is one of the major issue in wired and wireless network but due to
the presence of centralized administration not difficult to find out
misbehavior in network other than in Mobile Ad hoc Network due to the absence
of centralized management and frequently changes in topology security is one of
a major issue in MANET. Only prevention methods for attack are not enough. In
this paper a new Intrusion Detection System (IDS) algorithm has proposed
against selfish node attack to secure MANET. Here the behavior of selfish node
is unnecessary flooding the information in network and block all types of
packets transferring between the reliable nodes. Proposed IDS Algorithm
identifies the behavior of selfish node and also blocked their misbehavior
activities. In case of selfish node attack network performance is almost
negligible but after applying IDS on attack network performance is enhanced up
to 92% and provides 0% Infection rate from attack.
","Gaurav Soni|Kamlesh Chandravanshi","","http://arxiv.org/abs/1307.3638v1","http://arxiv.org/pdf/1307.3638v1","","5 tables, 4 figures, 13 references","International Journal on Computational Science &
  Applications(IJCSA) 2013 ISSN: 2200-0011 is a AIRCC journel","","cs.NI","cs.NI"
"465","1308.0056v1","2013-07-31 22:37:33","2013-07-31 22:37:33","SDN-like: The Next Generation of Pub/Sub","  Software-Defined Networking (SDN) has raised the boundaries of cloud
computing by offering unparalleled levels of control and flexibility to system
administrators over their virtualized environments. To properly embrace this
new era of SDN-driven network architectures, the research community must not
only consider the impact of SDN over the protocol stack, but also on its
overlying networked applications. In this big ideas paper, we study the impact
of SDN on the design of future message-oriented middleware, specifically
pub/sub systems. We argue that key concepts put forth by SDN can be applied in
a meaningful fashion to the next generation of pub/sub systems. First, pub/sub
can adopt a logically centralized controller model for maintenance, monitoring,
and control of the overlay network. We establish a parallel with existing work
on centralized pub/sub routing and discuss how the logically centralized
controller model can be implemented in a distributed manner. Second, we
investigate the separation of the control and data plane, which is integral to
SDN, which can be adopted to raise the level of decoupling in pub/sub. We
introduce a new model of pub/sub which separates the traditional publisher and
subscriber roles into flow regulators and producer/consumers of data. We then
present use cases that benefit from this approach and study the impact of
decoupling for performance.
","Kaiwen Zhang|Hans-Arno Jacobsen","","http://arxiv.org/abs/1308.0056v1","http://arxiv.org/pdf/1308.0056v1","","","","","cs.NI","cs.NI|cs.DC"
"466","1308.1440v1","2013-08-06 23:02:12","2013-08-06 23:02:12","Graywulf: A platform for federated scientific databases and services","  Many fields of science rely on relational database management systems to
analyze, publish and share data. Since RDBMS are originally designed for, and
their development directions are primarily driven by, business use cases they
often lack features very important for scientific applications. Horizontal
scalability is probably the most important missing feature which makes it
challenging to adapt traditional relational database systems to the ever
growing data sizes. Due to the limited support of array data types and metadata
management, successful application of RDBMS in science usually requires the
development of custom extensions. While some of these extensions are specific
to the field of science, the majority of them could easily be generalized and
reused in other disciplines. With the Graywulf project we intend to target
several goals. We are building a generic platform that offers reusable
components for efficient storage, transformation, statistical analysis and
presentation of scientific data stored in Microsoft SQL Server. Graywulf also
addresses the distributed computational issues arising from current RDBMS
technologies. The current version supports load balancing of simple queries and
parallel execution of partitioned queries over a set of mirrored databases.
Uniform user access to the data is provided through a web based query interface
and a data surface for software clients. Queries are formulated in a slightly
modified syntax of SQL that offers a transparent view of the distributed data.
The software library consists of several components that can be reused to
develop complex scientific data warehouses: a system registry, administration
tools to manage entire database server clusters, a sophisticated workflow
execution framework, and a SQL parser library.
","Laszlo Dobos|Alexander S. Szalay|Tamas Budavari|Istvan Csabai|Nolan Li","","http://arxiv.org/abs/1308.1440v1","http://arxiv.org/pdf/1308.1440v1","","SSDBM 2013 proceedings","","","cs.DB","cs.DB"
"467","1308.1482v1","2013-08-07 05:40:27","2013-08-07 05:40:27","Increasing Robustness of the Anesthesia Process from Difference
  Patient's Delay Using a State-Space Model Predictive Controller","  The process of anesthesia is nonlinear with time delay and also there are
some constraints which have to be considered in calculating administrative drug
dosage. We present an Extended Kalman Filter (EKF) observer to estimate drug
concentration in the patient's body and use this estimation in a state-space
based Model of Predictive Controller (MPC) for controlling the depth of
anesthesia. Bispectral Index (BIS) is used as a patient consciousness index and
propofol as an anesthetic agent. Performance evaluations of the proposed
controller, the results have been compared with those of a MPC controller. The
results demonstrate that state-space MPC including the EKF estimator for
controlling the anesthesia process can significantly increase the robustness in
encountering patients' delay deviations in comparison with the MPC.
","Saba Rezvaniana|Farzad Towhidkhah|Nematollah Ghahramani|Alireza Rezvanian","","http://arxiv.org/abs/1308.1482v1","http://arxiv.org/pdf/1308.1482v1","http://dx.doi.org/10.1016/j.proeng.2011.08.171","5 pages, 4 figures, Journal","Procedia Engineering 15 (2011) 928-932","10.1016/j.proeng.2011.08.171","cs.SY","cs.SY"
"468","1308.5115v3","2013-08-23 12:45:25","2014-11-24 07:52:37","Power-law models for infectious disease spread","  Short-time human travel behaviour can be described by a power law with
respect to distance. We incorporate this information in space-time models for
infectious disease surveillance data to better capture the dynamics of disease
spread. Two previously established model classes are extended, which both
decompose disease risk additively into endemic and epidemic components: a
spatio-temporal point process model for individual-level data and a
multivariate time-series model for aggregated count data. In both frameworks, a
power-law decay of spatial interaction is embedded into the epidemic component
and estimated jointly with all other unknown parameters using (penalised)
likelihood inference. Whereas the power law can be based on Euclidean distance
in the point process model, a novel formulation is proposed for count data
where the power law depends on the order of the neighbourhood of discrete
spatial units. The performance of the new approach is investigated by a
reanalysis of individual cases of invasive meningococcal disease in Germany
(2002-2008) and count data on influenza in 140 administrative districts of
Southern Germany (2001-2008). In both applications, the power law substantially
improves model fit and predictions, and is reasonably close to alternative
qualitative formulations, where distance and order of neighbourhood,
respectively, are treated as a factor. Implementation in the R package
surveillance allows the approach to be applied in other settings.
","Sebastian Meyer|Leonhard Held","","http://arxiv.org/abs/1308.5115v3","http://arxiv.org/pdf/1308.5115v3","http://dx.doi.org/10.1214/14-AOAS743","Published in at http://dx.doi.org/10.1214/14-AOAS743 the Annals of
  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of
  Mathematical Statistics (http://www.imstat.org)","Annals of Applied Statistics 2014, Vol. 8, No. 3, 1612-1639","10.1214/14-AOAS743","stat.ME","stat.ME|physics.data-an|physics.soc-ph|stat.AP"
"469","1308.5177v1","2013-08-23 16:41:56","2013-08-23 16:41:56","Policy Specification in Role based Access Control on Clouds","  Cloud Computing is a set of IT Services that are provided to a customer over
a network and these services are delivered by third party provider who owns the
infrastructure and reduce the burden at user's end. Nowadays researchers
devoted their work access control method to enhance the security on Cloud. RBAC
is attractive access model because the number of roles is significantly less
hence users can be easily classified according to their roles. The Role-based
Access Control (RBAC) model provides efficient way to manage access to
information while reducing the cost of security administration and complexity
in large networked applications. This paper specify various policies in RBAC on
clouds such as migration policy which helps the user to migrate the database
schema and roles easily to the Cloud using XML with more security. Restriction
policy provide the security enhancement in Role Based Access Model by
restricting the number of transaction per user and if the number of
transactions will increase the admin will come to know through its monitoring
system that unauthorized access has been made and it would be easier to take
action against such happening. This paper proposes backup and restoration
policy in Role Based Access Model in which if the main cloud is crashed or not
working properly then the backup and restoration facility will be available to
avoid the lost of important data. In this case chances of loss of data are very
less so enhance more security on Cloud Computing.
"," Gitanjali|Sukhjit Singh Sehra|Jaiteg Singh","","http://arxiv.org/abs/1308.5177v1","http://arxiv.org/pdf/1308.5177v1","http://dx.doi.org/10.5120/13078-0253","","International Journal of Computer Applications 75(1):39-43, August
  2013","10.5120/13078-0253","cs.DC","cs.DC|cs.CR"
"470","1308.5703v2","2013-08-26 21:26:00","2014-03-04 14:01:46","A Principled Approach to Bridging the Gap between Graph Data and their
  Schemas","  Although RDF graphs have schema information associated with them, in practice
it is very common to find cases in which data do not fully conform to their
schema. A prominent example of this is DBpedia, which is RDF data extracted
from Wikipedia, a publicly editable source of information. In such situations,
it becomes interesting to study the structural properties of the actual data,
because the schema gives an incomplete description of the organization of a
dataset. In this paper we have approached the study of the structuredness of an
RDF graph in a principled way: we propose a framework for specifying
structuredness functions, which gauge the degree to which an RDF graph conforms
to a schema. In particular, we first define a formal language for specifying
structuredness functions with expressions we call rules. This language allows a
user or a database administrator to state a rule to which an RDF graph may
fully or partially conform. Then we consider the issue of discovering a
refinement of a sort (type) by partitioning the dataset into subsets whose
structuredness is over a specified threshold. In particular, we prove that the
natural decision problem associated to this refinement problem is NP-complete,
and we provide a natural translation of this problem into Integer Linear
Programming (ILP). Finally, we test this ILP solution with two real world
datasets, DBpedia Persons and WordNet Nouns, and 4 different and intuitive
rules, which gauge the structuredness in different ways. The rules give
meaningful refinements of the datasets, showing that our language can be a
powerful tool for understanding the structure of RDF data.
","Marcelo Arenas|Gonzalo I. Diaz|Achille Fokoue|Anastasios Kementsietsidis|Kavitha Srinivas","","http://arxiv.org/abs/1308.5703v2","http://arxiv.org/pdf/1308.5703v2","","18 pages, 8 figures. To be published in PVLDB Vol. 8, No. 9","","","cs.DB","cs.DB"
"471","1309.5351v1","2013-09-02 13:11:42","2013-09-02 13:11:42","Human Resource Management System","  The paper titled HUMAN RESOURCE MANAGEMENT SYSTEM is basically concerned with
managing the Administrator of HUMAN RESOURCE Department in a company. A Human
Resource Management System, refers to the systems and processes at the
intersection between human resource management and information technology. It
merges HRM as a discipline and in particular its basic HR activities and
processes with the information technology field, whereas the programming of
data processing systems evolved into standardized routines and packages of
enterprise resource planning software. The main objective of this paper is to
reduce the effort of Administrator to keep the daily events such as attendance,
projects, works, appointments, etc. This paper deals with the process of
identifying the employees, recording their attendance hourly and calculating
their effective payable hours or days. This paper should maintain the records
of each and every employee and their time spend in to company, which can be
used for performance appraisal. Based on that transfer, removal, promotion can
be done.
","A. S. Syed Navaz|A. S. Syed Fiaz|C. Prabhadevi|V. Sangeetha|S. Gopalakrishnan","","http://arxiv.org/abs/1309.5351v1","http://arxiv.org/pdf/1309.5351v1","","10 Pages","","","cs.OH","cs.OH"
"472","1309.0969v1","2013-09-04 10:49:37","2013-09-04 10:49:37","Pervasive liquid metal direct writing electronics with roller-ball pen","  A roller-ball pen enabled direct writing electronics via room temperature
liquid metal ink was proposed. With the rolling to print mechanism, the
metallic inks were smoothly written on flexible polymer substrate to form
conductive tracks and electronic devices. The contact angle analyzer and
scanning electron microscope were implemented to probe the inner property of
the obtained electronics. An ever high writing resolution with line width and
thickness as 200{\mu}m and 80{\mu}m, respectively was realized. Further, with
the administration of external writing pressure, GaIn24.5 droplets embody
increasing wettability on polymer which demonstrates the pervasive adaptability
of the roller-ball pen electronics.
","Yi Zheng|Qin Zhang|Jing Liu","","http://arxiv.org/abs/1309.0969v1","http://arxiv.org/pdf/1309.0969v1","http://dx.doi.org/10.1063/1.4832220","","AIP Advances 3, 112117 (2013)","10.1063/1.4832220","cond-mat.mtrl-sci","cond-mat.mtrl-sci|cond-mat.mes-hall|physics.ins-det|physics.pop-ph"
"473","1309.1232v1","2013-09-05 04:37:16","2013-09-05 04:37:16","Bug Tracking and Reporting System","  This is the world of information. The ever growing field Information
Technology has its many advanced notable features which made it what it was now
today. In this world, the information has to be processed, clearly distributed
and must be efficiently reachable to the end users intended for that. Otherwise
we know it lead to disastrous situations. The other coin of the same phase is
it is absolutely necessary to know any bugs that are hither to faced by the end
users. The project Bug Tracking and Reporting System aims to provide the
solution for that. The Bug Tracker can be made from any two types. The first
one being the system side, the other being the services side. Our project deals
with the second one. The paper is wholly dedicated to tracking the bugs that
are hither by arise. The administrator maintains the master details regarding
to the bugs id, bugs type, bugs description, bugs severity, bugs status, user
details. The administrator too has the authority to update the master details
of severity level, status level, etc, modules of the paper. The administrator
adds the users and assign them responsibility of completing the paper. Finally
on analyzing the paper assigned to the particular user, the administrator can
track the bugs, and it is automatically added to the tables containing the
bugs, by order of severity and status. The administrator can know the
information in tact the various paper s assigned to various users, their bug
tracking status, their description etc in the form of reports from time to
time.
","A. S. Syed Fiaz|N. Devi|S. Aarthi","","http://arxiv.org/abs/1309.1232v1","http://arxiv.org/pdf/1309.1232v1","","4 Pages","","","cs.SE","cs.SE"
"474","1309.2351v1","2013-09-10 00:42:05","2013-09-10 00:42:05","Elementos de ingenieria de explotacion de la informacion aplicados
  a la investigacion tributaria fiscal","  By introducing elements of information mining to tax analysis, by means of
data mining software and advanced computational concepts of artificial
intelligence, the problem of tax evader's crime against public property has
been addressed. Through an empirical approach from a hypothetical case of use,
induction algorithms, neural networks and bayesian networks are applied to
determine the feasibility of its heuristic application by the tax public
administrator. Different strategies are explored to facilitate the work of
local and regional federal tax inspectors, considering their limited
computational capabilities, but equally effective for those social scientist
committed to handcrafting tax research.
  -----
  Apresentando a introdu\c{c}\~ao de elementos de explora\c{c}\~ao de
informa\c{c}\~oes para an\'alise fiscal, por meio de software de
minera\c{c}\~ao de dados e conceitos avan\c{c}ados computacionais de
intelig\^encia artificial, foi abordado o problema do crime de sonegador fiscal
contra o patrim\^onio p\'ublico. Atrav\'es de uma abordagem emp\'irica a partir
de um caso hipot\'etico de uso, os algoritmos de indu\c{c}\~ao, redes neurais e
redes bayesianas s\~ao aplicados para determinar a viabilidade de sua
aplica\c{c}\~ao heur\'istica pelo administrador p\'ublico tribut\'ario.
Diferentes estrat\'egias s\~ao exploradas para facilitar o trabalho dos
inspectores tribut\'arios federais locais e regionais, tendo em conta as suas
capacidades computacionais limitados, mas igualmente eficaz para aqueles
cientista social comprometido com a investiga\c{c}\~ao fiscal.
","Rodrigo Lopez-Pablos","Universidad Nacional de La Matanza y Universidad Tecnologica Nacional","http://arxiv.org/abs/1309.2351v1","http://arxiv.org/pdf/1309.2351v1","","30 pages, 7 figures, written in Castilian, Artificial Intelligence
  (cs.AI), Computers and Society (cs.CY)","","","cs.AI","cs.AI|I.2.4; K.4.1"
"475","1309.5393v1","2013-09-13 17:23:49","2013-09-13 17:23:49","How to Implement Access Rights in an MIS Project","  The MIS data is critical to an organization and should be protected from
misuse by wrong persons. Although The MIS data is typically meant for the
senior managers each MIS report may not be required by every manager. The
access to MIS data is determined by the role of an individual in the
organization and controlled by the MIS administrator accordingly. The access is
generally determined by the following parameters, (a) the type of user (such as
staff or manager etc.), (b) the type of data (whether general data or
managerial data), (c) level of access (read/ write/ admin access) and (d)
special access allocated by MIS admin. By combining all the above four
parameters, each individual user can be allocated exact specific rights
required to access the MIS.
","Umakant Mishra","","http://arxiv.org/abs/1309.5393v1","http://arxiv.org/pdf/1309.5393v1","http://dx.doi.org/10.2139/ssrn.2315570","15 pages, 8 figures, Available at SSRN:
  http://ssrn.com/abstract=2315570","Mishra, Umakant, How to Implement Access Rights in an MIS Project
  (August 24, 2013)","10.2139/ssrn.2315570","cs.CR","cs.CR"
"476","1309.4893v1","2013-09-19 08:56:02","2013-09-19 08:56:02","Mechanical Therapy as a Potential Green Way to Attack Cancer Disease","  Mechanical force is tightly connected to human health status, and the
occurrence of disease can generally be ascribed to certain loss of force
balance. However, the role of mechanical approaches in tumor therapy is largely
neglected, while the currently available cancer prevention and treatment
methods are generally either expensive or just cause too much side effect. In
this article, we present a systematic interpretation on a promising strategy
which was termed here as Mechanical Therapy for the first time based on the
fact that mechanical force closely accompanies the whole life (growth and
death) of a cell, and plays a crucial role in biological functions. In order to
mold the mechanical force as a practical tool for cancer therapy, we expound
the effects of the mechanical force related to the tumors from molecule to
tissue level and evaluate its feasibility for treatment purpose. It can be
conceived that given enough investigations, the mechanical therapy may generate
big potential to open new windows for cancer prevention, relieving or even
destroying if well administrated. As an easy going and convenient method, a
systematic and thorough study of mechanical therapy may clarify more options
for future clinical practices. Besides, when patients encounter various forces
in daily life, they can at least identify the good or bad forces for a better
and healthy life style based on the mechanisms thus disclosed.
","Li Ting Yi|Jing Liu","","http://arxiv.org/abs/1309.4893v1","http://arxiv.org/pdf/1309.4893v1","","","","","physics.med-ph","physics.med-ph|q-bio.TO"
"477","1309.7099v1","2013-09-27 01:47:37","2013-09-27 01:47:37","On the Internal Dynamics of the Shanghai Ranking","  The Academic Ranking of World Universities (ARWU) published by researchers at
Shanghai Jiao Tong University has become a major source of information for
university administrators, country officials, students and the public at large.
Recent discoveries regarding its internal dynamics allow the inversion of
published ARWU indicator scores to reconstruct raw scores for five hundred
world class universities. This paper explores raw scores in the ARWU and in
other contests to contrast the dynamics of rank-driven and score-driven tables,
and to explain why the ARWU ranking is a score-driven procedure. We show that
the ARWU indicators constitute sub-scales of a single factor accounting for
research performance, and provide an account of the system of gains and
non-linearities used by ARWU. The paper discusses the non-linearities selected
by ARWU, concluding that they are designed to represent the regressive
character of indicators measuring research performance. We propose that the
utility and usability of the ARWU could be greatly improved by replacing the
unwanted dynamical effects of the annual re-scaling based on raw scores of the
best performers.
","Domingo Docampo|Lawrence Cram","","http://arxiv.org/abs/1309.7099v1","http://arxiv.org/pdf/1309.7099v1","http://dx.doi.org/10.1007/s11192-013-1143-0","","","10.1007/s11192-013-1143-0","cs.DL","cs.DL|68P20|H.1.1; H.2.8; H.3.1"
"478","1310.1055v2","2013-10-03 18:39:27","2014-05-02 18:07:00","An epistemology and expectations survey about experimental physics:
  Development and initial results","  In response to national calls to better align physics laboratory courses with
the way physicists engage in research, we have developed an epistemology and
expectations survey to assess how students perceive the nature of physics
experiments in the contexts of laboratory courses and the professional research
laboratory. The Colorado Learning Attitudes about Science Survey for
Experimental Physics (E-CLASS) evaluates students' epistemology at the
beginning and end of a semester. Students respond to paired questions about how
they personally perceive doing experiments in laboratory courses and how they
perceive an experimental physicist might respond regarding their research.
Also, at the end of the semester, the E-CLASS assesses a third dimension of
laboratory instruction, students' reflections on their course's expectations
for earning a good grade. By basing survey statements on widely embraced
learning goals and common critiques of teaching labs, the E-CLASS serves as an
assessment tool for lab courses across the undergraduate curriculum and as a
tool for physics education research. We present the development, evidence of
validation, and initial formative assessment results from a sample that
includes 45 classes at 20 institutions. We also discuss feedback from
instructors and reflect on the challenges of large-scale online administration
and distribution of results.
","Benjamin M. Zwickl|Takako Hirokawa|Noah Finkelstein|H. J. Lewandowski","","http://arxiv.org/abs/1310.1055v2","http://arxiv.org/pdf/1310.1055v2","http://dx.doi.org/10.1103/PhysRevSTPER.10.010120","31 pages, 9 figures, 3 tables, submitted to Phys. Rev. - PER","","10.1103/PhysRevSTPER.10.010120","physics.ed-ph","physics.ed-ph"
"479","1310.2148v1","2013-10-03 22:49:06","2013-10-03 22:49:06","C2MS: Dynamic Monitoring and Management of Cloud Infrastructures","  Server clustering is a common design principle employed by many organisations
who require high availability, scalability and easier management of their
infrastructure. Servers are typically clustered according to the service they
provide whether it be the application(s) installed, the role of the server or
server accessibility for example. In order to optimize performance, manage load
and maintain availability, servers may migrate from one cluster group to
another making it difficult for server monitoring tools to continuously monitor
these dynamically changing groups. Server monitoring tools are usually
statically configured and with any change of group membership requires manual
reconfiguration; an unreasonable task to undertake on large-scale cloud
infrastructures.
  In this paper we present the Cloudlet Control and Management System (C2MS); a
system for monitoring and controlling dynamic groups of physical or virtual
servers within cloud infrastructures. The C2MS extends Ganglia - an open source
scalable system performance monitoring tool - by allowing system administrators
to define, monitor and modify server groups without the need for server
reconfiguration. In turn administrators can easily monitor group and individual
server metrics on large-scale dynamic cloud infrastructures where roles of
servers may change frequently. Furthermore, we complement group monitoring with
a control element allowing administrator-specified actions to be performed over
servers within service groups as well as introduce further customized
monitoring metrics. This paper outlines the design, implementation and
evaluation of the C2MS.
","Gary A. McGilvary|Josep Rius|Inigo Goiri|Francesc Solsona|Adam Barker|Malcolm Atkinson","","http://arxiv.org/abs/1310.2148v1","http://arxiv.org/pdf/1310.2148v1","http://dx.doi.org/10.1109/CloudCom.2013.45","Proceedings of the The 5th IEEE International Conference on Cloud
  Computing Technology and Science (CloudCom 2013), 8 pages","","10.1109/CloudCom.2013.45","cs.DC","cs.DC|cs.OS"
"480","1310.1829v2","2013-10-07 15:50:31","2013-12-20 06:30:57","Delineating geographical regions with networks of human interactions in
  an extensive set of countries","  Large-scale networks of human interaction, in particular country-wide
telephone call networks, can be used to redraw geographical maps by applying
algorithms of topological community detection. The geographic projections of
the emerging areas in a few recent studies on single regions have been
suggested to share two distinct properties: first, they are cohesive, and
second, they tend to closely follow socio-economic boundaries and are similar
to existing political regions in size and number. Here we use an extended set
of countries and clustering indices to quantify overlaps, providing ample
additional evidence for these observations using phone data from countries of
various scales across Europe, Asia, and Africa: France, the UK, Italy, Belgium,
Portugal, Saudi Arabia, and Ivory Coast. In our analysis we use the known
approach of partitioning country-wide networks, and an additional iterative
partitioning of each of the first level communities into sub-communities,
revealing that cohesiveness and matching of official regions can also be
observed on a second level if spatial resolution of the data is high enough.
The method has possible policy implications on the definition of the
borderlines and sizes of administrative regions.
","Stanislav Sobolevsky|Michael Szell|Riccardo Campari|Thomas Couronne|Zbigniew Smoreda|Carlo Ratti","","http://arxiv.org/abs/1310.1829v2","http://arxiv.org/pdf/1310.1829v2","http://dx.doi.org/10.1371/journal.pone.0081707","10 pages, 4 figures","PLOS ONE 8(12): e81707 (2013)","10.1371/journal.pone.0081707","cs.SI","cs.SI|physics.soc-ph"
"481","1310.2369v1","2013-10-09 06:30:21","2013-10-09 06:30:21","Semi Symmetric Method Of SAN Storage Virtualization","  Virtualization is one of the biggest buzzwords of the technology industry
right at this moment. The fast growth in storage capacity and processing power
in enterprise installations coupled with the need for high availability,
requires Storage Area Network (SAN) architecture to provide seamless addition
of storage and performance elements without downtime. The usual goal of
virtualization is to centralize administrative tasks while improving
scalability and work loads. This paper, describing about new proposed method
for virtualization, which would be overcome limitations of existed methods for
storage virtualization
","Dhanamma Jagli|Ramesh Solanki|Rohini Temkar|Laxmi Veshapogu","","http://arxiv.org/abs/1310.2369v1","http://arxiv.org/pdf/1310.2369v1","","","","","cs.SE","cs.SE|cs.DC"
"482","1310.7440v1","2013-10-09 08:29:32","2013-10-09 08:29:32","Neural perceptual model to global-local vision for recognition of the
  logical structure of administrative documents","  This paper gives the definition of Transparent Neural Network ""TNN"" for the
simulation of the globallocal vision and its application to the segmentation of
administrative document image. We have developed and have adapted a recognition
method which models the contextual effects reported from studies in
experimental psychology. Then, we evaluated and tested the TNN and the
multi-layer perceptron ""MLP"", which showed its effectiveness in the field of
the recognition, in order to show that the TNN is clearer for the user and more
powerful on the level of the recognition. Indeed, the TNN is the only system
which makes it possible to recognize the document and its structure.
","Boulbaba Ben Ammar","","http://arxiv.org/abs/1310.7440v1","http://arxiv.org/pdf/1310.7440v1","http://dx.doi.org/10.5121/ijaia","17 pages, International Journal of Artificial Intelligence &
  Applications (IJAIA), Vol. 4, No. 5, September 2013","","10.5121/ijaia","cs.CV","cs.CV"
"483","1310.2861v1","2013-10-10 15:47:16","2013-10-10 15:47:16","Distributed firewalls and IDS interoperability checking based on a
  formal approach","  To supervise and guarantee a network security, the administrator uses
different security components, such as firewalls, IDS and IPS. For a perfect
interoperability between these components, they must be configured properly to
avoid misconfiguration between them. Nevertheless, the existence of a set of
anomalies between filtering rules and alerting rules, particularly in
distributed multi-component architectures is very likely to degrade the network
security. The main objective of this paper is to check if a set of security
components are interoperable. A case study using a firewall and an IDS as
examples will illustrate the usefulness of our approach.
","Kamel Karoui|Fakher Ben Ftima|Henda Ben Ghezala","","http://arxiv.org/abs/1310.2861v1","http://arxiv.org/pdf/1310.2861v1","http://dx.doi.org/10.5121/ijcnc.2013.5508","Security component, relevancy, misconfigurations detection,
  interoperability cheking, formal correction,formal verification, projection,
  IDS, Firewall","International Journal of Computer Networks & Communications,
  September 2013, Volume 5. Number 5, pp 95-115","10.5121/ijcnc.2013.5508","cs.NI","cs.NI|cs.CR|C.2; C.2.0; C.2.1; C.2.3; D.2.12"
"484","1310.3040v3","2013-10-11 07:52:48","2014-01-31 07:03:27","Measuring Triple-Helix Synergy in the Russian Innovation Systems at
  Regional, Provincial, and National Levels","  We measure synergy for the Russian national, provincial, and regional
innovation systems as reduction of uncertainty using mutual information among
the three distributions of firm sizes, technological knowledge-bases of firms,
and geographical locations. Half a million data at firm level in 2011 were
obtained from the Orbis database of Bureau Van Dijk. The firm level data were
aggregated at the levels of eight Federal Districts, the regional level of 83
Federal Subjects, and the single level of the Russian Federation. Not
surprisingly, the knowledge base of the economy is concentrated in the Moscow
region (22.8%); St. Petersburg follows with 4.0%. Only 0.4% of the firms are
classified as high-tech, and 2.7% as medium-tech manufacturing (NACE, Rev. 2).
Except in Moscow itself, high-tech manufacturing does not add synergy to any
other unit at any of the various levels of geographical granularity; instead it
disturbs regional coordination even in the region surrounding Moscow (""Moscow
Region""). In the case of medium-tech manufacturing, there is also synergy in
St. Petersburg. Knowledge-intensive services (KIS; including laboratories)
contribute 12.8% to the economy in terms of establishments and contribute to
the synergy in all Federal Districts (except the North-Caucasian Federal
District), but only in 30 of the 83 Federal Subjects. The synergy in KIS is
concentrated in centers of administration. Unlike Western European countries,
the knowledge-intensive services (which are often state-affiliated) thus
provide backbone to an emerging knowledge-based economy at the level of Federal
Districts, but the economy is otherwise not knowledge-based (except for the
Moscow region).
","Loet Leydesdorff|Evgeniy Perevodchikov|Alexander Uvarov","","http://arxiv.org/abs/1310.3040v3","http://arxiv.org/pdf/1310.3040v3","","accepted for publication in the Journal of the Association for
  Information Science and Technology JASIST","","","cs.CY","cs.CY"
"485","1310.5255v1","2013-10-19 17:32:17","2013-10-19 17:32:17","Efficient and Robust Allocation Algorithms in Clouds under Memory
  Constraints","  We consider robust resource allocation of services in Clouds. More
specifically, we consider the case of a large public or private Cloud platform
that runs a relatively small set of large and independent services. These
services are characterized by their demand along several dimensions (CPU,
memory,...) and by their quality of service requirements, that have been
defined through an SLA in the case of a public Cloud or fixed by the
administrator in the case of a private Cloud. This quality of service defines
the required robustness of the service, by setting an upper limit on the
probability that the provider fails to allocate the required quantity of
resources. This maximum probability of failure can be transparently turned into
a pair (price,penalty). Failures can indeed hit the platform, and resilience is
provided through service replication. Our contribution is two-fold. First, we
propose a resource allocation strategy whose complexity is logarithmic in the
number of resources, what makes it very efficient for large platforms. Second,
we propose an efficient algorithm based on rare events detection techniques in
order to estimate the robustness of an allocation, a problem that has been
proven to be P-complete. Finally, we provide an analysis of the proposed
strategy through an extensive set of simulations, both in terms of the overall
number of allocated resources and in terms of time necessary to compute the
allocation.
","Olivier Beaumont|Lionel Eyraud-Dubois|Paul Renaud-Goud","INRIA Bordeaux - Sud-Ouest, LaBRI|INRIA Bordeaux - Sud-Ouest, LaBRI|INRIA Bordeaux - Sud-Ouest, LaBRI","http://arxiv.org/abs/1310.5255v1","http://arxiv.org/pdf/1310.5255v1","","","","","cs.DC","cs.DC"
"486","1310.5962v1","2013-10-22 15:35:37","2013-10-22 15:35:37","RBAC Architecture Design Issues in Institutions Collaborative
  Environment","  Institutional collaborative systems focus on providing the fast, and secure
connections to students, teaching and non-teaching staff members. Access
control is more important in these types of systems because different kind of
users access the system on different levels. So a proper architecture must be
there for these kinds of systems, for providing an efficient and secure system.
As lot of work was done in RBAC like for grouping, securing the system, ease of
use, and for enterprise etc but no one apply all these concepts as a whole on
institution level. So, this paper will be a step towards administrative load
sharing, securing the system, and ease of use.
","Muhammad Umar Aftab|Amna Nisar|Dr. Asif|Adeel Ashraf|Burhan Gill","","http://arxiv.org/abs/1310.5962v1","http://arxiv.org/pdf/1310.5962v1","","8 pages, 3 figures, 11 References","International Journal of Computer Science Issues, Volume 10, Issue
  4, July 2013","","cs.SE","cs.SE"
"487","1310.6775v1","2013-10-24 21:10:53","2013-10-24 21:10:53","Durkheim Project Data Analysis Report","  This report describes the suicidality prediction models created under the
DARPA DCAPS program in association with the Durkheim Project
[http://durkheimproject.org/]. The models were built primarily from
unstructured text (free-format clinician notes) for several hundred patient
records obtained from the Veterans Health Administration (VHA). The models were
constructed using a genetic programming algorithm applied to bag-of-words and
bag-of-phrases datasets. The influence of additional structured data was
explored but was found to be minor. Given the small dataset size,
classification between cohorts was high fidelity (98%). Cross-validation
suggests these models are reasonably predictive, with an accuracy of 50% to 69%
on five rotating folds, with ensemble averages of 58% to 67%. One particularly
noteworthy result is that word-pairs can dramatically improve classification
accuracy; but this is the case only when one of the words in the pair is
already known to have a high predictive value. By contrast, the set of all
possible word-pairs does not improve on a simple bag-of-words model.
","Linas Vepstas","","http://arxiv.org/abs/1310.6775v1","http://arxiv.org/pdf/1310.6775v1","http://dx.doi.org/10.1371/journal.pone.0085733.s001","43 pages, to appear as appendix of primary science publication
  Poulin, et al ""Predicting the risk of suicide by analyzing the text of
  clinical notes""","","10.1371/journal.pone.0085733.s001","cs.AI","cs.AI|cs.CL|cs.LG|I.2.1; I.2.2; I.2.7; J.4"
"488","1310.8383v1","2013-10-31 05:05:51","2013-10-31 05:05:51","Bioinformatics Knowledge Transmission (training, learning, and
  teaching): overview and flexible comparison of computer based training
  approaches","  The merger of computer science, mathematics, and life sciences has brought
about the discipline known as bioinformatics. However, the transmission (e.g.
training, learning, and teaching) of this knowledge becomes an important issue.
Many tools have been developed to help the bioinformatics community with that
transmission challenge. When selecting the best of these tools, called here
BKTMS (Bioinformatics Knowledge Transmission Management Systems), there may be
confusion. What makes a good BKTMS? How can we make this choice efficiently?
These questions remain unanswered for many users (e.g. learner, teacher and
student, trainer and trainee, administrator). This paper provides a critical
review of 32 existing BKTMS and a flexible comparison. This review and
evaluation will be used to gain insight into the tools, systems, and
capabilities that will be added to or excluded from a new proposed model for
the next generation of BKTMS, involving multidisciplinary, web semantic tools
(e.g. web services, workflow) and standards like LOM, or SCORM.
","Etienne Z. Gnimpieba|Douglas Jennewein|Luke Fuhrman|Carol M. Lushbough","","http://arxiv.org/abs/1310.8383v1","http://arxiv.org/pdf/1310.8383v1","","Proc. IKE'13, World Academy of Science","","","cs.CY","cs.CY|q-bio.OT"
"489","1311.5064v1","2013-11-07 09:13:04","2013-11-07 09:13:04","Graph measures and network robustness","  Network robustness research aims at finding a measure to quantify network
robustness. Once such a measure has been established, we will be able to
compare networks, to improve existing networks and to design new networks that
are able to continue to perform well when it is subject to failures or attacks.
In this paper we survey a large amount of robustness measures on simple,
undirected and unweighted graphs, in order to offer a tool for network
administrators to evaluate and improve the robustness of their network. The
measures discussed in this paper are based on the concepts of connectivity
(including reliability polynomials), distance, betweenness and clustering. Some
other measures are notions from spectral graph theory, more precisely, they are
functions of the Laplacian eigenvalues. In addition to surveying these graph
measures, the paper also contains a discussion of their functionality as a
measure for topological network robustness.
","W. Ellens|R. E. Kooij","","http://arxiv.org/abs/1311.5064v1","http://arxiv.org/pdf/1311.5064v1","","","","","cs.DM","cs.DM|cs.SI|math.CO|physics.soc-ph"
"490","1311.2677v1","2013-11-12 05:32:48","2013-11-12 05:32:48","Sampling Based Approaches to Handle Imbalances in Network Traffic
  Dataset for Machine Learning Techniques","  Network traffic data is huge, varying and imbalanced because various classes
are not equally distributed. Machine learning (ML) algorithms for traffic
analysis uses the samples from this data to recommend the actions to be taken
by the network administrators as well as training. Due to imbalances in
dataset, it is difficult to train machine learning algorithms for traffic
analysis and these may give biased or false results leading to serious
degradation in performance of these algorithms. Various techniques can be
applied during sampling to minimize the effect of imbalanced instances. In this
paper various sampling techniques have been analysed in order to compare the
decrease in variation in imbalances of network traffic datasets sampled for
these algorithms. Various parameters like missing classes in samples,
probability of sampling of the different instances have been considered for
comparison.
","Raman Singh|Harish Kumar|R. K. Singla","","http://arxiv.org/abs/1311.2677v1","http://arxiv.org/pdf/1311.2677v1","http://dx.doi.org/10.5121/csit.2013.3704","12 pages","","10.5121/csit.2013.3704","cs.NI","cs.NI|cs.CR|cs.LG"
"491","1311.4043v2","2013-11-16 10:07:59","2013-12-02 21:15:23","High Dimensional Tests Based on U-Statistics for Generalized Linear
  Models","  I propose two U-statistics to test coefficients in generalized linear models.
One of them is used to deal with global hypothesis and the other one to test
with the nuisance parameter. Both the statistics proposed are within
high-dimensional setting which means the number of coefficients is much larger
than the sample size. The statistics are based on quasi-likelihood function so
that they have wilder applications. I theoretically analyze the asymptotic
distribution of the statistics under the null hypothesis and the power
functions under the local and fixed alternatives. To serve as a comparison, the
power functions of the test proposed by Goeman et al. (2011) are also derived.
Some simulation studies are carried out and I apply my methods to an empirical
study.
","Gong Zi Jiang Nan","","http://arxiv.org/abs/1311.4043v2","http://arxiv.org/pdf/1311.4043v2","","This submission has been withdrawn by arXiv administrators due to
  disputed authorship","","","stat.AP","stat.AP|math.ST|stat.TH"
"492","1311.4211v3","2013-11-17 20:21:23","2014-04-03 13:35:44","Network communities within and across borders","  We investigate the impact of borders on the topology of spatially embedded
networks. Indeed territorial subdivisions and geographical borders
significantly hamper the geographical span of networks thus playing a key role
in the formation of network communities. This is especially important in
scientific and technological policy-making, highlighting the interplay between
pressure for the internationalization to lead towards a global innovation
system and the administrative borders imposed by the national and regional
institutions. In this study we introduce an outreach index to quantify the
impact of borders on the community structure and apply it to the case of the
European and US patent co-inventors networks. We find that (a) the US
connectivity decays as a power of distance, whereas we observe a faster
exponential decay for Europe; (b) European network communities essentially
correspond to nations and contiguous regions while US communities span multiple
states across the whole country without any characteristic geographic scale. We
confirm our findings by means of a set of simulations aimed at exploring the
relationship between different patterns of cross-border community structures
and the outreach index.
","Federica Cerina|Alessandro Chessa|Fabio Pammolli|Massimo Riccaboni","","http://arxiv.org/abs/1311.4211v3","http://arxiv.org/pdf/1311.4211v3","http://dx.doi.org/10.1038/srep04546","Scientific Reports 4, 2014","","10.1038/srep04546","physics.soc-ph","physics.soc-ph|cs.SI"
"493","1311.5874v2","2013-11-22 20:35:21","2014-05-09 20:41:32","Exploiting tumor shrinkage through temporal optimization of radiotherapy","  In multi-stage radiotherapy, a patient is treated in several stages separated
by weeks or months. This regimen has been motivated mostly by radiobiological
considerations, but also provides an approach to reduce normal tissue dose by
exploiting tumor shrinkage. The paper considers the optimal design of
multi-stage treatments, motivated by the clinical management of large liver
tumors for which normal liver dose constraints prohibit the administration of
an ablative radiation dose in a single treatment.
  We introduce a dynamic tumor model that incorporates three factors: radiation
induced cell kill, tumor shrinkage, and tumor cell repopulation. The design of
multi-stage radiotherapy is formulated as a mathematical optimization problem
in which the total dose to the liver is minimized, subject to delivering the
prescribed dose to the tumor. Based on the model, we gain insight into the
optimal administration of radiation over time, i.e. the optimal treatment gaps
and dose levels.
  We analyze treatments consisting of two stages in detail. The analysis
confirms the intuition that the second stage should be delivered just before
the tumor size reaches a minimum and repopulation overcompensates shrinking.
Furthermore, it was found that, for a large range of model parameters,
approximately one third of the dose should be delivered in the first stage. The
projected benefit of multi-stage treatments depends on model assumptions.
However, the model predicts large liver dose reductions by more than a factor
of two for plausible model parameters.
  The analysis of the tumor model suggests that substantial reduction in normal
tissue dose can be achieved by exploiting tumor shrinkage via an optimal design
of multi-stage treatments. This suggests taking a fresh look at multi-stage
radiotherapy for selected disease sites where substantial tumor regression
translates into reduced target volumes.
","Jan Unkelbach|David Craft|Theodore Hong|David Papp|Jagdish Ramakrishnan|Ehsan Salari|John Wolfgang|Thomas Bortfeld","","http://arxiv.org/abs/1311.5874v2","http://arxiv.org/pdf/1311.5874v2","http://dx.doi.org/10.1088/0031-9155/59/12/3059","","","10.1088/0031-9155/59/12/3059","physics.med-ph","physics.med-ph"
"494","1311.6251v1","2013-11-25 10:28:33","2013-11-25 10:28:33","A Comprehensive Assessment Strategy for Physics Laboratory Courses","  The objective of physics laboratory training is to develop, in students, a
variety of important cognitive and psycho-motor abilities related to
experimental physics. These include conceptual understanding, procedural
understanding, experimental skills and the experimental problem solving
ability. It has been noted that strategies adopted for the assessment of what
students learn and develop through a laboratory course are often inconsistent
with the objectives of the laboratory courses. The author has developed a
comprehensive assessment strategy which can be used at the school, college and
university level. The strategy is based on four tools of assessment, namely,
test on conceptual understanding, test on procedural understanding, an
experimental test, and the continuous assessment. The relative weightage for
each of the four tools depends on the level and emphasis of the laboratory
course. The four tools of assessment, with respect to the type of questions,
design, grading schemes, administration of each tool have been described with a
few sample questions for each tool of assessment. This assessment strategy is
being practiced and its effectiveness studied during a series of special
courses in experimental physics in India. Furthermore, a survey was carried out
with university teachers from across India to check the acceptability and
feasibility of using this strategy for larger number of students in
universities and most of the participating teachers supported the use of this
strategy.
","Rajesh B. Khaparde","","http://arxiv.org/abs/1311.6251v1","http://arxiv.org/pdf/1311.6251v1","","7 Pages","","","physics.ed-ph","physics.ed-ph"
"495","1311.6806v1","2013-11-26 20:59:35","2013-11-26 20:59:35","Prevalence of Earth-size planets orbiting Sun-like stars","  Determining whether Earth-like planets are common or rare looms as a
touchstone in the question of life in the universe. We searched for Earth-size
planets that cross in front of their host stars by examining the brightness
measurements of 42,000 stars from National Aeronautics and Space
Administration's Kepler mission. We found 603 planets, including 10 that are
Earth size (1-2 Earth-radii) and receive comparable levels of stellar energy to
that of Earth (within a factor of four). We account for Kepler's imperfect
detectability of such planets by injecting synthetic planet-caused dimmings
into the Kepler brightness measurements and recording the fraction detected. We
find that $11\pm4%$ of Sun-like stars harbor an Earth-size planet receiving
between one and four times the stellar intensity as Earth. We also find that
the occurrence of Earth-size planets is constant with increasing orbital period
(P), within equal intervals of logP up to $\sim200$ d. Extrapolating, one finds
$5.7^{+1.7}_{-2.2}%$ of Sun-like stars harbor an Earth-size planet with orbital
periods of 200-400 d.
","Erik A. Petigura|Andrew W. Howard|Geoffrey W. Marcy","","http://arxiv.org/abs/1311.6806v1","http://arxiv.org/pdf/1311.6806v1","http://dx.doi.org/10.1073/pnas.1319909110","Main text: 6 pages, 5 figures, 1 table. Supporting information: 54
  pages, 17 pages, 3 tables. Published in the Proceedings of the National
  Academy of Sciences available at
  http://www.pnas.org/cgi/doi/10.1073/pnas.1319909110","PNAS 2013 110 (48) 19175-19176","10.1073/pnas.1319909110","astro-ph.EP","astro-ph.EP"
"496","1312.0084v2","2013-11-30 10:20:15","2014-07-27 14:30:20","Crossing the hurdle: the determinants of individual scientific
  performance","  An original cross sectional dataset referring to a medium sized Italian
university is implemented in order to analyze the determinants of scientific
research production at individual level. The dataset includes 942 permanent
researchers of various scientific sectors for a three year time span (2008 -
2010). Three different indicators - based on the number of publications or
citations - are considered as response variables. The corresponding
distributions are highly skewed and display an excess of zero - valued
observations. In this setting, the goodness of fit of several Poisson mixture
regression models are explored by assuming an extensive set of explanatory
variables. As to the personal observable characteristics of the researchers,
the results emphasize the age effect and the gender productivity gap, as
previously documented by existing studies. Analogously, the analysis confirm
that productivity is strongly affected by the publication and citation
practices adopted in different scientific disciplines. The empirical evidence
on the connection between teaching and research activities suggests that no
univocal substitution or complementarity thesis can be claimed: a major
teaching load does not affect the odds to be a non-active researcher and does
not significantly reduce the number of publications for active researchers. In
addition, new evidence emerges on the effect of researchers administrative
tasks, which seem to be negatively related with researcher's productivity, and
on the composition of departments. Researchers' productivity is apparently
enhanced by operating in department filled with more administrative and
technical staff, and it is not significantly affected by the composition of the
department in terms of senior or junior researchers.
","Alberto Baccini|Lucio Barabesi|Martina Cioni|Caterina Pisani","","http://arxiv.org/abs/1312.0084v2","http://arxiv.org/pdf/1312.0084v2","http://dx.doi.org/10.1007/s11192-014-1395-3","Revised version accepted for publication by Scientometrics","","10.1007/s11192-014-1395-3","physics.soc-ph","physics.soc-ph|cs.DL|stat.AP|62J12"
"497","1312.0114v1","2013-11-30 15:06:21","2013-11-30 15:06:21","Extended Role Based Access Control with Blob Service on Cloud","  Role-based access control (RBAC) models have generated a great interest in
the security community as a powerful and generalized approach to security
management and ability to model organizational structure and their capability
to reduce administrative expenses. In this paper, we highlight the drawbacks of
latest developed RBAC models in terms of access control and authorization and
later provide a more viable extended-RBAC model, which enhances and extends its
powers to make any system more secure by adding valuable constraints. Later the
Blobs are stored on cloud server which is then accessed by the end users via
this Extended RBAC model.
","Mamoon Rashid|Er. Rishma Chawla","","http://arxiv.org/abs/1312.0114v1","http://arxiv.org/pdf/1312.0114v1","","6 page and 1 figure","","","cs.DC","cs.DC|cs.CR"
"498","1312.0910v1","2013-12-03 19:17:57","2013-12-03 19:17:57","MPWide: a light-weight library for efficient message passing over wide
  area networks","  We present MPWide, a light weight communication library which allows
efficient message passing over a distributed network. MPWide has been designed
to connect application running on distributed (super)computing resources, and
to maximize the communication performance on wide area networks for those
without administrative privileges. It can be used to provide message-passing
between application, move files, and make very fast connections in
client-server environments. MPWide has already been applied to enable
distributed cosmological simulations across up to four supercomputers on two
continents, and to couple two different bloodflow simulations to form a
multiscale simulation.
","Derek Groen|Steven Rieder|Simon Portegies Zwart","","http://arxiv.org/abs/1312.0910v1","http://arxiv.org/pdf/1312.0910v1","http://dx.doi.org/10.5334/jors.ah","accepted by the Journal Of Open Research Software, 13 pages, 4
  figures, 1 table","Journal of Open Research Software 1(1):e9, 2013","10.5334/jors.ah","cs.DC","cs.DC|cs.NI"
"499","1312.1603v1","2013-12-05 16:22:16","2013-12-05 16:22:16","US and European Technology Roadmap for a Mid-infrared Space
  Interferometer","  Studies of mid-infrared space interferometer concepts in the USA and in
Europe have converged on a single architecture. We address the question of how
the US and European communities could collaborate to advance technology efforts
leading to a future space mission. We present the current state of the art in
nulling interferometry, as demonstrated at ambient temperature and pressure in
the lab, and outline required steps to demonstrate its performance under space
conditions. Design studies of a cryogenic optical test bench under vacuum have
already been carried out. We highlight pre-conditions and constraints of a
collaborative effort, foreseeable practical and administrative challenges, and
possible strategies to meet those challenges.
","Peter A. Schuller|Peter R. Lawson|Olivier P. Lay|Alain Leger|Stefan R. Martin","","http://arxiv.org/abs/1312.1603v1","http://arxiv.org/pdf/1312.1603v1","","2 pages; proceedings of workshop ""Pathways Towards Habitable Planets""
  held 14 to 18 September 2009 in Barcelona, Spain","ASP Conference Series, vol. 430 (2010), p.536","","astro-ph.IM","astro-ph.IM"
"500","1312.2246v1","2013-12-08 19:29:06","2013-12-08 19:29:06","E-Governance, International Cooperation and Security - New Millennium
  Challenges for a Small Country","  In the paper we analyze the Information and Communication Technologies (ICT)
and their impact on developing countries (DCs), making a criticism of different
views on the supposed role of ICT for the future of human society. This
criticism is seen from the point of view of a small developing post-communist
country as Albania, hoping that the conclusions would throw some light for
developing countries in general, especially those in a transition stage. We
examine some aspects of international collaboration and security, where the ICT
implemented in the public administration may have an important impact. The
development policies and practices are examined, including relations between
public and private sectors. Following arguments of many authors, we identify or
redefine some crucial factors that negatively impact the role of ICT in the
development of the country, its relations with the international community, and
ways to push forward its development.
","Neki Frasheri","","http://arxiv.org/abs/1312.2246v1","http://arxiv.org/pdf/1312.2246v1","","","","","cs.CY","cs.CY"
"501","1312.2804v1","2013-12-10 14:05:42","2013-12-10 14:05:42","A Novel Software Tool for Analysing NT File System Permissions","  Administrating and monitoring New Technology File System (NTFS) permissions
can be a cumbersome and convoluted task. In today's data rich world there has
never been a more important time to ensure that data is secured against
unwanted access. This paper identifies the essential and fundamental
requirements of access control, highlighting the main causes of their
misconfiguration within the NTFS. In response, a number of features are
identified and an efficient, informative and intuitive software-based solution
is proposed for examining file system permissions. In the first year that the
software has been made freely available it has been downloaded and installed by
over four thousand users
","Simon Parkinson|Andrew Crampton","","http://arxiv.org/abs/1312.2804v1","http://arxiv.org/pdf/1312.2804v1","http://dx.doi.org/10.14569/IJACSA.2013.040635","","International Journal of Advanced Computer Science and
  Applications (IJACSA), Vol. 4, No. 6, 2013","10.14569/IJACSA.2013.040635","cs.SE","cs.SE"
"502","1312.5657v3","2013-12-19 17:37:21","2014-12-11 05:47:38","A Mathematical Programming Approach to the Fractionation Problem in
  Chemoradiotherapy","  In concurrent chemoradiotherapy, chemotherapeutic agents are administered
during the course of radiotherapy to enhance the primary tumor control.
However, that often comes at the expense of increased risk of normal-tissue
complications. The additional biological damage is mainly attributed to two
mechanisms of action, which are the independent cytotoxic activity of
chemotherapeutic agents and their interactive cooperation with radiation. The
goal of this study is to develop a mathematical framework to obtain drug and
radiation administration schedules that maximize the therapeutic gain for
concurrent chemoradiotherapy. In particular, we analyze the impact of
incorporating these two mechanisms into the radiation fractionation problem.
Considering each mechanism individually, we first derive closed-form
expressions for the optimal radiation fractionation regimen and the
corresponding drug administration schedule. We next study the case in which
both mechanisms are simultaneously present and develop a dynamic programming
framework to determine optimal treatment regimens. Results show that those
chemotherapeutic agents that interact with radiation may change optimal
radiation fractionation regimens. Moreover, administration of chemotherapeutic
agents possessing both mechanisms may give rise to optimal non-stationary
radiation fractionation schemes.
","Ehsan Salari|Jan Unkelbach|Thomas Bortfeld","","http://arxiv.org/abs/1312.5657v3","http://arxiv.org/pdf/1312.5657v3","","","","","physics.med-ph","physics.med-ph"
"503","1312.6036v1","2013-12-20 16:47:37","2013-12-20 16:47:37","Crowdsourced bi-directional disaster reporting and alerting on
  smartphones in Lao PDR","  Natural disasters are a large threat for people especially in developing
countries such as Laos. ICT-based disaster management systems aim at supporting
disaster warning and response efforts. However, the ability to directly
communicate in both directions between local and administrative level is often
not supported, and a tight integration into administrative workflows is
missing. In this paper, we present the smartphone-based disaster and reporting
system Mobile4D. It allows for bi-directional communication while being fully
involved in administrative processes. We present the system setup and discuss
integration into administrative structures in Lao PDR.
","Lutz Frommberger|Falko Schmid","","http://arxiv.org/abs/1312.6036v1","http://arxiv.org/pdf/1312.6036v1","","","","","cs.CY","cs.CY"
"504","1312.6443v2","2013-12-22 23:57:22","2014-03-08 22:39:56","Global inequality in energy consumption from 1980 to 2010","  We study the global probability distribution of energy consumption per capita
around the world using data from the U.S. Energy Information Administration
(EIA) for 1980-2010. We find that the Lorenz curves have moved up during this
time period, and the Gini coefficient G has decreased from 0.66 in 1980 to 0.55
in 2010, indicating a decrease in inequality. The global probability
distribution of energy consumption per capita in 2010 is close to the
exponential distribution with G=0.5. We attribute this result to the
globalization of the world economy, which mixes the world and brings it closer
to the state of maximal entropy. We argue that global energy production is a
limited resource that is partitioned among the world population. The most
probable partition is the one that maximizes entropy, thus resulting in the
exponential distribution function. A consequence of the latter is the law of
1/3: the top 1/3 of the world population consumes 2/3 of produced energy. We
also find similar results for the global probability distribution of CO2
emissions per capita.
","Scott Lawrence|Qin Liu|Victor M. Yakovenko","","http://arxiv.org/abs/1312.6443v2","http://arxiv.org/pdf/1312.6443v2","http://dx.doi.org/10.3390/e15125565","15 pages, 14 figures, 2 movies; v.2 correction to the inset in the
  movie Lorenz-Energy.mov, no changes in the paper","Entropy 15, 5565-5579 (2013)","10.3390/e15125565","physics.data-an","physics.data-an|q-fin.ST|stat.AP"
"505","1312.6481v1","2013-12-23 08:20:07","2013-12-23 08:20:07","Wildfire Prediction to Inform Fire Management: Statistical Science
  Challenges","  Wildfire is an important system process of the earth that occurs across a
wide range of spatial and temporal scales. A variety of methods have been used
to predict wildfire phenomena during the past century to better our
understanding of fire processes and to inform fire and land management
decision-making. Statistical methods have an important role in wildfire
prediction due to the inherent stochastic nature of fire phenomena at all
scales. Predictive models have exploited several sources of data describing
fire phenomena. Experimental data are scarce; observational data are dominated
by statistics compiled by government fire management agencies, primarily for
administrative purposes and increasingly from remote sensing observations.
Fires are rare events at many scales. The data describing fire phenomena can be
zero-heavy and nonstationary over both space and time. Users of fire modeling
methodologies are mainly fire management agencies often working under great
time constraints, thus, complex models have to be efficiently estimated. We
focus on providing an understanding of some of the information needed for fire
management decision-making and of the challenges involved in predicting fire
occurrence, growth and frequency at regional, national and global scales.
","S. W. Taylor|Douglas G. Woolford|C. B. Dean|David L. Martell","","http://arxiv.org/abs/1312.6481v1","http://arxiv.org/pdf/1312.6481v1","http://dx.doi.org/10.1214/13-STS451","Published in at http://dx.doi.org/10.1214/13-STS451 the Statistical
  Science (http://www.imstat.org/sts/) by the Institute of Mathematical
  Statistics (http://www.imstat.org)","Statistical Science 2013, Vol. 28, No. 4, 586-615","10.1214/13-STS451","stat.AP","stat.AP|physics.ao-ph"
"506","1312.7118v1","2013-12-26 15:13:27","2013-12-26 15:13:27","The Development of Educational Quality Administration: a Case of
  Technical College in Southern Thailand","  The purpose of this research were: to survey the needs of using the
information system for educational quality administration; to develop
Information System for Educational quality Administration (ISEs) in accordance
with quality assessment standard; to study the qualification of ISEs; and to
study satisfaction level of ISEs user. Subsequently, the tools of study have
been employed that there were the collection of 47 questionnaires and 5
interviews to specialist by responsible officers for Information center of
Technical colleges in Southern Thailand. The analysis of quantitative data has
employed descriptive statistics using mean and standard deviation as the tool
of measurement.
","Bangsuk Jantawan|Cheng-Fa Tsai","","http://arxiv.org/abs/1312.7118v1","http://arxiv.org/pdf/1312.7118v1","","","","","cs.CY","cs.CY"
"507","1312.7556v1","2013-12-29 16:41:56","2013-12-29 16:41:56","Stochastic Model for Tumor Control Probability: Effects of Cell Cycle
  and (A)symmetric Proliferation","  Estimating the required dose in radiotherapy is of crucial importance since
the administrated dose should be sufficient to eradicate the tumor and at the
same time should inflict minimal damage on normal cells. The probability that a
given dose and schedule of ionizing radiation eradicates all the tumor cells in
a given tissue is called the tumor control probability (TCP), and is often used
to compare various treatment strategies used in radiation therapy. In this
paper, we aim to investigate the effects of including cell-cycle phase on the
TCP by analyzing a stochastic model of a tumor comprised of actively dividing
cells and quiescent cells with different radiation sensitivities. We derive an
exact phase-diagram for the steady-state TCP of the model and show that at
high, clinically-relevant doses of radiation, the distinction between active
and quiescent tumor cells (i.e. accounting for cell-cycle effects) becomes of
negligible importance in terms of its effect on the TCP curve. However, for
very low doses of radiation, these proportions become significant determinants
of the TCP. Moreover, we use a novel numerical approach based on the method of
characteristics for partial differential equations, validated by the Gillespie
algorithm, to compute the TCP as a function of time. We observe that our
results differ from the results in the literature using similar existing
models, even though similar parameters values are used, and the reasons for
this are discussed.
","Andrew Dhawan|Kamran Kaveh|Mohammad Kohandel|Siv Sivaloganathan","","http://arxiv.org/abs/1312.7556v1","http://arxiv.org/pdf/1312.7556v1","","12 pages, 5 figures","","","q-bio.QM","q-bio.QM"
"508","1401.0608v1","2014-01-03 08:48:25","2014-01-03 08:48:25","A Framework for Creating a Distributed Rendering Environment on the
  Compute Clusters","  This paper discusses the deployment of existing render farm manager in a
typical compute cluster environment such as a university. Usually, both a
render farm and a compute cluster use different queue managers and assume total
control over the physical resources. But, taking out the physical resources
from an existing compute cluster in a university-like environment whose primary
use of the cluster is to run numerical simulations may not be possible. It can
potentially reduce the overall resource utilization in a situation where
compute tasks are more than rendering tasks. Moreover, it can increase the
system administration cost. In this paper, a framework has been proposed that
creates a dynamic distributed rendering environment on top of the compute
clusters using existing render farm managers without requiring the physical
separation of the resources.
","Ali Sheharyar|Othmane Bouhali","","http://arxiv.org/abs/1401.0608v1","http://arxiv.org/pdf/1401.0608v1","","","","","cs.DC","cs.DC|cs.GR"
"509","1401.0759v3","2014-01-04 00:36:07","2014-07-31 13:00:10","Adjusting models of ordered multinomial outcomes for nonignorable
  nonresponse in the occupational employment statistics survey","  An establishment's average wage, computed from administrative wage data, has
been found to be related to occupational wages. These occupational wages are a
primary outcome variable for the Bureau of Labor Statistics Occupational
Employment Statistics survey. Motivated by the fact that nonresponse in this
survey is associated with average wage even after accounting for other
establishment characteristics, we propose a method that uses the administrative
data for imputing missing occupational wage values due to nonresponse. This
imputation is complicated by the structure of the data. Since occupational wage
data is collected in the form of counts of employees in predefined wage ranges
for each occupation, weighting approaches to deal with nonresponse do not
adequately adjust the estimates for certain domains of estimation. To preserve
the current data structure, we propose a method to impute each missing
establishment's wage interval count data as an ordered multinomial random
variable using a separate survival model for each occupation. Each model
incorporates known auxiliary information for each establishment associated with
the distribution of the occupational wage data, including geographic and
industry characteristics. This flexible model allows the baseline hazard to
vary by occupation while allowing predictors to adjust the probabilities of an
employee's salary falling within the specified ranges. An empirical study and
simulation results suggest that the method imputes missing OES wages that are
associated with the average wage of the establishment in a way that more
closely resembles the observed association.
","Nicholas J. Horton|Daniell Toth|Polly Phipps","","http://arxiv.org/abs/1401.0759v3","http://arxiv.org/pdf/1401.0759v3","http://dx.doi.org/10.1214/14-AOAS714","Published in at http://dx.doi.org/10.1214/14-AOAS714 the Annals of
  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of
  Mathematical Statistics (http://www.imstat.org)","Annals of Applied Statistics 2014, Vol. 8, No. 2, 956-973","10.1214/14-AOAS714","stat.AP","stat.AP"
"510","1401.1663v1","2014-01-08 11:15:40","2014-01-08 11:15:40","Calibrated imputation of numerical data under linear edit restrictions","  A common problem faced by statistical institutes is that data may be missing
from collected data sets. The typical way to overcome this problem is to impute
the missing data. The problem of imputing missing data is complicated by the
fact that statistical data often have to satisfy certain edit rules and that
values of variables across units sometimes have to sum up to known totals. For
numerical data, edit rules are most often formulated as linear restrictions on
the variables. For example, for data on enterprises edit rules could be that
the profit and costs of an enterprise should sum up to its turnover and that
the turnover should be at least zero. The totals of some variables across units
may already be known from administrative data (e.g., turnover from a tax
register) or estimated from other sources. Standard imputation methods for
numerical data as described in the literature generally do not take such edit
rules and totals into account. In this article we describe algorithms for
imputing missing numerical data that take edit restrictions into account and
ensure that sums are calibrated to known totals. These algorithms are based on
a sequential regression approach that uses regression predictions to impute the
variables one by one. To assess the performance of the imputation methods, a
simulation study is carried out as well as an evaluation study based on a real
data set.
","Jeroen Pannekoek|Natalie Shlomo|Ton De Waal","","http://arxiv.org/abs/1401.1663v1","http://arxiv.org/pdf/1401.1663v1","http://dx.doi.org/10.1214/13-AOAS664","Published in at http://dx.doi.org/10.1214/13-AOAS664 the Annals of
  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of
  Mathematical Statistics (http://www.imstat.org)","Annals of Applied Statistics 2013, Vol. 7, No. 4, 1983-2006","10.1214/13-AOAS664","stat.AP","stat.AP"
"511","1401.4876v1","2014-01-20 12:31:21","2014-01-20 12:31:21","The critical factors affecting E-Government adoption: A Conceptual
  Framework in Vietnam","  Electronic government (e-government) has established as an effective
mechanism for increasing government productivity and efficiency and a key
enabler of citizen- centric services. However, e-government implementation is
surrounded by technological, governing and social issues, which have to be
considered and treated carefully in order to facilitate this change. This
research attempts to explore and investigate the key challenges that influence
e-government implementation and the factors influencing citizen adoption in
Vietnam. It develops a conceptual framework on the basis of existing
experiences drawn from administrative reforms. Survey data from public employee
will be used to test the proposed hypothesis and the model. Therefore, this
research has identified factors that determine if the citizen will adopt
E-government services and thereby aiding governments in accessing what is
required to increase adoption. We will also highlight several research,
practitioner and policy implications.
","Ngo Tan Vu Khanh","","http://arxiv.org/abs/1401.4876v1","http://arxiv.org/pdf/1401.4876v1","","","","","cs.CY","cs.CY"
"512","1401.5613v1","2014-01-22 10:33:27","2014-01-22 10:33:27","A precision of the sequential change point detection","  A random sequence having two segments being the homogeneous Markov processes
is registered. Each segment has his own transition probability law and the
length of the segment is unknown and random. The transition probabilities of
each process are known and a priori distribution of the disorder moment is
given. The decision maker aim is to detect the moment of the transition
probabilities change. The detection of the disorder rarely is precise. The
decision maker accepts some deviation in estimation of the disorder moment. In
the considered model the aim is to indicate the change point with fixed,
bounded error with maximal probability. The case with various precision for
over and under estimation of this point is analysed. The case when the disorder
does not appears with positive probability is also included. The results
insignificantly extends range of application, explain the structure of optimal
detector in various circumstances and shows new details of the solution
construction. The motivation for this investigation is the modelling of the
attacks in the node of networks. The objectives is to detect one of the attack
immediately or in very short time before or after it appearance with highest
probability. The problem is reformulated to optimal stopping of the observed
sequences. The detailed analysis of the problem is presented to show the form
of optimal decision function.
","A. Ochman-Gozdek|W. Sarnowski|K. J. Szajowski","","http://arxiv.org/abs/1401.5613v1","http://arxiv.org/pdf/1401.5613v1","","8 pages. The research has been supported by grant S30103/I-18. This
  paper was presented in part at 59th ISI World Statistics Congress 25-30
  August 2013, Hong Kong Special Administrative Region, China in the session
  CPS018","","","math.ST","math.ST|math.PR|stat.AP|stat.TH|60G40, Secondary: 60K99, 90D60"
"513","1401.5641v2","2014-01-22 12:29:27","2014-01-23 10:20:28","Tracing sunspot groups to determine angular momentum transfer on the Sun","  The goal of this paper is to investigate Reynolds stresses and to check if it
is plausible that they are responsible for angular momentum transfer toward the
solar equator. We also analysed meridional velocity, rotation velocity
residuals and correlation between the velocities. We used sunspot groups
position measurements from GPR (Greenwich Photographic Result) and
SOON/USAF/NOAA (Solar Observing Optical Network/United States Air
Force/National Oceanic and Atmospheric Administration) databases covering the
period from 1878 until 2011. In order to calculate velocities we used daily
motion of sunspot groups. The sample was also limited to $\pm$58\degr in
Central Meridian Distance (CMD) in order to avoid solar limb effects. We mainly
investigated velocity patterns depending on solar cycle phase and latitude. We
found that meridional motion of sunspot groups is toward the centre of activity
from all available latitudes and in all phases of the solar cycle. The range of
meridional velocities is $\pm10$ m s$^{-1}$. Horizontal Reynolds stress is
negative at all available latitudes and indicates that there is a minimum value
($q\approx$ - 3000 m$^2$ s$^{-2}$) located at $b\approx\pm$30$^{0}$. In our
convention this means that angular momentum is transported toward the solar
equator in agreement with the observed rotational profile of the Sun.
","D. Sudar|I. Skoki<U+0107>|D. Ru<U+017E>djak|R. Braj<U+0161>a|H. Wohl","","http://arxiv.org/abs/1401.5641v2","http://arxiv.org/pdf/1401.5641v2","http://dx.doi.org/10.1093/mnras/stu099","","","10.1093/mnras/stu099","astro-ph.SR","astro-ph.SR"
"514","1401.6344v1","2014-01-24 14:18:06","2014-01-24 14:18:06","Mechanic: the MPI/HDF code framework for dynamical astronomy","  We introduce the Mechanic, a new open-source code framework. It is designed
to reduce the development effort of scientific applications by providing
unified API (Application Programming Interface) for configuration, data storage
and task management. The communication layer is based on the well-established
Message Passing Interface (MPI) standard, which is widely used on variety of
parallel computers and CPU-clusters. The data storage is performed within the
Hierarchical Data Format (HDF5). The design of the code follows em core-module
approach which allows to reduce the user's codebase and makes it portable for
single- and multi-CPU environments. The framework may be used in a local user's
environment, without administrative access to the cluster, under the PBS or
Slurm job schedulers. It may become a helper tool for a wide range of
astronomical applications, particularly focused on processing large data sets,
such as dynamical studies of long-term orbital evolution of planetary systems
with Monte Carlo methods, dynamical maps or evolutionary algorithms. It has
been already applied in numerical experiments conducted for Kepler-11
(Migaszewski et al., 2012), and nuOctantis planetary systems (Go\'zdziewski et
al., 2013). In this paper we describe the basics of the framework, including
code listings for the implementation of a sample user's module. The code is
illustrated on a model Hamiltonian introduced by (Froeschle et al., 2000)
presenting the Arnold diffusion. The Arnold Web is shown with the help of the
MEGNO (Mean Exponential Growth of Nearby Orbits) fast indicator (Go\'zdziewski
et al., 2008a) applied onto symplectic SABA integrators family (Laskar and
Robutel, 2001).
","Mariusz S©©onina|Krzysztof Go<U+017A>dziewski|Cezary Migaszewski","","http://arxiv.org/abs/1401.6344v1","http://arxiv.org/pdf/1401.6344v1","http://dx.doi.org/10.1016/j.newast.2014.05.006","Submitted to New Astronomy (after first review; minor revision)","","10.1016/j.newast.2014.05.006","astro-ph.IM","astro-ph.IM"
"515","1401.6348v1","2014-01-24 14:33:26","2014-01-24 14:33:26","Fuzzy Logic Based Multi User Adaptive Test System","  The present proliferation of e-learning has been actively underway for the
last 10 years. Current research in Adaptive Testing System focuses on the
development of psychometric models with items selection strategies applicable
to adaptive testing processes. The key aspect of proposed Adaptive Testing
System is to develop an increasingly sophisticated latent trait model which can
assist users in developing and enhancing their skills. Computerized Adaptive
Test (CAT) System requires a lot of investment in time and effort to develop
analyze and administrate an adaptive test. In this paper a fuzzy logic based
Multi User Adaptive Test System (MUATS) is developed. Which is a Short
Messaging Service (SMS) based System, currently integrated with GSM network
based on the new psychometric model in education assessment. MUATS is not only
a platform independent Adaptive Test System but also it eases the evaluation
effort for adaptive test process. It further uses fuzzy logic to pick the most
appropriate question from the pool of database for a specific user to be asked
which makes the overall system an intelligent one.
","Atif Ali Khan|Oumair Naseer","","http://arxiv.org/abs/1401.6348v1","http://arxiv.org/pdf/1401.6348v1","http://dx.doi.org/10.7321/jscse.v2.n8.1","Published online: Aug 20, 2012","International Journal of Soft Computing And Software Engineering
  (JSCSE), e-ISSN: 2251-7545, Vol. 2, no. 8, 2012","10.7321/jscse.v2.n8.1","cs.CY","cs.CY"
"516","1401.7584v1","2014-01-28 20:38:05","2014-01-28 20:38:05","XLSearch: A Search Engine for Spreadsheets","  Spreadsheets are end-user programs and domain models that are heavily
employed in administration, financial forecasting, education, and science
because of their intuitive, flexible, and direct approach to computation. As a
result, institutions are swamped by millions of spreadsheets that are becoming
increasingly difficult to manage, access, and control.
  This note presents the XLSearch system, a novel search engine for
spreadsheets. It indexes spreadsheet formulae and efficiently answers formula
queries via unification (a complex query language that allows metavariables in
both the query as well as the index). But a web-based search engine is only one
application of the underlying technology: Spreadsheet formula export to web
standards like MathML combined with formula indexing can be used to find
similar spreadsheets or common formula errors.
","Michael Kohlhase|Corneliu Prodescu|Christian Liguda","","http://arxiv.org/abs/1401.7584v1","http://arxiv.org/pdf/1401.7584v1","","12 Pages. 10 B&W & Colour Figures. Proc. European Spreadsheet Risks
  Int. Grp. (EuSpRIG) 2013, ISBN: 978-1-9054045-1-3","","","cs.DB","cs.DB"
"517","1401.7720v2","2014-01-30 02:52:34","2014-06-16 22:43:51","Large cities are less green","  We study how urban quality evolves as a result of carbon dioxide emissions as
urban agglomerations grow. We employ a bottom-up approach combining two
unprecedented microscopic data on population and carbon dioxide emissions in
the continental US. We first aggregate settlements that are close to each other
into cities using the City Clustering Algorithm (CCA) defining cities beyond
the administrative boundaries. Then, we use data on $\rm{CO}_2$ emissions at a
fine geographic scale to determine the total emissions of each city. We find a
superlinear scaling behavior, expressed by a power-law, between $\rm{CO}_2$
emissions and city population with average allometric exponent $\beta = 1.46$
across all cities in the US. This result suggests that the high productivity of
large cities is done at the expense of a proportionally larger amount of
emissions compared to small cities. Furthermore, our results are substantially
different from those obtained by the standard administrative definition of
cities, i.e. Metropolitan Statistical Area (MSA). Specifically, MSAs display
isometric scaling emissions and we argue that this discrepancy is due to the
overestimation of MSA areas. The results suggest that allometric studies based
on administrative boundaries to define cities may suffer from endogeneity bias.
","E. A. Oliveira|J. S. Andrade Jr.|H. A. Makse","","http://arxiv.org/abs/1401.7720v2","http://arxiv.org/pdf/1401.7720v2","http://dx.doi.org/10.1038/srep04235","","Oliveira, E. A., Andrade Jr., J. S. and Makse, H. A. Large Cities
  are less green. Sci. Rep. 4, 4235 (2014)","10.1038/srep04235","physics.soc-ph","physics.soc-ph"
"518","1403.1180v2","2014-02-04 17:52:22","2014-09-25 09:13:02","A distributed Integrity Catalog for digital repositories","  Digital repositories, either digital preservation systems or archival
systems, periodically check the integrity of stored objects to assure users of
their correctness. To do so, prior solutions calculate integrity metadata and
require the repository to store it alongside the actual data objects. This
integrity metadata is essential for regularly verifying the correctness of the
stored data objects. To safeguard and detect damage to this metadata, prior
solutions rely on widely visible media, that is unaffiliated third parties, to
store and provide back digests of the metadata to verify it is intact. However,
they do not address recovery of the integrity metadata in case of damage or
attack by an adversary. In essence, they do not preserve this metadata. We
introduce IntegrityCatalog, a system that collects all integrity related
metadata in a single component, and treats them as first class objects,
managing both their integrity and their preservation. We introduce a
treap-based persistent authenticated dictionary managing arbitrary length
key/value pairs, which we use to store all integrity metadata, accessible
simply by object name. Additionally, IntegrityCatalog is a distributed system
that includes a network protocol that manages both corruption detection and
preservation of this metadata, using administrator-selected network peers with
two possible roles. Verifiers store and offer attestations on digests and have
minimal storage requirements, while preservers efficiently synchronize a
complete copy of the catalog to assist in recovery in case of a detected
catalog compromise on the local system. We describe our prototype
implementation of IntegrityCatalog, measure its performance empirically, and
demonstrate its effectiveness in real-world situations, with worst measured
throughput of approximately 1K insertions per second, and 2K verified search
operations per second.
","Nikos Chondros|Mema Roussopoulos","","http://arxiv.org/abs/1403.1180v2","http://arxiv.org/pdf/1403.1180v2","","","","","cs.DB","cs.DB|cs.DC|cs.DL"
"519","1402.1880v1","2014-02-08 19:29:33","2014-02-08 19:29:33","Design of Locally E-management System for Technical Education
  Foundation- Erbil","  Until now, there is no e-management and automation necessary for the
operations or procedures of the departments in the Technical Education
Foundation Erbil, and the foundation like any other organization in Kurdistan
region is not connected to the network, because there is not infrastructure for
that purpose. To solve this problem, comes the proposal DLMS4TEF, which
requirements are divided into hardware and software, as hardware will need
Fast-Ethernet (LAN) technology to connect the departments of the Foundation via
Client-Server network later, when an infrastructure is established for
e-governments or e-management, it may be extended to the campus network. The
software is represented by installing windows server to implement the proposal
design of DLMS4TEF, PHP script is used as web programming that supports the
server, where as the HTML and JavaScript are used to support the client side.
The dynamic DLMS4TEF will be based on relational database, which is created by
using MySQL, to support processing hundreds of queries per second, and the
Kurdish Unicode to support Kurdish fonts of GUI's, Moreover, for security
DLMS4TEF allows each department in the Foundation to enter its own section and
prevent accessing other sections by using HTAccessible program which allows the
user to access by using his IP address and his computer only. The important
conclusions and advantages of applying DLMS4TEF are making backup to DLMS4TEF's
databases using the option (zipped) which allows them to reach the size of (3%)
of the original database size, sufficient security techniques, through
achieving levels of security, hidden access to the administrator section, and
finally DLMS4TEF, when compared with the traditional methods and project of
Oman, shows the same efficiency of some, if not better, features of Oman.
Keywords- E-management, Client-Server network, Fast-Ethernet, PHP, MySQL
","Ayad Ghany Ismaeel|Dina Y. Mikhail","","http://arxiv.org/abs/1402.1880v1","http://arxiv.org/pdf/1402.1880v1","","11 pages, 11 figures, 1 tables","PolyTechnic, Vol. 1, No. 1, Oct. 2011, National Journal,
  Erbil-IRAQ","","cs.CY","cs.CY"
"520","1402.1952v1","2014-02-09 13:48:26","2014-02-09 13:48:26","Women in Italian astronomy","  This document gives some quantitative facts about the role of women in
Italian astronomy. More than 26% of Italian IAU members are women: this is the
largest fraction among the world leading countries in astronomy. Most of this
high fraction is due to their presence in INAF, where women make up 32% of the
research staff (289 out of 908) and 40% of the technical/administrative staff
(173 out of 433); the percentage is slightly lower among permanent research
staff (180 out of 599, about 30%). The presence of women is lower in the
Universities (27 out of 161, about 17%, among staff). In spite of these
(mildly) positive facts, we notice that similarly to other countries (e.g. USA
and Germany) career prospects for Italian astronomers are clearly worse for
women than for men. Within INAF, the fraction of women is about 35-40% among
non-permanent position, 36% for Investigators, 17% for Associato/Primo
Ricercatore, and only 13% among Ordinario/Dirigente di Ricerca. The situation
is even worse at University (only 6% of Professore Ordinario are women). We
found that similar trends are also present if researchers are ordered according
to citation rather than position: for instance, women make up only 15% among
the 100 most cited astronomers working in Italy, a percentage which is however
twice that over all Europe. A similar fraction is found among first authors of
most influential papers, which cannot be explained as a residual of a lower
female presence in the past. We conclude that implicit sex discrimination
factors probably dominate over explicit ones and are still strongly at work.
Finally, we discuss the possible connection between the typical career pattern
and these factors.
","Francesca Matteucci|Raffaele Gratton","","http://arxiv.org/abs/1402.1952v1","http://arxiv.org/pdf/1402.1952v1","","10 pages; document prepared for INAF-Astrophysical National
  Institute, Italy","","","astro-ph.IM","astro-ph.IM|physics.soc-ph"
"521","1402.1974v1","2014-02-09 18:19:59","2014-02-09 18:19:59","A Novel Approach to Detect Spam Worms Propagation with Monitoring the
  Footprinting","  One of the key security threats on the Internet are the compromised machines
that can be used to launch various security attacks such as spamming and
spreading malware, accessing useful information and DDoS. Attackers for
spamming activity are volunteer by large number of compromised machines. Our
main focus is on detection of the compromised machines in a network that may be
or are involved in the spamming activities; these machines are commonly known
as spam zombies. Activities such as port scan, DB scan and so on are treated as
malicious activity within the network. So to overcome that we develop one of
the most effective spam zombie detection system within the network based on the
behavior of other systems as if performing the above activities are treated as
zombies machines. If any system within the network try's to gather some
information about any other system then this is treated as a malicious activity
and should be not allowed to do so. SYN packets are used in order to initiate
communication within the network so as to establish connection. If any system
try's to flood the network with these packets we can make an assumption that
the system is trying to gather the information about other system. This is what
called footprinting. So we will try to detect any system involved in
footprinting and report to the administrator.
","Rajesh R Chauhan|G S Praveen Kumar","","http://arxiv.org/abs/1402.1974v1","http://arxiv.org/pdf/1402.1974v1","","","International Journal of Computer Trends and Technology (IJCTT)
  6(23):3-6, December 2013","","cs.NI","cs.NI|cs.CR"
"522","1402.2925v2","2014-02-12 18:38:16","2016-06-09 15:36:38","Modeling Switched Behavior with Hybrid Bond Graph: Application to a Tank
  system","  Different approaches have been used in the development of system models. In
addition, modeling and simulation approaches are essential for design,
analysis, control, and diagnosis of complex systems. This work presents a
Simulink model for systems with mixed continuous and discrete behaviors. The
model simulated was developed using the bond graph methodology and we model
hybrid systems using hybrid bond graphs (HBGs), that incorporates local
switching functions that enable the reconfiguration of energy flow paths. This
approach has been implemented as a software tool called the MOdeling and
Transformation of HBGs for Simulation (MOTHS) tool suite which incorporates a
model translator that create Simulink models. Simulation model of a three-tank
system that includes a switching component was developed using the bond graph
methodology, and MoTHS software were used to build a Simulink model of the
dynamic behavior.
","Slim Triki|Taher Mekki|Anas Kamoun","","http://arxiv.org/abs/1402.2925v2","http://arxiv.org/pdf/1402.2925v2","","arXiv admin note: submission has been withdrawn by arXiv
  administrators due to inappropriate text reuse from external sources","IJCSI International Journal of Computer Science Issues, Vol. 11,
  Issue 1, No 2,pp. 60-66, January 2014","","cs.SY","cs.SY"
"523","1402.5991v2","2014-02-24 21:37:25","2014-03-12 23:42:51","A predictive analytics approach to reducing avoidable hospital
  readmission","  Hospital readmission has become a critical metric of quality and cost of
healthcare. Medicare anticipates that nearly $17 billion is paid out on the 20%
of patients who are readmitted within 30 days of discharge. Although several
interventions such as transition care management and discharge reengineering
have been practiced in recent years, the effectiveness and sustainability
depends on how well they can identify and target patients at high risk of
rehospitalization. Based on the literature, most current risk prediction models
fail to reach an acceptable accuracy level; none of them considers patient's
history of readmission and impacts of patient attribute changes over time; and
they often do not discriminate between planned and unnecessary readmissions.
Tackling such drawbacks, we develop a new readmission metric based on
administrative data that can identify potentially avoidable readmissions from
all other types of readmission. We further propose a tree based classification
method to estimate the predicted probability of readmission that can directly
incorporate patient's history of readmission and risk factors changes over
time. The proposed methods are validated with 2011-12 Veterans Health
Administration data from inpatients hospitalized for heart failure, acute
myocardial infarction, pneumonia, or chronic obstructive pulmonary disease in
the State of Michigan. Results shows improved discrimination power compared to
the literature (c-statistics>80%) and good calibration.
","Issac Shams|Saeede Ajorlou|Kai Yang","","http://arxiv.org/abs/1402.5991v2","http://arxiv.org/pdf/1402.5991v2","","30 pages, 4 figures, 7 tables","","","stat.AP","stat.AP|cs.AI|60J28, 62P10, 68T05, 62G86, 90B22|I.2.1; I.2.6; H.2.8"
"524","1402.6497v1","2014-02-26 11:28:12","2014-02-26 11:28:12","Authentication Mechanism for Resistance to Password Stealing and Reuse
  Attack","  Considering computer systems, security is the major concern with usability.
Security policies need to be developed to protect information from unauthorized
access. Passwords and secrete codes used between users and information systems
for secure user authentication with the system. Playing a vital role in
security, easily guessed passwords are links to vulnerability. They allow
invader to put system resources significantly closer to access them, other
accounts on nearby machines and possibly even administrative privileges with
different threats and vulnerabilities (e.g., phishing, key logging and
malwares). The purpose of this system is to introduce the concept and
methodology which helps organization and users to implement stronger password
policies. This paper studies a password stealing and reuse issues of password
based authentication systems. Techniques and concepts of authentication are
discussed which gives rise to a novel approach of two-factor authentication.
Avoiding password reuse is a crucial issue in information systems which can at
some extent contribute to password stealing issue also. In the proposed system,
each participating website possesses a user's unique phone number,
telecommunication services in registration and recovery phases and a long-term
password used to generate one-time password for each login session on all
websites.
","Sharayu A. Aghav|RajneeshKaur Bedi","","http://arxiv.org/abs/1402.6497v1","http://arxiv.org/pdf/1402.6497v1","","6 pages, 3 figures, Third Post Graduate Symposium for Computer
  Engineering cPGCON 2014, 28-29 March, 2014, Nashik, India","","","cs.CR","cs.CR"
"525","1402.6666v1","2014-02-26 20:14:20","2014-02-26 20:14:20","An analytics approach to designing patient centered medical homes","  Recently the patient-centered medical home (PCMH) model has become a popular
team-based approach focused on delivering more streamlined care to patients. In
current practices of medical homes, a clinical-based prediction frame is
recommended because it can help match the portfolio capacity of PCMH teams with
the actual load generated by a set of patients. Without such balances in
clinical supply and demand, issues such as excessive under and over utilization
of physicians, long waiting time for receiving the appropriate treatment, and
non-continuity of care will eliminate many advantages of the medical home
strategy. In this paper, by extending the hierarchical generalized linear model
to include multivariate responses, we develop a clinical workload prediction
model for care portfolio demands in a Bayesian framework. The model allows for
heterogeneous variances and unstructured covariance matrices for nested random
effects that arise through complex hierarchical care systems. We show that
using a multivariate approach substantially enhances the precision of workload
predictions at both primary and non-primary care levels. We also demonstrate
that care demands depend not only on patient demographics but also on other
utilization factors, such as length of stay. Our analyses of a recent data from
Veteran Health Administration further indicate that risk adjustment for patient
health conditions can considerably improve the prediction power of the model.
","Saeede Ajorlou|Issac Shams|Kai Yang","","http://arxiv.org/abs/1402.6666v1","http://arxiv.org/pdf/1402.6666v1","","36 pages, 6 figures, 8 tables, submitted to health care management
  science","","","stat.AP","stat.AP|62P10, 62F15, 62J12, 90B50|I.2.1"
"526","1403.0674v1","2014-03-04 03:42:34","2014-03-04 03:42:34","A multivariate hierarchical Bayesian framework for healthcare
  predictions with application to medical home study in the Department of
  Veteran Affairs","  Recently the patient centered medical home (PCMH) model has become a popular
approach to deliver better care to patients. Current research shows that the
most important key for succession of this method is to make balance between
healthcare supply and demand. Without such balance in clinical supply and
demand, issues such as excessive under and over utilization of physicians, long
waiting time for receiving the appropriate treatment, and non continuity of
care will eliminate many advantages of the medical home strategy. To reach this
end we need to have information about both supply and demand in healthcare
system. Healthcare supply can be calculated easily based on head counts and
available hours which is offered by professionals for a specific time period
while healthcare demand is not easy to calculate, and it is affected by some
healthcare, diagnostic and demographic attributes. In this paper, by extending
the hierarchical generalized linear model to include multivariate responses, we
develop a clinical workload prediction model for care portfolio demands in a
Bayesian framework. Our analyses of a recent data from Veteran Health
Administration indicate that our prediction model works for clinical data with
high performance.
","Issac Shams|Saeede Ajorlou|Kai Yang","","http://arxiv.org/abs/1403.0674v1","http://arxiv.org/pdf/1403.0674v1","","5 pages, 3 tables, one figure, to be submitted to IEEE CASE 2014.
  arXiv admin note: text overlap with arXiv:1402.6666","","","stat.AP","stat.AP|62P10, 62F15"
"527","1403.4988v1","2014-03-19 21:39:53","2014-03-19 21:39:53","Bracing Heterogeneous Distributed Systems via Built-in Frameworks","  This paper introduces a novel architecture of distributed systems--called
framed distributed system, or FDS--that braces a given system via a built-in
virtual framework that controls the flow of messages between system components
and between them and their environment, while being oblivious of the code of
the communicating components. This control is carried out in a decentralized,
and thus scalable, manner. The FDS architecture is expected to have a
significant impact on the dependability and security of distributed systems,
and on the whole life cycle of such systems. Although this architecture has
been designed specifically for SOA-like heterogeneous and open systems--whose
components may be written in different languages, may run on different
platforms, and may be designed, constructed, and even maintained under
different administrative domains--it should be useful for distributed systems
in general.
","Naftaly Minsky","","http://arxiv.org/abs/1403.4988v1","http://arxiv.org/pdf/1403.4988v1","","","","","cs.DC","cs.DC|cs.SE"
"528","1403.5118v1","2014-03-20 12:48:24","2014-03-20 12:48:24","Geotagged tweets to inform a spatial interaction model: a case study of
  museums","  This paper explores the potential of volunteered geographical information
from social media for informing geographical models of behavior, based on a
case study of museums in Yorkshire, UK. A spatial interaction model of visitors
to 15 museums from 179 administrative zones is constructed to test this
potential. The main input dataset comprises geo-tagged messages harvested using
the Twitter Streaming Application Programming Interface (API), filtered,
analyzed and aggregated to allow direct comparison with the model's output.
Comparison between model output and tweet information allowed the calibration
of model parameters to optimize the fit between flows to museums inferred from
tweets and flow matrices generated by the spatial interaction model. We
conclude that volunteered geographic information from social media sites have
great potential for informing geographical models of behavior, especially if
the volume of geo-tagged social media messages continues to increase. However,
we caution that volunteered geographical information from social media has some
major limitations so should be used only as a supplement to more consistent
data sources or when official datasets are unavailable.
","Robin Lovelace|Nick Malleson|Kirk Harland|Mark Birkin","","http://arxiv.org/abs/1403.5118v1","http://arxiv.org/pdf/1403.5118v1","","A concise version of this article was submitted to GISRUK2014
  conference","","","stat.ME","stat.ME|cs.CY|cs.SI"
"529","1403.5557v1","2014-03-21 18:50:06","2014-03-21 18:50:06","Claude Bouchu, intendant de Bourgogne au 17eme siecle, a-t-il
  invente le mot ""statistique""","  The objective of this paper is to examine the assertion that the word
""statistics"" would have been used for the first time in the 17th century, in a
report written by Claude Bouchu, administrator of Bourgogne. A historical and
bibliographical analysis is carried out to judge the credibility of this
thesis. The physical inspection of the report then makes it possible to bring a
final answer.
","Dominique Pepin","CRIEF","http://arxiv.org/abs/1403.5557v1","http://arxiv.org/pdf/1403.5557v1","","in French","","","stat.OT","stat.OT"
"530","1404.1669v1","2014-04-07 07:15:40","2014-04-07 07:15:40","The Prospects for e-Examinations in Nigeria and Australia","  This paper compares the e-Examination system in Nigeria with that of
Australia. We consider the experiences of working with commercial firms such as
Electronic Testing Company (eTC) and using open-source software. It is
important to foster good relationships with accreditation authorities (such as
University Authorities, West African Examination Council (WAEC), Joint
Admissions and Matriculation Board (JAMB) etc. and the Tasmanian Qualifications
Authority) to assist in the transition from paperbased assessment to post-paper
assessment. The paper also considers the relative convenience for students,
administrators and lecturer/assessors; and to gauges the reliability and
security of the two systems in use. It examines the challenges in conducting
e-Examinations in both countries by juxtaposing the systems in the two
countries and suggests ways of developing more acceptable e-Examination
systems.
","Olawale S. Adebayo|Shafii M. Abdulhamid|Andrew Fluck","","http://arxiv.org/abs/1404.1669v1","http://arxiv.org/pdf/1404.1669v1","","13 pages, 3 figures, 1 table","International Journal of Advances in Management, Technology and
  Engineering Sciences, 2013","","cs.CY","cs.CY|cs.HC"
"531","1404.2153v1","2014-04-08 14:35:06","2014-04-08 14:35:06","Apple IOS Devices for Network Administrators","  As tablet devices continue to gain market share at the expense of the
traditional PC, they become a more integral part of the corporate landscape.
Tablets are no longer being utilized only by sales executives for presentation
purposes, or as addition to the traditional laptop. Users are attempting to
perform significant amounts of their daily work on tablet devices, some even
abandoning the ubiquitous laptop or desktop entirely. Operating exclusively
from a tablet device, specifically Apple IOS tablet devices creates unique
challenges in a corporate environment traditionally dominated by Microsoft
Windows operating systems. Interactions with file shares, presentation media,
VPN, and remote access present barriers that users and helpdesk support are
unfamiliar with in a relation to an iPad or iPhone. Many solutions are being
offered to these challenges some of which are analyzed by this manuscript.
","Timur Mirzoev|Gerard Gingo|Mike Stawchansky|Tracy White","","http://arxiv.org/abs/1404.2153v1","http://arxiv.org/pdf/1404.2153v1","","","World of Computer Science and Information Technology Journal
  (WCSIT)ISSN: 2221-0741 Vol. 3, No. 6, 114-119, 2013","","cs.OH","cs.OH"
"532","1404.2157v1","2014-04-08 14:41:51","2014-04-08 14:41:51","Leveraging VMware vCloud Director Virtual Applications (vApps) for
  Operational Expense (OpEx) Efficiency","  Virtualization technology has provided many benefits to organizations, but it
cannot provide automation. This causes operational expenditure (OpEx)
inefficiencies, which are solved by cloud computing (vCloud Director vApps).
Organizations have adopted virtualization technology to reduce IT costs and
meet business needs. In addition to improved CapEx efficiency, virtualization
has enabled organizations to respond to business needs faster. While
virtualization has dramatically optimized core IT infrastructures,
organizations struggle to reduce OpEx costs. Because virtualization only
addresses server consolidation, administrators are faced with the manual and
resource-intensive day-to-day tasks of managing the rest of the data center:
networking, storage, user management. This manuscript presents details on how
leverage vApps based on a virtualized platform to improve CapEx efficiency in
today s data center. The combination of virtualization and cloud computing can
transform the data center into a dynamic, scalable, and agile resource capable
of achieving significant CapEx and OpEx cost savings.
","Dr. Timur Mirzoev|Ramon Alvarez","","http://arxiv.org/abs/1404.2157v1","http://arxiv.org/pdf/1404.2157v1","","","World of Computer Science and Information Technology Journal
  (WCSIT)ISSN: 2221-0741 Vol. 3, No. 9, 156-163, 2013","","cs.DC","cs.DC"
"533","1404.5245v6","2014-04-21 17:08:42","2019-05-09 08:02:44","Size versus truthfulness in the House Allocation problem","  We study the House Allocation problem (also known as the Assignment problem),
i.e., the problem of allocating a set of objects among a set of agents, where
each agent has ordinal preferences (possibly involving ties) over a subset of
the objects. We focus on truthful mechanisms without monetary transfers for
finding large Pareto optimal matchings. It is straightforward to show that no
deterministic truthful mechanism can approximate a maximum cardinality Pareto
optimal matching with ratio better than 2. We thus consider randomised
mechanisms. We give a natural and explicit extension of the classical Random
Serial Dictatorship Mechanism (RSDM) specifically for the House Allocation
problem where preference lists can include ties. We thus obtain a universally
truthful randomised mechanism for finding a Pareto optimal matching and show
that it achieves an approximation ratio of $\frac{e}{e-1}$. The same bound
holds even when agents have priorities (weights) and our goal is to find a
maximum weight (as opposed to maximum cardinality) Pareto optimal matching. On
the other hand we give a lower bound of $\frac{18}{13}$ on the approximation
ratio of any universally truthful Pareto optimal mechanism in settings with
strict preferences. In the case that the mechanism must additionally be
non-bossy with an additional technical assumption, we show by utilising a
result of Bade that an improved lower bound of $\frac{e}{e-1}$ holds. This
lower bound is tight since RSDM for strict preference lists is non-bossy. We
moreover interpret our problem in terms of the classical secretary problem and
prove that our mechanism provides the best randomised strategy of the
administrator who interviews the applicants.
","Piotr Krysta|David Manlove|Baharak Rastegari|Jinshan Zhang","","http://arxiv.org/abs/1404.5245v6","http://arxiv.org/pdf/1404.5245v6","http://dx.doi.org/10.1007/s00453-019-00584-7","To appear in Algorithmica (preliminary version appeared in the
  Proceedings of EC 2014)","","10.1007/s00453-019-00584-7","cs.GT","cs.GT"
"534","1405.6627v2","2014-05-07 11:32:25","2019-01-16 21:54:57","Portable Camera-Based Product Label Reading For Blind People","  We propose a camera-based assistive text reading framework to help blind
persons read text labels and product packaging from hand-held objects in their
daily life. To isolate the object from untidy backgrounds or other surrounding
objects in the camera vision, we initially propose an efficient and effective
motion based method to define a region of interest (ROI) in the video by asking
the user to tremble the object. This scheme extracts moving object region by a
mixture-of-Gaussians-based background subtraction technique. In the extracted
ROI, text localization and recognition are conducted to acquire text details.
To automatically focus the text regions from the object ROI, we offer a novel
text localization algorithm by learning gradient features of stroke
orientations and distributions of edge pixels in an Adaboost model. Text
characters in the localized text regions are then binarized and recognized by
off-the-shelf optical character identification software. The renowned text
codes are converted into audio output to the blind users. Performance of the
suggested text localization algorithm is quantitatively evaluated on ICDAR-2003
and ICDAR-2011 Robust Reading Datasets. Experimental results demonstrate that
our algorithm achieves the highest level of developments at present time. The
proof-of-concept example is also evaluated on a dataset collected using ten
blind persons to evaluate the effectiveness of the scheme. We explore the user
interface issues and robustness of the algorithm in extracting and reading text
from different objects with complex backgrounds.
","Rajkumar N|Anand M. G|Barathiraja N","","http://arxiv.org/abs/1405.6627v2","http://arxiv.org/pdf/1405.6627v2","http://dx.doi.org/10.14445/22315381/IJETT-V10P303","This article has been withdrawn by arXiv administrators due to
  verbatim text overlap from external sources","IJETT,V10(11),521-524 April 2014.ISSN:2231-5381","10.14445/22315381/IJETT-V10P303","cs.HC","cs.HC|cs.CY"
"535","1405.1584v1","2014-05-07 12:27:59","2014-05-07 12:27:59","Modelling Delegation and Revocation Schemes in IDP","  In ownership-based access control frameworks with the possibility of
delegating permissions and administrative rights, chains of delegated accesses
will form. There are different ways to treat these delegation chains when
revoking rights, which give rise to different revocation schemes. In this
paper, we show how IDP - a knowledge base system that integrates technology
from ASP, SAT and CP - can be used to efficiently implement executable
revocation schemes for an ownership-based access control system based on a
declarative specification of their properties.
","Marcos Cramer|Pieter Van Hertum|Diego Agustin Ambrossio|Marc Denecker","","http://arxiv.org/abs/1405.1584v1","http://arxiv.org/pdf/1405.1584v1","","16 pages","","","cs.LO","cs.LO|cs.CR"
"536","1405.3381v1","2014-05-14 06:53:46","2014-05-14 06:53:46","Can Online MBA Programs Allow Professional Working Mothers to Balance
  Work, Family, and Career Progression? A Case Study in China","  Career progression is a general concern of professional working mothers in
China. The purpose of this paper is to report a qualitative study of Chinese
professional working mothers that explored the perceptions of online Master's
of Business Administration (MBA) programmes as a tool for career progression
for working mothers balancing work and family in China. The objective was to
examine existing work-family and career progression conflicts, the perceived
usefulness of online MBA in balancing work-family and career aspirations, and
the perceived ease of use of e-learning. Using Davis's (1989) technology
acceptance model (TAM), the research drew on in-depth interviews with 10 female
part-time MBA students from a university in Wuhan. The data were analysed
through coding and transcribing. The findings showed that conflicts arose where
demanding work schedules competed with family obligations, studies, and caring
for children and the elderly. Online MBA programmes were viewed as a viable
tool for balancing work and family and studying, given its flexible time
management capabilities. However, consideration must be given to address
students' motivation issues, lack of networking, lack of face-to-face
interaction, and quality. The research findings emphasise the pragmatic need to
re-align higher education policy and practice to position higher education
e-learning as a trustable education delivery channel in China. By shedding
light on the prevailing work-family conflict experienced by women seeking
career advancement, this study suggests developing better gender-supporting
policies and innovative e-learning practices to champion online MBA programme
for this target niche.
","Mboni Kibelloh|Yukun Bao","","http://arxiv.org/abs/1405.3381v1","http://arxiv.org/pdf/1405.3381v1","http://dx.doi.org/10.1007/s40299-013-0101-1","31 pages","","10.1007/s40299-013-0101-1","cs.CY","cs.CY"
"537","1405.3724v1","2014-05-15 02:05:56","2014-05-15 02:05:56","Integration of Inter-Connectivity of Information System (i3) using Web
  Services","  The growth and development of institutions day by day especially universities
are increasing very large number of peoples including employees as well as
students, working on different domain in distance between multiple integrated
institutes, and degrading the overall efficiency of system that cannot
facilitate better interoperability and automated integration. In many
universities, exist system does not have yet set of connections, in spite of
the fact they have different systems running on different platforms in
different administrative units or departments. University of Sindh (UoS) is the
one of them. In this paper we discuss about Integration of Inter-Connectivity
of Information System (i3) in the UoS which is based on Service Oriented
Architecture with web services. The proposed system (i3) can monitor and
exchange students information in support of verification along heterogeneous
and decentralized nature, and provide capability of interoperability in the
place of already deployed system in the UoS using different Languages as well
as Databases.
","Aftab Ahmed Chandio|Dingju Zhu|Ali Hassan Sodhro","","http://arxiv.org/abs/1405.3724v1","http://arxiv.org/pdf/1405.3724v1","","5 pages, 5 figures, submitted in the International MultiConference of
  Engineers and Computer Scientists (IMECS) 2012 March 12-16 Hong Kong","Lecture Notes in Engineering and Computer Science, Vol. 2195(1),
  2012","","cs.SE","cs.SE"
"538","1406.0198v2","2014-06-01 20:20:30","2014-06-06 23:18:51","Highly Unidirectional Uniform Optical Grating Couplers, Fabricated in
  Standard 45nm SOI-CMOS Foundry Process","  Removed by arXiv administrators because submission violated the terms of
arXiv's license agreement.
","Stevan Lj. Uro<U+0161>evi<U+0107>","","http://arxiv.org/abs/1406.0198v2","http://arxiv.org/pdf/1406.0198v2","","Removed by arXiv administrators because submission violated the terms
  of arXiv's license agreement","","","physics.optics","physics.optics"
"539","1406.1263v1","2014-06-05 04:11:37","2014-06-05 04:11:37","Direct synthesis of strong grating couplers for efficient integrated
  optical beam forming","  Removed by arXiv administrators because submission violated the terms of
arXiv's license agreement.
","Stevan Uro<U+0161>evi<U+0107>","","http://arxiv.org/abs/1406.1263v1","http://arxiv.org/pdf/1406.1263v1","","Removed by arXiv administrators because submission violated the terms
  of arXiv's license agreement","","","physics.optics","physics.optics"
"540","1406.1564v1","2014-06-06 01:48:40","2014-06-06 01:48:40","Rational Design of Antibiotic Treatment Plans","  The development of reliable methods for restoring susceptibility after
antibiotic resistance arises has proven elusive. A greater understanding of the
relationship between antibiotic administration and the evolution of resistance
is key to overcoming this challenge. Here we present a data-driven mathematical
approach for developing antibiotic treatment plans that can reverse the
evolution of antibiotic resistance determinants. We have generated adaptive
landscapes for 16 genotypes of the TEM beta-lactamase that vary from the wild
type genotype TEM-1 through all combinations of four amino acid substitutions.
We determined the growth rate of each genotype when treated with each of 15
beta-lactam antibiotics. By using growth rates as a measure of fitness, we
computed the probability of each amino acid substitution in each beta-lactam
treatment using two different models named the Correlated Probability Model
(CPM) and the Equal Probability Model (EPM). We then performed an exhaustive
search through the 15 treatments for substitution paths leading from each of
the 16 genotypes back to the wild type TEM-1. We identified those treatment
paths that returned the highest probabilities of selecting for reversions of
amino acid substitutions and returning TEM to the wild type state. For the CPM
model, the optimized probabilities ranged between 0.6 and 1.0. For the EPM
model, the optimized probabilities ranged between 0.38 and 1.0. For cyclical
CPM treatment plans in which the starting and ending genotype was the wild
type, the probabilities were between 0.62 and 0.7. Overall this study shows
that there is promise for reversing the evolution of resistance through
antibiotic treatment plans.
","Portia M. Mira|Kristina Crona|Devin Greene|Juan C. Meza|Bernd Sturmfels|Miriam Barlow","","http://arxiv.org/abs/1406.1564v1","http://arxiv.org/pdf/1406.1564v1","","52 pages, additional supplementary information can be requested from
  the authors","","","q-bio.PE","q-bio.PE"
"541","1406.2313v1","2014-06-09 10:22:30","2014-06-09 10:22:30","Context management strategies in wireless network","  Context management strategies in wireless technology are dependent upon the
collection of accurate information from the individual nodes. This information
(called context information) can be exploited by administrators or automated
systems in order to decide on specific management concerns. While traditional
approaches for fixed networks are more or less centralized, more complex
management strategies have been proposed for wireless networks, such as
hierarchical, fully distributed and hybrid ones. The reason for the
introduction of new strategies is based on the dynamic and unpredictable nature
of wireless networks and their (usually) limited resources, which do not
support centralized management solutions. In this work, efforts is being made
to uncovered some specific strategies that can be used to managed context
information that reaches the centre of decision making, the work is concluded
with a detail comparison of the strategies to enable context application
developers make right choice of strategy to be employed in a specific
situation.
","Vigale Leelanubari Giadom|Williams E. Edem","","http://arxiv.org/abs/1406.2313v1","http://arxiv.org/pdf/1406.2313v1","","","International Journal of advanced studies in Computer Science and
  Engineering IJASCSE, Volume 3, Issue 5, 2014","","cs.NI","cs.NI"
"542","1406.3876v1","2014-06-16 01:01:54","2014-06-16 01:01:54","Bringing Web Time Travel to MediaWiki: An Assessment of the Memento
  MediaWiki Extension","  We have implemented the Memento MediaWiki Extension Version 2.0, which brings
the Memento Protocol to MediaWiki, used by Wikipedia and the Wikimedia
Foundation. Test results show that the extension has a negligible impact on
performance. Two 302 status code datetime negotiation patterns, as defined by
Memento, have been examined for the extension: Pattern 1.1, which requires 2
requests, versus Pattern 2.1, which requires 3 requests. Our test results and
mathematical review find that, contrary to intuition, Pattern 2.1 performs
better than Pattern 1.1 due to idiosyncrasies in MediaWiki. In addition to
implementing Memento, Version 2.0 allows administrators to choose the optional
200-style datetime negotiation Pattern 1.2 instead of Pattern 2.1. It also
permits administrators the ability to have the Memento MediaWiki Extension
return full HTTP 400 and 500 status codes rather than using standard MediaWiki
error pages. Finally, version 2.0 permits administrators to turn off
recommended Memento headers if desired. Seeing as much of our work focuses on
producing the correct revision of a wiki page in response to a user's datetime
input, we also examine the problem of finding the correct revisions of the
embedded resources, including images, stylesheets, and JavaScript; identifying
the issues and discussing whether or not MediaWiki must be changed to support
this functionality.
","Shawn M. Jones|Michael L. Nelson|Harihar Shankar|Herbert Van de Sompel","","http://arxiv.org/abs/1406.3876v1","http://arxiv.org/pdf/1406.3876v1","","23 pages, 18 figures, 9 tables, 17 listings","","","cs.DL","cs.DL|H.3.7"
"543","1406.7866v1","2014-06-30 19:41:13","2014-06-30 19:41:13","Regional Pilot Study to Evaluate e-Readiness and Local e-Government
  Services","  The concept of local e-Government has become a key factor for delivering
services in an efficient, cost effective, transparent and convenient way, in
circumstances where a) citizens do not have enough time available to
communicate with local authorities in order to perform their responsibilities
and needs, and b) information and communication technologies significantly
facilitate administrative procedures and citizens-government interaction. This
paper aims to identify e-services that local authorities provide, and to
investigate their readiness for delivering these services. A pilot research has
been conducted to identify the offer of e-services by local authorities, along
with e-readiness in municipalities of the Pelagonia region in the Republic of
Macedonia. The survey was carried out by means of structured interview
questions based on a modified model proposed by Partnership on Measuring ICT
for Development - web analysis of municipal websites in the region has been
conducted, as well. The study reveals uneven distribution according to the age
group of users, lack of reliability and confidence for processing the needs and
requests electronically by a large part of the population, and improperly
developed set of ICT tools by local governments for providing a variety of
services that can be fully processed electronically.
","Marjan Angeleski|Pece Mitrevski|Slavica Rocheska|Ane Lashkoska","","http://arxiv.org/abs/1406.7866v1","http://arxiv.org/pdf/1406.7866v1","http://dx.doi.org/10.5121/ijmpict.2014.5201","10 pages, 3 figures, International Journal of Managing Public Sector
  Information and Communication Technologies (IJMPICT), Vol. 5, No. 2, June
  2014","","10.5121/ijmpict.2014.5201","cs.CY","cs.CY|K.4.0"
"544","1407.0462v2","2014-07-02 06:11:26","2016-10-21 16:07:58","Zigbee & IEEE 802.11b(WLAN)coexistence in ubiquitous network environment","  This submission has been withdrawn by arXiv administrators because it
contains excessive and unattributed reuse of content from other authors.
","Meenu Balodhi|Vishwanath Bijalwan|Banit Negi","","http://arxiv.org/abs/1407.0462v2","http://arxiv.org/pdf/1407.0462v2","","This submission has been withdrawn by arXiv administrators because it
  contains excessive and unattributed reuse of content from other authors","","","cs.NI","cs.NI"
"545","1407.0645v2","2014-07-02 16:53:04","2015-12-18 20:31:59","Branching Bisimilarity of Normed BPA Processes is in NEXPTIME","  Branching bisimilarity on normed BPA processes was recently shown to be
decidable by Yuxi Fu (ICALP 2013) but his proof has not provided any upper
complexity bound. We present a simpler approach based on relative prime
decompositions that leads to a nondeterministic exponential-time algorithm;
this is close to the known exponential-time lower bound.
","Wojciech Czerwi<U+0144>ski|Petr Jan<U+010D>ar","","http://arxiv.org/abs/1407.0645v2","http://arxiv.org/pdf/1407.0645v2","","This is the same text as in July 2014, but only with some
  acknowledgment added due to administrative needs","","","cs.LO","cs.LO|cs.FL|F.1.1; F.4.2"
"546","1407.3673v2","2014-07-03 10:55:52","2016-10-21 16:07:15","Enhanced EZW Technique for Compression of Image by Setting Detail
  Retaining Pass Number","  This submission has been withdrawn by arXiv administrators because it
contains excessive and unattributed reuse of content from other authors.
","Isha Tyagi|Ashish Nautiyal|Vishwanath Bijalwan|Meenu Balodhi","","http://arxiv.org/abs/1407.3673v2","http://arxiv.org/pdf/1407.3673v2","","This submission has been withdrawn by arXiv administrators because it
  contains excessive and unattributed reuse of content from other authors","","","cs.CV","cs.CV"
"547","1407.0981v2","2014-07-03 16:42:19","2016-07-12 15:59:26","A Centralized Mechanism to Make Predictions Based on Data From Multiple
  WSNs","  In this work, we present a method that exploits a scenario with
inter-Wireless Sensor Networks (WSNs) information exchange by making
predictions and adapting the workload of a WSN according to their outcomes. We
show the feasibility of an approach that intelligently utilizes information
produced by other WSNs that may or not belong to the same administrative
domain. To illustrate how the predictions using data from external WSNs can be
utilized, a specific use-case is considered, where the operation of a WSN
measuring relative humidity is optimized using the data obtained from a WSN
measuring temperature. Based on a dedicated performance score, the simulation
results show that this new approach can find the optimal operating point
associated to the trade-off between energy consumption and quality of
measurements. Moreover, we outline the additional challenges that need to be
overcome, and draw conclusions to guide the future work in this field.
","Gabriel Martins Dias|Simon Oechsner|Boris Bellalta","","http://arxiv.org/abs/1407.0981v2","http://arxiv.org/pdf/1407.0981v2","http://dx.doi.org/10.1007/978-3-319-23440-3_2","10 pages, simulation results and figures. Published in","Multiple Access Communications, Lecture Notes in Computer Science,
  Volume 9305, pp 19-32, 2015","10.1007/978-3-319-23440-3_2","cs.NI","cs.NI|cs.SY|D.2.11; C.2.1"
"548","1407.1199v1","2014-07-04 11:48:13","2014-07-04 11:48:13","Merlin: A Language for Provisioning Network Resources","  This paper presents Merlin, a new framework for managing resources in
software-defined networks. With Merlin, administrators express high-level
policies using programs in a declarative language. The language includes
logical predicates to identify sets of packets, regular expressions to encode
forwarding paths, and arithmetic formulas to specify bandwidth constraints. The
Merlin compiler uses a combination of advanced techniques to translate these
policies into code that can be executed on network elements including a
constraint solver that allocates bandwidth using parameterizable heuristics. To
facilitate dynamic adaptation, Merlin provides mechanisms for delegating
control of sub-policies and for verifying that modifications made to
sub-policies do not violate global constraints. Experiments demonstrate the
expressiveness and scalability of Merlin on real-world topologies and
applications. Overall, Merlin simplifies network administration by providing
high-level abstractions for specifying network policies and scalable
infrastructure for enforcing them.
","Robert Soule|Shrutarshi Basu|Parisa Jalili Marandi|Fernando Pedone|Robert Kleinberg|Emin Gun Sirer|Nate Foster","","http://arxiv.org/abs/1407.1199v1","http://arxiv.org/pdf/1407.1199v1","","","","","cs.NI","cs.NI"
"549","1407.1807v1","2014-07-04 19:28:35","2014-07-04 19:28:35","Building A Smart Academic Advising System Using Association Rule Mining","  In an academic environment, student advising is considered a paramount
activity for both advisors and student to improve the academic performance of
students. In universities of large numbers of students, advising is a
time-consuming activity that may take a considerable effort of advisors and
university administration in guiding students to complete their registration
successfully and efficiently. Current systems are traditional and depend
greatly on the effort of the advisor to find the best selection of courses to
improve students performance. There is a need for a smart system that can
advise a large number of students every semester. In this paper, we propose a
smart system that uses association rule mining to help both students and
advisors in selecting and prioritizing courses. The system helps students to
improve their performance by suggesting courses that meet their current needs
and at the same time improve their academic performance. The system uses
association rule mining to find associations between courses that have been
registered by students in many previous semesters. The system successfully
generates a list of association rules that guide a particular student to select
courses registered by similar students.
","Raed Shatnawi|Qutaibah Althebyan|Baraq Ghalib|Mohammed Al-Maolegi","","http://arxiv.org/abs/1407.1807v1","http://arxiv.org/pdf/1407.1807v1","","5 pages","","","cs.DB","cs.DB"
"550","1407.2125v2","2014-07-08 15:00:04","2016-08-25 16:23:14","Noisy Distance Measurements Using 3-D Localization with Rb-Rf Methods","  Wireless sensor networks are dynamically formed over the varying topologies.
Wireless sensor networks can assist in conducting the rescue operations and can
provide search in timely manner. Long time monitoring applications are
environment monitoring, security surveillance and habitat monitoring. Further,
where it can be deployed in time critical situations when disaster happens. As
we are dealing with the human lives here, we can not just rely on the
localization schemes that depend upon the connectivity information Rf i.e.
range-free algorithms only. Further, rescue operations are carried out in
highly noisy environments, so distance based Rb(range-based) localization
algorithms generate high error in distance measurements. An efficient algorithm
is needed that can measure the location of the sensor nodes near to the living
being or being attached to them in 3-D space with a high accuracy. To achieve
such kind of accuracy a combination of both the strategies is required. The
proposed method which incorporates both the Rb(range-based)&Rfrange-free
strategies that successfully localizes nodes in a sensor network with noisy
distance measurements. We also have depicted the effect of scalability on the
performance of the algorithm. Results show that as the scalability of the
network increases with the number of beacon nodes; the performance of the
algorithm goes high above 90 percent . The granularity of the areas estimated
may be easily adjusted by changing the system parameters which makes the
proposed algorithm flexible.
","Anubha Parashar|Susheel Kumar|Vinay S Bhaskar|Rajni Chinia","","http://arxiv.org/abs/1407.2125v2","http://arxiv.org/pdf/1407.2125v2","","This article has been withdrawn by arXiv administrators due to
  disputed authorship","","","cs.NI","cs.NI"
"551","1407.3912v1","2014-07-15 08:46:27","2014-07-15 08:46:27","Item selection by Latent Class-based methods","  The evaluation of nursing homes is usually based on the administration of
questionnaires made of a large number of polytomous items. In such a context,
the Latent Class (LC) model represents a useful tool for clustering subjects in
homogenous groups corresponding to different degrees of impairment of the
health conditions. It is known that the performance of model-based clustering
and the accuracy of the choice of the number of latent classes may be affected
by the presence of irrelevant or noise variables. In this paper, we show the
application of an item selection algorithm to real data collected within a
project, named ULISSE, on the quality-of-life of elderly patients hosted in
italian nursing homes. This algorithm, which is closely related to that
proposed by Dean and Raftery in 2010, is aimed at finding the subset of items
which provides the best clustering according to the Bayesian Information
Criterion. At the same time, it allows us to select the optimal number of
latent classes. Given the complexity of the ULISSE study, we perform a
validation of the results by means of a sensitivity analysis to different
specifications of the initial subset of items and of a resampling procedure.
","Francesco Bartolucci|Giorgio E. Montanari|Silvia Pandolfi","","http://arxiv.org/abs/1407.3912v1","http://arxiv.org/pdf/1407.3912v1","","","","","stat.AP","stat.AP"
"552","1407.4705v1","2014-07-17 15:20:26","2014-07-17 15:20:26","Sonification of a Network's Self-Organized Criticality","  Communication networks involve the transmission and reception of large
volumes of data. Research indicates that network traffic volumes will continue
to increase. These traffic volumes will be unprecedented and the behaviour of
global information infrastructures when dealing with these data volumes is
unknown. It has been shown that complex systems (including computer networks)
exhibit self-organized criticality under certain conditions. Given the
possibility in such systems of a sudden and spontaneous system reset the
development of techniques to inform system administrators of this behaviour
could be beneficial. This article focuses on the combination of two dissimilar
research concepts, namely sonification (a form of auditory display) and
self-organized criticality (SOC). A system is described that sonifies in real
time an information infrastructure's self-organized criticality to alert the
network administrators of both normal and abnormal network traffic and
operation.
","Paul Vickers|Chris Laing|Tom Fairfax","","http://arxiv.org/abs/1407.4705v1","http://arxiv.org/pdf/1407.4705v1","http://dx.doi.org/10.1016/j.displa.2016.05.002","","","10.1016/j.displa.2016.05.002","cs.HC","cs.HC|cs.NI|cs.SD|H.5.2; C.2.3; H.5.5"
"553","1407.5197v2","2014-07-19 15:04:44","2016-08-25 16:24:26","Design and Autonomous Control of the Active Adaptive Suspension System
  Rudra Mars Rover","  Semi or completely autonomous unmanned vehicles, remotely driven or
controlled through artificial intelligence, are instrumental to foster space
exploration. One of the most essential tasks of a rover is terrain traversing
which requires the need of efficient suspension systems. This communication
presents a suspension system giving degrees of freedom to every wheel with the
help of linear actuators connected through bell crank levers. The actuation of
linear actuators directly varies the height of every wheel from the chassis
hence offering articulation to the rover. A control system is developed
offering an algorithm for its autonomous actuation. This system proves
instrumental for leveling of the chassis where any kind of slope, roll or
pitch, may impute abstaining of payloads from efficient working. This was tried
and tested successfully as a part of the rover developed by Team RUDRA from SRM
University, INDIA (first Team from Asia and finishing at the fifth position) at
University Rover Challenge 2013, held at UTAH, USA in May-June.
","Karan Vaish|Shah Mihir Rajesh|K. Pasupatheeswaran|Anubha Parashar|Jyoti Chaturvedi","","http://arxiv.org/abs/1407.5197v2","http://arxiv.org/pdf/1407.5197v2","","This article has been withdrawn by arXiv administrators due to
  disputed authorship","","","cs.RO","cs.RO"
"554","1407.6090v2","2014-07-23 02:14:48","2016-08-25 16:24:39","Social and Business Intelligence Analysis Using PSO","  The goal of this paper is to elaborate swarm intelligence for business
intelligence decision making and the business rules management improvement.
.The swarm optimization, which is highly influenced by the behavior of
creature, performs in group. The Spatial data is defined as data that is
represented by 2D or 3D images. SQL Server supports only 2D images till now. As
we know that location is an essential part of any organizational data as well
as business data enterprises maintain customer address lists, own property,
ship goods from and to warehouses, manage transport flows among their
workforce, and perform many other activities. By means to say a lot of spatial
data is used and processed by enterprises, organizations and other bodies in
order to make the things more visible and self descriptive. From the
experiments, we found that PSO is can facilitate the intelligence in social and
business behavior.
","Jyoti Chaturvedi|Anubha Parashar|Amrita A Manjrekar|Vinay S Bhaskar","","http://arxiv.org/abs/1407.6090v2","http://arxiv.org/pdf/1407.6090v2","","This article has been withdrawn by arXiv administrators due to
  disputed authorship","","","cs.AI","cs.AI"
"555","1407.6498v1","2014-07-24 09:02:16","2014-07-24 09:02:16","Real-Time and Efficient Method for Accuracy Enhancement of Edge Based
  License Plate Recognition System","  License Plate Recognition plays an important role on the traffic monitoring
and parking management. Administration and restriction of those transportation
tools for their better service becomes very essential. In this paper, a fast
and real time method has an appropriate application to find plates that the
plat has tilt and the picture quality is poor. In the proposed method, at the
beginning, the image is converted into binary mode with use of adaptive
threshold. And with use of edge detection and morphology operation, plate
number location has been specified and if the plat has tilt; its tilt is
removed away. Then its characters are distinguished using image processing
techniques. Finally, K Nearest Neighbour (KNN) classifier was used for
character recognition. This method has been tested on available data set that
has different images of the background, considering distance, and angel of view
so that the correct extraction rate of plate reached at 98% and character
recognition rate achieved at 99.12%. Further we tested our character
recognition stage on Persian vehicle data set and we achieved 99% correct
recognition rate.
","Reza Azad|Babak Azad|Hamid Reza Shayegh","","http://arxiv.org/abs/1407.6498v1","http://arxiv.org/pdf/1407.6498v1","","2013 First International Conference on computer, Information
  Technology and Digital Media. arXiv admin note: substantial text overlap with
  arXiv:1407.6321","","","cs.CV","cs.CV"
"556","1407.8320v1","2014-07-31 08:55:20","2014-07-31 08:55:20","An Implementation of Web Services for Inter-Connectivity of Information
  Systems","  As educational institutions and their departments rapidly increase, a
communication between their end-users becomes more and more difficult in
traditional online management systems (OMS). However, the end-users, i.e.,
employees, teaching staff, and students are associated to different sub-domains
and using different subsystems that are executed on different platforms
following different administrative policies. Because of their
intercommunication is not automated integrated, consequently, the overall
efficiency of the system is degraded and the communication time is increased.
Therefore, a technique for better interoperability and automated integration of
departments is an urgent needed. Many of existing systems does not have a set
of connections yet, such as the system of the University of Sindh (UoS). In
this paper, we propose a system for the UoS, named integration of
inter-connectivity of information system (i3) based on service oriented
architecture (SOA) with web services. The system i3 monitors and exchanges the
students information in support of verification along heterogeneous and
decentralized nature. Moreover, the proposed system provides capability of
interoperability between their subsystems that are deployed in different
departments of UoS and using different programming languages and database
management systems (DBMS)
","Aftab Ahmed Chandio|Dingju Zhu|Ali Hassan Sodhro|Muhammad Umer Syed","","http://arxiv.org/abs/1407.8320v1","http://arxiv.org/pdf/1407.8320v1","","7 pages, 5 figures, (Accepted for the Int. J. Com. Dig. Sys. Vol. 3,
  No. 3, ISSN. 2210-142X)","International Journal of Computing and Digital Systems (IJCDS),
  Vol. 3, No. 3, pp. (2014)","","cs.SE","cs.SE"
"557","1408.0989v1","2014-08-04 15:09:35","2014-08-04 15:09:35","Delivery of Liquid Metal to the Target Vessels as Vascular Embolic Agent
  to Starve Diseased Tissues or Tumors to Death","  Tumor growth relies heavily on the continuous blood and nutrients supply.
Theoretically, it is an ideal therapeutic way of killing tumor by only vascular
embolization. However, most of the existing vascular embolic agents are still
rather insufficient to fulfill the real clinical need due to the reasons like:
incomplete filling of target vasculature, being easily washed away by blood or
body solution, or just producing toxicity to tissues. Here from an alternative
way, the body temperature liquid metal, a kind of soft and highly compliant
material, was proposed for the first time as blood vessel embolization agent
for tumor physical therapy. With its unique capability of easy phase transition
between liquid and solid state and sub-cooling behavior, such material can be
fluently injected into the tiny vessels including ending capillaries and fully
block them. The in vitro cytotoxicity experiments were performed which showed
that treating localized diseased tissues through liquid metal embolic agent is
acceptable. Endowed with a high density, the liquid metal-filled vessels are
highly visible under the CT scan, which offers the potential of
diagnosis-treatment integration. To further demonstrate the new conceptual
liquid metal vascular embolization therapy, several experiments on in vivo
vasculatures of rabbit ears and mouse tails were performed to provide evidences
of destroying the targeted tissues. To interpret the liquid metal starvation
therapy effects, a theoretical model was established to simulate the tumor
growth with zero, partial or complete filling of the metal agent inside the
vessels. All the results support that, given appropriate administration, the
liquid metal embolization is able to destruct the target regions and might
starve the tumors to death through a relatively easy way. This study lays the
foundation of a promising tumor starvation therapy in the coming time.
","Qian Wang|Yang Yu|Jing Liu","","http://arxiv.org/abs/1408.0989v1","http://arxiv.org/pdf/1408.0989v1","","21 pages, 10 figures","","","physics.med-ph","physics.med-ph"
"558","1408.6553v1","2014-08-11 17:21:41","2014-08-11 17:21:41","An Observational Study: The Effect of Diuretics Administration on
  Outcomes of Mortality and Mean Duration of I.C.U. Stay","  This thesis conducts an observational study into whether diuretics should be
administered to ICU patients with sepsis when length of stay in the ICU and
30-day post-hospital mortality are considered. The central contribution of the
thesis is a stepwise, reusable software-based approach for examining the
outcome of treatment vs no-treatment decisions with observational data. The
thesis implements, demonstrates and draws findings via three steps: Step 1.
Form a study group and prepare modeling variables. Step 2. Model the propensity
of the study group with respect to the administration of diuretics with a
propensity score function and create groups of patients balanced in this
propensity. Step 3. Statistically model each outcome with study variables to
decide whether the administration of diuretics has a significant impact.
Additionally, the thesis presents a preliminary machine learning based method
using Genetic Programming to predict mortality and length of stay in ICU
outcomes for the study group. The thesis finds, for its study group, in three
of five propensity balanced quintiles, a statistically significant longer
length of stay when diuretics are administered. For a less sick subset of
patients (SAPS ICU admission score < 17) the administration of diuretics has a
significant negative effect on mortality.
","Daniele Ramazzotti","","http://arxiv.org/abs/1408.6553v1","http://arxiv.org/pdf/1408.6553v1","","","","","stat.AP","stat.AP"
"559","1408.4987v2","2014-08-21 13:09:41","2014-08-22 07:48:50","The Influence of Social Movements on Space Astronomy Policy","  Public engagement (PE) initiatives can lead to a long term public support of
science. However most of the real impact of PE initiatives within the context
of long-term science policy is not completely understood. An examination of the
National Aeronautics and Space Administration's (NASA) Hubble Space Telescope,
James Webb Space Telescope, and International Sun-Earth Explorer 3 reveal how
large grassroots movements led by citizen scientists and space aficionados can
have profound effects on public policy. We explore the role and relevance of
public grassroots movements in the policy of space astronomy initiatives,
present some recent cases which illustrate policy decisions involving broader
interest groups, and consider new avenues of PE including crowdfunding and
crowdsourcing.
","Hannah E. Harris|Pedro Russo","","http://arxiv.org/abs/1408.4987v2","http://arxiv.org/pdf/1408.4987v2","http://dx.doi.org/10.1016/j.spacepol.2014.08.009","Accepted for publication in Space Policy journal","","10.1016/j.spacepol.2014.08.009","astro-ph.IM","astro-ph.IM"
"560","1408.6762v1","2014-08-28 16:01:10","2014-08-28 16:01:10","Chatbot for admissions","  The communication of potential students with a university department is
performed manually and it is a very time consuming procedure. The opportunity
to communicate with on a one-to-one basis is highly valued. However with many
hundreds of applications each year, one-to-one conversations are not feasible
in most cases. The communication will require a member of academic staff to
expend several hours to find suitable answers and contact each student. It
would be useful to reduce his costs and time.
  The project aims to reduce the burden on the head of admissions, and
potentially other users, by developing a convincing chatbot. A suitable
algorithm must be devised to search through the set of data and find a
potential answer. The program then replies to the user and provides a relevant
web link if the user is not satisfied by the answer. Furthermore a web
interface is provided for both users and an administrator.
  The achievements of the project can be summarised as follows. To prepare the
background of the project a literature review was undertaken, together with an
investigation of existing tools, and consultation with the head of admissions.
The requirements of the system were established and a range of algorithms and
tools were investigated, including keyword and template matching. An algorithm
that combines keyword matching with string similarity has been developed. A
usable system using the proposed algorithm has been implemented. The system was
evaluated by keeping logs of questions and answers and by feedback received by
potential students that used it.
","Nikolaos Polatidis","","http://arxiv.org/abs/1408.6762v1","http://arxiv.org/pdf/1408.6762v1","","","","","cs.CY","cs.CY|cs.CL"
"561","1409.0415v1","2014-09-01 13:24:14","2014-09-01 13:24:14","SSELab: A Plug-In-Based Framework for Web-Based Project Portals","  Tools are an essential part of every software engineering project. But the
number of tools that are used in all phases of the software development
life-cycle and their complexity is growing continually. Consequently, the setup
and maintenance of current tool chains and development environments requires
much effort and consumes a lot of time. One approach to counter this, is to
employ web-based systems for development tasks, because centralized systems
simplify the administration and the deployment of new features. But desktop
IDEs play an important role in software development projects today, and will
not be replaced entirely by web-based environments in the near future.
Therefore, supporting a mixture of hosted tools and tools integrated into
desktop IDEs is a sensible approach. In this paper, we present the SSELab, a
framework for web- based project portals that attempts to migrate more software
development tools from desktop to server environments, but still allows their
integration into modern desktop IDEs. It supports the deployment of tools as
hosted services using plug-in systems on the server-side. Additionally, it
provides access to these tools by a set of clients that can be used in
different contexts, either from the command line, from within IDEs such as
Eclipse, or from web pages. In the paper, we discuss the architecture and the
extensibility of the SSELab framework. Furthermore, we share our experiences
with creating an instance of the framework and integrating various tools for
our own software development projects.
","Christoph Herrmann|Thomas Kurpick|Bernhard Rumpe","","http://arxiv.org/abs/1409.0415v1","http://arxiv.org/pdf/1409.0415v1","","6 pages, 5 figures. Proceedings of the 2nd International Workshop on
  Developing Tools as Plug-Ins (TOPI 2012) at ICSE 2012, June 3, Zurich,
  Switzerland, 2012","","","cs.SE","cs.SE"
"562","1409.5282v1","2014-09-18 12:25:04","2014-09-18 12:25:04","Sonification Aesthetics and Listening for Network Situational Awareness","  This paper looks at the problem of using sonification to enable network
administrators to maintaining situational awareness about their network
environment. Network environments generate a lot of data and the need for
continuous monitoring means that sonification systems must be designed in such
a way as to maximise acceptance while minimising annoyance and listener
fatigue. It will be argued that solutions based on the concept of the
soundscape offer an ecological advantage over other sonification designs.
","Paul Vickers|Christopher Laing|Mohamed Debashi|Tom Fairfax","","http://arxiv.org/abs/1409.5282v1","http://arxiv.org/pdf/1409.5282v1","http://dx.doi.org/10.13140/2.1.4225.6648","Workshop paper presented at SoniHED --- Conference on Sonification of
  Health and Environmental Data, York, UK, 12 September, 2014","","10.13140/2.1.4225.6648","cs.HC","cs.HC|cs.CY"
"563","1409.6634v1","2014-09-22 12:29:08","2014-09-22 12:29:08","Evolving and Implanting Web-based E-Government Systems in Universities","  The Bologna Process has triggered a major restructuring of the current
university diploma into a bachelor/master system. As one effect, the
administration effort for the new system has increased dramatically. As a
second effect, students need and demand a much better information policy, given
the new possibilities of the internet. Both to increase efficiency of the
university's administration and to provide students as well as lecturers with
modern e government services, it is inevitable to evolve the current
IT-infrastructure of a university into a modern web-based landscape of systems
that support business processes on campus. In this paper, we describe the
approach taken at the Braunschweig University of Technology to evolve the
existing landscape of legacy systems by adding bridges between previously
unrelated parts, adding and customizing unused modules of existing software to
bring information and services online and to develop new software, where old
modules could not serve the necessary purposes. Most of all, both
implementation of the results in university's business processes and the
resulting quick feedback and wishes for feature enhancement are seen as part of
the software development processes and discussed in this paper.
","Dirk Rei©¬|Bernhard Rumpe|Marvin Schulze-Quester|Mark Stein","","http://arxiv.org/abs/1409.6634v1","http://arxiv.org/pdf/1409.6634v1","","14 pages, 3 figures","Proceedings to the 2nd International United Information Systems
  Conference (UNISCON) Klagenfurt, Austria, April 22-25, 2008 R. Kaschek, C.
  Kop, C. Steinberger, G. Fliedl: LNBIP 5, pp. 282-295 Springer-Verlag
  Berlin-Heidelberg 2008","","cs.CY","cs.CY|cs.SE"
"564","1409.6994v4","2014-09-24 15:36:28","2015-07-02 14:47:23","Bayesian complementary clustering, MCMC and Anglo-Saxon placenames","  Common cluster models for multi-type point processes model the aggregation of
points of the same type. In complete contrast, in the study of Anglo-Saxon
settlements it is hypothesized that administrative clusters involving
complementary names tend to appear. We investigate the evidence for such an
hypothesis by developing a Bayesian Random Partition Model based on clusters
formed by points of different types (complementary clustering).
  As a result we obtain an intractable posterior distribution on the space of
matchings contained in a k-partite hypergraph. We apply the Metropolis-Hastings
(MH) algorithm to sample from this posterior. We consider the problem of
choosing an efficient MH proposal distribution and we obtain consistent mixing
improvements compared to the choices found in the literature. Simulated
Tempering techniques can be used to overcome multimodality and a multiple
proposal scheme is developed to allow for parallel programming. Finally, we
discuss results arising from the careful use of convergence diagnostic
techniques.
  This allows us to study a dataset including locations and placenames of 1316
Anglo-Saxon settlements dated approximately around 750-850 AD. Without strong
prior knowledge, the model allows for explicit estimation of the number of
clusters, the average intra-cluster dispersion and the level of interaction
among placenames. The results support the hypothesis of organization of
settlements into administrative clusters based on complementary names.
","Giacomo Zanella","","http://arxiv.org/abs/1409.6994v4","http://arxiv.org/pdf/1409.6994v4","","33 pages, 13 figures. Version 4: minor revision","","","stat.AP","stat.AP|stat.CO"
"565","1409.7334v1","2014-09-25 17:15:33","2014-09-25 17:15:33","Radar in-Band Interference Effects on Macrocell LTE Uplink Deployments
  in the U.S. 3.5 GHz Band","  National Telecommunications and Information Administration (NTIA) has
proposed vast exclusions zones between radar and Worldwide Interoperability for
Microwave Access (WiMAX) systems which are also being considered as geographic
separations between radars and 3.5 GHz Long Term Evolution (LTE) systems
without investigating any changes induced by the distinct nature of LTE as
opposed to WiMAX. This paper performs a detailed system-level analysis of the
interference effects from shipborne radar systems into LTE systems. Even though
the results reveal impacts of radar interference on LTE systems performance,
they provide clear indications of conspicuously narrower exclusion zones for
LTE vis-\`a-vis those of WiMAX and pave the way toward deploying LTE at 3.5 GHz
within the coastline populous areas.
","Mo Ghorbanzadeh|Eugene Visotsky|Weidong Yang|Prakash Moorut|Charles Clancy","","http://arxiv.org/abs/1409.7334v1","http://arxiv.org/pdf/1409.7334v1","","","","","cs.NI","cs.NI"
"566","1410.2437v1","2014-10-09 12:16:32","2014-10-09 12:16:32","S.A.T.E.P. : Synchronous-Asynchronous Tele-education Platform","  S.A.T.E.P. means Synchronous Asynchronous Tele education Platform is a
software application for educational purposes, with a lot of parametrizing
features written entirely from scratch. It aims at the training and examination
of computer skills, a platform that can be adjusted to the needs of each
lesson. In the application the trainer and the administrator can define the
number of the lectures and upload files for each one of them. Furthermore, he
can insert, modify and delete questions which are used for evaluation tests but
also for the trainees examinations. The trainee can read and download the files
of each lesson and also test his knowledge on what he has studied through a
series of questions. A chat module where registered users as well as system
administrator can discuss and solve questions is also developed.
","Lazaros Lazaridis|Maria Papatsimouli|George F. Fragulis","","http://arxiv.org/abs/1410.2437v1","http://arxiv.org/pdf/1410.2437v1","","","International Journal of Smart Technology and Learning, 2019, 1.2:
  122-139","","cs.CY","cs.CY"
"567","1410.4019v1","2014-10-15 11:38:21","2014-10-15 11:38:21","An Automated Group Key Authentication System Using Secret Image Sharing
  Scheme","  In an open network environment, privacy of group communication and integrity
of the communication data are the two major issues related to secured
information exchange. The required level of security may be achieved by
authenticating a group key in the communication channel, where contribution
from each group member becomes a part of the overall group key. In the current
work, we have developed an authentication system through Central Administrative
Server (CAS) for automatic integration and validation of the group key. For
secured group communication, the CAS generates a secret alphanumeric group key
image. Using secret image sharing scheme, this group key image shares are
distributed among all the participating group members in the open network. Some
or all the secret shares may be merged to reconstruct the group key image at
CAS. A k-nearest neighbor classifier with 48 features to represent the images,
is used to validate the reconstructed image with the one stored in the CAS. 48
topological features are used to represent the reconstructed group key image.
We have achieved 99.1% classification accuracy for 26 printed English uppercase
characters and 10 numeric digits.
","Dipak Kumar Kole|Subhadip Basu","","http://arxiv.org/abs/1410.4019v1","http://arxiv.org/pdf/1410.4019v1","","","Proceedings of the International Conference on Recent trends in
  Information Systems (IRIS-06), pp. 98-106, Kovilpatti, Tamil Nadu, India,
  2006","","cs.CR","cs.CR"
"568","1410.4967v1","2014-10-18 14:59:59","2014-10-18 14:59:59","Pirus: A Web-based File Hosting Service with Object Oriented Logic in
  Cloud Computing","  In this paper a new Web-based File Hosting Service with Object Oriented Logic
in Cloud Computing called Pirus was developed. The service will be used by the
academic community of the University of Piraeus giving users the ability to
remotely store and access their personal files with no security compromises. It
also offers the administrators the ability to manage users and roles. The
objective was to deliver a fully operational service, using state-of-the-art
programming techniques to enable scalability and future development of the
existing functionality. The use of technologies such as .NET Framework, C#
programming language, CSS and jQuery, MSSQL for database hosting and the
support of Virtualization and Cloud Computing will contribute significantly in
compatibility, code reuse, reliability and reduce of maintenance costs and
resources. The service was installed and tested in a controlled environment to
ascertain the required functionality and the offered reliability and safety
with complete success.
  The technologies used and supported, allow future work in upgrading and
extending the service. Changes and improvements, in hardware and software, in
order to convert the service to a SaaS (Software as a Service) Cloud
application is a logical step in order to efficiently offer the service to a
wider community. Improved and added functionality offered by further
development will leverage the user experience.
","Dimitrios Kallergis|Konstantinos Chimos|Vizikidis Stefanos|Theodoros Karvounidis|Christos Douligeris","","http://arxiv.org/abs/1410.4967v1","http://arxiv.org/pdf/1410.4967v1","","6 pages, 3rd International Conference on Internet and Cloud Computing
  Technology (ICICCT2013), November 6-7 2013, Singapore","International Journal of Information Technology & Computer Science
  (IJITCS) 12 (3) (2013) 38-43","","cs.SE","cs.SE|cs.DC|68U99|C.2.4; D.1.5; D.2.3; D.4.3; H.3.5"
"569","1410.7709v1","2014-10-28 17:29:42","2014-10-28 17:29:42","Anomaly Detection Framework Using Rule Extraction for Efficient
  Intrusion Detection","  Huge datasets in cyber security, such as network traffic logs, can be
analyzed using machine learning and data mining methods. However, the amount of
collected data is increasing, which makes analysis more difficult. Many machine
learning methods have not been designed for big datasets, and consequently are
slow and difficult to understand. We address the issue of efficient network
traffic classification by creating an intrusion detection framework that
applies dimensionality reduction and conjunctive rule extraction. The system
can perform unsupervised anomaly detection and use this information to create
conjunctive rules that classify huge amounts of traffic in real time. We test
the implemented system with the widely used KDD Cup 99 dataset and real-world
network logs to confirm that the performance is satisfactory. This system is
transparent and does not work like a black box, making it intuitive for domain
experts, such as network administrators.
","Antti Juvonen|Tuomo Sipola","","http://arxiv.org/abs/1410.7709v1","http://arxiv.org/pdf/1410.7709v1","","35 pages, 12 figures, 7 tables","","","cs.LG","cs.LG|cs.CR"
"570","1411.0919v1","2014-11-04 14:30:05","2014-11-04 14:30:05","Inferential statistics, power estimates, and study design formalities
  continue to suppress biomedical innovation","  Innovation is the direct intended product of certain styles in research, but
not of others. Fundamental conflicts between descriptive vs inferential
statistics, deductive vs inductive hypothesis testing, and exploratory vs
pre-planned confirmatory research designs have been played out over decades,
with winners and losers and consequences. Longstanding warnings from both
academics and research-funding interests have failed to influence effectively
the course of these battles. The NIH publicly studied and diagnosed important
aspects of the problem a decade ago, resulting in outward changes in the grant
review process but not a definitive correction. Specific reforms could
deliberately abate the damage produced by the current overemphasis on
inferential statistics, power estimates, and prescriptive study design. Such
reform would permit a reallocation of resources to historically productive
rapid exploratory efforts and considerably,increase the chances for
higher-impact research discoveries. We can profit from the history and
foundation of these conflicts to make specific recommendations for
administrative objectives and the process of peer review in decisions regarding
research funding. (C) 2013 S. Kern
","Scott E. Kern","","http://arxiv.org/abs/1411.0919v1","http://arxiv.org/pdf/1411.0919v1","","","","","stat.AP","stat.AP"
"571","1411.1958v2","2014-11-07 15:59:47","2015-03-21 01:21:20","Checkpointing as a Service in Heterogeneous Cloud Environments","  A non-invasive, cloud-agnostic approach is demonstrated for extending
existing cloud platforms to include checkpoint-restart capability. Most cloud
platforms currently rely on each application to provide its own fault
tolerance. A uniform mechanism within the cloud itself serves two purposes: (a)
direct support for long-running jobs, which would otherwise require a custom
fault-tolerant mechanism for each application; and (b) the administrative
capability to manage an over-subscribed cloud by temporarily swapping out jobs
when higher priority jobs arrive. An advantage of this uniform approach is that
it also supports parallel and distributed computations, over both TCP and
InfiniBand, thus allowing traditional HPC applications to take advantage of an
existing cloud infrastructure. Additionally, an integrated health-monitoring
mechanism detects when long-running jobs either fail or incur exceptionally low
performance, perhaps due to resource starvation, and proactively suspends the
job. The cloud-agnostic feature is demonstrated by applying the implementation
to two very different cloud platforms: Snooze and OpenStack. The use of a
cloud-agnostic architecture also enables, for the first time, migration of
applications from one cloud platform to another.
","Jiajun Cao|Matthieu Simonin|Gene Cooperman|Christine Morin","","http://arxiv.org/abs/1411.1958v2","http://arxiv.org/pdf/1411.1958v2","","20 pages, 11 figures, appears in CCGrid, 2015","","","cs.DC","cs.DC|H.3.4; C.2.4"
"572","1411.3761v1","2014-11-13 23:05:41","2014-11-13 23:05:41","A Hybrid Approach to Finding Relevant Social Media Content for Complex
  Domain Specific Information Needs","  While contemporary semantic search systems offer to improve classical
keyword-based search, they are not always adequate for complex domain specific
information needs. The domain of prescription drug abuse, for example, requires
knowledge of both ontological concepts and 'intelligible constructs' not
typically modeled in ontologies. These intelligible constructs convey essential
information that include notions of intensity, frequency, interval, dosage and
sentiments, which could be important to the holistic needs of the information
seeker. We present a hybrid approach to domain specific information retrieval
(or knowledge-aware search) that integrates ontology-driven query
interpretation with synonym-based query expansion and domain specific rules, to
facilitate search in social media. Our framework is based on a context-free
grammar (CFG) that defines the query language of constructs interpretable by
the search system. The grammar provides two levels of semantic interpretation:
1) a top-level CFG that facilitates retrieval of diverse textual patterns,
which belong to broad templates and 2) a low-level CFG that enables
interpretation of certain specific expressions that belong to such patterns.
These low-level expressions occur as concepts from four different categories of
data: 1) ontological concepts, 2) concepts in lexicons (such as emotions and
sentiments), 3) concepts in lexicons with only partial ontology representation,
called lexico-ontology concepts (such as side effects and routes of
administration (ROA)), and 4) domain specific expressions (such as date, time,
interval, frequency and dosage) derived solely through rules. Our approach is
embodied in a novel Semantic Web platform called PREDOSE developed for
prescription drug abuse epidemiology.
  Keywords: Knowledge-Aware Search, Ontology, Semantic Search, Background
Knowledge, Context-Free Grammar
","Delroy Cameron|Amit Sheth|Nishita Jaykumar|Krishnaprasad Thirunarayan|Gaurish Anand|Gary A. Smith","","http://arxiv.org/abs/1411.3761v1","http://arxiv.org/pdf/1411.3761v1","","Accepted for publication: Journal of Web Semantics, Elsevier","","","cs.IR","cs.IR|H.3.3"
"573","1411.7364v3","2014-11-26 20:42:03","2015-01-27 15:37:59","The Western Africa Ebola virus disease epidemic exhibits both global
  exponential and local polynomial growth rates","  Background: While many infectious disease epidemics are initially
characterized by an exponential growth in time, we show that district-level
Ebola virus disease (EVD) outbreaks in West Africa follow slower
polynomial-based growth kinetics over several generations of the disease.
Methods: We analyzed epidemic growth patterns at three different spatial scales
(regional, national, and subnational) of the Ebola virus disease epidemic in
Guinea, Sierra Leone and Liberia by compiling publicly available weekly time
series of reported EVD case numbers from the patient database available from
the World Health Organization website for the period 05-Jan to 17-Dec 2014.
Results: We found significant differences in the growth patterns of EVD cases
at the scale of the country, district, and other subnational administrative
divisions. The national cumulative curves of EVD cases in Guinea, Sierra Leone,
and Liberia show periods of approximate exponential growth. In contrast, local
epidemics are asynchronous and exhibit slow growth patterns during 3 or more
EVD generations, which can be better approximated by a polynomial than an
exponential. Conclusions: The slower than expected growth pattern of local EVD
outbreaks could result from a variety of factors, including behavior changes,
success of control interventions, or intrinsic features of the disease such as
a high level of clustering. Quantifying the contribution of each of these
factors could help refine estimates of final epidemic size and the relative
impact of different mitigation efforts in current and future EVD outbreaks.
","Gerardo Chowell|Cecile Viboud|James M. Hyman|Lone Simonsen","","http://arxiv.org/abs/1411.7364v3","http://arxiv.org/pdf/1411.7364v3","","Published version in PLOS Currents Outbreaks. Jan 21st. 2015
  http://currents.plos.org/outbreaks/article/the-western-africa-ebola-virus-disease-epidemic-exhibits-both-global-exponential-and-local-polynomial-growth-rates/","PLOS Currents Outbreaks. 2015 Jan 21. Edition 1","","q-bio.PE","q-bio.PE"
"574","1412.0346v3","2014-12-01 04:46:41","2014-12-08 18:22:53","Single Channel Cutaneous Electrogastrography Parameters from Local White
  Rabbit (Oryctolagus cuniculus)","  EGG recordings performed on 13 local white rabbits (O. cuniculus) which
divided into 3 groups; acetosal 35 mg/kgBM receiver, reserpine 37.5 mg/kgBM
receiver, and control group. A total of 72 EGG recordings obtained from them,
which divided furthermore into 9 datasets based on prepandrial state,
postpandrial state, and post 1 hour drug administration state. EGG parameters
such as the number of cycle per minute ($cpm$), average voltage of action
potential segment ($\bar{V_a}$), root mean square voltage of action potential
segment ($A_{rms}$), root mean square voltage of all EGG segment ($V_{rms}$),
average period of action potential segment ($\bar{T_a}$), average period of
resting plateau ($\bar{T_i}$), average period difference among action potential
segment and resting plateau ($\bar{T_a - T_i}$), and dominant frequency ($f_d$)
are obtained. Insignificant difference of $f_d$ ($P$ = 0.9112993) and cpm ($P$
= 0.9382463) among 9 EGG datasets were found. These findings contrasted the
common practice of EGG assessment, which $f_d$ and $cpm$ are the main
parameters for diagnosis base. In other hand, significant difference between 9
EGG datasets found for $\bar{V_a}$, $A_{rms}$, and $V_{rms}$ parameter with $P$
= 0.0007346, 0.0039191, and 0.0000559 respectively. In conclusion, EGG
parameterization should not be limited to $f_d$ and $cpm$ only.
","Tyas Pandu Fiantoro|Lussya Eveline","","http://arxiv.org/abs/1412.0346v3","http://arxiv.org/pdf/1412.0346v3","","7 pages, 3 figures, typos","","","q-bio.QM","q-bio.QM"
"575","1412.0617v1","2014-12-01 20:02:34","2014-12-01 20:02:34","Radar In-Band and Out-of-Band Interference into LTE Macro and Small Cell
  Uplinks in the 3.5 GHz Band","  National Telecommunications and Information Administration (NTIA) has
proposed vast exclusions zones between radar and Worldwide Interoperability for
Microwave Access (WiMAX) (WiMAX) systems which are also being considered as
geographic separations between radars and 3.5 GHz Long Term Evolution (LTE)
systems without investigating any changes induced by the distinct nature of LTE
as opposed to WiMAX. This paper performs a detailed system-level analysis of
the interference effects from shipborne radar systems into LTE systems. Even
though the results reveal impacts of radar interference on LTE systems
performance, they provide clear indications of conspicuously narrower exclusion
zones for LTE vis-\`a-vis those for WiMAX and pave the way toward deploying LTE
at 3.5 GHz within the coastline populous areas.
","Mo Ghorbanzadeh|Eugene Visotsky|Weidong Yang|Prakash Moorut|Charles Clancy","","http://arxiv.org/abs/1412.0617v1","http://arxiv.org/pdf/1412.0617v1","","arXiv admin note: substantial text overlap with arXiv:1409.7334","","","cs.NI","cs.NI"
"576","1412.2200v2","2014-12-06 06:15:46","2015-01-22 20:53:34","Response to ""Comment on ""Ultra-low-energy non-volatile straintronic
  computing using single multiferroic composites"""" [Appl. Phys. Lett. 103,
  173110 (2013)]","  arXiv admin note: This submission has been withdrawn by arXiv administrators
due to unprofessional personal attack.
","Kuntal Roy","","http://arxiv.org/abs/1412.2200v2","http://arxiv.org/pdf/1412.2200v2","","arXiv admin note: This submission has been withdrawn by arXiv
  administrators due to unprofessional personal attack","","","cond-mat.mes-hall","cond-mat.mes-hall"
"577","1412.3034v3","2014-12-09 17:56:12","2015-08-29 15:19:00","A Framework for Transforming Departmental Culture to Support Educational
  Innovation","  This paper provides a research-based framework for promoting institutional
change in higher education. To date, most educational change efforts have
focused on relatively narrow subsets of the university system (e.g., faculty
teaching practices or administrative policies) and have been largely driven by
implicit change logics; both of these features have limited the success of such
efforts at achieving sustained, systemic change. Drawing from the literature on
organizational and cultural change, our framework encourages change agents to
coordinate their activities across three key levels of the university and to
ground their activities in the various change perspectives that emerge from
that literature. We use examples from a change project that we have been
carrying out at a large research university to illustrate how our framework can
be used as a basis for planning and implementing holistic change.
","Joel C. Corbo|Daniel L. Reinholz|Melissa H. Dancy|Stanley Deetz|Noah Finkelstein","","http://arxiv.org/abs/1412.3034v3","http://arxiv.org/pdf/1412.3034v3","","15 pages, 0 figures, submitted to Physical Review Special Topics:
  Physics Education Research","","","physics.ed-ph","physics.ed-ph"
"578","1412.3136v1","2014-12-09 22:09:48","2014-12-09 22:09:48","Distributed Protocols and Heterogeneous Trust: Technical Report","  The robustness of distributed systems is usually phrased in terms of the
number of failures of certain types that they can withstand. However, these
failure models are too crude to describe the different kinds of trust and
expectations of participants in the modern world of complex, integrated systems
extending across different owners, networks, and administrative domains. Modern
systems often exist in an environment of heterogeneous trust, in which
different participants may have different opinions about the trustworthiness of
other nodes, and a single participant may consider other nodes to differ in
their trustworthiness. We explore how to construct distributed protocols that
meet the requirements of all participants, even in heterogeneous trust
environments. The key to our approach is using lattice-based information flow
to analyse and prove protocol properties. To demonstrate this approach, we show
how two earlier distributed algorithms can be generalized to work in the
presence of heterogeneous trust: first, Heterogeneous Fast Consensus, an
adaptation of the earlier Bosco Fast Consensus protocol; and second, Nysiad, an
algorithm for converting crash-tolerant protocols to be Byzantine-tolerant.
Through simulations, we show that customizing a protocol to a heterogeneous
trust configuration yields performance improvements over the conventional
protocol designed for homogeneous trust.
","Isaac C. Sheff|Robbert van Renesse|Andrew C. Myers","","http://arxiv.org/abs/1412.3136v1","http://arxiv.org/pdf/1412.3136v1","","This is the technical report of a submission for EuroSys 2015. 26
  Pages 6 figures","","","cs.DC","cs.DC|cs.CR|cs.DB|cs.PL"
"579","1412.3695v1","2014-12-10 16:08:07","2014-12-10 16:08:07","Evaluating arbitration and conflict resolution mechanisms in the Spanish
  Wikipedia","  In open collaborative projects like Wikipedia, interactions among users can
produce tension and misunderstandings. Complex disputes require more
sophisticated mechanisms of conflict resolution. In this paper, we examine the
case of the Spanish Wikipedia and its Arbitration Committee, known as CRC, over
its two years of activity. We postulate that the high percentage of rejections
of cases presented by non-administrators, the lack of diversity inside the
committee (composed only by administrators), and the high number of cases
involving administrators played a central role in its eventual downfall. We
conclude that mechanisms that fail to acknowledge the ecosystem they are part
of cannot succceed. Therefore, further research is needed to determine if
granting more decision-making power to non-administrators may lead to more
effective conflict resolution mechanisms.
","Maria Sefidari|Felipe Ortega","","http://arxiv.org/abs/1412.3695v1","http://arxiv.org/pdf/1412.3695v1","","4 pages, 4 tables","","","cs.CY","cs.CY|H.5.3"
"580","1412.5847v1","2014-12-18 13:17:41","2014-12-18 13:17:41","ConGUSTo: (HT)Condor Graphical Unified Supervising Tool","  HTCondor is a distributed job scheduler developed by the University of
Wisconsin-Madison, which allows users to run their applications in other users'
machines when they are not being used, thus providing a considerably increase
in the overall computational power and a more efficient use of the computing
resources. Our institution has been successfully using HTCondor for more than
ten years, and HTCondor is nowadays the most used Supercomputing resource we
have. Although HTCondor provides a wide range of tools and options for its
management and administration, there are currently no tools that can show
detailed usage information and statistics in a clear, easy to interpret,
interactive set of graphics displays. For this reason, we have developed
ConGUSTo, a web-based tool that allows to collect HTCondor usage and statistics
data in an easy way, and present them using a variety of tabular and graphics
charts.
","Antonio Dorta|Nicola Caon|Jorge Andres Perez Prieto","","http://arxiv.org/abs/1412.5847v1","http://arxiv.org/pdf/1412.5847v1","","8 pages, 10 figures","","","cs.DC","cs.DC"
"581","1412.8013v2","2014-12-27 02:42:39","2015-01-21 02:26:27","An Effective Approach for Mobile ad hoc Network via I-Watchdog Protocol","  Mobile ad hoc network (MANET) is now days become very famous due to their
fixed infrastructure-less quality and dynamic nature. They contain a large
number of nodes which are connected and communicated to each other in wireless
nature. Mobile ad hoc network is a wireless technology that contains high
mobility of nodes and does not depend on the background administrator for
central authority, because they do not contain any infrastructure. Nodes of the
MANET use radio wave for communication and having limited resources and limited
computational power. The Topology of this network is changing very frequently
because they are distributed in nature and self-configurable. Due to its
wireless nature and lack of any central authority in the background, Mobile ad
hoc networks are always vulnerable to some security issues and performance
issues. The security imposes a huge impact on the performance of any network.
Some of the security issues are black hole attack, flooding, wormhole attack
etc. In this paper, we will discuss issues regarding low performance of
Watchdog protocol used in the MANET and proposed an improved Watchdog
mechanism, which is called by I-Watchdog protocol that overcomes the
limitations of Watchdog protocol and gives high performance in terms of
throughput, delay.
","Nidhi Lal","","http://arxiv.org/abs/1412.8013v2","http://arxiv.org/pdf/1412.8013v2","","","","","cs.NI","cs.NI"
"582","1412.8244v2","2014-12-29 02:08:14","2015-01-22 20:55:23","Comment on ""An error-resilient non-volatile magneto-elastic universal
  logic gate with ultralow energy-delay product"" [Sci. Rep. 4, 7553 (2014)]","  arXiv admin note: This submission has been withdrawn by arXiv administrators
due to unprofessional personal attack.
","Kuntal Roy","","http://arxiv.org/abs/1412.8244v2","http://arxiv.org/pdf/1412.8244v2","","arXiv admin note: This submission has been withdrawn by arXiv
  administrators due to unprofessional personal attack","","","cond-mat.mes-hall","cond-mat.mes-hall"
"583","1412.8412v2","2014-12-29 18:28:12","2014-12-31 15:22:04","Sanitization of Call Detail Records via Differentially-private Summaries","  In this work, we initiate the study of human mobility from sanitized call
detail records (CDRs). Such data can be extremely valuable to solve important
societal issues such as the improvement of urban transportation or the
understanding on the spread of diseases. One of the fundamental building block
for such study is the computation of mobility patterns summarizing how
individuals move during a given period from one area e.g., cellular tower or
administrative district) to another. However, such knowledge cannot be
published directly as it has been demonstrated that the access to this type of
data enable the (re-)identification of individuals. To answer this issue and to
foster the development of such applications in a privacy-preserving manner, we
propose in this paper a novel approach in which CDRs are summarized under the
form of a differentially-private Bloom filter for the purpose of privately
counting the number of mobile service users moving from one area (region) to
another in a given time frame. Our sanitization method is both time and space
efficient, and ensures differential privacy while solving the shortcomings of a
solution recently proposed to this problem. We also report on experiments
conducted with the proposed solution using a real life CDRs dataset. The
results obtained show that our method achieves - in most cases - a performance
similar to another method (linear counting sketch) that does not provide any
privacy guarantees. Thus, we conclude that our method maintains a high utility
while providing strong privacy guarantees.
","Mohammad Alaggan|Sebastien Gambs|Stan Matwin|Eriko Souza|Mohammed Tuhin","","http://arxiv.org/abs/1412.8412v2","http://arxiv.org/pdf/1412.8412v2","","Withdrawn due to some possible agreement issues","","","cs.CR","cs.CR"
"584","1501.00242v1","2015-01-01 03:27:08","2015-01-01 03:27:08","Combined 3D PET and Optical Projection Tomography Techniques for Plant
  Root Phenotyping","  New imaging techniques are in great demand for investigating underground
plant roots systems which play an important role in crop production. Compared
with other non-destructive imaging modalities, PET can image plant roots in
natural soil and produce dynamic 3D functional images which reveal the temporal
dynamics of plant-environment interactions. In this study, we combined PET with
optical projection tomography (OPT) to evaluate its potential for plant root
phenotyping. We used a dedicated high resolution plant PET imager that has a 14
cm transaxial and 10 cm axial field of views, and multi-bed imaging capability.
The image resolution is around 1.25 mm using ML-EM reconstruction algorithm.
B73 inbred maize seeds were germinated and then grown in a sealed jar with
transparent gel-based media. PET scanning started on the day when the first
green leaf appeared, and was carried out once a day for 5 days. Each morning,
around 10 mCi of 11CO2 was administrated into a custom built plant labeling
chamber. After 10 minutes, residual activity was flushed out with fresh air
before a 2-h PET scan started. For the OPT imaging, the jar was placed inside
an acrylic cubic container filled with water, illuminated with a uniform
surface light source, and imaged by a DSLR camera from 72 angles to acquire
optical images for OPT reconstruction. The same plant was imaged 3 times a day
by the OPT system. Plant roots growth is measured from the optical images.
Co-registered PET and optical images indicate that most of the hot spots
appeared in later time points of the PET images correspond to the most actively
growing root tips. The strong linear correlation between 11C allocation at root
tips measured by PET and eventual root growth measured by OPT suggests that we
can use PET as a phenotyping tool to measure how a plant makes subterranean
carbon allocation decisions in different environmental scenarios.
","Qiang Wang|Sergey Komarov|Aswin J. Mathews|Ke Li|Christopher Topp|Joseph A. O'Sullivan|Yuan-Chuan Tai","","http://arxiv.org/abs/1501.00242v1","http://arxiv.org/pdf/1501.00242v1","","5 pages, 10 figures","","","q-bio.QM","q-bio.QM|physics.bio-ph"
"585","1501.00310v1","2015-01-01 20:49:13","2015-01-01 20:49:13","Alignment of metabolic trajectories with application to metabonomic
  toxicology","  Geometry of the metabolic trajectories is characteristic of the biological
response (Keun, Ebbels et al. 2004). Yet, due to unavoidable inter-individual
variations, the exact trajectories characterising the biological responses
differ. We examined whether the differences seen between metabolic trajectories
of a specific treatment, correspond to the variations seen in the other
biological manifestations of the same treatment. Differences in trajectories
were measured via alignment procedures which introduced and implemented in this
study. Our study revealed strong correlation between the scales of the aligned
trajectories of metabolic responses and the severity of the hepatocelluar
lesions induced after administration of hydrazine. Thus the results confirm
that aligned trajectories are characteristic of a specific treatment. They then
can be used for comparison with other treatment specific or unknown metabolic
trajectories and can have many metabonomic applications such as preclinical
toxicological screening
","Mansour Taghavi Azar Sharabiani","","http://arxiv.org/abs/1501.00310v1","http://arxiv.org/pdf/1501.00310v1","","Final report of the first project of the Master of Research Programme
  (2005-2006), Imperial College London","","","q-bio.QM","q-bio.QM"
"586","1501.01477v1","2015-01-07 13:01:06","2015-01-07 13:01:06","The emergence of power-law distributions of inter-spike intervals
  characterizes status epilepticus induced by pilocarpine administration in
  rats","  Objective. To ascertain the existence of power-law distributions of
inter-spike intervals (ISI) occurring during the progression of status
epilepticus (SE), so that the emergence of critical states could be reasonably
hypothesized as being part of the intrinsic nature of the SE. Methods. Status
epilepticus was induced by pilocarpine administration in post-natal 21-day rats
(n=8). For each animal, 24 hours of EEG from the onset of the SE were analyzed
according to the analytical procedure suggested by Clauset et al. (2009) which
combines maximum-likelihood fitting methods with goodness-of-fit tests based on
the Kolmogorov-Smirnov statistics and likelihood ratios. The analytical
procedure was implemented by the freely available R software package
""poweRlaw"". Time of calculations was considerably shorten by the exploitation
of High-Throughput Computing technology, a.k.a. Grid Computing technology.
Results. The progression of the SE is characterized by the emergence of
power-law correlations of ISI whose likelihood of occurrence increases the more
the time from the onset of the SE elapses. Log-normal distribution of ISI is
however widely represented. Additionally, undetermined distributions of ISI are
represented as well, although confined within a restricted temporal window. The
final stage of SE appears dominated only by power-law and log-normal
distributions of ISI. Significance. The emergence of power-law correlations of
ISI concretely supports the concept of the occurrence of critical states during
the progression of SE. It is reasonably speculated, as a working hypothesis,
that the occurrence of power-law distributions of ISI within the early stages
of the SE could be a hallmark of the establishment of the route to
epileptogenesis.
","Massimo Rizzi","","http://arxiv.org/abs/1501.01477v1","http://arxiv.org/pdf/1501.01477v1","","","","","q-bio.NC","q-bio.NC"
"587","1501.01526v1","2015-01-07 15:42:59","2015-01-07 15:42:59","Social Interactions vs Revisions, What is important for Promotion in
  Wikipedia?","  In epistemic community, people are said to be selected on their knowledge
contribution to the project (articles, codes, etc.) However, the socialization
process is an important factor for inclusion, sustainability as a contributor,
and promotion. Finally, what does matter to be promoted? being a good
contributor? being a good animator? knowing the boss? We explore this question
looking at the process of election for administrator in the English Wikipedia
community. We modeled the candidates according to their revisions and/or social
attributes. These attributes are used to construct a predictive model of
promotion success, based on the candidates's past behavior, computed thanks to
a random forest algorithm.
  Our model combining knowledge contribution variables and social networking
variables successfully explain 78% of the results which is better than the
former models. It also helps to refine the criterion for election. If the
number of knowledge contributions is the most important element, social
interactions come close second to explain the election. But being connected
with the future peers (the admins) can make the difference between success and
failure, making this epistemic community a very social community too.
","Romain Picot-Clemente|Cecile Bothorel|Nicolas Jullien","","http://arxiv.org/abs/1501.01526v1","http://arxiv.org/pdf/1501.01526v1","","","","","cs.SI","cs.SI"
"588","1501.03215v1","2015-01-14 00:16:25","2015-01-14 00:16:25","Creating, Automating, and Assessing Online Homework in Introductory
  Statistics and Mathematics Classes","  Although textbook publishers offer course management systems, they do so to
promote brand loyalty, and while an open source tool such as WeBWorK is
promising, it requires administrative and IT buy-in. So supported in part by a
College Access Challenge Grant from the Department of Education, we
collaborated with other instructors to create online homework sets for three
classes: Elementary Algebra, Intermediate Algebra, and Statistics for
Behavioral Sciences I. After experimentation, some of these question pools are
now created by Mathematica programs that can generate data sets from specified
distributions, generate random polynomials that factor in a given way, create
image files of histograms, scatterplots, and so forth. These programs produce
files that can be read by the software package, Respondus, which then uploads
the questions into Blackboard Learn, the course management system used by the
Connecticut State University system. Finally, we summarize five classes worth
of student performance data along with lessons learned while working on this
project.
","Karen Santoro|Roger Bilisoly","","http://arxiv.org/abs/1501.03215v1","http://arxiv.org/pdf/1501.03215v1","","12 pages, 9 figures. Based on a presentation at the Joint Statistical
  Meetings, Section on Statistical Education, August, 2014, in Boston,
  Massachusetts, USA","","","stat.OT","stat.OT|math.HO"
"589","1501.04509v1","2015-01-19 14:59:10","2015-01-19 14:59:10","Responding to Retrieval: A Proposal to Use Retrieval Information for
  Better Presentation of Website Content","  Retrieval and content management are assumed to be mutually exclusive. In
this paper we suggest that they need not be so. In the usual information
retrieval scenario, some information about queries leading to a website (due to
`hits' or `visits') is available to the server administrator of the concerned
website. This information can used to better present the content on the
website. Further, we suggest that some more information can be shared by the
retrieval system with the content provider. This will enable the content
provider (any website) to have a more dynamic presentation of the content that
is in tune with the query trends, without violating the privacy of the querying
user. The result will be a better synchronization between retrieval systems and
content providers, with the purpose of improving the user's web search
experience. This will also give the content provider a say in this process,
given that the content provider is the one who knows much more about the
content than the retrieval system. It also means that the content presentation
may change in response to a query. In the end, the user will be able to find
the relevant content more easily and quickly.
","C Ravindranath Chowdary|Anil Kumar Singh|Anil Nelakanti","","http://arxiv.org/abs/1501.04509v1","http://arxiv.org/pdf/1501.04509v1","http://dx.doi.org/10.1007/978-3-319-24800-4_9","","Current Trends in Web Engineering. ICWE 2015. Lecture Notes in
  Computer Science, PP. 103--114, vol 9396. Springer, Cham","10.1007/978-3-319-24800-4_9","cs.IR","cs.IR"
"590","1501.05009v2","2015-01-20 22:44:04","2015-01-23 18:26:39","Comment on two recent arXiv postings [arXiv:1412.2200 and
  arXiv:1412.8244]","  This is a comment on two recent arXiv postings.
","Supriyo Bandyopadhyay|Jayasimha Atulasimha","","http://arxiv.org/abs/1501.05009v2","http://arxiv.org/pdf/1501.05009v2","","This posting was a response to two earlier postings (by the same
  author) containing personal attacks on us. Since those two offending postings
  have been administratively removed by arXiv because of unprofessional
  personal attacks, our response has become superfluous and is hence being
  withdrawn","","","cond-mat.mes-hall","cond-mat.mes-hall"
"591","1501.05414v1","2015-01-22 07:53:11","2015-01-22 07:53:11","A Task-Type-Based Algorithm for the Energy-Aware Profit Maximizing
  Scheduling Problem in Heterogeneous Computing Systems","  In this paper, we design an efficient algorithm for the energy-aware profit
maximizing scheduling problem, where the high performance computing system
administrator is to maximize the profit per unit time. The running time of the
proposed algorithm is depending on the number of task types, while the running
time of the previous algorithm is depending on the number of tasks. Moreover,
we prove that the worst-case performance ratio is close to 2, which maybe the
best result. Simulation experiments show that the proposed algorithm is more
accurate than the previous method.
","Weidong Li|Xi Liu|Xuejie Zhang|Xiaobo Cai","","http://arxiv.org/abs/1501.05414v1","http://arxiv.org/pdf/1501.05414v1","","","","","cs.DC","cs.DC|cs.DS"
"592","1501.06340v1","2015-01-26 11:31:55","2015-01-26 11:31:55","National Report for the International Association of Geodesy of the
  International Union of Geodesy and Geophysics 2007-2010","  This report submitted to the International Association of Geodesy (IAG) of
the International Union of Geodesy and Geophysics (IUGG) contains results
obtained by Russian geodesists in 2007-2010. In the report prepared for the XXV
General Assembly of IUGG (Australia, Melbourne, 28 June - 7 July 2011), the
results of principal researches in geodesy, geodynamics, gravimetry, in the
studies of geodetic reference frame creation and development, Earth's shape and
gravity field, Earth's rotation, geodetic theory, its application and some
other directions are briefly described. The period from 2007 to 2010 was still
difficult for Russian geodesy mainly due to the permanent reformation of state
geodetic administration as well as state education structure and organization.
The report is organized as a sequence of abstracts of principal publications
and presentations for symposia, conferences, workshops, etc. Each of the report
paragraphs includes a list of scientific papers published in 2007-2010
including those prepared in cooperation of Russian scientists and their
colleagues from other countries. Some interesting international and national
scientific events are reflected in the text too. For some objective reasons not
all results obtained by Russian scientists on the problems of geodesy are
included in the report.
","E. A. Boyarsky|L. F. Vitushkin|M. D. Gerasimenko|G. V. Demianov|M. B. Kaufman|V. I. Kaftan|E. M. Mazurova|Z. M. Malkin|S. M. Molodenskii|Yu. M. Neyman|A. K. Pevnev|V. P. Savinykh|G. M. Steblov|S. K. Tatevian|S. A. Tolchel'nikova|N. V Shestakov","","http://arxiv.org/abs/1501.06340v1","http://arxiv.org/pdf/1501.06340v1","","","Geo Science, 2011, No. 1, pp. 5-36","","physics.geo-ph","physics.geo-ph"
"593","1501.07506v1","2015-01-29 16:47:10","2015-01-29 16:47:10","Accuracy of areal interpolation methods for count data","  The combination of several socio-economic data bases originating from
different administrative sources collected on several different partitions of a
geographic zone of interest into administrative units induces the so called
areal interpolation problem. This problem is that of allocating the data from a
set of source spatial units to a set of target spatial units. A particular case
of that problem is the re-allocation to a single target partition which is a
regular grid. At the European level for example, the EU directive 'INSPIRE', or
INfrastructure for SPatial InfoRmation, encourages the states to provide
socio-economic data on a common grid to facilitate economic studies across
states. In the literature, there are three main types of such techniques:
proportional weighting schemes, smoothing techniques and regression based
interpolation. We propose a stochastic model based on Poisson point patterns to
study the statistical accuracy of these techniques for regular grid targets in
the case of count data. The error depends on the nature of the target variable
and its correlation with the auxiliary variable. For simplicity, we restrict
attention to proportional weighting schemes and Poisson regression based
methods. Our conclusion is that there is no technique which always dominates.
","Van Huyen Do|Christine Thomas-Agnan|Anne Vanhems","","http://arxiv.org/abs/1501.07506v1","http://arxiv.org/pdf/1501.07506v1","","","","","stat.ME","stat.ME"
"594","1502.00172v3","2015-01-31 22:10:50","2018-10-01 14:41:48","Leakage-resilient Cryptography with key derived from sensitive data","  In this paper we address the problem of large space consumption for protocols
in the Bounded Retrieval Model (BRM), which require users to store large secret
keys subject to adversarial leakage. We propose a method to derive keys for
such protocols on-the-fly from weakly random private data (like text documents
or photos, users keep on their disks anyway for non-cryptographic purposes) in
such a way that no extra storage is needed. We prove that any leakage-resilient
protocol (belonging to a certain, arguably quite broad class) when run with a
key obtained this way retains a similar level of security as the original
protocol had. Additionally, we guarantee privacy of the data the actual keys
are derived from. That is, an adversary can hardly gain any knowledge about the
private data except that he could otherwise obtain via leakage. Our reduction
works in the Random Oracle model.
","Konrad Durnoga|Tomasz Kazana|Micha©© Zaj<U+0105>c|Maciej Zdanowicz","","http://arxiv.org/abs/1502.00172v3","http://arxiv.org/pdf/1502.00172v3","","Withdrawn by arXiv administrators as version 2 is missing the
  required authorship and affiliation in the full text","","","cs.CR","cs.CR|cs.IT|math.IT"
"595","1502.02065v1","2015-02-06 22:09:19","2015-02-06 22:09:19","Participatory Militias: An Analysis of an Armed Movement's Online
  Audience","  Armed groups of civilians known as ""self-defense forces"" have ousted the
powerful Knights Templar drug cartel from several towns in Michoacan. This
militia uprising has unfolded on social media, particularly in the ""VXM""
(""Valor por Michoacan,"" Spanish for ""Courage for Michoacan"") Facebook page,
gathering more than 170,000 fans. Previous work on the Drug War has documented
the use of social media for real-time reports of violent clashes. However, VXM
goes one step further by taking on a pro-militia propagandist role, engaging in
two-way communication with its audience. This paper presents a descriptive
analysis of VXM and its audience. We examined nine months of posts, from VXM's
inception until May 2014, totaling 6,000 posts by VXM administrators and more
than 108,000 comments from its audience. We describe the main conversation
themes, post frequency and relationships with offline events and public
figures. We also characterize the behavior of VXM's most active audience
members. Our work illustrates VXM's online mobilization strategies, and how its
audience takes part in defining the narrative of this armed conflict. We
conclude by discussing possible applications of our findings for the design of
future communication technologies.
","Saiph Savage|Andres Monroy-Hernandez","","http://arxiv.org/abs/1502.02065v1","http://arxiv.org/pdf/1502.02065v1","http://dx.doi.org/10.1145/2675133.2675295","Participatory Militias: An Analysis of an Armed Movement's Online
  Audience. Saiph Savage, Andres Monroy-Hernandez. CSCW: ACM Conference on
  Computer-Supported Cooperative Work 2015","","10.1145/2675133.2675295","cs.SI","cs.SI|cs.CY|H.5.3"
"596","1502.03431v1","2015-02-11 20:36:30","2015-02-11 20:36:30","Time Petri Net Models for a New Queuless and Uncentralized Resource
  Discovery System","  In this report, we detail the model using Petri Nets of a new fully
distributed resource reservation system. The basic idea of the considered
distributed system is to let a user reserve a set of resources on a local
network and to use them, without any specific, central administration component
such as a front-end node. Resources can be, for instance, computing resources
(cores, nodes, GPUs...) or some memory on a server. In order to verify some
qualitative and quantitative properties provided by this system, we need to
model it. We detail the algorithms used by this system and the Petri Net models
we made of it.
","Camille Coti|Sami Evangelista|Kais Klai","","http://arxiv.org/abs/1502.03431v1","http://arxiv.org/pdf/1502.03431v1","","","","","cs.DC","cs.DC"
"597","1502.03609v1","2015-02-12 11:37:19","2015-02-12 11:37:19","Correcting for non-ignorable missingness in smoking trends","  Data missing not at random (MNAR) is a major challenge in survey sampling. We
propose an approach based on registry data to deal with non-ignorable
missingness in health examination surveys. The approach relies on follow-up
data available from administrative registers several years after the survey.
For illustration we use data on smoking prevalence in Finnish National FINRISK
study conducted in 1972-1997. The data consist of measured survey information
including missingness indicators, register-based background information and
register-based time-to-disease survival data. The parameters of missingness
mechanism are estimable with these data although the original survey data are
MNAR. The underlying data generation process is modelled by a Bayesian model.
The results indicate that the estimated smoking prevalence rates in Finland may
be significantly affected by missing data.
","Juho Kopra|Tommi Harkanen|Hanna Tolonen|Juha Karvanen","","http://arxiv.org/abs/1502.03609v1","http://arxiv.org/pdf/1502.03609v1","http://dx.doi.org/10.1002/sta4.73","in Stat, 2015","","10.1002/sta4.73","stat.AP","stat.AP|stat.ME"
"598","1503.01087v4","2015-02-26 19:51:47","2016-07-04 18:05:40","Fundamental properties of High-Mass X-ray Binaries","  The aim of this PhD Thesis is to characterize a representative sample of
Supergiant X-ray Binaries (SGXBs) formed by 4 sources: XTE J1855-026, a
classical SGXB with long-term stable X-ray flux; AX J1841.0-0535 and AX
J1845.0-0433, two supergiant fast X-ray transients (SFXTs) with the X-ray
emission mostly dominated by flaring; and IGR J00370+6122, something in between
these 2 sub-groups. The physical processes that produce these observable
differences are still a matter of debate. In this PhD Thesis I performed a
study of these 4 different systems to provide new data to constrain the models.
This study consists of:(i) the determination of the orbital solution,(ii) a
systematic study of the wind behavior along the orbit by the measure of Halpha
variations,(iii) a model of stellar atmospheres of the donor star,(iv)
establish whether there are X-ray flux variations modulated by the orbital
period. The study of the wind shows that Halpha variations are dominated by
intrinsic wind processes. The stellar atmospheres study shows that the
supergiant stars that harbor these binaries have a higher projected rotational
velocity, higher He abundance and higher N/C ratio than that of isolated
supergiant stars. The results of this study show that the eccentricity of the
binary does not have a simple correlation with the differences in the X-ray
flux of the different sub-groups. Therefore, the idea of a more complex
scenario is consolidated. The discovery of a new type of system, IGR
J00370+6122, which properties do not fit in any of the established sub-groups,
reinforces the idea of a continuum in the observed properties more than a
strict classification of systems. Furthermore, I have developed a pipeline to
reduce spectra of the FRODOSpec spectrograph at the Liverpool Telescope
optimized for the reduction of the red spectra of the obscured supergiants that
we find in these SGXBs.
","A. Gonzalez-Galan","","http://arxiv.org/abs/1503.01087v4","http://arxiv.org/pdf/1503.01087v4","","This article has been withdrawn by arXiv administrators due to a
  deliberately incomplete replacement by the author to avoid overlap detection,
  in violation of submission policy. For the complete text see
  arXiv:1503.01087v2","","","astro-ph.HE","astro-ph.HE"
"599","1503.02732v1","2015-03-09 23:30:26","2015-03-09 23:30:26","Detecting Incompleteness, Conflicting and Unreachability XACML Policies
  using Answer Set Programming","  Recently, XACML is a popular access control policy language that is used
widely in many applications. Policies in XACML are built based on many
components over distributed resources. Due to the expressiveness of XACML, it
is not trivial for policy administrators to understand the overall effect and
consequences of XACML policies they have written. In this paper we show a
mechanism and a tool how to analyses big access control policies sets such as
(i) incompleteness policies, (ii) conflicting policies, and (iii) unreachable
policies. To detect these problems we present a method using Answer Set
Programming (ASP) in the context of XACML 3.0.
","Carroline Dewi Puspa Kencana Ramli","","http://arxiv.org/abs/1503.02732v1","http://arxiv.org/pdf/1503.02732v1","","arXiv admin note: text overlap with arXiv:1206.5327","","","cs.CR","cs.CR"
"600","1503.03297v2","2015-03-11 12:19:40","2015-03-12 13:03:38","Uncertainty in Test Score Data and Classically Defined Reliability of
  Tests and Test Batteries, using a New Method for Test Dichotomisation","  As with all measurements, the measurement of examinee ability, in terms of
scores that the examinee obtains in a test, is also error-ridden. The
quantification of such error or uncertainty in the test score data--or rather
the complementary test reliability--is pursued within the paradigm of Classical
Test Theory in a variety of ways, with no existing method of finding
reliability, isomorphic to the theoretical definition that parametrises
reliability as the ratio of the true score variance and observed (i.e.
error-ridden) score variance. Thus, multiple reliability coefficients for the
same test have been advanced. This paper describes a much needed method of
obtaining reliability of a test as per its theoretical definition, via a single
administration of the test, by using a new fast method of splitting of a given
test into parallel halves, achieving near-coincident empirical distributions of
the two halves. The method has the desirable property of achieving splitting on
the basis of difficulty of the questions (or items) that constitute the test,
thus allowing for fast computation of reliability even for very large test data
sets, i.e. test data obtained by a very large examinee sample. An interval
estimate for the true score is offered, given an examinee score, subsequent to
the determination of the test reliability. This method of finding test
reliability as per the classical definition can be extended to find reliability
of a set or battery of tests; a method for determination of the weights
implemented in the computation of the weighted battery score is discussed. We
perform empirical illustration of our method on real and simulated tests, and
on a real test battery comprising two constituent tests.
","Satyendra Nath Chakrabartty|Kangrui Wang|Dalia Chakrabarty","","http://arxiv.org/abs/1503.03297v2","http://arxiv.org/pdf/1503.03297v2","","30 pages","","","stat.AP","stat.AP"
"601","1503.04277v1","2015-03-14 06:33:19","2015-03-14 06:33:19","Approximate Discovery of Service Nodes by Duplicate Detection in Flows","  Knowledge about which nodes provide services is of critical importance for
network administrators. Discovery of service nodes can be done by making full
use of duplicate element detection in flows. Because the amount of traffic
across network is massive, especially in large ISPs or campus networks, we
propose an approximate algorithm with Round-robin Buddy Bloom Filters(RBBF) for
service detection using NetFlow data solely. The properties and analysis of
RBBF data structure are also given. Our method has better time/space efficiency
than conventional algorithm with a small false positive rate.%portion of false
positive. We also demonstrate the contributions through a prototype system by
real world case studies.
","Zhou Changling|Xiao Jianguo|Cui Jian|Zhang Bei|Li Feng","","http://arxiv.org/abs/1503.04277v1","http://arxiv.org/pdf/1503.04277v1","","15 pages","China Communications, 2012, 9(5): 75-89","","cs.NI","cs.NI"
"602","1503.04864v1","2015-03-16 21:35:18","2015-03-16 21:35:18","GeomRDF: A Geodata Converter with a Fine-Grained Structured
  Representation of Geometry in the Web","  In recent years, with the advent of the web of data, a growing number of
national mapping agencies tend to publish their geospatial data as Linked Data.
However, differences between traditional GIS data models and Linked Data model
can make the publication process more complicated. Besides, it may require, to
be done, the setting of several parameters and some expertise in the semantic
web technologies. In addition, the use of standards like GeoSPARQL (or ad hoc
predicates) is mandatory to perform spatial queries on published geospatial
data. In this paper, we present GeomRDF, a tool that helps users to convert
spatial data from traditional GIS formats to RDF model easily. It generates
geometries represented as GeoSPARQL WKT literal but also as structured
geometries that can be exploited by using only the RDF query language, SPARQL.
GeomRDF was implemented as a module in the RDF publication platform Datalift. A
validation of GeomRDF has been realized against the French administrative units
dataset (provided by IGN France).
","Faycal Hamdi|Nathalie Abadie|Benedicte Bucher|Abdelfettah Feliachi","","http://arxiv.org/abs/1503.04864v1","http://arxiv.org/pdf/1503.04864v1","","12 pages, 2 figures, the 1st International Workshop on Geospatial
  Linked Data (GeoLD 2014) - SEMANTiCS 2014","","","cs.DB","cs.DB|cs.AI"
"603","1503.06154v1","2015-03-20 16:47:51","2015-03-20 16:47:51","Relationship-Based Access Control for OpenMRS","  Inspired by the access control models of social network systems,
Relationship-Based Access Control (ReBAC) was recently proposed as a
general-purpose access control paradigm for application domains in which
authorization must take into account the relationship between the access
requestor and the resource owner. The healthcare domain is envisioned to be an
archetypical application domain in which ReBAC is sorely needed: e.g., my
patient record should be accessible only by my family doctor, but not by all
doctors.
  In this work, we demonstrate for the first time that ReBAC can be
incorporated into a production-scale medical records system, OpenMRS, with
backward compatibility to the legacy RBAC mechanism. Specifically, we extend
the access control mechanism of OpenMRS to enforce ReBAC policies. Our
extensions incorporate and extend advanced ReBAC features recently proposed by
Crampton and Sellwood. In addition, we designed and implemented the first
administrative model for ReBAC. In this paper, we describe our ReBAC
implementation, discuss the system engineering lessons learnt as a result, and
evaluate the experimental work we have undertaken. In particular, we compare
the performance of the various authorization schemes we implemented, thereby
demonstrating the feasibility of ReBAC.
","Syed Zain Rizvi|Philip W. L. Fong|Jason Crampton|James Sellwood","","http://arxiv.org/abs/1503.06154v1","http://arxiv.org/pdf/1503.06154v1","","","","","cs.CR","cs.CR"
"604","1503.06654v1","2015-03-23 14:29:34","2015-03-23 14:29:34","United Nations Basic Space Science Initiative (UNBSSI) 1991-2012 and
  Beyond","  This paper contains an overview and summary on the achievements of the United
Nations basic space science initiative in terms of donated and provided
planetariums, astronomical telescopes, and space weather instruments,
particularly operating in developing nations. This scientific equipment has
been made available to respective host countries, particularly developing
nations, through the series of twenty basic space science workshops, organized
through the United Nations Programme on Space Applications since 1991.
Organized by the United Nations, the European Space Agency (ESA), the National
Aeronautics and Space Administration (NASA) of the United States of America,
and the Japan Aerospace Exploration Agency (JAXA), the basic space science
workshops were organized as a series of workshops that focused on basic space
science (1991-2004), the International Heliophysical Year 2007 (2005-2009), and
the International Space Weather Initiative (2010-2012) proposed by the
Committee on the Peaceful Uses of Outer Space on the basis of discussions of
its Scientific and Technical Subcommittee, as reflected in the reports of the
Subcommittee.
","A. M. Mathai|H. J. Haubold|W. R. Balogh","","http://arxiv.org/abs/1503.06654v1","http://arxiv.org/pdf/1503.06654v1","","51 pages","","","physics.space-ph","physics.space-ph"
"605","1503.07568v2","2015-03-25 22:28:21","2015-03-27 13:31:51","Router-level community structure of the Internet Autonomous Systems","  The Internet is composed of routing devices connected between them and
organized into independent administrative entities: the Autonomous Systems. The
existence of different types of Autonomous Systems (like large connectivity
providers, Internet Service Providers or universities) together with
geographical and economical constraints, turns the Internet into a complex
modular and hierarchical network. This organization is reflected in many
properties of the Internet topology, like its high degree of clustering and its
robustness.
  In this work, we study the modular structure of the Internet router-level
graph in order to assess to what extent the Autonomous Systems satisfy some of
the known notions of community structure. We show that the modular structure of
the Internet is much richer than what can be captured by the current community
detection methods, which are severely affected by resolution limits and by the
heterogeneity of the Autonomous Systems. Here we overcome this issue by using a
multiresolution detection algorithm combined with a small sample of nodes. We
also discuss recent work on community structure in the light of our results.
","Mariano G. Beiro|Sebastian P. Grynberg|J. Ignacio Alvarez-Hamelin","","http://arxiv.org/abs/1503.07568v2","http://arxiv.org/pdf/1503.07568v2","http://dx.doi.org/10.1140/epjds/s13688-015-0048-y","","","10.1140/epjds/s13688-015-0048-y","cs.NI","cs.NI|cs.DS|cs.SI|physics.data-an|68U99, 05C85|I.5.3; G.2.2"
"606","1503.07757v2","2015-03-26 14:51:17","2015-08-13 08:13:07","Geographies of an online social network","  How is online social media activity structured in the geographical space?
Recent studies have shown that in spite of earlier visions about the ""death of
distance"", physical proximity is still a major factor in social tie formation
and maintenance in virtual social networks. Yet, it is unclear, what are the
characteristics of the distance dependence in online social networks. In order
to explore this issue the complete network of the former major Hungarian online
social network is analyzed. We find that the distance dependence is weaker for
the online social network ties than what was found earlier for phone
communication networks. For a further analysis we introduced a coarser
granularity: We identified the settlements with the nodes of a network and
assigned two kinds of weights to the links between them. When the weights are
proportional to the number of contacts we observed weakly formed, but spatially
based modules resembling to the borders of macro-regions, the highest level of
regional administration in the country. If the weights are defined relative to
an uncorrelated null model, the next level of administrative regions, counties
are reflected.
","Balazs Lengyel|Attila Varga|Bence Sagvari|Akos Jakobi|Janos Kertesz","","http://arxiv.org/abs/1503.07757v2","http://arxiv.org/pdf/1503.07757v2","http://dx.doi.org/10.1371/journal.pone.0137248","19 pages pdf file. Minor changes mainly about users abroad plus maps
  redrawn","","10.1371/journal.pone.0137248","physics.soc-ph","physics.soc-ph|cs.SI"
"607","1503.09149v1","2015-03-31 18:09:28","2015-03-31 18:09:28","Mass campaigns with antimalarial drugs: a modelling comparison of
  artemether-lumefantrine and DHA-piperaquine with and without primaquine as
  tools for malaria control and elimination","  Antimalarial drugs are a powerful tool for malaria control and elimination.
Artemisinin-based combination therapies (ACTs) can reduce transmission when
widely distributed in a campaign setting. Modelling mass antimalarial campaigns
can elucidate how to most effectively deploy drug-based interventions and
quantitatively compare the effects of cure, prophylaxis, and
transmission-blocking in suppressing parasite prevalence. A previously
established agent-based model that includes innate and adaptive immunity was
used to simulate malaria infections and transmission. Pharmacokinetics of
artemether, lumefantrine, dihydroartemisinin, piperaquine, and primaquine were
modelled with a double-exponential distribution-elimination model including
weight-dependent parameters and age-dependent dosing. Drug killing of asexual
parasites and gametocytes was calibrated to clinical data. Mass distribution of
ACTs and primaquine was simulated with seasonal mosquito dynamics at a range of
transmission intensities. A single mass campaign with antimalarial drugs is
insufficient to permanently reduce malaria prevalence when transmission is
high. Current diagnostics are insufficiently sensitive to accurately identify
asymptomatic infections, and mass-screen-and-treat campaigns are much less
efficacious than mass drug administrations. Improving campaign coverage leads
to decreased prevalence one month after the end of the campaign, while
increasing compliance lengthens the duration of protection against reinfection.
Use of a long-lasting prophylactic as part of a mass drug administration
regimen confers the most benefit under conditions of high transmission and
moderately high coverage. Addition of primaquine can reduce prevalence but
exerts its largest effect when coupled with a long-lasting prophylactic.
","Jaline Gerardin|Philip Eckhoff|Edward A. Wenger","","http://arxiv.org/abs/1503.09149v1","http://arxiv.org/pdf/1503.09149v1","http://dx.doi.org/10.1186/s12879-015-0887-y","14 pages, 5 figures","Gerardin et al. BMC Infectious Diseases (2015) 15:144","10.1186/s12879-015-0887-y","q-bio.PE","q-bio.PE"
"608","1504.00229v1","2015-04-01 13:41:30","2015-04-01 13:41:30","Modelling and Managing SSD Write-amplification","  How stable is the performance of your flash-based Solid State Drives (SSDs)?
This question is central for database designers and administrators, cloud
service providers, and SSD constructors. The answer depends on
write-amplification, i.e., garbage collection overhead. More specifically, the
answer depends on how write-amplification evolves in time.
  How then can one model and manage write-amplification, especially when
application workloads change? This is the focus of this paper. Managing
write-amplification boils down to managing the surplus physical space, called
over-provisioned space. Modern SSDs essentially separate the physical space
into several partitions, based on the update frequency of the pages they
contain, and divide the over-provisioned space among the groups so as to
minimize write-amplification. We introduce Wolf, a block manager that allocates
over-provisioned space to SSD partitions using a near-optimal closed-form
expression, based on the sizes and update frequencies of groups of pages. Our
evaluation shows that Wolf is robust to workloads change, with an improvement
factor of 2 with respect to the state-of-the-art. We also show that Wolf
performs comparably and even slightly better than the state of the art with
stable workloads (over 20% improvement with a TPC-C workload).
","Niv Dayan|Luc Bouganim|Philippe Bonnet","","http://arxiv.org/abs/1504.00229v1","http://arxiv.org/pdf/1504.00229v1","","","","","cs.DB","cs.DB"
"609","1504.02124v2","2015-04-08 20:47:12","2017-05-26 01:37:58","Hyak Mortality Monitoring System: Innovative Sampling and Estimation
  Methods - Proof of Concept by Simulation","  Traditionally health statistics are derived from civil and/or vital
registration. Civil registration in low-income countries varies from partial
coverage to essentially nothing at all. Consequently the state of the art for
public health information in low-income countries is efforts to combine or
triangulate data from different sources to produce a more complete picture
across both time and space - data amalgamation. Data sources amenable to this
approach include sample surveys, sample registration systems, health and
demographic surveillance systems, administrative records, census records,
health facility records and others.
  We propose a new statistical framework for gathering health and population
data - Hyak - that leverages the benefits of sampling and longitudinal,
prospective surveillance to create a cheap, accurate, sustainable monitoring
platform. Hyak has three fundamental components:
  1) Data Amalgamation: a sampling and surveillance component that organizes
two or more data collection systems to work together: a) data from HDSS with
frequent, intense, linked, prospective follow-up and b) data from sample
surveys conducted in large areas surrounding the Health and Demographic
Surveillance System sites using informed sampling so as to capture as many
events as possible;
  2) Cause of Death: verbal autopsy to characterize the distribution of deaths
by cause at the population level; and
  3) SES: measurement of socioeconomic status in order to characterize poverty
and wealth.
  We conduct a simulation study of the informed sampling component of Hyak
based on the Agincourt HDSS site in South Africa. Compared to traditional
cluster sampling, Hyak's informed sampling captures more deaths, and when
combined with an estimation model that includes spatial smoothing, produces
estimates mortality that have lower variance and small bias.
","Samuel J. Clark|Jon Wakefield|Tyler McCormick|Michelle Ross","","http://arxiv.org/abs/1504.02124v2","http://arxiv.org/pdf/1504.02124v2","","Updated version including new simulation study with two-stage cluster
  and optimum allocation sampling","","","stat.OT","stat.OT|stat.AP"
"610","1504.02615v1","2015-04-10 09:41:30","2015-04-10 09:41:30","Detecting and Refactoring Operational Smells within the Domain Name
  System","  The Domain Name System (DNS) is one of the most important components of the
Internet infrastructure. DNS relies on a delegation-based architecture, where
resolution of names to their IP addresses requires resolving the names of the
servers responsible for those names. The recursive structures of the inter
dependencies that exist between name servers associated with each zone are
called dependency graphs. System administrators' operational decisions have far
reaching effects on the DNSs qualities. They need to be soundly made to create
a balance between the availability, security and resilience of the system. We
utilize dependency graphs to identify, detect and catalogue operational bad
smells. Our method deals with smells on a high-level of abstraction using a
consistent taxonomy and reusable vocabulary, defined by a DNS Operational
Model. The method will be used to build a diagnostic advisory tool that will
detect configuration changes that might decrease the robustness or security
posture of domain names before they become into production.
","Marwan Radwan|Reiko Heckel","University of Leicester|University of Leicester","http://arxiv.org/abs/1504.02615v1","http://arxiv.org/pdf/1504.02615v1","http://dx.doi.org/10.4204/EPTCS.181.8","In Proceedings GaM 2015, arXiv:1504.02448","EPTCS 181, 2015, pp. 113-128","10.4204/EPTCS.181.8","cs.NI","cs.NI|cs.CR"
"611","1504.04796v2","2015-04-19 05:40:44","2015-10-30 06:39:11","Infection Spreading and Source Identification: A Hide and Seek Game","  The goal of an infection source node (e.g., a rumor or computer virus source)
in a network is to spread its infection to as many nodes as possible, while
remaining hidden from the network administrator. On the other hand, the network
administrator aims to identify the source node based on knowledge of which
nodes have been infected. We model the infection spreading and source
identification problem as a strategic game, where the infection source and the
network administrator are the two players. As the Jordan center estimator is a
minimax source estimator that has been shown to be robust in recent works, we
assume that the network administrator utilizes a source estimation strategy
that can probe any nodes within a given radius of the Jordan center. Given any
estimation strategy, we design a best-response infection strategy for the
source. Given any infection strategy, we design a best-response estimation
strategy for the network administrator. We derive conditions under which a Nash
equilibrium of the strategic game exists. Simulations in both synthetic and
real-world networks demonstrate that our proposed infection strategy infects
more nodes while maintaining the same safety margin between the true source
node and the Jordan center source estimator.
","Wuqiong Luo|Wee Peng Tay|Mei Leng","","http://arxiv.org/abs/1504.04796v2","http://arxiv.org/pdf/1504.04796v2","http://dx.doi.org/10.1109/TSP.2016.2558168","","","10.1109/TSP.2016.2558168","cs.SI","cs.SI"
"612","1504.06011v3","2015-04-23 00:07:31","2015-08-07 23:30:51","A Robust Approach to Chance Constrained Optimal Power Flow with
  Renewable Generation","  Optimal Power Flow (OPF) dispatches controllable generation at minimum cost
subject to operational constraints on generation and transmission assets. The
uncertainty and variability of intermittent renewable generation is challenging
current deterministic OPF approaches. Recent formulations of OPF use chance
constraints to limit the risk from renewable generation uncertainty, however,
these new approaches typically assume the probability distributions which
characterize the uncertainty and variability are known exactly. We formulate a
Robust Chance Constrained (RCC) OPF that accounts for uncertainty in the
parameters of these probability distributions by allowing them to be within an
uncertainty set. The RCC OPF is solved using a cutting-plane algorithm that
scales to large power systems. We demonstrate the RRC OPF on a modified model
of the Bonneville Power Administration network, which includes 2209 buses and
176 controllable generators. Deterministic, chance constrained (CC), and RCC
OPF formulations are compared using several metrics including cost of
generation, area control error, ramping of controllable generators, and
occurrence of transmission line overloads as well as the respective
computational performance.
","Miles Lubin|Yury Dvorkin|Scott Backhaus","","http://arxiv.org/abs/1504.06011v3","http://arxiv.org/pdf/1504.06011v3","http://dx.doi.org/10.1109/TPWRS.2015.2499753","","","10.1109/TPWRS.2015.2499753","math.OC","math.OC"
"613","1504.06836v1","2015-04-26 14:57:05","2015-04-26 14:57:05","Monitoring Extreme-scale Lustre Toolkit","  We discuss the design and ongoing development of the Monitoring Extreme-scale
Lustre Toolkit (MELT), a unified Lustre performance monitoring and analysis
infrastructure that provides continuous, low-overhead summary information on
the health and performance of Lustre, as well as on-demand, in- depth problem
diagnosis and root-cause analysis. The MELT infrastructure leverages a
distributed overlay network to enable monitoring of center-wide Lustre
filesystems where clients are located across many network domains. We preview
interactive command-line utilities that help administrators and users to
observe Lustre performance at various levels of resolution, from individual
servers or clients to whole filesystems, including job-level reporting.
Finally, we discuss our future plans for automating the root-cause analysis of
common Lustre performance problems.
","Michael J. Brim|Joshua K. Lothian","","http://arxiv.org/abs/1504.06836v1","http://arxiv.org/pdf/1504.06836v1","","International Workshop on the Lustre Ecosystem: Challenges and
  Opportunities, March 2015, Annapolis MD","","","cs.DC","cs.DC|cs.OS"
"614","1504.07012v1","2015-04-27 10:11:14","2015-04-27 10:11:14","Information Technology in New Zealand: Review of Emerging Social Trends,
  Current Issues, and Policies","  This paper discusses the general state of information technology in New
Zealand society, current issues, and policies. It is a qualitative study that
reviews recent scholarly articles, periodicals, and surveys in order to create
an understanding of some of the information technology issues and trends in New
Zealand. After reviewing previous research, it assesses the potential existence
and nature of a 'digital divide' in New Zealand society whilst also evaluating
possible strategic responses to the issue. New Zealand society has rapidly
accepted emerging online trends as well as achieving an overall high level of
Internet provision nationally. Through government policy and education, this
small island nation has remained at the forefront of information technology and
can be considered somewhat of an e-democracy. However, despite these positives,
there is a risk of low-income communities being left behind as New Zealand
society becomes increasingly dependent on IT in the workplace and in
governmental administration.
","Emre Erturk|Derwyn Fail","","http://arxiv.org/abs/1504.07012v1","http://arxiv.org/pdf/1504.07012v1","","","","","cs.CY","cs.CY"
"615","1504.07135v2","2015-04-27 15:39:17","2015-07-08 23:09:20","Systems-theoretic Safety Assessment of Robotic Telesurgical Systems","  Robotic telesurgical systems are one of the most complex medical
cyber-physical systems on the market, and have been used in over 1.75 million
procedures during the last decade. Despite significant improvements in design
of robotic surgical systems through the years, there have been ongoing
occurrences of safety incidents during procedures that negatively impact
patients. This paper presents an approach for systems-theoretic safety
assessment of robotic telesurgical systems using software-implemented
fault-injection. We used a systemstheoretic hazard analysis technique (STPA) to
identify the potential safety hazard scenarios and their contributing causes in
RAVEN II robot, an open-source robotic surgical platform. We integrated the
robot control software with a softwareimplemented fault-injection engine which
measures the resilience of the system to the identified safety hazard scenarios
by automatically inserting faults into different parts of the robot control
software. Representative hazard scenarios from real robotic surgery incidents
reported to the U.S. Food and Drug Administration (FDA) MAUDE database were
used to demonstrate the feasibility of the proposed approach for safety-based
design of robotic telesurgical systems.
","Homa Alemzadeh|Daniel Chen|Andrew Lewis|Zbigniew Kalbarczyk|Jaishankar Raman|Nancy Leveson|Ravishankar K. Iyer","","http://arxiv.org/abs/1504.07135v2","http://arxiv.org/pdf/1504.07135v2","","Revise based on reviewers feedback. To appear in the the
  International Conference on Computer Safety, Reliability, and Security
  (SAFECOMP) 2015","","","cs.RO","cs.RO|cs.CR|cs.SE"
"616","1504.08318v3","2015-04-30 17:53:16","2016-04-18 12:46:40","Cities and Regions in Britain through hierarchical percolation","  Urban systems present hierarchical structures at many different scales. These
are observed as administrative regional delimitations which are the outcome of
complex geographical, political and historical processes which leave almost
indelible footprints on infrastructure such as the street network. In this work
we uncover a set of hierarchies in Britain at different scales using
percolation theory on the street network and on its intersections which are the
primary points of interaction and urban agglomeration. At the larger scales,
the observed hierarchical structures can be interpreted as regional fractures
of Britain, observed in various forms, from natural boundaries, such as
National Parks, to regional divisions based on social class and wealth such as
the well-known North-South divide. At smaller scales, cities are generated
through recursive percolations on each of the emerging regional clusters. We
examine the evolution of the morphology of the system as a whole, by measuring
the fractal dimension of the clusters at each distance threshold in the
percolation. We observe that this reaches a maximum plateau at a specific
distance. The clusters defined at this distance threshold are in excellent
correspondence with the boundaries of cities recovered from satellite images,
and from previous methods using population density.
","Elsa Arcaute|Carlos Molinero|Erez Hatna|Roberto Murcio|Camilo Vargas-Ruiz|A. Paolo Masucci|Michael Batty","","http://arxiv.org/abs/1504.08318v3","http://arxiv.org/pdf/1504.08318v3","http://dx.doi.org/10.1098/rsos.150691","20 pages, 14 figures","R. Soc. open sci. 3: 150691, 2016","10.1098/rsos.150691","physics.soc-ph","physics.soc-ph"
"617","1505.02656v1","2015-05-11 14:59:12","2015-05-11 14:59:12","Distributed Lustre activity tracking","  Numerous administration tools and techniques require near real time vision of
the activity occurring on a distributed filesystem. The changelog facility
provided by Lustre to address this need suffers limitations in terms of
scalability and flexibility. We have been working on reducing those limitations
by enhancing Lustre itself and developing external tools such as Lustre
ChangeLog Aggregate and Publish (LCAP) proxy. Beyond the ability to distribute
changelog processing, this effort aims at opening new prospectives by making
the changelog stream simpler to leverage for various purposes.
","Henri Doreau","","http://arxiv.org/abs/1505.02656v1","http://arxiv.org/pdf/1505.02656v1","","International Workshop on the Lustre Ecosystem: Challenges and
  Opportunities, March 2015, Annapolis MD","","","cs.DC","cs.DC"
"618","1505.04623v1","2015-05-18 13:13:34","2015-05-18 13:13:34","Identifying Geographic Clusters: A Network Analytic Approach","  In recent years there has been a growing interest in the role of networks and
clusters in the global economy. Despite being a popular research topic in
economics, sociology and urban studies, geographical clustering of human
activity has often studied been by means of predetermined geographical units
such as administrative divisions and metropolitan areas. This approach is
intrinsically time invariant and it does not allow one to differentiate between
different activities. Our goal in this paper is to present a new methodology
for identifying clusters, that can be applied to different empirical settings.
We use a graph approach based on k-shell decomposition to analyze world
biomedical research clusters based on PubMed scientific publications. We
identify research institutions and locate their activities in geographical
clusters. Leading areas of scientific production and their top performing
research institutions are consistently identified at different geographic
scales.
","Roberto Catini|Dmytro Karamshuk|Orion Penner|Massimo Riccaboni","","http://arxiv.org/abs/1505.04623v1","http://arxiv.org/pdf/1505.04623v1","http://dx.doi.org/10.1016/j.respol.2015.01.011","","","10.1016/j.respol.2015.01.011","physics.soc-ph","physics.soc-ph|cs.SI"
"619","1505.05960v1","2015-05-22 06:19:54","2015-05-22 06:19:54","Privacy-preserving Cross-domain Routing Optimization -- A Cryptographic
  Approach","  Today's large-scale enterprise networks, data center networks, and wide area
networks can be decomposed into multiple administrative or geographical
domains. Domains may be owned by different administrative units or
organizations. Hence protecting domain information is an important concern.
Existing general-purpose Secure Multi-Party Computation (SMPC) methods that
preserves privacy for domains are extremely slow for cross-domain routing
problems. In this paper we present PYCRO, a cryptographic protocol specifically
designed for privacy-preserving cross-domain routing optimization in Software
Defined Networking (SDN) environments. PYCRO provides two fundamental routing
functions, policy-compliant shortest path computing and bandwidth allocation,
while ensuring strong protection for the private information of domains. We
rigorously prove the privacy guarantee of our protocol. We have implemented a
prototype system that runs PYCRO on servers in a campus network. Experimental
results using real ISP network topologies show that PYCRO is very efficient in
computation and communication costs.
","Qingjun Chen|Chen Qian|Sheng Zhong","","http://arxiv.org/abs/1505.05960v1","http://arxiv.org/pdf/1505.05960v1","","","","","cs.CR","cs.CR"
"620","1505.08154v1","2015-05-29 19:09:49","2015-05-29 19:09:49","Dynamic Modeling for Representing Access Control Policies Effect","  In large databases, creating user interface for browsing or performing
insertion, deletion or modification of data is very costly in terms of
programming. In addition, each modification of an access control policy causes
many potential and unpredictable side effects which cause rule conflicts or
security breaches that affect the corresponding user interfaces as well. While
changes to access control policies in databases are inevitable, having a
dynamic system that generates interface according to the latest access control
policies become increasingly valuable. Lack of such a system leads to
unauthorized access to data and eventually violates the privacy of data owners.
In this work, we discuss a dynamic interface that applies Role Based Access
Control (RBAC) policies as the output of policy analysis and limits the amount
of information that users have access according to the policies defined for
roles. This interface also shows security administrators the effect of their
changes from the user's point of view while minimizing the cost by generating
the interface automatically.
","Kambiz Ghazinour|Mehdi Ghayoumi","","http://arxiv.org/abs/1505.08154v1","http://arxiv.org/pdf/1505.08154v1","","6 Pages, ICCS , 2015","","","cs.CR","cs.CR|94A60"
"621","1506.02822v2","2015-06-09 08:30:23","2015-07-26 18:50:32","Reproducible and User-Controlled Software Environments in HPC with Guix","  Support teams of high-performance computing (HPC) systems often find
themselves between a rock and a hard place: on one hand, they understandably
administrate these large systems in a conservative way, but on the other hand,
they try to satisfy their users by deploying up-to-date tool chains as well as
libraries and scientific software. HPC system users often have no guarantee
that they will be able to reproduce results at a later point in time, even on
the same system-software may have been upgraded, removed, or recompiled under
their feet, and they have little hope of being able to reproduce the same
software environment elsewhere. We present GNU Guix and the functional package
management paradigm and show how it can improve reproducibility and sharing
among researchers with representative use cases.
","Ludovic Courtes|Ricardo Wurmus","INRIA Bordeaux - Sud-Ouest|","http://arxiv.org/abs/1506.02822v2","http://arxiv.org/pdf/1506.02822v2","","2nd International Workshop on Reproducibility in Parallel Computing
  (RepPar), Aug 2015, Vienne, Austria. http://reppar.org/","","","cs.DC","cs.DC|cs.OS|cs.SE"
"622","1506.03828v1","2015-06-11 20:34:36","2015-06-11 20:34:36","Strategies for fostering Knowledge Management Programs in Public
  Organizations","  Knowledge Management (KM) is an approach to achieving strategic objectives by
visualizing, sharing, and using intangible resources of an organization and its
stakeholders. There are many studies that analyze specific factors for the
successful implementation of KM programs, and the evaluation of such factors is
considered a strategic tool for Public Organizations (POs) for efficiently
directing the implementation of a KM program. Nevertheless, there are cultural
problems such as weak trust, bad collaboration; technological problems as KM
systems difficult to use, nor interconnected or interoperable; and strategic
problems as political changes, not inter-administration continuity or lack of
political willingness. In this research we provide an overview of the key
factors for facilitating the implementation of KM programs. To this end, we
conducted a Systematic Literature Review (SLR) of success factors to create a
KM program for POs. Analyzing the twenty related studies we summarize benefits
and quality attributes of KM. Moreover, we obtained from an ongoing project
evaluation results of the KM factors ranked cultural, technological and
strategic. The results show that the Mexican POs have strategic and
technological issues such as: a misalignment between knowledge management and
organizational goals, it seems that POs are barely reusing their knowledge to
execute their daily activities or to take decisions, and is not possible to
know who and how a knowledge asset has been used. Finally, according to the
gaps and difficulties of the SLR, we provide strategies to successfully
implement KM programs in Mexico.
","Hugo A. Mitre-Hernandez|Arturo Mora-Soto|Hector Perez Lopez-Portillo|Carlos Lara-Alvarez","","http://arxiv.org/abs/1506.03828v1","http://arxiv.org/pdf/1506.03828v1","","","","","cs.CY","cs.CY"
"623","1506.04112v1","2015-06-12 18:51:25","2015-06-12 18:51:25","Owning Your Home Network: Router Security Revisited","  In this paper we investigate the Web interfaces of several DSL home routers
that can be used to manage their settings via a Web browser. Our goal is to
change these settings by using primary XSS and UI redressing attacks. This
study evaluates routers from 10 different manufacturers (TP-Link, Netgear,
Huawei, D-Link, Linksys, LogiLink, Belkin, Buffalo, Fritz!Box, and Asus). We
were able to circumvent the security of all of them. To demonstrate how all
devices are able to be attacked, we show how to do fast fingerprinting attacks.
Furthermore, we provide countermeasures to make administration interfaces and
therefore the use of routers more secure.
","Marcus Niemietz|Joerg Schwenk","","http://arxiv.org/abs/1506.04112v1","http://arxiv.org/pdf/1506.04112v1","","In Proceedings of the 9th Workshop on Web 2.0 Security and Privacy
  (W2SP) 2015","","","cs.CR","cs.CR|cs.NI"
"624","1507.02896v2","2015-06-30 09:39:35","2019-05-03 06:58:07","Energy analysis in ice hockey arenas and analytical formula for the
  temperature profile in the ice pad with transient boundary conditions","  The energy efficiency of ice hockey arenas is a central concern for the
administrations, as these buildings are well known to consume a large amount of
energy. Since they are composite, complex systems, solutions to such a problem
can be approached from many different areas, from managerial to technological
to more strictly physical.
  In this paper we consider heat transfer processes in an ice hockey hall,
during operating conditions, with a bottom-up approach based upon on-site
measurements. Detailed heat flux, relative humidity and temperature data for
the ice pad and the indoor air are used for a heat balance calculation in the
steady-state regime, which quantifies the impact of each single heat source. We
then solve the heat conduction equation for the ice pad in transient regime,
and obtain a generic analytical formula for the temperature profile that can be
used in practical applications.
  We then apply this formula to the resurfacing process for validation, and
find good agreement with an analogous numerical solution. Since it is given
with implicit initial condition and boundary conditions, it can be used not
only in ice hockey halls, but in a large variety of engineering applications.
","Andrea Ferrantelli|Klaus Viljanen|Jarek Kurnitski","","http://arxiv.org/abs/1507.02896v2","http://arxiv.org/pdf/1507.02896v2","http://dx.doi.org/10.1080/17512549.2019.1615549","23 pages, 18 figures, 1 table. Updated version with new title and
  improved descriptions, Table 1 and bibliography. Published in Advances in
  Building Energy Research","Advances in Building Energy Research (2019)","10.1080/17512549.2019.1615549","physics.gen-ph","physics.gen-ph"
"625","1506.09191v1","2015-06-30 18:22:41","2015-06-30 18:22:41","Coddlers, Scientists, Adventurers, and Opportunists: Personas to Inform
  Online Health Community Development","  As online health communities (OHCs) grow, users find it challenging to
properly search, read, and contribute to the community because of its
overwhelming content. Our goal is to understand OHC users' needs and
requirements for better delivering large-scale OHC content. We interviewed 14
OHC users with interests in diabetes to investigate their attitudes and needs
towards using OHCs and 2 OHC administrators to assess our findings. Four
personas -Coddlers, Scientists, Adventurers, and Opportunists- emerged, which
inform users' interaction behavior and attitudes with OHCs. An individual can
possess the characteristics of multiple personas, which can also change over
time. Our personas uniquely describe users' OHC participation intertwined with
illness contexts compared to existing social types in general online
communities. We discuss broader implications back to the literature and how our
findings apply to other illness contexts in OHCs. We end with requirements for
personalized delivery of large-scale OHC content.
","Jina Huh|Bum Chul Kwon|Jaegul Choo|Sung-Hee Kim|Ji Soo Yi","","http://arxiv.org/abs/1506.09191v1","http://arxiv.org/pdf/1506.09191v1","","10 pages, 1 table","","","cs.HC","cs.HC"
"626","1602.07662v1","2015-07-01 18:40:17","2015-07-01 18:40:17","Features of formation of a distributive infrastructure of e-commerce in
  Russia","  Article about objective laws of formation of a distributive infrastructure of
e-commerce. The distributive infrastructure of e-commerce, according to the
author, plays an important role in formation of network economy. The author
opens strategic value of institutional regulation of distributive logistics for
the decision problems of modernization of Russian economy.
","Mikhail Kaluzhsky","","http://arxiv.org/abs/1602.07662v1","http://arxiv.org/pdf/1602.07662v1","","18 pages, in Russian. ISSN 2075-1826","Management and Business Administration. 2014. N 2. pp. 38-55","","cs.CY","cs.CY"
"627","1507.00565v1","2015-07-02 13:10:09","2015-07-02 13:10:09","A Hierarchical Dynamic Beta Regression Model of School Performance in
  the Brazilian Mathematical Olympiads for Public Schools","  The Brazilian Mathematical Olympiads for Public Schools (OBMEP) is held every
year since 2005. In the 2013 edition there were over 47,000 schools registered
involving nearly 19.2 million students. The Brazilian public educational system
is structured into three administrative levels: federal, state and municipal.
Students participating in the OBMEP come from three educational levels, two in
primary and one in secondary school. We aim at studying the performance of
Brazilian public schools which have been taking part of the OBMEP from 2006
until 2013. We propose a standardization of the mean scores of schools per year
and educational level which is modeled through a hierarchical dynamic beta
regression model. Both the mean and precision of the beta distribution are
modeled as a function of covariates whose effects evolve smoothly with time.
Results show that, regardless of the educational level, federal schools have
better performance than municipal or state schools. The mean performance of
schools increases with the human development index (HDI) of the municipality
the school is located in. Moreover, the difference in mean performance between
federal and state or municipal schools tends to increase with the HDI. Schools
with higher proportion of boys tend to have better mean performance in the
second and third educational levels of OBMEP.
","Alexandra M. Schmidt|Caroline P. de Moraes|Helio S. Migon","","http://arxiv.org/abs/1507.00565v1","http://arxiv.org/pdf/1507.00565v1","","","","","stat.AP","stat.AP"
"628","1507.00576v2","2015-07-02 13:41:17","2015-09-14 20:10:13","Flip the Cloud: Cyber-Physical Signaling Games in the Presence of
  Advanced Persistent Threats","  Access to the cloud has the potential to provide scalable and cost effective
enhancements of physical devices through the use of advanced computational
processes run on apparently limitless cyber infrastructure. On the other hand,
cyber-physical systems and cloud-controlled devices are subject to numerous
design challenges; among them is that of security. In particular, recent
advances in adversary technology pose Advanced Persistent Threats (APTs) which
may stealthily and completely compromise a cyber system. In this paper, we
design a framework for the security of cloud-based systems that specifies when
a device should trust commands from the cloud which may be compromised. This
interaction can be considered as a game between three players: a cloud
defender/administrator, an attacker, and a device. We use traditional signaling
games to model the interaction between the cloud and the device, and we use the
recently proposed FlipIt game to model the struggle between the defender and
attacker for control of the cloud. Because attacks upon the cloud can occur
without knowledge of the defender, we assume that strategies in both games are
picked according to prior commitment. This framework requires a new equilibrium
concept, which we call Gestalt Equilibrium, a fixed-point that expresses the
interdependence of the signaling and FlipIt games. We present the solution to
this fixed-point problem under certain parameter cases, and illustrate an
example application of cloud control of an unmanned vehicle. Our results
contribute to the growing understanding of cloud-controlled systems.
","Jeffrey Pawlick|Sadegh Farhang|Quanyan Zhu","","http://arxiv.org/abs/1507.00576v2","http://arxiv.org/pdf/1507.00576v2","http://dx.doi.org/10.13140/RG.2.1.3128.9446","To be presented at the 2015 Conference on Decision and Game Theory
  for Security (GameSec 2015)","","10.13140/RG.2.1.3128.9446","cs.CR","cs.CR|cs.GT"
"629","1507.01272v1","2015-07-05 20:53:26","2015-07-05 20:53:26","VEWS: A Wikipedia Vandal Early Warning System","  We study the problem of detecting vandals on Wikipedia before any human or
known vandalism detection system reports flagging potential vandals so that
such users can be presented early to Wikipedia administrators. We leverage
multiple classical ML approaches, but develop 3 novel sets of features. Our
Wikipedia Vandal Behavior (WVB) approach uses a novel set of user editing
patterns as features to classify some users as vandals. Our Wikipedia
Transition Probability Matrix (WTPM) approach uses a set of features derived
from a transition probability matrix and then reduces it via a neural net
auto-encoder to classify some users as vandals. The VEWS approach merges the
previous two approaches. Without using any information (e.g. reverts) provided
by other users, these algorithms each have over 85% classification accuracy.
Moreover, when temporal recency is considered, accuracy goes to almost 90%. We
carry out detailed experiments on a new data set we have created consisting of
about 33K Wikipedia users (including both a black list and a white list of
editors) and containing 770K edits. We describe specific behaviors that
distinguish between vandals and non-vandals. We show that VEWS beats ClueBot NG
and STiki, the best known algorithms today for vandalism detection. Moreover,
VEWS detects far more vandals than ClueBot NG and on average, detects them 2.39
edits before ClueBot NG when both detect the vandal. However, we show that the
combination of VEWS and ClueBot NG can give a fully automated vandal early
warning system with even higher accuracy.
","Srijan Kumar|Francesca Spezzano|V. S. Subrahmanian","","http://arxiv.org/abs/1507.01272v1","http://arxiv.org/pdf/1507.01272v1","http://dx.doi.org/10.1145/2783258.2783367","To appear in Proceedings of the 21st ACM SIGKDD Conference of
  Knowledge Discovery and Data Mining (KDD 2015)","","10.1145/2783258.2783367","cs.SI","cs.SI|physics.soc-ph|H.2.8"
"630","1507.03780v1","2015-07-14 09:16:46","2015-07-14 09:16:46","Monitoring in IOT enabled devices","  As network size continues to grow exponentially, there has been a
proportionate increase in the number of nodes in the corresponding network.
With the advent of Internet of things (IOT), it is assumed that many more
devices will be connected to the existing network infrastructure. As a result,
monitoring is expected to get more complex for administrators as networks tend
to become more heterogeneous. Moreover, the addressing for IOTs would be more
complex given the scale at which devices will be added to the network and hence
monitoring is bound to become an uphill task due to management of larger range
of addresses. This paper will throw light on what kind of monitoring mechanisms
can be deployed in internet of things (IOTs) and their overall effectiveness.
","Udit Gupta","","http://arxiv.org/abs/1507.03780v1","http://arxiv.org/pdf/1507.03780v1","","6 pages, 1 table","","","cs.NI","cs.NI"
"631","1507.07086v1","2015-07-25 09:54:16","2015-07-25 09:54:16","On Liveness of Dynamic Storage","  Dynamic distributed storage algorithms such as DynaStore, Reconfigurable
Paxos, RAMBO, and RDS, do not ensure liveness (wait-freedom) in asynchronous
runs with infinitely many reconfigurations. We prove that this is inherent for
asynchronous dynamic storage algorithms, including ones that use $\Omega$ or
$\diamond S$ oracles. Our result holds even if only one process may fail,
provided that machines that were successfully removed from the system's
configuration may be switched off by an administrator. Intuitively, the
impossibility relies on the fact that a correct process can be suspected to
have failed at any time, i.e., its failure is indistinguishable to other
processes from slow delivery of its messages, and so the system should be able
to reconfigure without waiting for this process to complete its pending
operations.
  To circumvent this result, we define a dynamic eventually perfect failure
detector, and present an algorithm that uses it to emulate wait-free dynamic
atomic storage (with no restrictions on reconfiguration rate). Together, our
results thus draw a sharp line between oracles like $\Omega$ and $\diamond S$,
which allow some correct process to continue to be suspected forever, and a
dynamic eventually perfect one, which does not.
","Alexander Spiegelman|Idit Keidar","","http://arxiv.org/abs/1507.07086v1","http://arxiv.org/pdf/1507.07086v1","","","","","cs.DC","cs.DC"
"632","1507.08499v1","2015-07-30 13:42:56","2015-07-30 13:42:56","Low Delay Random Linear Coding and Scheduling Over Multiple Interfaces","  Multipath transport protocols like MPTCP transfer data across multiple routes
in parallel and deliver it in order at the receiver. When the delay on one or
more of the paths is variable, as is commonly the case, out of order arrivals
are frequent and head of line blocking leads to high latency. This is
exacerbated when packet loss, which is also common with wireless links, is
tackled using ARQ. This paper introduces Stochastic Earliest Delivery Path
First (S-EDPF), a resilient low delay packet scheduler for multipath transport
protocols. S-EDPF takes explicit account of the stochastic nature of paths and
uses this to minimise in-order delivery delay. S-EDPF also takes account of
FEC, jointly scheduling transmission of information and coded packets and in
this way allows lossy links to reduce delay and improve resiliency, rather than
degrading performance as usually occurs with existing multipath systems. We
implement S-EDPF as a multi-platform application that does not require
administration privileges nor modifications to the operating system and has
negligible impact on energy consumption. We present a thorough experimental
evaluation in both controlled environments and into the wild, revealing
dramatic gains in delay performance compared to existing approaches.
","Andres Garcia-Saavedra|Mohammad Karzand|Douglas J. Leith","","http://arxiv.org/abs/1507.08499v1","http://arxiv.org/pdf/1507.08499v1","","","","","cs.NI","cs.NI|cs.IT|cs.PF|math.IT"
"633","1508.00078v1","2015-08-01 05:05:38","2015-08-01 05:05:38","Tirocinio Formativo Attivo: il Laboratorio Pedagogico Didattico TIC
  della Classe A049 nell'Ateneo di Palermo","  We discuss the organization of the course of Laboratorio Pedagogico Didattico
TIC of the Tirocinio Formativo Attivo at the University of Palermo. The course
takes into account the development of information and communication
technologies both for the management/administration of school and their
application in Mathematics and Physics education. In particular, we will
discuss in detail the various possibilities of renewing the teaching given by
communication and sharing tools of web 2.0, the use of the digital whiteboard,
and the use of technologies in Mathematics and Physics laboratories, up to the
use of multimedia mobile devices as experimental tools.
","Lucia Lupo|Aurelio Agliolo Gallitto","","http://arxiv.org/abs/1508.00078v1","http://arxiv.org/pdf/1508.00078v1","","14 pages, 7 embedded figures, in Italian language","","","physics.ed-ph","physics.ed-ph"
"634","1508.01086v1","2015-08-05 14:24:23","2015-08-05 14:24:23","Km4City Ontology Building vs Data Harvesting and Cleaning for Smart-city
  Services","  Presently, a very large number of public and private data sets are available
from local governments. In most cases, they are not semantically interoperable
and a huge human effort would be needed to create integrated ontologies and
knowledge base for smart city. Smart City ontology is not yet standardized, and
a lot of research work is needed to identify models that can easily support the
data reconciliation, the management of the complexity, to allow the data
reasoning. In this paper, a system for data ingestion and reconciliation of
smart cities related aspects as road graph, services available on the roads,
traffic sensors etc., is proposed. The system allows managing a big data volume
of data coming from a variety of sources considering both static and dynamic
data. These data are mapped to a smart-city ontology, called KM4City (Knowledge
Model for City), and stored into an RDF-Store where they are available for
applications via SPARQL queries to provide new services to the users via
specific applications of public administration and enterprises. The paper
presents the process adopted to produce the ontology and the big data
architecture for the knowledge base feeding on the basis of open and private
data, and the mechanisms adopted for the data verification, reconciliation and
validation. Some examples about the possible usage of the coherent big data
knowledge base produced are also offered and are accessible from the RDF-Store
and related services. The article also presented the work performed about
reconciliation algorithms and their comparative assessment and selection.
","Pierfrancesco Bellini|Monica Benigni|Riccardo Billero|Paolo Nesi|Nadia Rauch","","http://arxiv.org/abs/1508.01086v1","http://arxiv.org/pdf/1508.01086v1","http://dx.doi.org/10.1016/j.jvlc.2014.10.023","","","10.1016/j.jvlc.2014.10.023","cs.DB","cs.DB|cs.AI|cs.CY"
"635","1508.04973v1","2015-08-20 13:09:56","2015-08-20 13:09:56","InstaCluster: Building A Big Data Cluster in Minutes","  Deploying, configuring, and managing large clusters is very a demanding and
cumbersome task due to the complexity of such systems and the variety of skills
needed. One needs to perform low-level configuration of the cluster nodes to
ensure their interoperability and connectivity, as well as install, configure
and provision the needed services.
  In this paper we address this problem and demonstrate how to build a Big Data
analytic platform on Amazon EC2 in a matter of minutes. Moreover, to use our
tool, embedded into a public Amazon Machine Image, the user does not need to be
an expert in system administration or Big Data service configuration. Our tool
dramatically reduces the time needed to provision clusters, as well as the cost
of the infrastructure. Researchers enjoy an additional benefit of having a
simple way to specify the experimental environments they use, so that their
experiments can be easily reproduced by anyone using our tool.
","Giovanni Paolo Gibilisco|Srdan Krstic","","http://arxiv.org/abs/1508.04973v1","http://arxiv.org/pdf/1508.04973v1","","5 pages, 1 figure","","","cs.DC","cs.DC|68M14|D.2.7"
"636","1508.05457v1","2015-08-22 02:24:14","2015-08-22 02:24:14","Analisis Keamanan Protokol Secure Socket Layer (SSL) Terhadap Proses
  Sniffing di Jaringan","  Development of information technology, especially in the field of computer
network allows the exchange of information faster and more complex and the data
that is exchanged can vary. Security of data on communication in the network is
a major thing. Secure socket layer (SSL) is the solution to the problem, but
further research on the security of the SSL protocol transactions should be
done to determine the extent of SSL can secure the data on the network. When
the computer sends data across the network, the data is transmitted in packets.
Sniffing is a technique of monitoring of every packet traversing the network.
Security threat presented by sniffers is their ability to capture all incoming
and outgoing packets through the network, which includes the passwords,
usernames and other sensitive issues. Packet sniffer captures the data
addressed to other devices, which will then be stored for later analysis later.
Sniffing can also be used by system administrators to monitor the network and
solve problems in the network.
","Heru Pranata|Leon Andretti Abdillah|Usman Ependi","","http://arxiv.org/abs/1508.05457v1","http://arxiv.org/pdf/1508.05457v1","","6 pages, Student Colloquium Sistem Informasi & Teknik Informatika
  (SC-SITI) 2015","","","cs.CR","cs.CR"
"637","1508.05502v1","2015-08-22 12:31:33","2015-08-22 12:31:33","Evaluating the quality of survey and administrative data with
  generalized multitrait-multimethod models","  Administrative register data are increasingly important in statistics, but,
like other types of data, may contain measurement errors. To prevent such
errors from invalidating analyses of scientific interest, it is therefore
essential to estimate the extent of measurement errors in administrative data.
Currently, however, most approaches to evaluate such errors involve either
prohibitively expensive audits or comparison with a survey that is assumed
perfect.
  We introduce the ""generalized multitrait-multimethod"" (GMTMM) model, which
can be seen as a general framework for evaluating the quality of administrative
and survey data simultaneously. This framework allows both survey and register
to contain random and systematic measurement errors. Moreover, it accommodates
common features of administrative data such as discreteness, nonlinearity, and
nonnormality, improving similar existing models. The use of the GMTMM model is
demonstrated by application to linked survey-register data from the German
Federal Employment Agency on income from and duration of employment, and a
simulation study evaluates the estimates obtained.
  KEY WORDS: Measurement error, Latent Variable Models, Official statistics,
Register data, Reliability
","Daniel Leonard Oberski|Antje Kirchner|Stephanie Eckman|Frauke Kreuter","","http://arxiv.org/abs/1508.05502v1","http://arxiv.org/pdf/1508.05502v1","","","","","stat.AP","stat.AP"
"638","1508.07509v2","2015-08-29 22:19:53","2016-04-03 19:56:59","Statistically-estimated tree composition for the northeastern United
  States at the time of Euro-American settlement","  We present a gridded 8 km-resolution data product of the estimated
composition of tree taxa at the time of Euro-American settlement of the
northeastern United States and the statistical methodology used to produce the
product from trees recorded by land surveyors. Composition is defined as the
proportion of stems larger than approximately 20 cm diameter at breast height
for 22 tree taxa, generally at the genus level. The data come from
settlement-era public survey records that are transcribed and then aggregated
spatially, giving count data. The domain is divided into two regions, eastern
(Maine to Ohio) and midwestern (Indiana to Minnesota). Public Land Survey point
data in the midwestern region (ca. 0.8-km resolution) are aggregated to a
regular 8 km grid, while data in the eastern region, from Town Proprietor
Surveys, are aggregated at the township level in irregularly-shaped local
administrative units. The product is based on a Bayesian statistical model fit
to the count data that estimates composition on a regular 8 km grid across the
entire domain. The statistical model is designed to handle data from both the
regular grid and the irregularly-shaped townships and allows us to estimate
composition at locations with no data and to smooth over noise caused by
limited counts in locations with data. The model also allows us to quantify
uncertainty in our composition estimates, making the product suitable for
applications employing data assimilation. We expect this data product to be
useful for understanding the state of vegetation in the northeastern United
States prior to large-scale Euro-American settlement. In addition to specific
regional questions, the data product can also serve as a baseline against which
to investigate how forests and ecosystems change after intensive settlement.
The data product is available at the NIS data portal as version 1.0.
","Christopher J. Paciorek|Simon J. Goring|Andrew L. Thurman|Charles V. Cogbill|John W. Williams|David J. Mladenoff|Jody A. Peters|Jun Zhu|Jason S. McLachlan","","http://arxiv.org/abs/1508.07509v2","http://arxiv.org/pdf/1508.07509v2","http://dx.doi.org/10.1371/journal.pone.0150087","23 pages, 5 tables, 3 figures","PLoS ONE (2016) 11(2): e0150087","10.1371/journal.pone.0150087","stat.AP","stat.AP"
"639","1509.00755v1","2015-09-02 15:51:34","2015-09-02 15:51:34","Optimal population-level infection detection strategies for malaria
  control and elimination in a spatial model of malaria transmission","  Mass campaigns with antimalarial drugs are potentially a powerful tool for
local elimination of malaria, yet current diagnostic technologies are
insufficiently sensitive to identify all individuals who harbor infections. At
the same time, overtreatment of uninfected individuals increases the risk of
accelerating emergence of drug resistance and losing community acceptance.
Local heterogeneity in transmission intensity may allow campaign strategies
that respond to index cases to successfully target subpatent infections while
simultaneously limiting overtreatment. While selective targeting of hotspots of
transmission has been proposed as a strategy for malaria control, such
targeting has not been tested in the context of malaria elimination. Using
household locations, demographics, and prevalence data from a survey of four
health facility catchment areas in southern Zambia and an agent-based model of
malaria transmission and immunity acquisition, a transmission intensity was fit
to each household based on neighborhood age-dependent malaria prevalence. A set
of individual infection trajectories was constructed for every household in
each catchment area, accounting for heterogeneous exposure and immunity.
Various campaign strategies (mass drug administration, mass screen and treat,
focal mass drug administration, snowball reactive case detection, pooled
sampling, and a hypothetical serological diagnostic) were simulated and
evaluated for performance at finding infections, minimizing overtreatment,
reducing clinical case counts, and interrupting transmission. For malaria
control, presumptive treatment leads to substantial overtreatment without
additional morbidity reduction under all but the highest transmission
conditions. Selective targeting of hotspots with drug campaigns is an
ineffective tool for elimination due to limited sensitivity of available field
diagnostics.
","Jaline Gerardin|Caitlin A. Bever|Busiku Hamainza|John M. Miller|Philip A. Eckhoff|Edward A. Wenger","","http://arxiv.org/abs/1509.00755v1","http://arxiv.org/pdf/1509.00755v1","http://dx.doi.org/10.1371/journal.pcbi.1004707","","","10.1371/journal.pcbi.1004707","q-bio.PE","q-bio.PE"
"640","1509.03977v1","2015-09-14 07:52:00","2015-09-14 07:52:00","Optimization of anemia treatment in hemodialysis patients via
  reinforcement learning","  Objective: Anemia is a frequent comorbidity in hemodialysis patients that can
be successfully treated by administering erythropoiesis-stimulating agents
(ESAs). ESAs dosing is currently based on clinical protocols that often do not
account for the high inter- and intra-individual variability in the patient's
response. As a result, the hemoglobin level of some patients oscillates around
the target range, which is associated with multiple risks and side-effects.
This work proposes a methodology based on reinforcement learning (RL) to
optimize ESA therapy.
  Methods: RL is a data-driven approach for solving sequential decision-making
problems that are formulated as Markov decision processes (MDPs). Computing
optimal drug administration strategies for chronic diseases is a sequential
decision-making problem in which the goal is to find the best sequence of drug
doses. MDPs are particularly suitable for modeling these problems due to their
ability to capture the uncertainty associated with the outcome of the treatment
and the stochastic nature of the underlying process. The RL algorithm employed
in the proposed methodology is fitted Q iteration, which stands out for its
ability to make an efficient use of data.
  Results: The experiments reported here are based on a computational model
that describes the effect of ESAs on the hemoglobin level. The performance of
the proposed method is evaluated and compared with the well-known Q-learning
algorithm and with a standard protocol. Simulation results show that the
performance of Q-learning is substantially lower than FQI and the protocol.
  Conclusion: Although prospective validation is required, promising results
demonstrate the potential of RL to become an alternative to current protocols.
","Pablo Escandell-Montero|Milena Chermisi|Jose M. Martinez-Martinez|Juan Gomez-Sanchis|Carlo Barbieri|Emilio Soria-Olivas|Flavio Mari|Joan Vila-Frances|Andrea Stopper|Emanuele Gatti|Jose D. Martin-Guerrero","","http://arxiv.org/abs/1509.03977v1","http://arxiv.org/pdf/1509.03977v1","http://dx.doi.org/10.1016/j.artmed.2014.07.004","17 pages, 10 figures","Artificial Intelligence in Medicine, Volume 62, Issue 1, September
  2014, Pages 47-60","10.1016/j.artmed.2014.07.004","stat.ML","stat.ML|cs.AI|cs.LG"
"641","1509.04350v1","2015-09-14 23:28:23","2015-09-14 23:28:23","Nonparametric estimation of a mixing distribution for a family of linear
  stochastic dynamical systems","  In this paper we develop a nonparametric maximum likelihood estimate of the
mixing distribution of the parameters of a linear stochastic dynamical system.
This includes, for example, pharmacokinetic population models with process and
measurement noise that are linear in the state vector, input vector and the
process and measurement noise vectors. Most research in mixing distributions
only considers measurement noise. The advantages of the models with process
noise are that, in addition to the measurements errors, the uncertainties in
the model itself are taken into the account. For example, for deterministic
pharmacokinetic models, errors in dose amounts, administration times, and
timing of blood samples are typically not included. For linear stochastic
models, we use linear Kalman-Bucy filtering to calculate the likelihood of the
observations and then employ a nonparametric adaptive grid algorithm to find
the nonparametric maximum likelihood estimate of the mixing distribution. We
then use the directional derivatives of the estimated mixing distribution to
show that the result found attains a global maximum. A simple example using a
one compartment pharmacokinetic linear stochastic model is given. In addition
to population pharmacokinetics, this research also applies to empirical Bayes
estimation.
","Alona Kryshchenko|Alan Schumitzky|Mike van Guilder|Michael Neely","","http://arxiv.org/abs/1509.04350v1","http://arxiv.org/pdf/1509.04350v1","","","","","stat.ME","stat.ME"
"642","1509.05500v1","2015-09-18 03:56:18","2015-09-18 03:56:18","On Reconstructability of Quadratic Utility Functions from the Iterations
  in Gradient Methods","  In this paper, we consider a scenario where an eavesdropper can read the
content of messages transmitted over a network. The nodes in the network are
running a gradient algorithm to optimize a quadratic utility function where
such a utility optimization is a part of a decision making process by an
administrator. We are interested in understanding the conditions under which
the eavesdropper can reconstruct the utility function or a scaled version of it
and, as a result, gain insight into the decision-making process. We establish
that if the parameter of the gradient algorithm, i.e.,~the step size, is chosen
appropriately, the task of reconstruction becomes practically impossible for a
class of Bayesian filters with uniform priors. We establish what step-size
rules should be employed to ensure this.
","Farhad Farokhi|Iman Shames|Michael G. Rabbat|Mikael Johansson","","http://arxiv.org/abs/1509.05500v1","http://arxiv.org/pdf/1509.05500v1","","","","","math.OC","math.OC|cs.CR|math.PR"
"643","1509.06035v1","2015-09-20 18:14:04","2015-09-20 18:14:04","Image Retrieval Based on LBP Pyramidal Multiresolution using Reversible
  Watermarking","  In the medical field, images are increasingly used to facilitate diagnosis of
diseases. These images are stored in multimedia databases accompanied by doctor
s prescriptions and other information related to patients.Search for medical
images has become for clinical applications an essential tool to bring
effective aid in diagnosis. Content Based Image Retrieval (CBIR) is one of the
possible solutions to effectively manage these databases. Our contribution is
to define a relevant descriptor to retrieve images based on multiresolution
analysis of texture using Local Binary Pattern LBP. This descriptor once
calculated and information s relating to the patient; will be placed in the
image using the technique of reversible watermarking. Thereby, the image,
descriptor of its contents, the BFILE locator and patientrelated information
become a single entity, so even the administrator cannot have access to the
patient private data.
","H. Ouahi|K. Afdel|M. Machkour","","http://arxiv.org/abs/1509.06035v1","http://arxiv.org/pdf/1509.06035v1","","","","","cs.CV","cs.CV"
"644","1509.07616v1","2015-09-25 08:06:42","2015-09-25 08:06:42","A Cyberinfrastructure-based Approach to Real Time Water Temperature
  Prediction","  The prediction of water temperature is crucial for aquatic ecosystem studies
and management. In this paper, we raise challenging issues in supporting real
time water temperature prediction and present a system called WT-Agabus to
address those issues. The WT-Agabus system is designed to be a
cyberinfrastructure and to support various prediction models in a uniform way.
In addition, we present a neural network-based water temperature prediction
model to use only data available online from Korea Meteorological
Administration (KMA). In this paper, we also show the current prototype
implementation of the WT-Agabus system to support the prediction model
","Jounghyun Lee|Keun Young Lee|Karpjoo Jeong|Meilan Jiang|Bomchul Kim|Suntae Hwang","","http://arxiv.org/abs/1509.07616v1","http://arxiv.org/pdf/1509.07616v1","","10 pages, 14 figures, PRAGMA-ICDS-15","","","cs.DC","cs.DC"
"645","1509.08231v1","2015-09-28 08:21:30","2015-09-28 08:21:30","Building a Virtual HPC Cluster with Auto Scaling by the Docker","  Solving the software dependency issue under the HPC environment has always
been a difficult task for both computing system administrators and application
scientists. This work would like to tackle the issue by introducing the modern
container technology, the Docker, to be specific. By integrating the
auto-scaling feature of service discovery with the light-weight virtualization
tool, the Docker, the construction of a virtual cluster on top of physical
cluster hardware is attempted. Thus, through the isolation of computing
environment, a remedy of software dependency of HPC environment is possible.
","Hsi-En Yu|Weicheng Huang","","http://arxiv.org/abs/1509.08231v1","http://arxiv.org/pdf/1509.08231v1","","PRAGMA-ICDS 15","","","cs.DC","cs.DC"
"646","1510.00842v5","2015-10-03 16:02:09","2018-03-31 10:42:16","Mortality Rate Estimation and Standardization for Public Reporting:
  Medicare's Hospital Compare","  Bayesian models are increasing fit to large administrative data sets and then
used to make individualized recommendations. For instance, Medicare's Hospital
Compare webpage provides information to patients about specific hospital
mortality rates for a heart attack or Acute Myocardial Infarction (AMI).
Hospital Compare's current recommendations are based on a random effects logit
model with a random hospital indicator and patient risk factors. By checking
the out of sample calibration of their individualized predictions against
general empirical advice, we are led to substantial revisions of the Hospital
Compare model for AMI mortality. As opposed to Hospital Compare, our revised
models incorporate information about hospital volume, nursing staff, medical
residents, and the hospital's ability to perform cardiovascular procedures,
information that is clearly needed if a model is to make appropriately
calibrated predictions. Additionally, we contrast several methods for
summarizing a model's predictions for use by the public. We find that indirect
standardization, as currently used by Hospital Compare, fails to adequately
control for differences in patient risk factors, whereas direct standardization
provides good control and is easy to interpret.
","E. I. George|V. Rockova|P. R. Rosenbaum|V. A. Satopaa|J. H. Silber","","http://arxiv.org/abs/1510.00842v5","http://arxiv.org/pdf/1510.00842v5","http://dx.doi.org/10.1080/01621459.2016.1276021","Main paper: 31 pages, 7 figures, 4 tables Supplemental Material: 4
  pages, 2 figures, 1 table","Journal of the American Statistical Association (2017), 112:519,
  933-947","10.1080/01621459.2016.1276021","stat.AP","stat.AP"
"647","1510.04507v1","2015-10-15 12:42:13","2015-10-15 12:42:13","Satellite Quantum Communication via the Alphasat Laser Communication
  Terminal","  By harnessing quantum effects, we nowadays can use encryption that is in
principle proven to withstand any conceivable attack. These fascinating quantum
features have been implemented in metropolitan quantum networks around the
world. In order to interconnect such networks over long distances, optical
satellite communication is the method of choice. Standard telecommunication
components allow one to efficiently implement quantum communication by
measuring field quadratures (continuous variables). This opens the possibility
to adapt our Laser Communication Terminals (LCTs) to quantum key distribution
(QKD). First satellite measurement campaigns are currently validating our
approach.
","Dominique Elser|Kevin Gunthner|Imran Khan|Birgit Stiller|Christoph Marquardt|Gerd Leuchs|Karen Saucke|Daniel Trondle|Frank Heine|Stefan Seel|Peter Greulich|Herwig Zech|Bjorn Gutlich|Ines Richter|Rolf Meyer","Quantum Information Processing Group|Quantum Information Processing Group|Quantum Information Processing Group|Quantum Information Processing Group|Quantum Information Processing Group|Quantum Information Processing Group|Tesat-Spacecom GmbH & Co. KG, Backnang, Germany|Tesat-Spacecom GmbH & Co. KG, Backnang, Germany|Tesat-Spacecom GmbH & Co. KG, Backnang, Germany|Tesat-Spacecom GmbH & Co. KG, Backnang, Germany|Tesat-Spacecom GmbH & Co. KG, Backnang, Germany|Tesat-Spacecom GmbH & Co. KG, Backnang, Germany|Space Administration, German Aerospace Center|Space Administration, German Aerospace Center|Space Administration, German Aerospace Center","http://arxiv.org/abs/1510.04507v1","http://arxiv.org/pdf/1510.04507v1","http://dx.doi.org/10.1109/ICSOS.2015.7425077","International Conference on Space Optical Systems and Applications
  (IEEE ICSOS 2015), October 27 and 28, 2015, New Orleans, USA, 4 pages, 5
  figures","","10.1109/ICSOS.2015.7425077","quant-ph","quant-ph|cs.CR|cs.ET|physics.ao-ph|physics.optics"
"648","1510.04967v4","2015-10-16 18:09:56","2016-10-06 20:07:22","A simple agent-based spatial model of the economy: tools for policy","  This study simulates the evolution of artificial economies in order to
understand the tax relevance of administrative boundaries in the quality of
life of its citizens. The modeling involves the construction of a computational
algorithm, which includes citizens, bounded into families; firms and
governments; all of them interacting in markets for goods, labor and real
estate. The real estate market allows families to move to dwellings with higher
quality or lower price when the families capitalize property values. The goods
market allows consumers to search on a flexible number of firms choosing by
price and proximity. The labor market entails a matching process between firms
(location) and candidates (qualification). The government may be configured
into one, four or seven distinct sub-national governments. The role of
government is to collect taxes on the value added of firms in its territory and
invest the taxes into higher levels of quality of life for residents. The model
does not have a credit market. The results suggest that the configuration of
administrative boundaries is relevant to the levels of quality of life arising
from the reversal of taxes. The model with seven regions is more dynamic, with
higher GDP values, but more unequal and heterogeneous across regions. The
simulation with only one region is more homogeneously poor. The study seeks to
contribute to a theoretical and methodological framework as well as to
describe, operationalize and test computer models of public finance analysis,
with explicitly spatial and dynamic emphasis. Several alternatives of expansion
of the model for future research are described. Moreover, this study adds to
the existing literature in the realm of simple microeconomic computational
models, specifying structural relationships between local governments and
firms, consumers and dwellings mediated by distance.
","Bernardo Alves Furtado|Isaque Daniel Rocha Eberhardt","","http://arxiv.org/abs/1510.04967v4","http://arxiv.org/pdf/1510.04967v4","","27 pages, 25 figures, includes ODD Protocol and pseudocodes","","","cs.MA","cs.MA|q-fin.GN|I.6.5; J.4"
"649","1510.06150v1","2015-10-21 07:21:13","2015-10-21 07:21:13","Matching Mechanisms For Real-Time Computational Resource Exchange
  Markets","  In this paper we describe matching mechanisms for a real-time computational
resource exchange market, Chital, that incentivizes participating clients to
perform computation for their peers in exchange for overall improved
performance. The system is designed to discourage dishonest behavior via a
credit system, while simultaneously minimizing the use of dedicated computing
servers and the number of verifications performed by the administrating
servers. We describe the system in the context of a pre-existing system (under
development), Vedalia \cite{715Project}, for analyzing and visualizing product
reviews, by using machine learning such as topic models. We extend this context
to general computing tasks, describe a list of matching algorithms, and
evaluate their performance in a simulated environment. In addition, we design a
matching algorithm that optimizes the amount of time a participant could save
compared to computing a task on their own, and show empirically that this
algorithm results in a situation in which it is almost always optimal for a
user to join the exchange than do computation alone. Lastly, we use a top-down
approach to derive a theoretically near-optimal matching algorithm under
certain distributional assumptions on query frequency.
","Joseph W. Robinson|Aaron Q. Li","","http://arxiv.org/abs/1510.06150v1","http://arxiv.org/pdf/1510.06150v1","","","","","cs.GT","cs.GT"
"650","1510.06262v2","2015-10-21 14:18:33","2015-10-26 20:53:49","IAU 2015 Resolution B2 on Recommended Zero Points for the Absolute and
  Apparent Bolometric Magnitude Scales","  The XXIXth IAU General Assembly in Honolulu adopted IAU 2015 Resolution B2 on
recommended zero points for the absolute and apparent bolometric magnitude
scales. The resolution was proposed by the IAU Inter-Division A-G Working Group
on Nominal Units for Stellar and Planetary Astronomy after consulting with a
broad spectrum of researchers from the astronomical community. Resolution B2
resolves the long-standing absence of an internationally-adopted zero point for
the absolute and apparent bolometric magnitude scales. Resolution B2 defines
the zero point of the absolute bolometric magnitude scale such that a radiation
source with $M_{\rm Bol}$ = 0 has luminosity L$_{\circ}$ = 3.0128e28 W. The
zero point of the apparent bolometric magnitude scale ($m_{\rm Bol}$ = 0)
corresponds to irradiance $f_{\circ}$ = 2.518021002e-8 W/m$^2$. The zero points
were chosen so that the nominal solar luminosity (3.828e26 W) adopted by IAU
2015 Resolution B3 corresponds approximately to $M_{\rm Bol}$(Sun) = 4.74, the
value most commonly adopted in recent literature. The nominal total solar
irradiance (1361 W/m$^2$) adopted in IAU 2015 Resolution B3 corresponds
approximately to apparent bolometric magnitude $m_{\rm bol}$(Sun) = -26.832.
Implicit in the IAU 2015 Resolution B2 definition of the apparent bolometric
magnitude scale is an exact definition for the parsec (648000/$\pi$ au) based
on the IAU 2012 Resolution B2 definition of the astronomical unit.
","E. E. Mamajek|G. Torres|A. Prsa|P. Harmanec|M. Asplund|P. D. Bennett|N. Capitaine|J. Christensen-Dalsgaard|E. Depagne|W. M. Folkner|M. Haberreiter|S. Hekker|J. L. Hilton|V. Kostov|D. W. Kurtz|J. Laskar|B. D. Mason|E. F. Milone|M. M. Montgomery|M. T. Richards|J. Schou|S. G. Stewart","IAU Inter-Division A-G Working Group on Nominal Units for Stellar & Planetary Astronomy|IAU Inter-Division A-G Working Group on Nominal Units for Stellar & Planetary Astronomy|IAU Inter-Division A-G Working Group on Nominal Units for Stellar & Planetary Astronomy|IAU Inter-Division A-G Working Group on Nominal Units for Stellar & Planetary Astronomy|IAU Inter-Division A-G Working Group on Nominal Units for Stellar & Planetary Astronomy|IAU Inter-Division A-G Working Group on Nominal Units for Stellar & Planetary Astronomy|IAU Inter-Division A-G Working Group on Nominal Units for Stellar & Planetary Astronomy|IAU Inter-Division A-G Working Group on Nominal Units for Stellar & Planetary Astronomy|IAU Inter-Division A-G Working Group on Nominal Units for Stellar & Planetary Astronomy|IAU Inter-Division A-G Working Group on Nominal Units for Stellar & Planetary Astronomy|IAU Inter-Division A-G Working Group on Nominal Units for Stellar & Planetary Astronomy|IAU Inter-Division A-G Working Group on Nominal Units for Stellar & Planetary Astronomy|IAU Inter-Division A-G Working Group on Nominal Units for Stellar & Planetary Astronomy|IAU Inter-Division A-G Working Group on Nominal Units for Stellar & Planetary Astronomy|IAU Inter-Division A-G Working Group on Nominal Units for Stellar & Planetary Astronomy|IAU Inter-Division A-G Working Group on Nominal Units for Stellar & Planetary Astronomy|IAU Inter-Division A-G Working Group on Nominal Units for Stellar & Planetary Astronomy|IAU Inter-Division A-G Working Group on Nominal Units for Stellar & Planetary Astronomy|IAU Inter-Division A-G Working Group on Nominal Units for Stellar & Planetary Astronomy|IAU Inter-Division A-G Working Group on Nominal Units for Stellar & Planetary Astronomy|IAU Inter-Division A-G Working Group on Nominal Units for Stellar & Planetary Astronomy|IAU Inter-Division A-G Working Group on Nominal Units for Stellar & Planetary Astronomy","http://arxiv.org/abs/1510.06262v2","http://arxiv.org/pdf/1510.06262v2","","4 pages, IAU 2015 Resolution B2, passed by the XXIXth IAU General
  Assembly in Honolulu, 13 August 2015. All IAU resolutions can be viewed at
  https://www.iau.org/administration/resolutions/general_assemblies/","","","astro-ph.SR","astro-ph.SR|astro-ph.EP|astro-ph.GA|astro-ph.IM"
"651","1510.06413v1","2015-10-21 20:12:14","2015-10-21 20:12:14","Automatic Detection of Magnetic delta in Sunspot Groups","  Large and magnetically complex sunspot groups are known to be associated with
flares. To date, the Mount Wilson scheme has been used to classify sunspot
groups based on their morphological and magnetic properties. The most flare
prolific class, the delta sunspot-group, is characterised by opposite polarity
umbrae within a common penumbra, separated by less than 2 degrees. In this
article, we present a new system, called the Solar Monitor Active Region
Tracker - Delta Finder (SMART-DF), that can be used to automatically detect and
classify magnetic deltas in near-realtime. Using continuum images and
magnetograms from the Helioseismic and Magnetic Imager (HMI) onboard NASA's
Solar Dynamics Observatory (SDO), we first estimate distances between opposite
polarity umbrae. Opposite polarity pairs having distances of less that 2
degrees are then identified, and if these pairs are found to share a common
penumbra, they are identified as a magnetic delta configuration. The algorithm
was compared to manual delta detections reported by the Space Weather
Prediction Center (SWPC), operated by the National Oceanic and Atmospheric
Administration (NOAA). SMART-DF detected 21 out of 23 active regions (ARs) that
were marked as delta spots by NOAA during 2011 - 2012 (within +/- 60 degrees
longitude). SMART-DF in addition detected five ARs which were not announced as
delta spots by NOAA. The near-relatime operation of SMART-DF resulted in many
deltas being identified in advance of NOAA's daily notification. SMART-DF will
be integrated with SolarMonitor (www.solarmonitor.org) and the near-realtime
information will be available to the public.
","Sreejith Padinhatteeri|Paul A. Higgins|D. Shaun Bloomfield|Peter T. Gallagher","","http://arxiv.org/abs/1510.06413v1","http://arxiv.org/pdf/1510.06413v1","http://dx.doi.org/10.1007/s11207-015-0808-7","14 pages, 5 figures, accepted for publication in Solarphysics","","10.1007/s11207-015-0808-7","astro-ph.SR","astro-ph.SR"
"652","1510.07674v1","2015-10-26 20:52:26","2015-10-26 20:52:26","IAU 2015 Resolution B3 on Recommended Nominal Conversion Constants for
  Selected Solar and Planetary Properties","  Astronomers commonly quote the properties of celestial objects in units of
parameters for the Sun, Jupiter, or the Earth. The resolution presented here
was proposed by the IAU Inter-Division Working Group on Nominal Units for
Stellar and Planetary Astronomy and passed by the XXIXth IAU General Assembly
in Honolulu. IAU 2015 Resolution B3 adopts a set of nominal solar, terrestrial,
and jovian conversion constants for stellar and (exo)planetary astronomy which
are defined to be exact SI values. While the nominal constants are based on
current best estimates (CBEs; which have uncertainties, are not secularly
constant, and are updated regularly using new observations), they should be
interpreted as standard values and not as CBEs. IAU 2015 Resolution B3 adopts
five solar conversion constants (nominal solar radius, nominal total solar
irradiance, nominal solar luminosity, nominal solar effective temperature, and
nominal solar mass parameter) and six planetary conversion constants (nominal
terrestrial equatorial radius, nominal terrestrial polar radius, nominal jovian
equatorial radius, nominal jovian polar radius, nominal terrestrial mass
parameter, and nominal jovian mass parameter).
","E. E. Mamajek|A. Prsa|G. Torres|P. Harmanec|M. Asplund|P. D. Bennett|N. Capitaine|J. Christensen-Dalsgaard|E. Depagne|W. M. Folkner|M. Haberreiter|S. Hekker|J. L. Hilton|V. Kostov|D. W. Kurtz|J. Laskar|B. D. Mason|E. F. Milone|M. M. Montgomery|M. T. Richards|J. Schou|S. G. Stewart","IAU Inter-Division A-G Working Group on Nominal Units for Stellar & Planetary Astronomy|IAU Inter-Division A-G Working Group on Nominal Units for Stellar & Planetary Astronomy|IAU Inter-Division A-G Working Group on Nominal Units for Stellar & Planetary Astronomy|IAU Inter-Division A-G Working Group on Nominal Units for Stellar & Planetary Astronomy|IAU Inter-Division A-G Working Group on Nominal Units for Stellar & Planetary Astronomy|IAU Inter-Division A-G Working Group on Nominal Units for Stellar & Planetary Astronomy|IAU Inter-Division A-G Working Group on Nominal Units for Stellar & Planetary Astronomy|IAU Inter-Division A-G Working Group on Nominal Units for Stellar & Planetary Astronomy|IAU Inter-Division A-G Working Group on Nominal Units for Stellar & Planetary Astronomy|IAU Inter-Division A-G Working Group on Nominal Units for Stellar & Planetary Astronomy|IAU Inter-Division A-G Working Group on Nominal Units for Stellar & Planetary Astronomy|IAU Inter-Division A-G Working Group on Nominal Units for Stellar & Planetary Astronomy|IAU Inter-Division A-G Working Group on Nominal Units for Stellar & Planetary Astronomy|IAU Inter-Division A-G Working Group on Nominal Units for Stellar & Planetary Astronomy|IAU Inter-Division A-G Working Group on Nominal Units for Stellar & Planetary Astronomy|IAU Inter-Division A-G Working Group on Nominal Units for Stellar & Planetary Astronomy|IAU Inter-Division A-G Working Group on Nominal Units for Stellar & Planetary Astronomy|IAU Inter-Division A-G Working Group on Nominal Units for Stellar & Planetary Astronomy|IAU Inter-Division A-G Working Group on Nominal Units for Stellar & Planetary Astronomy|IAU Inter-Division A-G Working Group on Nominal Units for Stellar & Planetary Astronomy|IAU Inter-Division A-G Working Group on Nominal Units for Stellar & Planetary Astronomy|IAU Inter-Division A-G Working Group on Nominal Units for Stellar & Planetary Astronomy","http://arxiv.org/abs/1510.07674v1","http://arxiv.org/pdf/1510.07674v1","","6 pages, IAU 2015 Resolution B3, passed by the XXIXth IAU General
  Assembly in Honolulu, 13 August 2015. All IAU resolutions can be viewed at
  https://www.iau.org/administration/resolutions/general_assemblies/","","","astro-ph.SR","astro-ph.SR|astro-ph.EP|astro-ph.GA|astro-ph.HE|astro-ph.IM"
"653","1510.08831v1","2015-10-29 19:27:27","2015-10-29 19:27:27","Antisocial pool rewarding does not deter public cooperation","  Rewarding cooperation is in many ways expected behaviour from social players.
However, strategies that promote antisocial behaviour are also surprisingly
common, not just in human societies, but also among eusocial insects and
bacteria. Examples include sanctioning of individuals who behave prosocially,
or rewarding of freeriders who do not contribute to collective enterprises. We
therefore study the public goods game with antisocial and prosocial pool
rewarding in order to determine the potential negative consequences on the
effectiveness of positive incentives to promote cooperation. Contrary to a
naive expectation, we show that the ability of defectors to distribute rewards
to their like does not deter public cooperation as long as cooperators are able
to do the same. Even in the presence of antisocial rewarding the spatial
selection for cooperation in evolutionary social dilemmas is enhanced. Since
the administration of rewards to either strategy requires a considerable degree
of aggregation, cooperators can enjoy the benefits of their prosocial
contributions as well as the corresponding rewards. Defectors when aggregated,
on the other hand, can enjoy antisocial rewards, but due to their lack of
contributions to the public good they ultimately succumb to their inherent
inability to secure a sustainable future. Strategies that facilitate the
aggregation of akin players, even if they seek to promote antisocial behaviour,
thus always enhance the long-term benefits of cooperation.
","Attila Szolnoki|Matjaz Perc","","http://arxiv.org/abs/1510.08831v1","http://arxiv.org/pdf/1510.08831v1","http://dx.doi.org/10.1098/rspb.2015.1975","9 two-column pages, 5 figures; accepted for publication in
  Proceedings of the Royal Society B","Proc. R. Soc. B 282 (2015) 20151975","10.1098/rspb.2015.1975","q-bio.PE","q-bio.PE|physics.soc-ph"
"654","1511.02183v1","2015-11-06 18:26:42","2015-11-06 18:26:42","Transcranial electrical stimulation: An introduction","  The main objective of the electrical stimulation of the brain is to generate
action potentials from the application of electromagnetic fields. Among the
available techniques, transcranial electrical stimulation (TES) represents a
popular method of administration that has the advantage of being non-invasive
and economically more affordable. This article aims to briefly introduce the
reader into the understanding of TES in terms of the physics involved as well
as for some of the relevant results of studies applying this technique.
","Carlos G. Tarazona|Carlos E. Guerra|Laura Chavez|Sebastian Andrade","","http://arxiv.org/abs/1511.02183v1","http://arxiv.org/pdf/1511.02183v1","","5 pages, in Spanish","","","physics.med-ph","physics.med-ph"
"655","1511.03555v2","2015-11-11 16:20:56","2017-02-13 19:49:04","A Location-Based Privacy Preserving Framework for M-Learning Adoption to
  Enhance Distance Education in Kenya: Literature Review","  The aim of this paper is to study m-learning literature in order to propose
and develop a privacy-preserving framework which can be used to foster
sustainable deployment of mobile learning within open and distance education in
Kenya. Location-based privacy in mobile learning is essential to retain users
trust, key to influencing usage intention. Any risk on privacy can negatively
affect users perceptions of a systems reliability and trustworthiness. While
extant studies have proposed frameworks for mobile technologies adoption into
learning, few have integrated privacy aspects and their influence on m-learning
implementation. The framework would provide University management with informed
approach to consider privacy preserving aspects in m-learning implementation.
Also, it could provide enlightened guidance to mobile learning application
developers on the need to cater for learners privacy aspects.
","Peter B. Obiria|Micheal W. Kimwele|Wilson K. Cheruiyot|Gitau Mwangi","","http://arxiv.org/abs/1511.03555v2","http://arxiv.org/pdf/1511.03555v2","","arXiv admin note: submission has been withdrawn by arXiv
  administrators due to inappropriate text reuse from external sources","","","cs.CY","cs.CY"
"656","1511.03609v1","2015-11-11 19:17:38","2015-11-11 19:17:38","Automated Dynamic Firmware Analysis at Scale: A Case Study on Embedded
  Web Interfaces","  Embedded devices are becoming more widespread, interconnected, and
web-enabled than ever. However, recent studies showed that these devices are
far from being secure. Moreover, many embedded systems rely on web interfaces
for user interaction or administration. Unfortunately, web security is known to
be difficult, and therefore the web interfaces of embedded systems represent a
considerable attack surface.
  In this paper, we present the first fully automated framework that applies
dynamic firmware analysis techniques to achieve, in a scalable manner,
automated vulnerability discovery within embedded firmware images. We apply our
framework to study the security of embedded web interfaces running in
Commercial Off-The-Shelf (COTS) embedded devices, such as routers, DSL/cable
modems, VoIP phones, IP/CCTV cameras. We introduce a methodology and implement
a scalable framework for discovery of vulnerabilities in embedded web
interfaces regardless of the vendor, device, or architecture. To achieve this
goal, our framework performs full system emulation to achieve the execution of
firmware images in a software-only environment, i.e., without involving any
physical embedded devices. Then, we analyze the web interfaces within the
firmware using both static and dynamic tools. We also present some interesting
case-studies, and discuss the main challenges associated with the dynamic
analysis of firmware images and their web interfaces and network services. The
observations we make in this paper shed light on an important aspect of
embedded devices which was not previously studied at a large scale.
  We validate our framework by testing it on 1925 firmware images from 54
different vendors. We discover important vulnerabilities in 185 firmware
images, affecting nearly a quarter of vendors in our dataset. These
experimental results demonstrate the effectiveness of our approach.
","Andrei Costin|Apostolis Zarras|Aurelien Francillon","","http://arxiv.org/abs/1511.03609v1","http://arxiv.org/pdf/1511.03609v1","","","","","cs.CR","cs.CR|cs.DC|cs.NI|D.4.6; K.4.4; K.6.5; K.6.m; C.3; D.2.7; C.1.4; H.3.5"
"657","1511.03650v3","2015-11-11 20:54:28","2015-12-07 14:20:01","Piecewise Linear Activation Functions For More Efficient Deep Networks","  This submission has been withdrawn by arXiv administrators because it is
intentionally incomplete, which is in violation of our policies.
","Cheng-Yang Fu|Alexander C. Berg","","http://arxiv.org/abs/1511.03650v3","http://arxiv.org/pdf/1511.03650v3","","Withdrawn by arXiv admins","","","cs.CV","cs.CV"
"658","1511.05615v2","2015-11-17 23:21:20","2016-05-31 15:42:43","A pragmatic approach to sovereignty on Mars","  Rising interest in Mars colonization from both private and public sectors
necessitates a renewed discussion about sovereignty in space. The
non-appropriation principle of the Outer Space Treaty currently prohibits any
sovereign claims to celestial bodies, but it remains unclear how this principle
should be applied to the peaceful colonization of Mars. Here we develop a
pragmatic approach to guide the settlement of Mars, which is based upon a
""bounded first possession"" model with mandatory planetary parks. Scientists,
experts, and leaders will establish planetary park locations and regulations
through worldwide community solicitation in order to protect sites of
scientific, aesthetic, historical, cultural, environmental, spiritual value.
Colonization parties may occupy limited plots of martian land and may claim
exclusive economic rights within this zone, while still refraining from any
claims to sovereignty. All colonists remain under the legal jurisdiction of
their host nation, with conflicts to be resolved diplomatically or through a
temporary tribunal system composed of representatives from other Mars colonies.
We also propose the formation of a Mars Secretariat as an administrative body
with limited power to facilitate communication among parties. Our model for
Mars colonization remains consistent with the Outer Space Treaty, but we also
recommend revisiting or amending the non-appropriation and province of mankind
principles to resolve the ambiguity of how nations, corporations, and
individuals may utilize the resources of space.
","Sara Bruhns|Jacob Haqq-Misra","","http://arxiv.org/abs/1511.05615v2","http://arxiv.org/pdf/1511.05615v2","http://dx.doi.org/10.1016/j.spacepol.2016.05.008","Published in Space Policy, 14 pages","","10.1016/j.spacepol.2016.05.008","physics.pop-ph","physics.pop-ph|physics.soc-ph"
"659","1511.05911v2","2015-11-18 19:03:41","2015-11-19 16:39:41","Behavior Query Discovery in System-Generated Temporal Graphs","  Computer system monitoring generates huge amounts of logs that record the
interaction of system entities. How to query such data to better understand
system behaviors and identify potential system risks and malicious behaviors
becomes a challenging task for system administrators due to the dynamics and
heterogeneity of the data. System monitoring data are essentially heterogeneous
temporal graphs with nodes being system entities and edges being their
interactions over time. Given the complexity of such graphs, it becomes
time-consuming for system administrators to manually formulate useful queries
in order to examine abnormal activities, attacks, and vulnerabilities in
computer systems.
  In this work, we investigate how to query temporal graphs and treat query
formulation as a discriminative temporal graph pattern mining problem. We
introduce TGMiner to mine discriminative patterns from system logs, and these
patterns can be taken as templates for building more complex queries. TGMiner
leverages temporal information in graphs to prune graph patterns that share
similar growth trend without compromising pattern quality. Experimental results
on real system data show that TGMiner is 6-32 times faster than baseline
methods. The discovered patterns were verified by system experts; they achieved
high precision (97%) and recall (91%).
","Bo Zong|Xusheng Xiao|Zhichun Li|Zhenyu Wu|Zhiyun Qian|Xifeng Yan|Ambuj K. Singh|Guofei Jiang","","http://arxiv.org/abs/1511.05911v2","http://arxiv.org/pdf/1511.05911v2","","The full version of the paper ""Behavior Query Discovery in
  System-Generated Temporal Graphs"", to appear in VLDB'16","","","cs.SI","cs.SI|cs.AI|cs.DB"
"660","1511.06420v2","2015-11-19 22:15:46","2015-11-24 02:30:16","Skip-Thought Memory Networks","  Question Answering (QA) is fundamental to natural language processing in that
most nlp problems can be phrased as QA (Kumar et al., 2015). Current weakly
supervised memory network models that have been proposed so far struggle at
answering questions that involve relations among multiple entities (such as
facebook's bAbi qa5-three-arg-relations in (Weston et al., 2015)). To address
this problem of learning multi-argument multi-hop semantic relations for the
purpose of QA, we propose a method that combines the jointly learned long-term
read-write memory and attentive inference components of end-to-end memory
networks (MemN2N) (Sukhbaatar et al., 2015) with distributed sentence vector
representations encoded by a Skip-Thought model (Kiros et al., 2015). This
choice to append Skip-Thought Vectors to the existing MemN2N framework is
motivated by the fact that Skip-Thought Vectors have been shown to accurately
model multi-argument semantic relations (Kiros et al., 2015).
","Ethan Caballero","","http://arxiv.org/abs/1511.06420v2","http://arxiv.org/pdf/1511.06420v2","","Removed by arXiv administrators because submission violated the terms
  of arXiv's license agreement","","","cs.NE","cs.NE|cs.CL|cs.LG"
"661","1511.07473v2","2015-11-20 19:09:54","2016-08-11 17:56:35","High-speed FSK Modulator Using Switched-capacitor Resonators","  In this paper, an ultra-fast frequency shift-keying (FSK) modulation
technique based on switched capacitor resonators is presented. It is
demonstrated that switching a reactive component such as a capacitor, in a
high-Q resonator with proper switching signal can preserve the stored energy
and shift it to a different frequency. Switching boundaries are found by
continuity of electric charge and magnetic flux. It is shown that if switching
time is synchronous with zero crossing of the voltage signal across the
switched capacitor, impulsive components can be avoided and continuity of
electric charge is satisfied without energy dissipation. We use this property
to realize a fast binary frequency-shift keying (FSK) modulator with only a
single RF source. In this technique, the modulation rate is independent of the
resonator bandwidth and can be as high as the lower carrier frequency.
Experimental results are presented to validate the simulations.
","Mohsen Salehi","","http://arxiv.org/abs/1511.07473v2","http://arxiv.org/pdf/1511.07473v2","http://dx.doi.org/10.1002/cta.2157","This article has been administratively withdrawn because of
  authorship dispute and retraction by the journal","","10.1002/cta.2157","physics.ins-det","physics.ins-det"
"662","1511.06896v3","2015-11-21 16:25:27","2015-12-04 11:48:22","Bayesian binary quantile regression for the analysis of Bachelor-Master
  transition","  The multi-cycle organization of modern university systems stimulates the
interest in studying the progression to higher level degree courses during the
academic career. In particular, after the achievement of the first level
qualification (Bachelor degree), students have to decide whether to continue
their university studies, by enrolling in a second level (Master) programme, or
to conclude their training experience. In this work we propose a binary
quantile regression approach to analyze the Bachelor-Master transition
phenomenon with the adoption of the Bayesian inferential perspective. In
addition to the traditional predictors of academic outcomes, such as the
personal characteristics and the field of study, different aspects of the
student's performance are considered. Moreover, a new contextual variable,
indicating the type of university regulations, is taken into account in the
model specification. The utility of the Bayesian binary quantile regression to
characterize the non-continuation decision after the first cycle studies is
illustrated with an application to administrative data of Bachelor graduates at
the School of Economics of Sapienza University of Rome and compared with a more
conventional logistic regression approach.
","Cristina Mollica|Lea Petrella","","http://arxiv.org/abs/1511.06896v3","http://arxiv.org/pdf/1511.06896v3","","24 pages, 7 figures and 3 tables","","","stat.AP","stat.AP"
"663","1511.06952v2","2015-11-22 02:40:42","2015-12-04 19:58:32","Minimality in General PDE","  This submission has been withdrawn by arXiv administrators as it is a
machine-generated paper.
","Eli D. Sadoff","","http://arxiv.org/abs/1511.06952v2","http://arxiv.org/pdf/1511.06952v2","","withdrawn by arXiv administrators","","","math.AP","math.AP"
"664","1512.01925v2","2015-12-07 06:18:06","2017-08-03 15:36:08","The Baltic Meetings 1957 to 1967","  The Baltic meetings of astronomers from Northern Germany and Scandinavia
began in 1957 and gathered up to 70 participants. Reports of the presentations
are available from all meetings, providing an overview of the interests of
astronomers in this part of the world 50 years ago. Most interesting to see for
a young astronomer in our days, I think, is that a large part of the time was
about astrometry. This focus on astrometry was the basis for the scientific
knowhow which made the idea of space astrometry realistic, resulting in the
approval by ESA of the first astrometry satellite Hipparcos in 1980 which
brought a revolution of high-precision astrometry of positions, motions and
distances of stars. The correspondence with ten observatories shows that only
one of them has any archive of letters at all from the 1950s, that is in
Copenhagen where about 7000 letters on scientific and administrative matters
are extant. - These letters have now been stored in the Rigsarkivet.
","Erik H©ªg","","http://arxiv.org/abs/1512.01925v2","http://arxiv.org/pdf/1512.01925v2","","11 pages, 3 figures. In 2017 the invalid references to dropbox
  changed to my website. Accepted for publication in Nuncius Hamburgensis,
  Volume 38 (2016)","","","physics.hist-ph","physics.hist-ph|astro-ph.IM"
"665","1512.03934v1","2015-12-12 16:48:38","2015-12-12 16:48:38","Fast and flexible interpolation via PUM with applications in population
  dynamics","  In this paper the Partition of Unity Method (PUM) is efficiently performed
using Radial Basis Functions (RBFs) as local approximants. In particular, we
present a new space-partitioning data structure extremely useful in
applications because of its independence from the problem geometry. Moreover,
we study, in the context of wild herbivores in forests, an application of such
algorithm. This investigation shows that the ecosystem of the considered
natural park is in a very delicate situation, for which the animal population
could become extinguished. The determination of the so-called sensitivity
surfaces, obtained with the new fast and flexible interpolation tool, indicates
some possible preventive measures to the park administrators.
","Alessandra De Rossi|Roberto Cavoretto|Emma Perracchione","","http://arxiv.org/abs/1512.03934v1","http://arxiv.org/pdf/1512.03934v1","","","","","math.NA","math.NA"
"666","1512.09370v1","2015-12-12 19:02:03","2015-12-12 19:02:03","Wild herbivores in forests: four case studies","  A three population system with a top predator population, i.e. the
herbivores, and two prey populations, grass and trees, is considered to model
the interaction of herbivores with natural resources. We apply the model for
four natural mountain parks in Northern Italy, three located in the Eastern
Alps, two of which in the Dolomites and one in the Julian Alps, and one in the
Maritime Alps, Northwest Italy. The simulations, based on actual data gathered
from contacts with rangers and parks administrators, field samplings and
published material, provide useful information on the behavior of the
vegetation-wild herbivores interactions and the possible medium-long term
evolution of these ecosystems. At the same time they show that these ecosystems
are in a very delicate situation, for which the animal populations could become
extinguished in case of adverse environmental conditions. The determination of
the so called sensitivity surfaces support our findings and indicate some
possible preventive measures to the park admistrators.
","Giorgio Sabetta|Emma Perracchione|Ezio Venturino","","http://arxiv.org/abs/1512.09370v1","http://arxiv.org/pdf/1512.09370v1","","","","","q-bio.PE","q-bio.PE"
"667","1512.04177v3","2015-12-14 05:36:07","2016-07-20 02:12:59","Conflict and Computation on Wikipedia: a Finite-State Machine Analysis
  of Editor Interactions","  What is the boundary between a vigorous argument and a breakdown of
relations? What drives a group of individuals across it? Taking Wikipedia as a
test case, we use a hidden Markov model to approximate the computational
structure and social grammar of more than a decade of cooperation and conflict
among its editors. Across a wide range of pages, we discover a bursty war/peace
structure where the systems can become trapped, sometimes for months, in a
computational subspace associated with significantly higher levels of
conflict-tracking ""revert"" actions. Distinct patterns of behavior characterize
the lower-conflict subspace, including tit-for-tat reversion. While a fraction
of the transitions between these subspaces are associated with top-down actions
taken by administrators, the effects are weak. Surprisingly, we find no
statistical signal that transitions are associated with the appearance of
particularly anti-social users, and only weak association with significant news
events outside the system. These findings are consistent with transitions being
driven by decentralized processes with no clear locus of control. Models of
belief revision in the presence of a common resource for information-sharing
predict the existence of two distinct phases: a disordered high-conflict phase,
and a frozen phase with spontaneously-broken symmetry. The bistability we
observe empirically may be a consequence of editor turn-over, which drives the
system to a critical point between them.
","Simon DeDeo","","http://arxiv.org/abs/1512.04177v3","http://arxiv.org/pdf/1512.04177v3","http://dx.doi.org/10.3390/fi8030031","23 pages, 3 figures. Matches published version. Code for HMM fitting
  available at http://bit.ly/sfihmm ; time series and derived finite state
  machines at bit.ly/wiki_hmm","Future Internet 2016, 8(3), 31","10.3390/fi8030031","cs.SI","cs.SI|physics.soc-ph|q-bio.PE"
"668","1512.06896v1","2015-12-21 22:35:18","2015-12-21 22:35:18","A mathematical model of granulopoiesis incorporating the negative
  feedback dynamics and kinetics of G-CSF/neutrophil binding and
  internalisation","  We develop a physiological model of granulopoiesis which includes explicit
modelling of the kinetics of the cytokine granulocyte colony-stimulating factor
(G-CSF) incorporating both the freely circulating concentration and the
concentration of the cytokine bound to mature neutrophils. G-CSF concentrations
are used to directly regulate neutrophil production, with the rate of
differentiation of stem cells to neutrophil precursors, the effective
proliferation rate in mitosis, the maturation time, and the release rate from
the mature marrow reservoir into circulation all dependent on the level of
G-CSF in the system. The dependence of the maturation time on the cytokine
concentration introduces a state-dependent delay into our differential equation
model, and we show how this is derived from an age-structured partial
differential equation model of the mitosis and maturation, and also detail the
derivation of the rest of our model. The model and its estimated parameters are
shown to successfully predict the neutrophil and G-CSF responses to a variety
of treatment scenarios, including the combined administration of chemotherapy
and exogenous G-CSF. This concomitant treatment was reproduced without any
additional fitting to characterise drug-drug interactions.
","Morgan Craig|Antony R Humphries|Michael C Mackey","","http://arxiv.org/abs/1512.06896v1","http://arxiv.org/pdf/1512.06896v1","http://dx.doi.org/10.1007/s11538-016-0179-8","","Bulletin of Mathematical Biology, 78(12). 2304-2357 (2016)","10.1007/s11538-016-0179-8","q-bio.CB","q-bio.CB|q-bio.TO"
"669","1512.08982v1","2015-12-30 15:46:30","2015-12-30 15:46:30","Technical Report: a tool for measuring Prosodic Accommodation","  This article has been withdrawn by arXiv administrators because the submitter
did not have the legal authority to grant the license applied to the work.
","Sucheta Ghosh","","http://arxiv.org/abs/1512.08982v1","http://arxiv.org/pdf/1512.08982v1","","Withdrawn by arXiv administrators","","","cs.SD","cs.SD|cs.CL"
"670","1601.00709v1","2016-01-05 00:32:06","2016-01-05 00:32:06","Materials Design for New Superconductors","  Since the announcement in 2011 of the Materials Genome Initiative by the
Obama administration, much attention has been given to the subject of materials
design to accelerate the discovery of new materials that could have
technological implications. Although having its biggest impact for more applied
materials like batteries, there is increasing interest in applying these ideas
to predict new superconductors. This is obviously a challenge, given that
superconductivity is a many body phenomenon, with whole classes of known
superconductors lacking a quantitative theory. Given this caveat, various
efforts to formulate materials design principles for superconductors are
reviewed here, with a focus on surveying the periodic table in an attempt to
identify cuprate analogues.
","M. R. Norman","","http://arxiv.org/abs/1601.00709v1","http://arxiv.org/pdf/1601.00709v1","http://dx.doi.org/10.1088/0034-4885/79/7/074502","mini-review for Reports on Progress in Physics special issue on
  Strongly Correlated Electron Systems","Rep. Prog. Phys. 79, 074502 (2016)","10.1088/0034-4885/79/7/074502","cond-mat.supr-con","cond-mat.supr-con"
"671","1601.01396v3","2016-01-07 04:25:27","2016-05-06 17:30:25","On the Computation of the Optimal Connecting Points in Road Networks","  In this paper we consider a set of travelers, starting from likely different
locations towards a common destination within a road network, and propose
solutions to find the optimal connecting points for them. A connecting point is
a vertex of the network where a subset of the travelers meet and continue
traveling together towards the next connecting point or the destination. The
notion of optimality is with regard to a given aggregated travel cost, e.g.,
travel distance or shared fuel cost. This problem by itself is new and we make
it even more interesting (and complex) by considering affinity factors among
the users, i.e., how much a user likes to travel together with another one.
This plays a fundamental role in determining where the connecting points are
and how subsets of travelers are formed. We propose three methods for
addressing this problem, one that relies on a fast and greedy approach that
finds a sub-optimal solution, and two others that yield globally optimal
solution. We evaluate all proposed approaches through experiments, where
collections of real datasets are used to assess the trade-offs, behavior and
characteristics of each method.
","George Tsatsanifos","","http://arxiv.org/abs/1601.01396v3","http://arxiv.org/pdf/1601.01396v3","","This submission has been withdrawn by arXiv administrators due to
  disputed authorship","","","cs.DS","cs.DS|05C85|H.3.3"
"672","1601.01556v2","2016-01-07 15:09:40","2016-01-12 17:54:26","Towards a Semantic Administrative Shell for Industry 4.0 Components","  In the engineering and manufacturing domain, there is currently an atmosphere
of departure to a new era of digitized production. In different regions,
initiatives in these directions are known under different names, such as
industrie du futur in France, industrial internet in the US or Industrie 4.0 in
Germany. While the vision of digitizing production and manufacturing gained
much traction lately, it is still relatively unclear how this vision can
actually be implemented with concrete standards and technologies. Within the
German Industry 4.0 initiative, the concept of an Administrative Shell was
devised to respond to these requirements. The Administrative Shell is planned
to provide a digital representation of all information being available about
and from an object which can be a hardware system or a software platform. In
this paper, we present an approach to developing such a digital representation
based on semantic knowledge representation formalisms such as RDF, RDF Schema
and OWL. We present our concept of a Semantic I4.0 Component which addresses
the communication and comprehension challenges in Industry 4.0 scenarios using
semantic technologies. Our approach is illustrated with a concrete example
showing its benefits in a real-world use case.
","Irlan Grangel-Gonzalez|Lavdim Halilaj|Gokhan Coskun|Soren Auer|Diego Collarana|Michael Hoffmeister","","http://arxiv.org/abs/1601.01556v2","http://arxiv.org/pdf/1601.01556v2","","","","","cs.SE","cs.SE"
"673","1601.01747v1","2016-01-08 02:07:47","2016-01-08 02:07:47","An HCI View of Configuration Problems","  In recent years, configuration problems have drawn tremendous attention
because of their increasing prevalence and their big impact on system
availability. We believe that many of these problems are attributable to
today's configuration interfaces that have not evolved to accommodate the
enormous shift of the system administrator group. Plain text files, as the de
facto configuration interfaces, assume administrators' understanding of the
system under configuration. They ask administrators to directly edit the
corresponding entries with little guidance or assistance. However, this
assumption no longer holds for todays administrator group which has expanded
greatly to include non- and semi-professional administrators. In this paper, we
provide an HCI view of today's configuration problems, and articulate system
configuration as a new HCI problem. Moreover, we present the top obstacles to
correctly and efficiently configuring software systems, and most importantly
their implications on the design and implementation of new-generation
configuration interfaces.
","Tianyin Xu|Vineet Pandey|Scott Klemmer","","http://arxiv.org/abs/1601.01747v1","http://arxiv.org/pdf/1601.01747v1","","9 pages of exploratory research on understanding system configuration
  problems using Human-Computer Interaction principles","","","cs.HC","cs.HC|cs.SE"
"674","1601.02130v1","2016-01-09 16:52:23","2016-01-09 16:52:23","SENDIM for Incremental Development of Cloud Networks","  Due to the limited and varying availability of cheap infrastructure and
resources, cloud network systems and applications are tested in simulation and
emulation environments prior to physical deployments, at different stages of
development. Configuration management tools manage deployments and migrations
across different cloud platforms, mitigating tedious system administration
efforts. However, currently a cloud networking simulation cannot be migrated as
an emulation, or vice versa, without rewriting and manually re-deploying the
simulated application. This paper presents SENDIM (Sendim is a northeastern
Portuguese town close to the Spanish border, where the rare Mirandese language
is spoken), a Simulation, Emulation, aNd Deployment Integration Middleware for
cloud networks. As an orchestration platform for incrementally building
Software-Defined Cloud Networks (SDCN), SENDIM manages the development and
deployment of algorithms and architectures the entire length from
visualization, simulation, emulation, to physical deployments. Hence, SENDIM
optimizes the evaluation of cloud networks.
","Pradeeban Kathiravelu|Luis Veiga","","http://arxiv.org/abs/1601.02130v1","http://arxiv.org/pdf/1601.02130v1","","","","","cs.NI","cs.NI|cs.DC"
"675","1601.05329v2","2016-01-20 17:01:17","2016-01-23 11:21:52","Review and Analysis of Networking Challenges in Cloud Computing","  Cloud Computing offers virtualized computing, storage, and networking
resources, over the Internet, to organizations and individual users in a
completely dynamic way. These cloud resources are cheaper, easier to manage,
and more elastic than sets of local, physical, ones. This encourages customers
to outsource their applications and services to the cloud. The migration of
both data and applications outside the administrative domain of customers into
a shared environment imposes transversal, functional problems across distinct
platforms and technologies. This article provides a contemporary discussion of
the most relevant functional problems associated with the current evolution of
Cloud Computing, mainly from the network perspective. The paper also gives a
concise description of Cloud Computing concepts and technologies. It starts
with a brief history about cloud computing, tracing its roots. Then,
architectural models of cloud services are described, and the most relevant
products for Cloud Computing are briefly discussed along with a comprehensive
literature review. The paper highlights and analyzes the most pertinent and
practical network issues of relevance to the provision of high-assurance cloud
services through the Internet, including security. Finally, trends and future
research directions are also presented.
","Jose Moura|David Hutchison","","http://arxiv.org/abs/1601.05329v2","http://arxiv.org/pdf/1601.05329v2","http://dx.doi.org/10.1016/j.jnca.2015.11.015","","Journal of Network and Computer Applications, vol. 60, pp.
  113-129, 2016","10.1016/j.jnca.2015.11.015","cs.NI","cs.NI"
"676","1601.05664v1","2016-01-21 14:56:31","2016-01-21 14:56:31","Defining urban agglomerations to detect agglomeration economies","  Agglomeration economies are a persistent subject of debate among economists
and urban planners. Their definition turns on whether or not larger cities and
regions are more efficient and more productive than smaller ones. We complement
existing discussion on agglomeration economies and the urban wage premium here
by providing a sensitivity analysis of estimated coefficients to different
delineations of urban agglomeration as well as to different definitions of the
economic measure that summarises the urban premium. This quantity can consist
of total wages measured at the place of work, or of income registered at the
place of residence. The chosen option influences the scaling behaviour of city
size as well as the spatial distribution of the phenomenon at the city level.
Spatial discrepancies between the distribution of jobs and the distribution of
households at different economic levels makes city definitions crucial to the
estimation of economic relations which vary with city size. We argue this point
by regressing measures of income and wage over about five thousands different
definitions of cities in France, based on our algorithmic aggregation of
administrative spatial units at regular cutoffs which reflect density,
population thresholds and commuting flows. We also go beyond aggregated
observations of wages and income by searching for evidence of larger
inequalities and economic segregation in the largest cities. This paper
therefore considers the spatial and economic complexity of cities with respect
to discussion about how we measure agglomeration economies. It provides a basis
for reflection on alternative ways to model the processes which lead to
observed variations, and this can provide insights for more comprehensive
regional planning.
","Clementine Cottineau|Olivier Finance|Erez Hatna|Elsa Arcaute|Michael Batty","","http://arxiv.org/abs/1601.05664v1","http://arxiv.org/pdf/1601.05664v1","http://dx.doi.org/10.1177/2399808318755146","21 pages, 13 figures, working paper","Cottineau, C., Finance, O., Hatna, E., Arcaute, E., & Batty, M.
  (2018). Defining urban clusters to detect agglomeration economies.
  Environment and Planning B: Urban Analytics and City Science,
  2399808318755146","10.1177/2399808318755146","physics.soc-ph","physics.soc-ph"
"677","1601.07896v3","2016-01-28 20:50:31","2016-06-09 14:38:00","Alternative model for the administration and analysis of research-based
  assessments","  Research-based assessments represent a valuable tool for both instructors and
researchers interested in improving undergraduate physics education. However,
the historical model for disseminating and propagating conceptual and
attitudinal assessments developed by the physics education research (PER)
community has not resulted in widespread adoption of these assessments within
the broader community of physics instructors. Within this historical model,
assessment developers create high quality, validated assessments, make them
available for a wide range of instructors to use, and provide minimal (if any)
support to assist with administration or analysis of the results. Here, we
present and discuss an alternative model for assessment dissemination, which is
characterized by centralized data collection and analysis. This model provides
a greater degree of support for both researchers and instructors in order to
more explicitly support adoption of research-based assessments. Specifically,
we describe our experiences developing a centralized, automated system for an
attitudinal assessment we previously created to examine students'
epistemologies and expectations about experimental physics. This system
provides a proof-of-concept that we use to discuss the advantages associated
with centralized administration and data collection for research-based
assessments in PER. We also discuss the challenges that we encountered while
developing, maintaining, and automating this system. Ultimately, we argue that
centralized administration and data collection for standardized assessments is
a viable and potentially advantageous alternative to the default model
characterized by decentralized administration and analysis. Moreover, with the
help of online administration and automation, this model can support the
long-term sustainability of centralized assessment systems.
","Bethany R. Wilcox|Benjamin M. Zwickl|Robert D. Hobbs|John M. Aiken|Nathan M. Welch|H. J. Lewandowski","","http://arxiv.org/abs/1601.07896v3","http://arxiv.org/pdf/1601.07896v3","http://dx.doi.org/10.1103/PhysRevPhysEducRes.12.010139","7 pages, 1 figure, accepted in Phys. Rev. PER","","10.1103/PhysRevPhysEducRes.12.010139","physics.ed-ph","physics.ed-ph"
"678","1601.08049v2","2016-01-29 10:54:56","2016-02-23 12:45:12","Individual Bibliometric Assessment @ University of Vienna: From Numbers
  to Multidimensional Profiles","  This paper shows how bibliometric assessment can be implemented at individual
level. This has been successfully done at the University of Vienna carried out
by the Bibliometrics and Publication Strategies Department of the Vienna
University Library. According to the department's philosophy, bibliometrics is
not only a helpful evaluation instrument in order to complement the peer review
system. It is also meant as a compass for researchers in the ""publish or
perish"" dilemma in order to increase general visibility and to optimize
publication strategies. The individual assessment comprises of an interview
with the researcher under evaluation, the elaboration of a bibliometric report
of the researcher's publication output, the discussion and validation of the
obtained results with the researcher under evaluation as well as further
optional analyses. The produced bibliometric reports are provided to the
researchers themselves and inform them about the quantitative aspects of their
research output. They also serve as a basis for further discussion concerning
their publication strategies. These reports are eventually intended for
informed peer review practices, and are therefore forwarded to the quality
assurance and the rector's office and finally sent to the peers. The most
important feature of the generated bibliometric report is its multidimensional
and individual character. It relies on a variety of basic indicators and
further control parameters in order to foster comprehensibility. Researchers,
administrative staff and peers alike have confirmed the usefulness of this
bibliometric approach. An increasing demand is noticeable. In total, 33
bibliometric reports have been delivered so far. Moreover, similar reports have
also been produced for the bibliometric assessment of two faculties with great
success.
","Juan Gorraiz|Martin Wieland|Christian Gumpenberger","","http://arxiv.org/abs/1601.08049v2","http://arxiv.org/pdf/1601.08049v2","http://dx.doi.org/10.5281/zenodo.45402","Preprint","","10.5281/zenodo.45402","cs.DL","cs.DL"
"679","1602.00258v1","2016-01-31 15:12:57","2016-01-31 15:12:57","Placebo Response is Driven by UCS Revaluation: Evidence,
  Neurophysiological Consequences and a Quantitative Model","  Despite growing scientific interest in the placebo effect and increasing
understanding of neurobiological mechanisms, theoretical modeling of the
placebo response remains poorly developed. The most extensively accepted
theories are expectation and conditioning, involving both conscious and
unconscious information processing. However, it is not completely understood
how these mechanisms can shape the placebo response. We focus here on neural
processes which can account for key properties of the response to substance
intake. It is shown that placebo response can be conceptualized as a reaction
of a distributed neural system within the central nervous system. Such a
reaction represents an integrated component of the response to open substance
administration (or to substance intake) and is updated through ""unconditioned
stimulus (UCS) revaluation"" learning. The analysis leads to a theorem, which
proves the existence of two distinct quantities coded within the brain, these
are the expected or prediction outcome and the reactive response. We show that
the reactive response is updated automatically by implicit revaluation lerning,
while the expected outcome can also be modulated through conscious information
processing. Conceptualizing the response to substance intake in terms of UCS
revaluation learning leads to the theoretical formulation of a potential
neuropharmacological treatment for increasing unlimitedly the effectiveness of
a given drug.
","Luca Puviani|Sidita Rama","","http://arxiv.org/abs/1602.00258v1","http://arxiv.org/pdf/1602.00258v1","","","","","q-bio.NC","q-bio.NC"
"680","1602.01888v2","2016-02-04 23:45:36","2016-02-08 03:25:38","Construction of Nano-Assembled Microcapsules Embedded with Gold
  Nanoparticles for use in Novel Drug Delivery Systems","  Coronary stents have changed the way in which coronary artery diseases are
managed. Although bare-metal stents can be traced back to 1994, long term
efficacy of these stents has been shattered by ISR (In-Stent restenosis) and
late stent thrombosis. Research on finding a solution to these issues has led
to the development of DESs (Drug eluting stents). However long term
effectiveness of DESs and various drug delivery systems have raised concerns.
Also current DESs does not have the capability to adjust the drug dose or
release kinetics that corresponds to the diseased status of the affected
vessel. Through the use of a drug delivery system which employs controlled drug
release, it may be possible to control the release rate of the pharmacological
drugs or substances of interest, and therefore one might be able to circumvent
the range of events that leads to ISR, thus preventing the need for further
invasive interventions. To overcome the limitations of drug delivery systems,
the administration of drugs using nanoparticle based microcapsules as carriers
is being researched for their ability to facilitate a sustained, prolonged and
controlled release of drugs. But still much research is needed to evaluate and
fix issues relating toxicity, chemical and mechanical properties of the
nanoparticles. This project combines a range of nanobiotechnological techniques
to synthesise nano-components which are finally assembled together to get the
microcapsules which could be used in drug delivery systems. The microcapsules
were engineered keeping in mind their potential use in an ultrasound based drug
delivery system.
","Abraham Samuel Finny","","http://arxiv.org/abs/1602.01888v2","http://arxiv.org/pdf/1602.01888v2","","62 pages, 20 figures, MSc thesis, University of Strathclyde, December
  2014, Submitted, Accepted and Published at http://suprimo.lib.strath.ac.uk","","","physics.med-ph","physics.med-ph|cond-mat.mtrl-sci|physics.bio-ph|physics.chem-ph|q-bio.BM"
"681","1602.06666v1","2016-02-22 06:58:36","2016-02-22 06:58:36","Helping Students Learn Quantum Mechanics for Quantum Computing","  Quantum information science and technology is a rapidly growing
interdisciplinary field drawing researchers from science and engineering
fields. Traditional instruction in quantum mechanics is insufficient to prepare
students for research in quantum computing because there is a lack of emphasis
in the current curriculum on quantum formalism and dynamics. We are
investigating the difficulties students have with quantum mechanics and are
developing and evaluating quantum interactive learning tutorials (QuILTs) to
reduce the difficulties. Our investigation includes interviews with individual
students and the development and administration of free-response and
multiple-choice tests. We discuss the implications of our research and
development project on helping students learn quantum mechanics relevant for
quantum computing.
","Chandralekha Singh","","http://arxiv.org/abs/1602.06666v1","http://arxiv.org/pdf/1602.06666v1","http://dx.doi.org/10.1063/1.2508687","4 pages, PACS: 1.40Fk,01.40.gb,01.40G-, Keywords: physics education
  research, pedagogy, teaching, learning, quantum mechanics, Proceedings of the
  2006 Physics Education Research Conference, Syracuse, NY, AIP Conference
  Proceedings, Melville New York 883, 42-45, (2007)","","10.1063/1.2508687","physics.ed-ph","physics.ed-ph"
"682","1603.01450v2","2016-03-04 13:25:56","2016-05-26 15:36:22","Sequential ranking under random semi-bandit feedback","  In many web applications, a recommendation is not a single item suggested to
a user but a list of possibly interesting contents that may be ranked in some
contexts. The combinatorial bandit problem has been studied quite extensively
these last two years and many theoretical results now exist : lower bounds on
the regret or asymptotically optimal algorithms. However, because of the
variety of situations that can be considered, results are designed to solve the
problem for a specific reward structure such as the Cascade Model. The present
work focuses on the problem of ranking items when the user is allowed to click
on several items while scanning the list from top to bottom.
","Hossein Vahabi|Paul Lagree|Claire Vernade|Olivier Cappe","","http://arxiv.org/abs/1603.01450v2","http://arxiv.org/pdf/1603.01450v2","","This submission has been withdrawn by arXiv administrators due to
  irreconcilable authorship dispute","","","cs.DS","cs.DS|cs.LG"
"683","1603.02034v2","2016-03-07 12:41:01","2016-04-06 15:26:51","Comments on ""Overdemodulation for High-Performance Receivers with
  Low-Resolution ADC""","  This submission has been withdrawn by arXiv administrators due to
misrepresentation of authorship.
","Tristan Afortiori","","http://arxiv.org/abs/1603.02034v2","http://arxiv.org/pdf/1603.02034v2","","submission withdrawn by arXiv administrators","","","cs.IT","cs.IT|math.IT"
"684","1603.02373v2","2016-03-08 03:59:53","2018-03-02 20:09:34","On point processes in multitarget tracking","  The finite-set statistics (FISST) approach to multitarget tracking was
introduced in the mid-1990s. Its current extended form dates from 2001. In
2008, an ""elementary"" alternative to FISST was proposed, based on ""finite point
processes"" rather than RFS's. This was accompanied by single-sensor and
multisensor versions of a claimed generalization of the PHD filter, the
""iFilter."" Then in 2013 in the Journal of Advances in Information Fusion (JAIF)
and elsewhere, the same author went on to claim that the FISST
p.g.fl./functional derivative approach is actually ""due to"" (a ""corollary"" of)
a 50-year-old pure-mathematics paper by Moyal; and described a ""point process""
p.g.fl./functional derivative approach to multitarget tracking supposedly based
on it. In this paper it is shown that: (1)non-RFS point processes are a
phenomenologically erroneous foundation for multitarget tracking; (2) nearly
every equation, concept, discussion, derivation, and methodology in the JAIF
paper originally appeared in FISST publications, without being so attributed;
(3) FISST cannot possibly be ""due to Moyal""; (4) the ""point process"" approach
described in JAIF differs from FISST only in regard to terminology and
notation, and thus in this sense appears to be an obscured, phenomenologically
erroneous, and improperly attributed copy of FISST. It is also shown that the
derivations of the single-sensor and multisensory iFilter appear to have had
major errors, as did a subsequent recasting of the multisensor iFilter as a
""traffic mapping filter.""
","Ronald Mahler","","http://arxiv.org/abs/1603.02373v2","http://arxiv.org/pdf/1603.02373v2","","This submission has been withdrawn by arXiv administrators for
  unprofessional language","","","stat.OT","stat.OT"
"685","1603.02604v1","2016-03-08 17:43:48","2016-03-08 17:43:48","Observing Trends in Automated Multilingual Media Analysis","  Any large organisation, be it public or private, monitors the media for
information to keep abreast of developments in their field of interest, and
usually also to become aware of positive or negative opinions expressed towards
them. At least for the written media, computer programs have become very
efficient at helping the human analysts significantly in their monitoring task
by gathering media reports, analysing them, detecting trends and - in some
cases - even to issue early warnings or to make predictions of likely future
developments. We present here trend recognition-related functionality of the
Europe Media Monitor (EMM) system, which was developed by the European
Commission's Joint Research Centre (JRC) for public administrations in the
European Union (EU) and beyond. EMM performs large-scale media analysis in up
to seventy languages and recognises various types of trends, some of them
combining information from news articles written in different languages and
from social media posts. EMM also lets users explore the huge amount of
multilingual media data through interactive maps and graphs, allowing them to
examine the data from various view points and according to multiple criteria. A
lot of EMM's functionality is accessibly freely over the internet or via apps
for hand-held devices.
","Ralf Steinberger|Aldo Podavini|Alexandra Balahur|Guillaume Jacquet|Hristo Tanev|Jens Linge|Martin Atkinson|Michele Chinosi|Vanni Zavarella|Yaniv Steiner|Erik van der Goot","","http://arxiv.org/abs/1603.02604v1","http://arxiv.org/pdf/1603.02604v1","","Proceedings of the Symposium on New Frontiers of Automated Content
  Analysis in the Social Sciences (ACA'2015), Z\""urich, Switzerland, 1-3 July
  2015 (20 pages)","","","cs.CL","cs.CL|H.3.1; H.3.3; H.3.6; I.2.7; I.5.4; J.4"
"686","1603.04387v1","2016-03-14 18:49:53","2016-03-14 18:49:53","NetMemex: Providing Full-Fidelity Traffic Archival","  NetMemex explores efficient network traffic archival without any loss of
information. Unlike NetFlow-like aggregation, NetMemex allows retrieving the
entire packet data including full payload, which makes it useful in forensic
analysis, networked and distributed system research, and network
administration. Different from packet trace dumps, NetMemex performs
sophisticated data compression for small storage space use and optimizes the
data layout for fast query processing. NetMemex takes advantage of high-speed
random access of flash drives and inexpensive storage space of hard disk
drives. These efforts lead to a cost-effective yet high-performance full
traffic archival system. We demonstrate that NetMemex can record full-fidelity
traffic at near-Gbps rates using a single commodity machine, handling common
queries at up to 90.1 K queries/second, at a low storage cost comparable to
conventional hard disk-only traffic archival solutions.
","Hyeontaek Lim|Vyas Sekar|Yoshihisa Abe|David G. Andersen","","http://arxiv.org/abs/1603.04387v1","http://arxiv.org/pdf/1603.04387v1","","A reformatted version of the ACM SIGCOMM 2013 submission","","","cs.NI","cs.NI"
"687","1603.04500v1","2016-03-14 23:13:45","2016-03-14 23:13:45","Optimal designs for dose response curves with common parameters","  A common problem in Phase II clinical trials is the comparison of dose
response curves corresponding to different treatment groups. If the effect of
the dose level is described by parametric regression models and the treatments
differ in the administration frequency (but not in the sort of drug) a
reasonable assumption is that the regression models for the different
treatments share common parameters. This paper develops optimal design theory
for the comparison of different regression models with common parameters. We
derive upper bounds on the number of support points of admissible designs, and
explicit expressions for $D$-optimal designs are derived for frequently used
dose response models with a common location parameter. If the location and
scale parameter in the different models coincide, minimally supported designs
are determined and sufficient conditions for their optimality in the class of
all designs derived. The results are illustrated in a dose-finding study
comparing monthly and weekly administration.
","Chrystel Feller|Kirsten Schorning|Holger Dette|Georgina Bermann|Bjorn Bornkamp","","http://arxiv.org/abs/1603.04500v1","http://arxiv.org/pdf/1603.04500v1","","Keywords and Phrases: Nonlinear regression, different treatment
  groups, $D$-optimal design, models with common parameters, admissible design,
  Bayesian optimal design AMS Subject Classification: Primary 62K05; Secondary
  62F03","","","math.ST","math.ST|stat.ME|stat.TH"
"688","1603.04773v1","2016-03-15 17:37:23","2016-03-15 17:37:23","Malaria elimination campaigns in the Lake Kariba region of Zambia: a
  spatial dynamical model","  Background As more regions approach malaria elimination, understanding how
different interventions interact to reduce transmission becomes critical. The
Lake Kariba area of Southern Province, Zambia, is part of a multi-country
elimination effort and presents a particular challenge as it is an
interconnected region of variable transmission intensities.
  Methods In 2012-13, six rounds of mass-screen-and-treat drug campaigns were
carried out in the Lake Kariba region. A spatial dynamical model of malaria
transmission in the Lake Kariba area, with transmission and climate modeled at
the village scale, was calibrated to the 2012-13 prevalence survey data, with
case management rates, insecticide-treated net usage, and drug campaign
coverage informed by surveillance. The model was used to simulate the effect of
various interventions implemented in 2014-22 on reducing regional transmission,
achieving elimination by 2022, and maintaining elimination through 2028.
  Findings The model captured the spatio-temporal trends of decline and rebound
in malaria prevalence in 2012-13 at the village scale. Simulations predicted
that elimination required repeated mass drug administrations coupled with
simultaneous increase in net usage. Drug campaigns targeted only at high-burden
areas were as successful as campaigns covering the entire region.
  Interpretation Elimination in the Lake Kariba region is possible through
coordinating mass drug campaigns with high-coverage vector control. Targeting
regional hotspots is a viable alternative to global campaigns when human
migration within an interconnected area is responsible for maintaining
transmission in low-burden areas.
","Milen Nikolov|Caitlin A. Bever|Alexander Upfill-Brown|Busiku Hamainza|John M. Miller|Philip A. Eckhoff|Edward A. Wenger|Jaline Gerardin","","http://arxiv.org/abs/1603.04773v1","http://arxiv.org/pdf/1603.04773v1","http://dx.doi.org/10.1371/journal.pcbi.1005192","","","10.1371/journal.pcbi.1005192","q-bio.PE","q-bio.PE"
"689","1603.06200v1","2016-03-20 11:07:48","2016-03-20 11:07:48","Assessing the Navigational Effects of Click Biases and Link Insertion on
  the Web","  Websites have an inherent interest in steering user navigation in order to,
for example, increase sales of specific products or categories, or to guide
users towards specific information. In general, website administrators can use
the following two strategies to influence their visitors' navigation behavior.
First, they can introduce click biases to reinforce specific links on their
website by changing their visual appearance, for example, by locating them on
the top of the page. Second, they can utilize link insertion to generate new
paths for users to navigate over. In this paper, we present a novel approach
for measuring the potential effects of these two strategies on user navigation.
Our results suggest that, depending on the pages for which we want to increase
user visits, optimal link modification strategies vary. Moreover, simple
topological measures can be used as proxies for assessing the impact of the
intended changes on the navigation of users, even before these changes are
implemented.
","Florian Geigl|Kristina Lerman|Simon Walk|Markus Strohmaier|Denis Helic","","http://arxiv.org/abs/1603.06200v1","http://arxiv.org/pdf/1603.06200v1","","This paper is currently under review at ACM Hypertext 2016","","","cs.SI","cs.SI"
"690","1603.06988v1","2016-03-22 21:22:16","2016-03-22 21:22:16","On a Shape-Invariant Hazard Regression Model","  In survival analysis, Cox model is widely used for most clinical trial data.
Alternatives include the additive hazard model, the accelerated failure time
(AFT) model and a more general transformation model. All these models assume
that the effects for all covariates are on the same scale. However, it is
possible that for different covariates, the effects are on different scales. In
this paper, we propose a shape-invariant hazard regression model that allows us
to estimate the multiplicative treatment effect with adjustment of covariates
that have non-multiplicative effects. We propose moment-based inference
procedures for the regression parameters. We also discuss the risk prediction
and goodness of fit test for our proposed model. Numerical studies show good
finite sample performance of our proposed estimator. We applied our method to
Veteran's Administration (VA) lung cancer data and the HIVNET 012 data. For the
latter, we found that single-dose nevirapine treatment has a significant
improvement for 18-month survival with appropriate adjustment for maternal CD4
counts and virus load.
","Cheng Zheng|Ying Qing Chen","","http://arxiv.org/abs/1603.06988v1","http://arxiv.org/pdf/1603.06988v1","","","","","stat.ME","stat.ME"
"691","1603.07355v1","2016-03-23 20:55:08","2016-03-23 20:55:08","Clinical Informatics: Recursive Concurrence Intravenous Medication
  Administration Systems Protocols for Addressing the Potential Problem of
  Recessive Lethal Autonomy in Smart Infusion Systems","  Smart Infusion Pumps are vital tools for use in administering a broad range
of parenteral/intravenous medications. Safe and effective use of smart infusion
pumps depends upon their integration with Pump Servers, Computerized Physician
Order Entry Systems, Pharmacy Information Systems, DERS/MERS Dose/Medication
Error Reduction Systems( and the digital Drug Libraries that they use),
eMARS/electronic Medication Administration Records Systems, etc. More computer
systems result in more computer network connections, with systems that run on
ever more lines of computer software code. Resulting in more chaos and more
complexity, and inevitably a greater likelihood for systems failure and/or
malfunction - this in effect runs contrary to the objective of deploying
clinical informatics solutions such as smart infusion systems, due to their
inherent technical and chronic performance problems as well as error prone
usage that may occasion Recessive(unintended but underlying) Lethal Autonomy.
The Recursive Concurrence protocols suggested in this paper are a means for
reducing if not eliminating the challenging phenomenon of Recessive Lethal
Autonomy in clinical informatics devices and/or systems.
","Nyagudi Musandu Nyagudi","","http://arxiv.org/abs/1603.07355v1","http://arxiv.org/pdf/1603.07355v1","","26 pages, 1 schematic graph","","","cs.CY","cs.CY"
"692","1604.00002v2","2016-03-31 19:12:04","2016-12-01 13:44:09","Characterization of Large Scale Functional Brain Networks During
  Ketamine-Medetomidine Anesthetic Induction","  Several experiments evidence that specialized brain regions functionally
interact and reveal that the brain processes and integrates information in a
specific and structured manner. Networks can be used to model brain functional
activities constituting a way to characterize and quantify this structured form
of organization. Reports state that different physiological states or even
diseases that affect the central nervous system may be associated to
alterations on those networks, that might reflect in graphs of different
architectures. However, the relation of their structure to different states or
conditions of the organism is not well comprehended. Thus, experiments that
involve the estimation of functional neural networks of subjects exposed to
different controlled conditions are of great relevance. Within this context,
this research has sought to model large scale functional brain networks during
an anesthetic induction process. The experiment was based on intra-cranial
recordings of neural activities of an old world macaque of the species Macaca
fuscata. Neural activity was recorded during a Ketamine-Medetomidine anesthetic
induction process. Networks were serially estimated in time intervals of five
seconds. Changes were observed in various networks properties within about one
and a half minutes after the administration of the anesthetics. These changes
reveal the occurrence of a transition on the networks architecture. During
general anesthesia a reduction in the functional connectivity and network
integration capabilities were verified in both local and global levels. It was
also observed that the brain shifted to a highly specific and dynamic state.
The results bring empirical evidence and report the relation of the induced
state of anesthesia to properties of functional networks, thus, they contribute
for the elucidation of some new aspects of neural correlates of consciousness.
","Eduardo C. Padovani","","http://arxiv.org/abs/1604.00002v2","http://arxiv.org/pdf/1604.00002v2","","28 pages , 9 figures, 7 tables; - English errors were corrected;
  Figures 1,3,4,5,6,8 and 9 were replaced by (exact the same)figures of higher
  resolution; Three(3) references were added on the introduction section","","","q-bio.NC","q-bio.NC"
"693","1604.02620v1","2016-04-10 00:14:43","2016-04-10 00:14:43","Surveying Turkish high school and university student attitudes and
  approaches to physics problem solving","  Student attitudes and approaches to problem solving can impact how well they
learn physics. Prior research in the US using a validated Attitude and
Approaches to Problem Solving (AAPS) survey suggests that there are major
differences between students in introductory physics and astronomy courses and
physics experts in terms of their attitudes and approaches to physics problem
solving. Here we discuss the validation, administration and analysis of data
for the Turkish version of the AAPS survey for high school and university
students in Turkey. After the validation and administration of the Turkish
version of the survey, the analysis of the data was conducted by grouping the
data by grade level, school type, and gender. While there are no statistically
significant differences between the averages of various groups on the survey,
overall, the university students in Turkey were more expert-like than
vocational high school students. On an item by item basis, there are
statistically differences between the averages of the groups on many items. For
example, on average, the university students demonstrated less expert-like
attitudes about the role of equations and formulas in problem solving, in
solving difficult problems, and in knowing when the solution is not correct,
whereas they displayed more expert-like attitudes and approaches on items
related to meta-cognition in physics problem solving. A principal component
analysis on the data yields item clusters into which the student responses on
various survey items can be grouped. A comparison of the responses of the
Turkish and American university students enrolled in algebra-based introductory
physics courses shows that on more than half of the items, the responses of
these two groups were statistically significantly different with the US
students on average responding to the items in more expert-like manner.
","Nuri Balta|Andrew Mason|Chandralekha Singh","","http://arxiv.org/abs/1604.02620v1","http://arxiv.org/pdf/1604.02620v1","http://dx.doi.org/10.1103/PhysRevPhysEducRes.12.010129","16 pages, Keywords: Physics Education Research, Attitudes and
  approaches to problem solving, Turkish students, American students, factor
  analysis, principal component analysis","Physical Review Physics Education Research, 12, 010129, 2016","10.1103/PhysRevPhysEducRes.12.010129","physics.ed-ph","physics.ed-ph"
"694","1604.04465v1","2016-04-15 12:48:05","2016-04-15 12:48:05","PRI: Privacy Preserving Inspection of Encrypted Network Traffic","  Traffic inspection is a fundamental building block of many security solutions
today. For example, to prevent the leakage or exfiltration of confidential
insider information, as well as to block malicious traffic from entering the
network, most enterprises today operate intrusion detection and prevention
systems that inspect traffic. However, the state-of-the-art inspection systems
do not reflect well the interests of the different involved autonomous roles.
For example, employees in an enterprise, or a company outsourcing its network
management to a specialized third party, may require that their traffic remains
confidential, even from the system administrator. Moreover, the rules used by
the intrusion detection system, or more generally the configuration of an
online or offline anomaly detection engine, may be provided by a third party,
e.g., a security research firm, and can hence constitute a critical business
asset which should be kept confidential. Today, it is often believed that
accounting for these additional requirements is impossible, as they contradict
efficiency and effectiveness. We in this paper explore a novel approach, called
Privacy Preserving Inspection (PRI), which provides a solution to this problem,
by preserving privacy of traffic inspection and confidentiality of inspection
rules and configurations, and e.g., also supports the flexible installation of
additional Data Leak Prevention (DLP) rules specific to the company.
","Liron Schiff|Stefan Schmid","","http://arxiv.org/abs/1604.04465v1","http://arxiv.org/pdf/1604.04465v1","","","","","cs.CR","cs.CR|cs.NI|C.2.0"
"695","1605.02093v1","2016-04-26 17:39:16","2016-04-26 17:39:16","A Perspective Study on Content Management in E-Learning and M-Learning","  This is the era of Information and Communication Technology (ICT). Nowadays,
there is no limit to learn, people can learn anywhere and anytime with the
enhancement of technology. Electronic Learning (E-learning) and Mobile Learning
(M-learning) are the two vital buzz terms in modern education particularly in
Education Enhanced Technology and Technologies Supported Learning. E-learning
is defined as the instructional content or learning experience delivered or
enabled by electronic technologies whereas, M-learning is defined simply as
learning via mobile devices such as cell phones, smart phones, palmtops, and
handheld computers. There are many similarities between the two technologies as
both are modern learning tools. Moreover, the latter is an extension and a
subset of the former. However, there are few limitations or differences still
exist in mobile learning tools, especially in the design, development and the
technology usability. In this paper we have mainly focused on how the digital
content is administrated (Content Management) in these two technologies.
Additionally, the content management in E and Mlearning are compared and their
similarities and differences are figured out.
","RD. Balaji|Fatma Al-Mahri|R. Malathi","","http://arxiv.org/abs/1605.02093v1","http://arxiv.org/pdf/1605.02093v1","","7 pages","","","cs.CY","cs.CY"
"696","1604.08330v1","2016-04-28 07:33:40","2016-04-28 07:33:40","Server Consolidation for Internet Applications in Virtualized Data
  Centers","  Server consolidation based on virtualization technology simplifies system
administration and improves energy efficiency by improving resource
utilizations and reducing the physical machine (PM) number in contemporary
service-oriented data centers. The elasticity of Internet applications changes
the consolidation technologies from addressing virtual machines (VMs) to PMs
mapping schemes which must know the VMs statuses, i.e. the number of VMs and
the profiling data of each VM, into providing the application-to-VM-to-PM
mapping. In this paper, we study on the consolidation of multiple Internet
applications, minimizing the number of PMs with required performance. We first
model the consolidation providing the application-to-VM-to-PM mapping to
minimize the number of PMs as an integer linear programming problem, and then
present a heuristic algorithm to solve the problem in polynomial time.
Extensive experimental results show that our heuristic algorithm consumes less
than 4.3% more resources than the optimal amounts with few overheads. Existing
consolidation technologies using the input of the VM statuses output by our
heuristic algorithm consume 1.06% more PMs.
","Bo Wang|Ying Song|Yuzhong Sun|Jun Liu","","http://arxiv.org/abs/1604.08330v1","http://arxiv.org/pdf/1604.08330v1","","8 pages, 6 figures, In Proceedings of the Symposium on High
  Performance Computing (HPC '16). Society for Computer Simulation
  International, Pasadena, CA, USA","","","cs.DC","cs.DC"
"697","1605.00328v1","2016-05-02 01:23:37","2016-05-02 01:23:37","Core Course Analysis for Undergraduate Students in Mathematics","  In this work, we develop statistical tools to understand core courses at the
university level. Traditionally, professors and administrators label courses as
""core"" when the courses contain foundational material. Such courses are often
required to complete a major, and, in some cases, allocated additional
educational resources. We identify two key attributes which we expect core
courses to have. Namely, we expect core courses to be highly correlated with
and highly impactful on a student's overall mathematics GPA. We use two
statistical procedures to measure the strength of these attributes across
courses. The first of these procedures fashions a metric out of standard
correlation measures. The second utilizes sparse regression. We apply these
methods on student data coming from the University of California, Los Angeles
(UCLA) department of mathematics to compare core and non-core coursework.
","Ritvik Kharkar|Jessica Tran|Charles Z. Marshak","","http://arxiv.org/abs/1605.00328v1","http://arxiv.org/pdf/1605.00328v1","","","","","math.HO","math.HO|stat.AP"
"698","1605.00814v2","2016-05-03 09:40:24","2017-01-18 12:09:09","Using Ecological Propensity Score to Adjust for Missing Confounders in
  Small Area Studies","  Small area ecological studies are commonly used in epidemiology to assess the
impact of area level risk factors on health outcomes when data are only
available in an aggregated form. However the resulting estimates are often
biased due to unmeasured confounders, which typically are not available from
the standard administrative registries used for these studies. Extra
information on confounders can be provided through external datasets such as
surveys or cohorts, where the data are available at the individual level rather
than at the area level; however such data typically lack the geographical
coverage of administrative registries. We develop a framework of analysis which
combines ecological and individual level data from different sources to provide
an adjusted estimate of area level risk factors which is less biased. Our
method (i) summarises all available individual level confounders into an area
level scalar variable, which we call ecological propensity score (EPS), (ii)
implements a hierarchical structured approach to predict the values of EPS
whenever they are missing, (iii) includes the estimated and predicted EPS into
the ecological regression linking the risk factors to the health outcome.
Through a simulation study we show that integrating individual level data into
small area analyses via EPS is a promising method to reduce the bias intrinsic
in ecological studies due to unmeasured confounders; we also apply the method
to a real case study to evaluate the effect of air pollution on coronary heart
disease hospital admissions in Greater London.
","Yingbo Wang|Sylvia Richardson|Anna Hansell|Marta Blangiardo","","http://arxiv.org/abs/1605.00814v2","http://arxiv.org/pdf/1605.00814v2","","","","","stat.AP","stat.AP"
"699","1605.01116v1","2016-05-03 23:46:48","2016-05-03 23:46:48","An evaluation of randomized machine learning methods for redundant data:
  Predicting short and medium-term suicide risk from administrative records and
  risk assessments","  Accurate prediction of suicide risk in mental health patients remains an open
problem. Existing methods including clinician judgments have acceptable
sensitivity, but yield many false positives. Exploiting administrative data has
a great potential, but the data has high dimensionality and redundancies in the
recording processes. We investigate the efficacy of three most effective
randomized machine learning techniques random forests, gradient boosting
machines, and deep neural nets with dropout in predicting suicide risk. Using a
cohort of mental health patients from a regional Australian hospital, we
compare the predictive performance with popular traditional approaches
clinician judgments based on a checklist, sparse logistic regression and
decision trees. The randomized methods demonstrated robustness against data
redundancies and superior predictive performance on AUC and F-measure.
","Thuong Nguyen|Truyen Tran|Shivapratap Gopakumar|Dinh Phung|Svetha Venkatesh","","http://arxiv.org/abs/1605.01116v1","http://arxiv.org/pdf/1605.01116v1","","","","","stat.ML","stat.ML|cs.LG"
"700","1605.01701v1","2016-05-05 19:30:52","2016-05-05 19:30:52","A Survey and Evaluation of Data Center Network Topologies","  Data centers are becoming increasingly popular for their flexibility and
processing capabilities in the modern computing environment. They are managed
by a single entity (administrator) and allow dynamic resource provisioning,
performance optimization as well as efficient utilization of available
resources. Each data center consists of massive compute, network and storage
resources connected with physical wires. The large scale nature of data centers
requires careful planning of compute, storage, network nodes, interconnection
as well as inter-communication for their effective and efficient operations. In
this paper, we present a comprehensive survey and taxonomy of network
topologies either used in commercial data centers, or proposed by researchers
working in this space. We also compare and evaluate some of those topologies
using mininet as well as gem5 simulator for different traffic patterns, based
on various metrics including throughput, latency and bisection bandwidth.
","Brian Lebiednik|Aman Mangal|Niharika Tiwari","","http://arxiv.org/abs/1605.01701v1","http://arxiv.org/pdf/1605.01701v1","","","","","cs.DC","cs.DC|cs.NI"
"701","1606.04139v1","2016-05-08 02:12:26","2016-05-08 02:12:26","Credit allocation based on journal impact factor and coauthorship
  contribution","  Some research institutions demand researchers to distribute the incomes they
earn from publishing papers to their researchers and/or co-authors. In this
study, we deal with the Impact Factor-based ranking journal as a criteria for
the correct distribution of these incomes. We also include the Authorship
Credit factor for distribution of the incomes among authors, using the
geometric progression of Cantor's theory and the Harmonic Credit Index.
Depending on the ranking of the journal, the proposed model develops a proper
publication credit allocation among all authors. Moreover, our tool can be
deployed in the evaluation of an institution for a funding program, as well as
calculating the amounts necessary to incentivize research among personnel.
","Javier E. Contreras-Reyes","","http://arxiv.org/abs/1606.04139v1","http://arxiv.org/pdf/1606.04139v1","http://dx.doi.org/10.1453/jsas.v3i2.809","9 pages; 3 figures; 2 tables","Journal of Social and Administrative Sciences (2016), 3(2),
  111-118","10.1453/jsas.v3i2.809","cs.DL","cs.DL|q-fin.GN"
"702","1605.02917v1","2016-05-10 10:05:40","2016-05-10 10:05:40","Web Spam Detection Using Multiple Kernels in Twin Support Vector Machine","  Search engines are the most important tools for web data acquisition. Web
pages are crawled and indexed by search Engines. Users typically locate useful
web pages by querying a search engine. One of the challenges in search engines
administration is spam pages which waste search engine resources. These pages
by deception of search engine ranking algorithms try to be showed in the first
page of results. There are many approaches to web spam pages detection such as
measurement of HTML code style similarity, pages linguistic pattern analysis
and machine learning algorithm on page content features. One of the famous
algorithms has been used in machine learning approach is Support Vector Machine
(SVM) classifier. Recently basic structure of SVM has been changed by new
extensions to increase robustness and classification accuracy. In this paper we
improved accuracy of web spam detection by using two nonlinear kernels into
Twin SVM (TSVM) as an improved extension of SVM. The classifier ability to data
separation has been increased by using two separated kernels for each class of
data. Effectiveness of new proposed method has been experimented with two
publicly used spam datasets called UK-2007 and UK-2006. Results show the
effectiveness of proposed kernelized version of TSVM in web spam page
detection.
","Seyed Hamid Reza Mohammadi|Mohammad Ali Zare Chahooki","","http://arxiv.org/abs/1605.02917v1","http://arxiv.org/pdf/1605.02917v1","","","","","cs.IR","cs.IR|cs.LG"
"703","1605.07323v1","2016-05-24 07:32:23","2016-05-24 07:32:23","On model of information system for management of information flows","  In this article are discussed some problems in developing software related to
the management of information flows. We presented the basic stages in their
development. We bold a methodology for conceptual modeling and design of
information systems of this type. In order to demonstrate the effectiveness of
the proposed model is an information system for administrative services of
graduate students in the university.
","Radoslva Kraleva|Velin Kralev","","http://arxiv.org/abs/1605.07323v1","http://arxiv.org/pdf/1605.07323v1","","8 pages, in Bulgarian, 4 figures in II Balkan Scientific Conference
  The Science, the Education and the Art in 21st Century, Bulgaria,
  Blagoevgrad, 26-27 September 2009, pp. 203-210","","","cs.SE","cs.SE"
"704","1605.07343v1","2016-05-24 09:11:27","2016-05-24 09:11:27","An Innovative, Open, Interoperable Citizen Engagement Cloud Platform for
  Smart Government and Users' Interaction","  This paper introduces an open, interoperable, and cloud-computing-based
citizen engagement platform for the management of administrative processes of
public administrations, which also increases the engagement of citizens. The
citizen engagement platform is the outcome of a 3-year Italian national project
called PRISMA (Interoperable cloud platforms for smart government). The aim of
the project is to constitute a new model of digital ecosystem that can support
and enable new methods of interaction among public administrations, citizens,
companies, and other stakeholders surrounding cities. The platform has been
defined by the media as a flexible (enable the addition of any kind of
application or service) and open (enable access to open services) Italian
""cloud"" that allows public administrations to access to a vast knowledge base
represented as linked open data to be reused by a stakeholder community with
the aim of developing new applications (""Cloud Apps"") tailored to the specific
needs of citizens. The platform has been used by Catania and Syracuse
municipalities, two of the main cities of southern Italy, located in the
Sicilian region. The fully adoption of the platform is rapidly spreading around
the whole region (local developers have already used available application
programming interfaces (APIs) to create additional services for citizens and
administrations) to such an extent that other provinces of Sicily and Italy in
general expressed their interest for its usage. The platform is available
online and, as mentioned above, is open source and provides APIs for full
exploitation.
","Diego Reforgiato Recupero|Mario Castronovo|Sergio Consoli|Tarcisio Costanzo|Aldo Gangemi|Luigi Grasso|Giorgia Lodi|Gianluca Merendino|Misael Mongiovi|Valentina Presutti|Salvatore Davide Rapisarda|Salvo Rosa|Emanuele Spampinato","","http://arxiv.org/abs/1605.07343v1","http://arxiv.org/pdf/1605.07343v1","http://dx.doi.org/10.1007/s13132-016-0361-0","23 pages, 7 figures, journal paper","Journal of the Knowledge Economy, 2016, 7(2):388-412","10.1007/s13132-016-0361-0","cs.CY","cs.CY"
"705","1605.08841v1","2016-05-28 03:58:52","2016-05-28 03:58:52","Surviving an ""Eternal September"" - How an Online Community Managed a
  Surge of Newcomers","  We present a qualitative analysis of interviews with participants in the
NoSleep community within Reddit where millions of fans and writers of horror
fiction congregate. We explore how the community handled a massive, sudden, and
sustained increase in new members. Although existing theory and stories like
Usenet's infamous ""Eternal September"" suggest that large influxes of newcomers
can hurt online communities, our interviews suggest that NoSleep survived
without major incident. We propose that three features of NoSleep allowed it to
manage the rapid influx of newcomers gracefully: (1) an active and
well-coordinated group of administrators, (2) a shared sense of community which
facilitated community moderation, and (3) technological systems that mitigated
norm violations. We also point to several important trade-offs and limitations.
","Charles Kiene|Andres Monroy-Hernandez|Benjamin Mako Hill","","http://arxiv.org/abs/1605.08841v1","http://arxiv.org/pdf/1605.08841v1","","In Proceedings of the 34th Annual ACM Conference on Human Factors in
  Computing Systems (CHI 2016). ACM, New York, NY, USA","","","cs.HC","cs.HC|cs.CY"
"706","1605.09115v1","2016-05-30 06:33:15","2016-05-30 06:33:15","The Mathematical Foundations for Mapping Policies to Network Devices
  (Technical Report)","  A common requirement in policy specification languages is the ability to map
policies to the underlying network devices. Doing so, in a provably correct
way, is important in a security policy context, so administrators can be
confident of the level of protection provided by the policies for their
networks. Existing policy languages allow policy composition but lack formal
semantics to allocate policy to network devices.
  Our research tackles this from first principles: we ask how network policies
can be described at a high-level, independent of firewall-vendor and network
minutiae. We identify the algebraic requirements of the policy mapping process
and propose semantic foundations to formally verify if a policy is implemented
by the correct set of policy-arbiters. We show the value of our proposed
algebras in maintaining concise network-device configurations by applying them
to real-world networks.
","Dinesha Ranathunga|Matthew Roughan|Phil Kernick|Nick Falkner","","http://arxiv.org/abs/1605.09115v1","http://arxiv.org/pdf/1605.09115v1","","","","","cs.CR","cs.CR"
"707","1606.05248v1","2016-06-06 17:05:06","2016-06-06 17:05:06","Leadership Network and Team Performance in Interactive Contests","  Over the years, the concept of leadership has experienced a paradigm shift -
from solitary leader (centralized leadership) to de-centralized leadership or
distributed leadership. This paper explores the idea that centralized
leadership, as earlier suggested, negatively impacts team performance. I
applied the hypothesis to cricket, a sport in which leaders play an important
role in team's success. I generated batting partnership network and evaluated
the central-most player in the team, applying tools of social network analysis.
Analyzing 3420 matches in one day international cricket and 1979 Test matches
involving 10 teams, I examined the impact of centralized leadership in outcome
of a contest. I observed that the odds for winning a one day international
match under centralized leadership is 30% higher than the odds for winning
under de-centralized leadership. In both forms of cricket (Test and one day
international ), I failed to find evidence that distributed leadership is
associated with higher team performance. These results suggest important
implications for cricket administrators in development and management of
working teams.
","Satyam Mukherjee","","http://arxiv.org/abs/1606.05248v1","http://arxiv.org/pdf/1606.05248v1","http://dx.doi.org/10.1016/j.socnet.2016.05.003","8 pages, 4 figures","Social Networks 2016 85-92","10.1016/j.socnet.2016.05.003","cs.SI","cs.SI|physics.soc-ph"
"708","1606.02562v1","2016-06-08 14:08:21","2016-06-08 14:08:21","DialPort: Connecting the Spoken Dialog Research Community to Real User
  Data","  This paper describes a new spoken dialog portal that connects systems
produced by the spoken dialog academic research community and gives them access
to real users. We introduce a distributed, multi-modal, multi-agent prototype
dialog framework that affords easy integration with various remote resources,
ranging from end-to-end dialog systems to external knowledge APIs. To date, the
DialPort portal has successfully connected to the multi-domain spoken dialog
system at Cambridge University, the NOAA (National Oceanic and Atmospheric
Administration) weather API and the Yelp API.
","Tiancheng Zhao|Kyusong Lee|Maxine Eskenazi","","http://arxiv.org/abs/1606.02562v1","http://arxiv.org/pdf/1606.02562v1","","Under Peer Review of SigDial 2016","","","cs.AI","cs.AI|cs.CL"
"709","1606.02700v1","2016-06-08 19:44:24","2016-06-08 19:44:24","Distance, Borders, and Time: The Diffusion and Permeability of Political
  Violence in North and West Africa","  This paper explores the spatial and temporal diffusion of political violence
in North and West Africa. It does so by endeavoring to represent the mental
landscape that lives in the back of a group leader's mind as he contemplates
strategic targeting. We assume that this representation is a combination of the
physical geography of the target environment, and the mental and physical cost
of following a seemingly random pattern of attacks. Focusing on the distance
and time between attacks and taking into consideration the transaction costs
that state boundaries impose, we wish to understand what constrains a group
leader to attack at a location other than the one that would seem to yield the
greatest overt payoff. By its very nature, the research problem defies the
collection of a full set of structural data. Instead, we leverage functional
data from the Armed Conflict Location and Event Data project (ACLED) dataset
that, inter alia, meticulously catalogues violent extremist incidents in North
and West Africa since 1997, to generate a network whose nodes are
administrative regions. These nodes are connected by edges of qualitatively
different types: undirected edges representing geographic distance, undirected
edges representing borders, and directed edges representing consecutive attacks
by the same group at the two endpoints. We analyze the resulting network using
novel spectral embedding techniques that are able to account fully for the
different types of edges. The result is a map of North and West Africa that
depicts the permeability to violence. A better understanding of how location,
time, and borders condition attacks enables planning, prepositioning, and
response.
","David Skillicorn|Olivier Walther|Quan Zheng|Christian Leuprecht","","http://arxiv.org/abs/1606.02700v1","http://arxiv.org/pdf/1606.02700v1","","","","","cs.SI","cs.SI|physics.soc-ph|91D30|E.1"
"710","1606.03182v1","2016-06-10 05:22:08","2016-06-10 05:22:08","Cyber Attack Thread: A Control-flow Based Approach to Deconstruct and
  Mitigate Cyber Threats","  Attacks in cyberspace have got attention due to risk at privacy, breach of
trust and financial losses for individuals as well as organizations. In recent
years, these attacks have become more complex to analyze technically, as well
as to detect and prevent from accessing confidential data. Although there are
many methodologies and mechanisms which have been suggested for cyber-attack
detection and prevention, but not from the perspective of an attacker. This
paper presents the cyber-defence as hindrances, faced by the attacker, by
understanding attack thread and defence possibilities with existing security
mechanisms. Seven phases of Cyber Attack Thread are introduced and technical
aspects are discussed with reference to APT attacks. The paper aims for
security practitioner and administrators as well as for the general audience to
understand the attack scenario and defensive security measures.
","Koustav Sadhukhan|Rao Arvind Mallari|Tarun Yadav","","http://arxiv.org/abs/1606.03182v1","http://arxiv.org/pdf/1606.03182v1","http://dx.doi.org/10.1109/CoCoNet.2015.7411183","9 Pages, 1 Figure, 2015 International Conference on Computing and
  Network Communications (CoCoNet), The final publication is available at IEEE
  Xplore via http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=7411183","","10.1109/CoCoNet.2015.7411183","cs.NI","cs.NI|cs.CR"
"711","1606.03503v1","2016-06-10 23:56:13","2016-06-10 23:56:13","Green Information Technology as Administrative innovation -
  Organizational factors for successful implementation: Literature Review","  There is a considerable amount of awareness of environmental issues and
corporate responsibility for sustainability. As such, from a technological
viewpoint, Green IT has become an important topic in contemporary
organizations. Consequently, organisations are expected to be innovative in
their business practices to become more sustainable. Yet, the popularity and
adoption of such initiatives amongst employees remain low. Furthermore, the
management practices for adhering to Green IT are largely dormant, lacking
active incentives for employees to engage in Green IT initiatives. This study
observes the phenomenon of Green IT through administrative innovation. In doing
so this paper performs a comprehensive analysis of 137 papers published between
2007 and 2015. The paper reveals organizational factors for successful
implementation of Green IT as administrative innovation that can be useful to
both academia and practice.
","Badrunnesa Zaman|Darshana Sedera","","http://arxiv.org/abs/1606.03503v1","http://arxiv.org/pdf/1606.03503v1","","ISBN# 978-0-646-95337-3 Presented at the Australasian Conference on
  Information Systems 2015 (arXiv:1605.01032)","","","cs.CY","cs.CY"
"712","1606.03661v2","2016-06-12 03:10:31","2016-06-17 18:10:18","Existence of Self-Cheeger Sets on Riemannian Manifolds","  Let $(\mathcal{M}, g)$ be a compact Riemannian manifold of dimension $N\geq
2$. We prove the existence of a family $(\Omega_\varepsilon)_{\varepsilon\in
(0,\varepsilon_0)}$ of self-Cheeger sets in $(\mathcal{M}, g)$ . The domains
$\Omega_\varepsilon\subset\mathcal{M}$ are perturbations of geodesic balls of
radius $\varepsilon$ centered at $p \in \mathcal{M}$, and in particular, if
$p_0$ is a non-degenerate critical point of the scalar curvature of $g$, then
the family $( \partial\Omega_\varepsilon)_{\varepsilon\in (0,\varepsilon_0)}$
constitutes a smooth foliation of a neighborhood of $p_0$.
","Ignace Aristide Minlend","","http://arxiv.org/abs/1606.03661v2","http://arxiv.org/pdf/1606.03661v2","","arXiv admin note: this article has been withdrawn by arXiv
  administrators because it is identical to arXiv:1603.00204v2","","","math.DG","math.DG"
"713","1606.04277v1","2016-06-14 09:41:04","2016-06-14 09:41:04","How hot is .brussels? Impact of the uptake of the .brussels top-level
  domain name extension","  The opening up of the top-level domain name market in 2012 has offered new
perspectives for companies, administrations and individuals to include a
geographic component within the domain name of their website. Little to no
research has been carried out since then to analyse the uptake of the new
top-level domain names (TLDN). Based on the specific case of the TLDN
.brussels, this article proposes an empirical study of how the opening up of
the top-level domain name market actually impacts registration practices. By
making use of freely available software tools such as OpenRefine and Natural
Language Processing (NLP) methods, the entire corpus of the .brussels domain
names (6300) was analysed from a quantitative perspective. Based on a
statistically representative sample, a qualitative interpretation allowed for a
more fine-grained analysis of how the new TLDN is being used in practice. By
doing so, the article gives a detailed insight into the impact of the recent
changes of the rules concerning domain name registration. Researchers, policy
makers, investors and anyone who cares about the Brussels identity in the
digital realm can gain through this analysis a better understanding of the
state of play of the .brussels TLDN.
","Margot Waty|Seth van Hooland|Simon Hengchen|Mathias Coeckelbergs|Max De Wilde","","http://arxiv.org/abs/1606.04277v1","http://arxiv.org/pdf/1606.04277v1","","","","","cs.CY","cs.CY"
"714","1606.04719v1","2016-06-15 10:56:10","2016-06-15 10:56:10","Characterization of the Community Structure of Large Scale Functional
  Brain Networks During Ketamine-Medetomidine Anesthetic Induction","  One of the central questions in neuroscience is to understand the way
communication is organized in the brain, trying to comprehend how cognitive
capacities or physiological states of the organism are potentially related to
brain activities involving interactions of several brain areas. One important
characteristic of the functional brain networks is that they are modularly
structured, being this modular architecture regarded to account for a series of
properties and functional dynamics. In the neurobiological context, communities
may indicate brain regions that are involved in one same activity, representing
neural segregated processes. Several studies have demonstrated the modular
character of organization of brain activities. However, empirical evidences
regarding to its dynamics and relation to different levels of consciousness
have not been reported yet. Within this context, this research sought to
characterize the community structure of functional brain networks during an
anesthetic induction process. The experiment was based on intra-cranial
recordings of neural activities of an old world macaque of the species Macaca
fuscata during a Ketamine-Medetomidine anesthetic induction process. Networks
were serially estimated in time intervals of five seconds. Changes were
observed within about one and a half minutes after the administration of the
anesthetics, revealing the occurrence of a transition on the community
structure. The awake state was characterized by the presence of large clusters
involving frontal and parietal regions, while the anesthetized state by the
presence of communities in the primary visual and motor cortices, being the
areas of the secondary associative cortex most affected. The results report the
influence of general anesthesia on the structure of functional clusters,
contributing for understanding some new aspects of neural correlates of
consciousness.
","Eduardo C. Padovani","","http://arxiv.org/abs/1606.04719v1","http://arxiv.org/pdf/1606.04719v1","","24 pages, 8 figures. arXiv admin note: text overlap with
  arXiv:1604.00002","","","q-bio.NC","q-bio.NC"
"715","1606.07497v2","2016-06-23 22:09:37","2016-10-12 18:12:43","Testing Modeling Assumptions in the West Africa Ebola Outbreak","  The Ebola virus in West Africa has infected almost 30,000 and killed over
11,000 people. Recent models of Ebola Virus Disease (EVD) have often made
assumptions about how the disease spreads, such as uniform transmissibility and
homogeneous mixing within a population. In this paper, we test whether these
assumptions are necessarily correct, and offer simple solutions that may
improve disease model accuracy. First, we use data and models of West African
migration to show that EVD does not homogeneously mix, but spreads in a
predictable manner. Next, we estimate the initial growth rate of EVD within
country administrative divisions and find that it significantly decreases with
population density. Finally, we test whether EVD strains have uniform
transmissibility through a novel statistical test, and find that certain
strains appear more often than expected by chance.
","Keith Burghardt|Christopher Verzijl|Junming Huang|Matthew Ingram|Binyang Song|Marie-Pierre Hasne","","http://arxiv.org/abs/1606.07497v2","http://arxiv.org/pdf/1606.07497v2","http://dx.doi.org/10.1038/srep34598","16 pages, 14 figures","Sci. Rep., 6: 34598 (2016)","10.1038/srep34598","q-bio.PE","q-bio.PE"
"716","1606.07537v1","2016-06-24 01:20:27","2016-06-24 01:20:27","Sistem Informasi Pengarsipan Menggunakan Algoritma Levensthein String
  pada Kecamatan Seberang Ulu II","  Archival information systems in government agency is one of the most used
applications for daily acitivities. One feature in application management
information document is searching. This feature serves to search for documents
from a collection of available information based on keywords entered by the
user. But some researches on a search engine (searching) concluded that the
average user error in the search is quite high due to several factors.
Therefore, we need a development on this feature as search suggestion. This
study discusses the application of the method of approximate string matching
algorithm using levenshtein distance. Levensthein distance algorithm is capable
of calculating the minimum distance conversion of a string into another string
to the optimum. Archiving information system using Levensthein Algorithm String
is an application that will be built to address these problems, this
application will help, especially in the administration to enter or save a
document, locate and make a report that will be seen by government agencies.
","Pratiwi Vidyarsih|Leon Andretti Abdillah|Ari Muzakir","","http://arxiv.org/abs/1606.07537v1","http://arxiv.org/pdf/1606.07537v1","","6 pages","Vidyarsih, P., Abdillah, L. A., & Muzakir, A. (2016). Sistem
  Informasi Pengarsipan Menggunakan Algoritma Levensthein String pada Kecamatan
  Seberang Ulu II. Paper presented at the SHaP-SITI2016, Palembang","","cs.CY","cs.CY"
"717","1606.07803v1","2016-06-24 01:31:50","2016-06-24 01:31:50","Penerapan E-Service Berbasis Android pada Divisi Pelayanan Perbaikan
  Komputer CV Ria Kencana Ungu (RKU)","  Archival information systems in government agency is one of the most used
applications for daily acitivities. One feature in application management
information document is searching. This feature serves to search for documents
from a collection of available information based on keywords entered by the
user. But some researches on a search engine (searching) concluded that the
average user error in the search is quite high due to several factors.
Therefore, we need a development on this feature as search suggestion. This
study discusses the application of the method of approximate string matching
algorithm using levenshtein distance. Levensthein distance algorithm is capable
of calculating the minimum distance conversion of a string into another string
to the optimum. Archiving information system using Levensthein Algorithm String
is an application that will be built to address these problems, this
application will help, especially in the administration to enter or save a
document, locate and make a report that will be seen by government agencies.
","Febri Valentina|Leon Andretti Abdillah|Nurul Adha Oktarini Saputri","","http://arxiv.org/abs/1606.07803v1","http://arxiv.org/pdf/1606.07803v1","","6 pages, Paper presented at the SHaP-SITI2016, Palembang (2016)","","","cs.CY","cs.CY"
"718","1606.08378v1","2016-06-27 17:34:02","2016-06-27 17:34:02","Mitigating Data Exfiltration in Storage-as-a-Service Clouds","  Existing processes and methods for incident handling are geared towards
infrastructures and operational models that will be increasingly outdated by
cloud computing. Research has shown that to adapt incident handling to cloud
computing environments, cloud customers must establish clarity about their
requirements on Cloud Service Providers (CSPs) for successful handling of
incidents and contract CSPs accordingly. Secondly, CSPs must strive to support
these requirements and mirror them in their Service Level Agreements. Intrusion
Detection Systems (IDS) have been used widely to detect malicious behaviors in
network communication and hosts. Facing new application scenarios in Cloud
Computing, the IDS approaches yield several problems since the operator of the
IDS should be the user, not the administrator of the Cloud infrastructure.
Cloud providers need to enable possibilities to deploy and configure IDS for
the user - which poses its own challenges. Current research and commercial
solutions primarily focus on protecting against Denial of Service attacks and
attacks against the Cloud's virtual infrastructure. To counter these
challenges, we propose a capability that aims to both detect and prevent the
potential of data exfiltration by using a novel deception-based methodology. We
also introduce a method of increasing the data protection level based on
various threat conditions.
","Duane Wilson|Jeff Avery","","http://arxiv.org/abs/1606.08378v1","http://arxiv.org/pdf/1606.08378v1","","","","","cs.CR","cs.CR"
"719","1606.08683v1","2016-06-28 13:07:29","2016-06-28 13:07:29","Space Weather impact on the degradation of NOAA POES MEPED proton
  detectors","  The Medium Energy Proton and Electron Detector (MEPED) on board the National
Oceanic and Atmospheric Administration Polar Orbiting Environmental Satellites
(NOAA POES) is known to degrade with time. In recent years a lot of effort has
been put into calibrating the degraded proton detectors. We make use of
previous work and show that the degradation of the detectors can be attributed
to the radiation dose of each individual instrument. However, the effectiveness
of the radiation in degrading the detector is modulated when it is weighted by
the mean $\textit{ap}$ index, increasing the degradation rate in periods with
high geomagnetic activity, and decreasing it through periods of low activity.
When taking $\textit{ap}$ and the radiation dose into account, we find that the
degradation rate is independent of spacecraft and detector pointing direction.
We have developed a model to estimate the correction factor for all the MEPED
detectors as a function of accumulated corrected flux and the $\textit{ap}$
index. We apply the routine to NOAA POES spacecraft starting with NOAA-15,
including the European satellites MetOp-02 and MetOp-01, and estimate
correction factors.
"," ¨ªdegaard|L. -K. G.|H. N. Tyss©ªy|M. I. J. Sandanger|J. Stadsnes|F. S©ªraas","","http://arxiv.org/abs/1606.08683v1","http://arxiv.org/pdf/1606.08683v1","http://dx.doi.org/10.1051/swsc/2016020","","","10.1051/swsc/2016020","astro-ph.IM","astro-ph.IM|physics.space-ph"
"720","1607.00655v2","2016-07-03 16:44:48","2016-11-21 06:22:10","The End of a Myth: Distributed Transactions Can Scale","  The common wisdom is that distributed transactions do not scale. But what if
distributed transactions could be made scalable using the next generation of
networks and a redesign of distributed databases? There would be no need for
developers anymore to worry about co-partitioning schemes to achieve decent
performance. Application development would become easier as data placement
would no longer determine how scalable an application is. Hardware provisioning
would be simplified as the system administrator can expect a linear scale-out
when adding more machines rather than some complex sub-linear function, which
is highly application specific.
  In this paper, we present the design of our novel scalable database system
NAM-DB and show that distributed transactions with the very common Snapshot
Isolation guarantee can indeed scale using the next generation of RDMA-enabled
network technology without any inherent bottlenecks. Our experiments with the
TPC-C benchmark show that our system scales linearly to over 6.5 million
new-order (14.5 million total) distributed transactions per second on 56
machines.
","Erfan Zamanian|Carsten Binnig|Tim Kraska|Tim Harris","","http://arxiv.org/abs/1607.00655v2","http://arxiv.org/pdf/1607.00655v2","","12 pages","","","cs.DB","cs.DB"
"721","1607.01990v1","2016-07-07 12:35:31","2016-07-07 12:35:31","A Maturity Model for Public Administration as Open Translation Data
  Providers","  Any public administration that produces translation data can be a provider of
useful reusable data to meet its own translation needs and the ones of other
public organizations and private companies that work with texts of the same
domain. These data can also be crucial to produce domain-tuned Machine
Translation systems. The organization's management of the translation process,
the characteristics of the archives of the generated resources and of the
infrastructure available to support them determine the efficiency and the
effectiveness with which the materials produced can be converted into reusable
data. However, it is of utmost importance that the organizations themselves
first become aware of the goods they are producing and, second, adapt their
internal processes to become optimal providers. In this article, we propose a
Maturity Model to help these organizations to achieve it by identifying the
different stages of the management of translation data that determine the path
to the aforementioned goal.
","Nuria Bel|Mikel L. Forcada|Asuncion Gomez-Perez","","http://arxiv.org/abs/1607.01990v1","http://arxiv.org/pdf/1607.01990v1","","","","","cs.CY","cs.CY|cs.CL"
"722","1607.03200v2","2016-07-11 23:42:36","2018-04-08 11:49:19","Qualitative Judgement of Research Impact: Domain Taxonomy as a
  Fundamental Framework for Judgement of the Quality of Research","  The appeal of metric evaluation of research impact has attracted considerable
interest in recent times. Although the public at large and administrative
bodies are much interested in the idea, scientists and other researchers are
much more cautious, insisting that metrics are but an auxiliary instrument to
the qualitative peer-based judgement. The goal of this article is to propose
availing of such a well positioned construct as domain taxonomy as a tool for
directly assessing the scope and quality of research. We first show how
taxonomies can be used to analyse the scope and perspectives of a set of
research projects or papers. Then we proceed to define a research team or
researcher's rank by those nodes in the hierarchy that have been created or
significantly transformed by the results of the researcher. An experimental
test of the approach in the data analysis domain is described. Although the
concept of taxonomy seems rather simplistic to describe all the richness of a
research domain, its changes and use can be made transparent and subject to
open discussions.
","Fionn Murtagh|Michael Orlov|Boris Mirkin","","http://arxiv.org/abs/1607.03200v2","http://arxiv.org/pdf/1607.03200v2","http://dx.doi.org/10.1007/s00357-018-9247-0","22 pages, 7 figures, Journal of Classification, Online First, March
  25, 2018","","10.1007/s00357-018-9247-0","cs.DL","cs.DL|stat.ML|68P01|H.0, I.5.3, G.3"
"723","1607.08344v1","2016-07-28 08:02:29","2016-07-28 08:02:29","AUGURY: A time-series based application for the analysis and forecasting
  of system and network performance metrics","  This paper presents AUGURY, an application for the analysis of monitoring
data from computers, servers or cloud infrastructures. The analysis is based on
the extraction of patterns and trends from historical data, using elements of
time-series analysis. The purpose of AUGURY is to aid a server administrator by
forecasting the behaviour and resource usage of specific applications and in
presenting a status report in a concise manner. AUGURY provides tools for
identifying network traffic congestion and peak usage times, and for making
memory usage projections. The application data processing specialises in two
tasks: the parametrisation of the memory usage of individual applications and
the extraction of the seasonal component from network traffic data. AUGURY uses
a different underlying assumption for each of these two tasks. With respect to
the memory usage, a limited number of single-valued parameters are assumed to
be sufficient to parameterize any application being hosted on the server.
Regarding the network traffic data, long-term patterns, such as hourly or daily
exist and are being induced by work-time schedules and automatised
administrative jobs. In this paper, the implementation of each of the two tasks
is presented, tested using locally-generated data, and applied to data from
weather forecasting applications hosted on a web server. This data is used to
demonstrate the insight that AUGURY can add to the monitoring of server and
cloud infrastructures.
","Nicolas Gutierrez|Manuela Wiesinger-Widi","","http://arxiv.org/abs/1607.08344v1","http://arxiv.org/pdf/1607.08344v1","http://dx.doi.org/10.1109/SYNASC.2016.062","8 pages, 9 figures, submitted to SYNASC2016","","10.1109/SYNASC.2016.062","cs.DC","cs.DC|cs.PF"
"724","1608.00647v3","2016-08-02 00:09:22","2016-09-20 21:55:00","Multi-task Prediction of Disease Onsets from Longitudinal Lab Tests","  Disparate areas of machine learning have benefited from models that can take
raw data with little preprocessing as input and learn rich representations of
that raw data in order to perform well on a given prediction task. We evaluate
this approach in healthcare by using longitudinal measurements of lab tests,
one of the more raw signals of a patient's health state widely available in
clinical data, to predict disease onsets. In particular, we train a Long
Short-Term Memory (LSTM) recurrent neural network and two novel convolutional
neural networks for multi-task prediction of disease onset for 133 conditions
based on 18 common lab tests measured over time in a cohort of 298K patients
derived from 8 years of administrative claims data. We compare the neural
networks to a logistic regression with several hand-engineered, clinically
relevant features. We find that the representation-based learning approaches
significantly outperform this baseline. We believe that our work suggests a new
avenue for patient risk stratification based solely on lab results.
","Narges Razavian|Jake Marcus|David Sontag","","http://arxiv.org/abs/1608.00647v3","http://arxiv.org/pdf/1608.00647v3","","Presented at 2016 Machine Learning and Healthcare Conference (MLHC
  2016), Los Angeles, CA","","","cs.LG","cs.LG"
"725","1608.02658v3","2016-08-08 23:46:59","2016-12-21 16:33:44","Revisiting Causality Inference in Memory-less Transition Networks","  Several methods exist to infer causal networks from massive volumes of
observational data. However, almost all existing methods require a considerable
length of time series data to capture cause and effect relationships. In
contrast, memory-less transition networks or Markov Chain data, which refers to
one-step transitions to and from an event, have not been explored for causality
inference even though such data is widely available. We find that causal
network can be inferred from characteristics of four unique distribution zones
around each event. We call this Composition of Transitions and show that cause,
effect, and random events exhibit different behavior in their compositions. We
applied machine learning models to learn these different behaviors and to infer
causality. We name this new method Causality Inference using Composition of
Transitions (CICT). To evaluate CICT, we used an administrative inpatient
healthcare dataset to set up a network of patients transitions between
different diagnoses. We show that CICT is highly accurate in inferring whether
the transition between a pair of events is causal or random and performs well
in identifying the direction of causality in a bi-directional association.
","Abbas Shojaee|Isuru Ranasinghe|Alireza Ani","","http://arxiv.org/abs/1608.02658v3","http://arxiv.org/pdf/1608.02658v3","","This edition is improved with further details in the discussion
  section and Figure 1. Other authors will be added in final revision; For
  feedback, opinions, or questions please contact: abbas.shojaee@gmail.com OR
  abbas.shojaee@yale.edu","","","stat.ML","stat.ML|cs.AI|nlin.CD|physics.data-an"
"726","1608.04841v2","2016-08-17 03:11:07","2016-08-21 20:35:08","Safe Serializable Secure Scheduling: Transactions and the Trade-off
  Between Security and Consistency","  Modern applications often operate on data in multiple administrative domains.
In this federated setting, participants may not fully trust each other. These
distributed applications use transactions as a core mechanism for ensuring
reliability and consistency with persistent data. However, the coordination
mechanisms needed for transactions can both leak confidential information and
allow unauthorized influence.
  By implementing a simple attack, we show these side channels can be
exploited. However, our focus is on preventing such attacks. We explore secure
scheduling of atomic, serializable transactions in a federated setting. While
we prove that no protocol can guarantee security and liveness in all settings,
we establish conditions for sets of transactions that can safely complete under
secure scheduling. Based on these conditions, we introduce staged commit, a
secure scheduling protocol for federated transactions. This protocol avoids
insecure information channels by dividing transactions into distinct stages. We
implement a compiler that statically checks code to ensure it meets our
conditions, and a system that schedules these transactions using the staged
commit protocol. Experiments on this implementation demonstrate that realistic
federated transactions can be scheduled securely, atomically, and efficiently.
","Isaac Sheff|Tom Magrino|Jed Liu|Andrew C. Myers|Robbert van Renesse","","http://arxiv.org/abs/1608.04841v2","http://arxiv.org/pdf/1608.04841v2","","Technical Report","","","cs.DC","cs.DC|cs.CR"
"727","1608.05564v2","2016-08-19 10:44:42","2017-09-19 12:06:13","Living in Parallel Realities -- Co-Existing Schema Versions with a
  Bidirectional Database Evolution Language","  We introduce end-to-end support of co-existing schema versions within one
database. While it is state of the art to run multiple versions of a
continuously developed application concurrently, it is hard to do the same for
databases. In order to keep multiple co-existing schema versions alive; which
are all accessing the same data set; developers usually employ handwritten
delta code (e.g. views and triggers in SQL). This delta code is hard to write
and hard to maintain: if a database administrator decides to adapt the physical
table schema, all handwritten delta code needs to be adapted as well, which is
expensive and error-prone in practice. In this paper, we present InVerDa:
developers use the simple bidirectional database evolution language BiDEL,
which carries enough information to generate all delta code automatically.
Without additional effort, new schema versions become immediately accessible
and data changes in any version are visible in all schema versions at the same
time. InVerDa also allows for easily changing the physical table design without
affecting the availability of co-existing schema versions. This greatly
increases robustness (orders of magnitude less lines of code) and allows for
significant performance optimization. A main contribution is the formal
evaluation that each schema version acts like a common full-fledged database
schema independently of the chosen physical table design.
","Kai Herrmann|Hannes Voigt|Andreas Behrend|Jonas Rausch|Wolfgang Lehner","","http://arxiv.org/abs/1608.05564v2","http://arxiv.org/pdf/1608.05564v2","http://dx.doi.org/10.1145/3035918.3064046","","","10.1145/3035918.3064046","cs.DB","cs.DB"
"728","1608.07232v1","2016-08-24 15:41:20","2016-08-24 15:41:20","Toward a closed-loop subcutaneous delivery of L-DOPA","  L-DOPA has been the gold standard treatment for Parkinson's disease since 50
years. Being the direct biochemical precursor of dopamine, L-DOPA is
effectively converted in the brain, but two major phenomena reduce its
therapeutic action: i) competition with amino acids in the gut wall and in the
blood brain barrier and ii) its fast kinetics (absorption, distribution,
metabolism, and elimination). Continuous administration of L-DOPA, such as
jejunal pumps, have addressed the issue of fast absorption. Considering a
subcutaneous delivery of L-DOPA allows to bypass the gastrointestinal tract and
avoid competition with dietary amino acids. Remains the competition at the
blood barrier between amino acids and L-DOPA, which we address by proposing a
closed-loop controlled, continuous subcutaneous delivery pump. In the
proof-of-concept format, the delivery strategy evaluated on comprehensive model
of L-DOPA kinetics, holds the promise of improving the treatment of late-stage
Parkinson's disease patients.
","Marouen Ben Guebila|Johan Thunberg","","http://arxiv.org/abs/1608.07232v1","http://arxiv.org/pdf/1608.07232v1","","","","","q-bio.TO","q-bio.TO"
"729","1608.07658v1","2016-08-27 03:43:50","2016-08-27 03:43:50","TopoMan: Global Network Visibility in the Presence of Middleboxes (A
  Graybox Approach)","  Software Defined Networks (SDN) provide vital benefits to network
administrators by offering global visibility and network-wide control over the
switching infrastructure of the network. It is rather much difficult to obtain
the same benefits in the presence of middleboxes (MBs), due to (i) lack of a
proper topology discovery mechanism in environments with a mix of forwarding
devices and middleboxes. (ii) lack of generic APIs to abstract and gain control
on these rigid and heterogeneous third-party middleboxes (iii) lack of a
generic network infrastructure framework to monitor and verify any specific
device or path connectivity status in the network. These limitations make
automation of network operations such as, network-wide monitoring, policy
enforcement and rule-placement much difficult to handle. Hence, there is a
greater urge even from middlebox vendors, to better handle the control and
visibility aspects of the network in presence of middleboxes.
  In this paper, we propose a Unified network infrastructure framework for
gaining global network visibility, by discovering the network topology in the
presence of middleboxes, along with a framework to support the end-to-end path
connectivity verification, independent of SDN. We have also addressed security
aspects and provided necessary APIs to support our framework.
","Vasudevan Nagendra|Shubhada Patil|Michalis Polychronakis|Samir R. Das","","http://arxiv.org/abs/1608.07658v1","http://arxiv.org/pdf/1608.07658v1","","","","","cs.NI","cs.NI"
"730","1608.08096v1","2016-08-29 15:03:05","2016-08-29 15:03:05","Clinical trials with rescue medication applied according to a
  deterministic rule","  Clinical trials in specific indications require the administration of rescue
medication in case a patient does not sufficiently respond to investigational
treatment. The application of additional treatment on an as needed basis causes
problems to the analysis and interpretation of the results of these studies
since the effect of the investigational treatment can be confounded by the
additional medication. Following-up all patients until study end and capturing
all data is not fully addressing the issue. We present an analysis that takes
care of the fact that rescue is a study outcome and not a covariate when rescue
medication is administered according to a deterministic rule. This approach
allows to clearly define a biological effect. For normally distributed
longitudinal data a practically unbiased estimator of the biological effect can
be obtained. The results are compared to an ITT analysis and an analysis on all
patients not receiving rescue.
","Gerd K. Rosenkranz","","http://arxiv.org/abs/1608.08096v1","http://arxiv.org/pdf/1608.08096v1","","","","","stat.ME","stat.ME"
"731","1608.08387v3","2016-08-30 09:38:22","2017-03-30 08:16:52","Breakdown of local information processing may underlie isoflurane
  anesthesia effects","  The disruption of coupling between brain areas has been suggested as the
mechanism underlying loss of consciousness in anesthesia. This hypothesis has
been tested previously by measuring the information transfer between brain
areas, and by taking reduced information transfer as a proxy for decoupling.
Yet, information transfer is a function of the amount of information available
in the information source-such that transfer decreases even for unchanged
coupling when less source information is available. Therefore, we asked whether
impaired local information processing leads to a loss of information transfer.
An important prediction of this alternative hypothesis is that changes in
locally available information (signal entropy) should be at least as pronounced
as changes in information transfer. We tested this prediction by recording
local field potentials in two ferrets after administration of isoflurane in
concentrations of 0.0 %, 0.5 %, and 1.0 %.
  We found strong decreases in the source entropy under isoflurane in area V1
and the prefrontal cortex (PFC)-as predicted by our alternative hypothesis. The
decrease in source entropy was stronger in PFC. Information transfer between V1
and PFC was reduced bidirectionally, but with a stronger decrease from PFC to
V1. This links the stronger decrease in information transfer to the stronger
decrease in source entropy, suggesting reduced source entropy reduces
information transfer. This conclusion fits the observation that the synaptic
targets of isoflurane are located in local cortical circuits rather than on the
synapses formed by interareal axonal projections. Thus, changes in information
transfer under isoflurane seem to be a consequence of changes in local
processing more than of decoupling between brain areas. We suggest that source
entropy changes must be considered whenever interpreting changes in information
transfer as decoupling.
","Patricia Wollstadt|Kristin K. Sellers|Lucas Rudelt|Viola Priesemann|Axel Hutt|Flavio Frohlich|Michael Wibral","","http://arxiv.org/abs/1608.08387v3","http://arxiv.org/pdf/1608.08387v3","http://dx.doi.org/10.1371/journal.pcbi.1005511","48 pages, 11 Figures","","10.1371/journal.pcbi.1005511","q-bio.NC","q-bio.NC"
"732","1609.00056v5","2016-08-31 22:14:14","2018-09-18 20:35:51","Linear Quadratic Mean Field Teams: Optimal and Approximately Optimal
  Decentralized Solutions","  We consider team optimal control of decentralized systems with linear
dynamics, quadratic costs, and arbitrary disturbance that consist of multiple
sub-populations with exchangeable agents (i.e., exchanging two agents within
the same sub-population does not affect the dynamics or the cost). Such a
system is equivalent to one where the dynamics and costs are coupled across
agents through the mean-field (or empirical mean) of the states and actions
(even when the primitive random variables are non-exchangeable). Two
information structures are investigated. In the first, all agents observe their
local state and the mean-field of all sub-populations, in the second, all
agents observe their local state but the mean-field of only a subset of the
sub-populations. Both information structures are non-classical and not
partially nested. Nonetheless, it is shown that linear control strategies are
optimal for the first and approximately optimal for the second, the
approximation error is inversely proportional to the size of the
sub-populations whose mean-fields are not observed. The corresponding gains are
determined by the solution of K+1 decoupled standard Riccati equations, where K
is the number of sub-populations. The dimensions of the Riccati equations do
not depend on the size of the sub-populations, thus the solution complexity is
independent of the number of agents. Generalizations to major-minor agents,
tracking cost, weighted mean-field, and infinite horizon are provided. The
results are illustrated using an example of demand response in smart grids.
","Jalal Arabneydi|Aditya Mahajan","","http://arxiv.org/abs/1609.00056v5","http://arxiv.org/pdf/1609.00056v5","","This article has been withdrawn by arXiv administrators due to an
  unresolvable authorship dispute","","","cs.SY","cs.SY|cs.IT|math.IT"
"733","1609.00099v2","2016-09-01 03:38:16","2016-09-03 22:45:33","Design and Implementation of A Network Security Management System","  In recent years, the emerged network worms and attacks have distributive
characteristic, which can spread globally in a very short time. Security
management crossing network to co-defense network-wide attacks and improve
efficiency of security administration is urgently needed. This paper proposes a
hierarchical distributed network security management system (HD-NSMS), which
can centrally manage security across networks. First describes the system in
macrostructure and microstructure; then discusses three key problems when
building HD-NSMS: device model, alert mechanism and emergency response
mechanism; at last, describes the implementation of HD-NSMS. The paper is
valuable for implementing NSMS in that it derives from a practical network
security management system (NSMS).
","Zhiyong Shan|Bin Liao","","http://arxiv.org/abs/1609.00099v2","http://arxiv.org/pdf/1609.00099v2","","","","","cs.CR","cs.CR|cs.NI"
"734","1609.00100v2","2016-09-01 03:45:39","2016-09-03 22:40:31","Suspicious-Taint-Based Access Control for Protecting OS from Network
  Attacks","  Today, security threats to operating systems largely come from network.
Traditional discretionary access control mechanism alone can hardly defeat
them. Although traditional mandatory access control models can effectively
protect the security of OS, they have problems of being incompatible with
application software and complex in administration. In this paper, we propose a
new model, Suspicious-Taint-Based Access Control (STBAC) model, for defeating
network attacks while being compatible, simple and maintaining good system
performance. STBAC regards the processes using Non-Trustable-Communications as
the starting points of suspicious taint, traces the activities of the
suspiciously tainted processes by taint rules, and forbids the suspiciously
tainted processes to illegally access vital resources by protection rules. Even
in the cases when some privileged processes are subverted, STBAC can still
protect vital resources from being compromised by the intruder. We implemented
the model in the Linux kernel and evaluated it through experiments. The
evaluation showed that STBAC could protect vital resources effectively without
significant impact on compatibility and performance.
","Zhiyong Shan","","http://arxiv.org/abs/1609.00100v2","http://arxiv.org/pdf/1609.00100v2","","","","","cs.CR","cs.CR|cs.OS"
"735","1609.00266v1","2016-09-01 15:12:51","2016-09-01 15:12:51","Retrofitting Applications with Provenance-Based Security Monitoring","  Data provenance is a valuable tool for detecting and preventing cyber attack,
providing insight into the nature of suspicious events. For example, an
administrator can use provenance to identify the perpetrator of a data leak,
track an attacker's actions following an intrusion, or even control the flow of
outbound data within an organization. Unfortunately, providing relevant data
provenance for complex, heterogenous software deployments is challenging,
requiring both the tedious instrumentation of many application components as
well as a unified architecture for aggregating information between components.
  In this work, we present a composition of techniques for bringing affordable
and holistic provenance capabilities to complex application workflows, with
particular consideration for the exemplar domain of web services. We present
DAP, a transparent architecture for capturing detailed data provenance for web
service components. Our approach leverages a key insight that minimal knowledge
of open protocols can be leveraged to extract precise and efficient provenance
information by interposing on application components' communications, granting
DAP compatibility with existing web services without requiring instrumentation
or developer cooperation. We show how our system can be used in real time to
monitor system intrusions or detect data exfiltration attacks while imposing
less than 5.1 ms end-to-end overhead on web requests. Through the introduction
of a garbage collection optimization, DAP is able to monitor system activity
without suffering from excessive storage overhead. DAP thus serves not only as
a provenance-aware web framework, but as a case study in the non-invasive
deployment of provenance capabilities for complex applications workflows.
","Adam Bates|Kevin Butler|Alin Dobra|Brad Reaves|Patrick Cable|Thomas Moyer|Nabil Schear","","http://arxiv.org/abs/1609.00266v1","http://arxiv.org/pdf/1609.00266v1","","","","","cs.CR","cs.CR"
"736","1609.00992v1","2016-09-04 21:23:22","2016-09-04 21:23:22","Performance Evaluation of a Natural Language Processing approach applied
  in White Collar crime investigation","  In today world we are confronted with increasing amounts of information every
day coming from a large variety of sources. People and co-operations are
producing data on a large scale, and since the rise of the internet, e-mail and
social media the amount of produced data has grown exponentially. From a law
enforcement perspective we have to deal with these huge amounts of data when a
criminal investigation is launched against an individual or company. Relevant
questions need to be answered like who committed the crime, who were involved,
what happened and on what time, who were communicating and about what? Not only
the amount of available data to investigate has increased enormously, but also
the complexity of this data has increased. When these communication patterns
need to be combined with for instance a seized financial administration or
corporate document shares a complex investigation problem arises. Recently,
criminal investigators face a huge challenge when evidence of a crime needs to
be found in the Big Data environment where they have to deal with large and
complex datasets especially in financial and fraud investigations. To tackle
this problem, a financial and fraud investigation unit of a European country
has developed a new tool named LES that uses Natural Language Processing (NLP)
techniques to help criminal investigators handle large amounts of textual
information in a more efficient and faster way. In this paper, we present
briefly this tool and we focus on the evaluation its performance in terms of
the requirements of forensic investigation: speed, smarter and easier for
investigators. In order to evaluate this LES tool, we use different performance
metrics. We also show experimental results of our evaluation with large and
complex datasets from real-world application.
","Maarten Banerveld|Nhien-An Le-Khac|Tahar Kechadi","","http://arxiv.org/abs/1609.00992v1","http://arxiv.org/pdf/1609.00992v1","","","","","cs.IR","cs.IR|cs.CY"
"737","1609.01821v1","2016-09-05 23:26:23","2016-09-05 23:26:23","Bring-Your-Own-Device (BYOD): An Evaluation of Associated Risks to
  Corporate Information Security","  This study evaluates the cyber-risks to Business Information Assets posed by
the adoption of Bring-Your-Own-Device (BYOD) to the workplace. BYOD is an
emerging trend where employees bring and use personal computing devices on the
companys network to access applications and sensitive data like emails,
calendar and scheduling applications, documents, etc. Employees are captivated
by BYOD because they can have access to private items as well as perform
certain job functions while being unrestricted to their desks. This is however
usually done on the blind side of management or the system administrator; a
situation that tends to expose vital and sensitive corporate information to
various threats like unwanted network traffic, unknown applications, malwares,
and viruses. Expert opinions were elicited in this exploratory study. The study
evaluated the characteristics of BYOD, assessed associated risks, threats and
vulnerabilities. The findings indicate that little or no security measures were
instituted to mitigate risks associated with BYOD. Though, profound benefits
abound with BYOD adoption, they could be eroded by security threats and costs
of mitigation in curing breaches. The most significant risk was found to be
Data Loss which was in consonance with similar studies on Smartphone security
risks. Some mitigation measures are then recommended.
","Ezer Osei Yeboah-Boateng|Francis Edmund Boaten","","http://arxiv.org/abs/1609.01821v1","http://arxiv.org/pdf/1609.01821v1","","","International Journal of Information Technology (IT) &
  Engineering, Vol. 4, Issue 8, pp 12-30, 2016","","cs.CR","cs.CR"
"738","1609.02775v1","2016-09-09 12:58:43","2016-09-09 12:58:43","Werner Heisenberg and the German Uranium Project 1939 - 1945. Myths and
  Facts","  The results of a careful analysis of all the available information on the
activities of Heisenberg and of his talks during the years 1939 to 1945 can be
summarized in the following way. Like several other German physicists
Heisenberg was drafted by German Army Ordnance when war began in Europe in
September 1939 to investigate whether the energy from splitting Uranium nuclei
by neutrons could be used for technical and military purposes. Heisenberg found
that this is possible in principle but that military use would require such
enormous industrial expenditures that it would take many years and would be
impracticable while the war lasted. The project was therefore dropped by the
Nazi government in 1942. Heisenberg even refrained from calculating a precise
value for the critical mass of U 235. He was relieved that he was thus spared a
moral decision between obeying an order to build the bomb or risking his life
by refusing to be involved in the project or sabotaging it. He was happy to be
confined to a project of building a small test reactor under civilian
administration that the government had approved. In 1941 Heisenberg tried to
get the opinion of Niels Bohr in Copenhagen on what the international community
of nuclear physicist could possibly do or prevent regarding the long-range
technical feasibility of making nuclear weapons. Bohr completely misunderstood
the cautious approach of Heisenberg.
","Klaus Gottstein","","http://arxiv.org/abs/1609.02775v1","http://arxiv.org/pdf/1609.02775v1","","20 pages","","","physics.hist-ph","physics.hist-ph|physics.soc-ph"
"739","1609.04450v2","2016-09-14 21:22:53","2018-08-10 18:10:21","An Introduction to Graphene Plasmonics","  This book is meant as an introduction to graphene plasmonics and aims at the
advanced undergraduate and graduate students entering the field of plasmonics
in graphene. In it different theoretical methods are introduced, starting with
an elementary description of graphene plasmonics and evolving towards more
advanced topics. This book is essentially self-contained and brings together a
number of different topics about the field that are scattered in the vast
literature. The text is composed of eleven chapters and of a set of detailed
appendices. It can be read in two different ways: Reading only the chapters to
get acquainted with the field of plasmonics in graphene or reading the chapters
and studying the appendices to get a working knowledge of the topic. The study
of the material in this book will bring the students to the forefront of the
research in this field.
","P. A. D. Goncalves|N. M. R. Peres","","http://arxiv.org/abs/1609.04450v2","http://arxiv.org/pdf/1609.04450v2","http://dx.doi.org/10.1142/9948","arXiv admin note: submission withdrawn by arXiv administrators due to
  inappropriate format","","10.1142/9948","cond-mat.mes-hall","cond-mat.mes-hall|physics.optics"
"740","1610.01645v1","2016-09-19 08:07:21","2016-09-19 08:07:21","Administration Costs in the Management of Research Funds; A Case Study
  of a Public Fund for the Promotion of Industrial Innovation","  Research funding agencies routinely use a proportion of their total revenues
to support internal administration and marketing costs. The ratio of
administration to total costs, referred to as the administration ratio, is
highly variable and within any single fund depends on many factors including
the number and average size of projects and the overall efficiency of the
funding agency. In this study, the standard agency activities have been
identified and used to develop a model of administration costs against expected
outcomes. In particular, the model has been designed to estimate the optimum
portfolio success rate and administration ratio as a function of a range of key
input variables including the project size, the complexity of proposal
evaluation and project management, the risk tolerance of the sponsor and the
targeted research domain.
","David R Walwyn","","http://arxiv.org/abs/1610.01645v1","http://arxiv.org/pdf/1610.01645v1","","16 pages, 5000 words, 2 tables, 8 figures","","","q-fin.GN","q-fin.GN|65.05|J.1"
"741","1609.05702v1","2016-09-19 13:04:18","2016-09-19 13:04:18","Monitor, Detect, Mitigate: Combating BGP Prefix Hijacking in Real-Time
  with ARTEMIS","  The Border Gateway Protocol (BGP) is globally used by Autonomous Systems
(ASes) to establish route paths for IP prefixes in the Internet. Due to the
lack of authentication in BGP, an AS can hijack IP prefixes owned by other ASes
(i.e., announce illegitimate route paths), impacting thus the Internet routing
system and economy. To this end, a number of hijacking detection systems have
been proposed. However, existing systems are usually third party services that
-inherently- introduce a significant delay between the hijacking detection (by
the service) and its mitigation (by the network administrators). To overcome
this shortcoming, in this paper, we propose ARTEMIS, a tool that enables an AS
to timely detect hijacks on its own prefixes, and automatically proceed to
mitigation actions. To evaluate the performance of ARTEMIS, we conduct real
hijacking experiments. To our best knowledge, it is the first time that a
hijacking detection/mitigation system is evaluated through extensive
experiments in the real Internet. Our results (a) show that ARTEMIS can detect
(mitigate) a hijack within a few seconds (minutes) after it has been launched,
and (b) demonstrate the efficiency of the different control-plane sources used
by ARTEMIS, towards monitoring routing changes.
","Pavlos Sermpezis|Gavriil Chaviaras|Petros Gigis|Xenofontas Dimitropoulos","","http://arxiv.org/abs/1609.05702v1","http://arxiv.org/pdf/1609.05702v1","http://dx.doi.org/10.1145/2934872.2959078","","In Proceedings of the ACM SIGCOMM 2016 Conference (SIGCOMM '16),
  625-626","10.1145/2934872.2959078","cs.NI","cs.NI"
"742","1609.06676v1","2016-09-21 18:44:48","2016-09-21 18:44:48","Detecting Anomalous User Behavior Using an Extended Isolation Forest
  Algorithm: An Enterprise Case Study","  Anomalous user behavior detection is the core component of many information
security systems, such as intrusion detection, insider threat detection and
authentication systems. Anomalous behavior will raise an alarm to the system
administrator and can be further combined with other information to determine
whether it constitutes an unauthorised or malicious use of a resource. This
paper presents an anomalous user behaviour detection framework that applies an
extended version of Isolation Forest algorithm. Our method is fast and scalable
and does not require example anomalies in the training data set. We apply our
method to an enterprise dataset. The experimental results show that the system
is able to isolate anomalous instances from the baseline user model using a
single feature or combined features.
","Li Sun|Steven Versteeg|Serdar Boztas|Asha Rao","","http://arxiv.org/abs/1609.06676v1","http://arxiv.org/pdf/1609.06676v1","","","","","cs.CR","cs.CR"
"743","1609.08393v1","2016-09-27 13:04:44","2016-09-27 13:04:44","Semi Automatic Color Segmentation of Document Pages","  -This paper presents a semi automatic method used to segment color documents
into different uniform color plans. The practical application is dedicated to
administrative documents segmentation. In these documents, like in many other
cases, color has a semantic meaning: it is then possible to identify some
specific regions like manual annotations, rubber stamps or colored
highlighting. A first step of user-controlled learning of the desired color
plans is made on few sample documents. An automatic process can then be
performed on the much bigger set as a batch. Our experiments show very
interesting results in with a very competitive processing time.
","Stephane Bres|Veronique Eglin|Vincent Poulain","imagine|imagine|LOCEAN","http://arxiv.org/abs/1609.08393v1","http://arxiv.org/pdf/1609.08393v1","","","International Workshop on Document Analysis System, Apr 2016,
  Santorini, France","","cs.CV","cs.CV"
"744","1610.02101v1","2016-10-06 23:51:29","2016-10-06 23:51:29","On the automated verification of web applications with embedded SQL","  A large number of web applications is based on a relational database together
with a program, typically a script, that enables the user to interact with the
database through embedded SQL queries and commands. In this paper, we introduce
a method for formal automated verification of such systems which connects
database theory to mainstream program analysis. We identify a fragment of SQL
which captures the behavior of the queries in our case studies, is
algorithmically decidable, and facilitates the construction of weakest
preconditions. Thus, we can integrate the analysis of SQL queries into a
program analysis tool chain. To this end, we implement a new decision procedure
for the SQL fragment that we introduce. We demonstrate practical applicability
of our results with three case studies, a web administrator, a simple firewall,
and a conference management system.
","Shachar Itzhaky|Tomer Kotek|Noam Rinetzky|Mooly Sagiv|Orr Tamir|Helmut Veith|Florian Zuleger","","http://arxiv.org/abs/1610.02101v1","http://arxiv.org/pdf/1610.02101v1","","25 pages","","","cs.LO","cs.LO|68P15, 68Q60|D.3.2; F.3.1"
"745","1610.02708v1","2016-10-09 19:31:44","2016-10-09 19:31:44","Population patterns in World's administrative units","  While there has been an extended discussion concerning city population
distribution, little has been said about administrative units. Even though
there might be a correspondence between cities and administrative divisions,
they are conceptually different entities and the correspondence breaks as
artificial divisions form and evolve. In this work we investigate the
population distribution of second level administrative units for 150 countries
and propose the Discrete Generalized Beta Distribution (DGBD) rank-size
function to describe the data. After testing the goodness of fit of this two
parameter function against power law, which is the most common model for city
population, DGBD is a good statistical model for 73% of our data sets and
better than power law in almost every case. Particularly, DGBD is better than
power law for fitting country population data. The fitted parameters of this
function allow us to construct a phenomenological characterization of countries
according to the way in which people are distributed inside them. We present a
computational model to simulate the formation of administrative divisions and
give numerical evidence that DGBD arises from it. This model along with the
DGBD function prove adequate to reproduce and describe local unit evolution and
its effect on population distribution.
","Oscar Fontanelli|Pedro Miramontes|Germinal Cocho|Wentian Li","","http://arxiv.org/abs/1610.02708v1","http://arxiv.org/pdf/1610.02708v1","","","","","stat.AP","stat.AP|physics.soc-ph"
"746","1610.02902v1","2016-10-10 13:22:28","2016-10-10 13:22:28","Content Based Image Retrieval (CBIR) in Remote Clinical Diagnosis and
  Healthcare","  Content-Based Image Retrieval (CBIR) locates, retrieves and displays images
alike to one given as a query, using a set of features. It demands accessible
data in medical archives and from medical equipment, to infer meaning after
some processing. A problem similar in some sense to the target image can aid
clinicians. CBIR complements text-based retrieval and improves evidence-based
diagnosis, administration, teaching, and research in healthcare. It facilitates
visual/automatic diagnosis and decision-making in real-time remote
consultation/screening, store-and-forward tests, home care assistance and
overall patient surveillance. Metrics help comparing visual data and improve
diagnostic. Specially designed architectures can benefit from the application
scenario. CBIR use calls for file storage standardization, querying procedures,
efficient image transmission, realistic databases, global availability, access
simplicity, and Internet-based structures. This chapter recommends important
and complex aspects required to handle visual content in healthcare.
","Albany E. Herrmann|Vania Vieira Estrela","","http://arxiv.org/abs/1610.02902v1","http://arxiv.org/pdf/1610.02902v1","http://dx.doi.org/10.4018/978-1-4666-9978-6.ch039","28 pages, 6 figures, Book Chapter from ""Encyclopedia of E-Health and
  Telemedicine""","Encyclopedia of E-Health and Telemedicine. IGI Global, 2016.
  495-520. Web. 10 Oct. 2016","10.4018/978-1-4666-9978-6.ch039","cs.CV","cs.CV"
"747","1610.03450v1","2016-10-11 18:07:38","2016-10-11 18:07:38","A Distributed Multi Agents Based Platform for High Performance Computing
  Infrastructures","  This work introduces a novel, modular, layered web based platform for
managing machine learning experiments on grid-based High Performance Computing
infrastructures. The coupling of the communication services offered by the
grid, with an administration layer and conventional web server programming, via
a data synchronization utility, leads to the straightforward development of a
web-based user interface that allows the monitoring and managing of diverse
online distributed computing applications. It also introduces an experiment
generation and monitoring tool particularly suitable for investigating machine
learning in game playing. The platform is demonstrated with experiments for two
different games.
","Chairi Kiourt|Dimitris Kalles","","http://arxiv.org/abs/1610.03450v1","http://arxiv.org/pdf/1610.03450v1","","12 pages,4 figures, Conference: Workshop Parallel and Distributed
  Computing for Knowledge Discovery in Data Bases, a workshop of the European
  Conference on Machine Learning and Principles and Practice of Knowledge
  Discovery, At Porto, Portugal, 2015","","","cs.DC","cs.DC|cs.MA"
"748","1610.04005v1","2016-10-13 10:08:37","2016-10-13 10:08:37","Stream Reasoning-Based Control of Caching Strategies in CCN Routers","  Content-Centric Networking (CCN) research addresses the mismatch between the
modern usage of the Internet and its outdated architecture. Importantly, CCN
routers may locally cache frequently requested content in order to speed up
delivery to end users. Thus, the issue of caching strategies arises, i.e.,
which content shall be stored and when it should be replaced. In this work, we
employ novel techniques towards intelligent administration of CCN routers that
autonomously switch between existing strategies in response to changing content
request patterns. In particular, we present a router architecture for CCN
networks that is controlled by rule-based stream reasoning, following the
recent formal framework LARS which extends Answer Set Programming for streams.
The obtained possibility for flexible router configuration at runtime allows
for faster experimentation and may thus help to advance the further development
of CCN. Moreover, the empirical evaluation of our feasibility study shows that
the resulting caching agent may give significant performance gains.
","Harald Beck|Bruno Bierbaumer|Minh Dao-Tran|Thomas Eiter|Hermann Hellwagner|Konstantin Schekotihin","","http://arxiv.org/abs/1610.04005v1","http://arxiv.org/pdf/1610.04005v1","","21 pages, 8 figures","","","cs.AI","cs.AI|cs.NI"
"749","1610.07294v2","2016-10-24 06:42:31","2016-11-09 21:27:26","Molecular solutions for the Maximum K-colourable Sub graph Problem in
  Adleman-Lipton model","  Adleman showed that deoxyribonucleic acid DNA strands could be employed
towards calculating solutions to an instance of the Hamiltonian path problem .
Lipton also demonstrated that Adleman techniques could be used to solve the
Satisfiability problem. In this paper, we use Adleman Lipton model for
developing a DNA algorithm to solve Maximum k-colourable Sub graph problem. In
spite of the NP-hardness of Maximum k-colourable Sub graph problem our DNA
procedures is done in a polynomial time.
","Akbar Moazzam|Babak Dalvand","","http://arxiv.org/abs/1610.07294v2","http://arxiv.org/pdf/1610.07294v2","","This article has been withdrawn by arXiv administrators due to
  excessive unattributed and verbatim text overlap from external sources","","","cs.DS","cs.DS|cs.ET"
"750","1610.08036v1","2016-10-25 19:42:46","2016-10-25 19:42:46","Modeling pressure distribution and heat in the body tissue and extract
  the relationship between them in order to improve treatment planning in HIFU","  In high intensity focused ultrasound (HIFU) systems using non-ionizing
methods in cancer treatment, if the device is applied to the body externally,
the HIFU beam can damage nearby healthy tissues and burn skin due to lack of
knowledge about the viscoelastic properties of patient tissue and failure to
consider the physical properties of tissue in treatment planning. Addressing
this problem by using various methods, such as MRI or ultrasound, elastography
can effectively measure visco-elastic properties of tissue and fits within the
pattern of stimulation and total treatment planning. In this paper, in a linear
path of HIFU propagation, and by considering the smallest part of the path,
including voxel with three mechanical elements of mass, spring and damper,
which represents the properties of viscoelasticity of tissue, by creating waves
of HIFU in the wire environment of MATLAB mechanics and stimulating these
elements, pressure and heat transfer due to stimulation in the hypothetical
voxel was obtained. Through the repeatability of these three-dimensional
elements, tissue is created. The measurement was performed on three layers. The
values of these elements for liver tissue and kidney of sheep in a practical
example and outside the body are measured, and pressure and heat for three
layers of liver and kidney tissue of an organism were obtained by applying
ultrasound signals with a designed model. This action is repeated in three
different directions, and the results are then compared with simulation
software for ultrasound, as a reference to U.S. Food and Drug Administration
(FDA) measures for HIFU, as well as comparisons of results with an operational
method for an HIFU cell.
","Saeed Reza Hajian|Ali Abbaspour Tehrani Fard|Majid Pouladian|Gholam Reza Hemmasi","","http://arxiv.org/abs/1610.08036v1","http://arxiv.org/pdf/1610.08036v1","","","","","physics.med-ph","physics.med-ph"
"751","1610.08570v1","2016-10-26 23:20:56","2016-10-26 23:20:56","TrustBase: An Architecture to Repair and Strengthen Certificate-based
  Authentication","  We describe TrustBase, an architecture that provides certificate-based
authentication as an operating system service. TrustBase enforces best
practices for certificate validation for all applications and transparently
enables existing applications to be strengthened against failures of the CA
system. The TrustBase system allows simple deployment of authentication systems
that harden the CA system. This enables system administrators, for example, to
require certificate revocation checks on all TLS connections, or require
STARTTLS for email servers that support it. TrustBase is the first system that
is able to secure all TLS traffic, using an approach compatible with all
operating systems. We design and evaluate a prototype implementation of
TrustBase on Linux, evaluate its security, and demonstrate that it has
negligible overhead and universal compatibility with applications. To
demonstrate the utility of TrustBase, we have developed six authentication
services that strengthen certificate validation for all applications.
","Mark O'Neill|Scott Heidbrink|Jordan Whitehead|Scott Ruoti|Dan Bunker|Kent Seamons|Daniel Zappala","","http://arxiv.org/abs/1610.08570v1","http://arxiv.org/pdf/1610.08570v1","","15 pages, 4 figures","","","cs.CR","cs.CR"
"752","1611.00791v1","2016-11-02 20:34:56","2016-11-02 20:34:56","Predicting Domain Generation Algorithms with Long Short-Term Memory
  Networks","  Various families of malware use domain generation algorithms (DGAs) to
generate a large number of pseudo-random domain names to connect to a command
and control (C&C) server. In order to block DGA C&C traffic, security
organizations must first discover the algorithm by reverse engineering malware
samples, then generating a list of domains for a given seed. The domains are
then either preregistered or published in a DNS blacklist. This process is not
only tedious, but can be readily circumvented by malware authors using a large
number of seeds in algorithms with multivariate recurrence properties (e.g.,
banjori) or by using a dynamic list of seeds (e.g., bedep). Another technique
to stop malware from using DGAs is to intercept DNS queries on a network and
predict whether domains are DGA generated. Such a technique will alert network
administrators to the presence of malware on their networks. In addition, if
the predictor can also accurately predict the family of DGAs, then network
administrators can also be alerted to the type of malware that is on their
networks. This paper presents a DGA classifier that leverages long short-term
memory (LSTM) networks to predict DGAs and their respective families without
the need for a priori feature extraction. Results are significantly better than
state-of-the-art techniques, providing 0.9993 area under the receiver operating
characteristic curve for binary classification and a micro-averaged F1 score of
0.9906. In other terms, the LSTM technique can provide a 90% detection rate
with a 1:10000 false positive (FP) rate---a twenty times FP improvement over
comparable methods. Experiments in this paper are run on open datasets and code
snippets are provided to reproduce the results.
","Jonathan Woodbridge|Hyrum S. Anderson|Anjum Ahuja|Daniel Grant","","http://arxiv.org/abs/1611.00791v1","http://arxiv.org/pdf/1611.00791v1","","","","","cs.CR","cs.CR|cs.AI"
"753","1611.01279v1","2016-11-04 07:23:03","2016-11-04 07:23:03","3.5-Year Monitoring of 225 GHz Opacity at the Summit of Greenland","  We present the 3.5-yr monitoring results of 225 GHz opacity at the summit of
the Greenland ice sheet (Greenland Summit Camp) at an altitude of 3200 m using
a tipping radiometer. We chose this site as our submillimeter telescope
(Greenland Telescope; GLT) site, because its location offers favorable
baselines to existing submillimeter telescopes for global-scale VLBI. The site
shows a clear seasonal variation with the average opacity lower by a factor of
two during winter. For the winter quartiles of 25% and 50%, the Greenland site
is about 10%-30% worse than the ALMA or the South Pole sites. Estimated
atmospheric transmission spectra in winter season are similar to the ALMA site
at lower frequencies (<450 GHz), which are transparent enough to perform
astronomical observations almost all of the winter time with opacities <0.5,
but 10%-25% higher opacities at higher frequencies (>450 GHz) than those at the
ALMA site. This is due to the lower altitude of the Greenland site.
Nevertheless, half of the winter time at the Greenland site can be used for
astronomical observations at frequencies between 450 GHz and 1000 GHz with
opacities <1.2, and 10% of the time show >10% transmittance in the THz (1035
GHz, 1350 GHz, and 1500 GHz) windows. One major advantage of the Greenland site
in winter is that there is no diurnal variation due to the polar night
condition, and therefore the durations of low-opacity conditions are
significantly longer than at the ALMA site. Opacities lower than 0.05 or 0.04
can continue for more than 100 hours. Such long stable opacity conditions do
not occur as often even at the South Pole; it happens only for the opacity
lower than 0.05. Since the opacity variation is directly related to the sky
temperature (background) variation, the Greenland site is suitable for
astronomical observations that need unusually stable sky background.
","Satoki Matsushita|Keiichi Asada|Pierre L. Martin-Cocher|Ming-Tang Chen|Paul T. P. Ho|Makoto Inoue|Patrick M. Koch|Scott N. Paine|David D. Turner","Academia Sinica Institute of Astronomy and Astrophysics|Academia Sinica Institute of Astronomy and Astrophysics|Academia Sinica Institute of Astronomy and Astrophysics|Academia Sinica Institute of Astronomy and Astrophysics|Academia Sinica Institute of Astronomy and Astrophysics|Academia Sinica Institute of Astronomy and Astrophysics|Academia Sinica Institute of Astronomy and Astrophysics|Harvard-Smithsonian Center for Astrophysics|National Oceanic and Atmospheric Administration","http://arxiv.org/abs/1611.01279v1","http://arxiv.org/pdf/1611.01279v1","http://dx.doi.org/10.1088/1538-3873/129/972/025001","9 pages, 8 figures. Accepted in PASP","PASP, 129, 025001 (2017)","10.1088/1538-3873/129/972/025001","astro-ph.IM","astro-ph.IM"
"754","1611.01880v1","2016-11-07 03:02:01","2016-11-07 03:02:01","Inductive decision based Real Time Occupancy detector in University
  Buildings","  The ability to estimate College Campus Occupancy for Classrooms and Labs in
real time has become one of the major concerns for various Academicians,
authorities and administrators,where still a manual attendance marking system
is being followed. Using a low budget multiple sensor setup installed in a
college auditorium, the goal is to build a real-time occupancy detector. This
paper presents an Inductive real time Decision tree based classifier using
multiple sensor dataset to detect occupancy. Using simple feature based
thresholds, Reverberation time which comes out to be a novel as well as most
distinguishing feature sampled at various frequencies over a given time
interval was used to detect the occupancy with an accuracy of %.Addition of
various other sensor data, decreased the accuracy of classification results.
The detector setup can be used in various college buildings to provide real
time centralised occupancy status thus automating the manual attendance system
being used.
","Nkita Jain|Rachita Gupta","","http://arxiv.org/abs/1611.01880v1","http://arxiv.org/pdf/1611.01880v1","","7 Pages 9 Figures, International Journal of Computer Science and
  Information Security Vol 14 No 10 2016","International Journal of Computer Science and Information Security
  14 (10) 2016","","cs.CY","cs.CY"
"755","1611.02319v2","2016-11-07 22:00:23","2016-11-10 15:14:43","Performance Improvements in Heterogeneous Wireless Networks for First
  Responders","  Efficient communications are crucial for disaster response and recovery.
However, most current public safety land mobile radio (LMR) networks only
provide narrowband voice service with limited support of low-speed data
services. In this paper, we study to enhance the interoperability of LMR with
commercial wireless cellular networks, by which a wide variety of benefits can
be offered to disaster responders, including new multimedia services, increased
data rates and low cost devices. Our approach is based on Session Initial
Protocol (SIP) and a joint radio resource management framework. In addition, an
optimal radio resource management scheme is proposed to maximize the overall
radio resource utilization and at the same time guarantee service availability
and continuity quality of service (QoS) for disaster responders. The
effectiveness of the proposed approach is illustrated by numerical examples.
","Jianqiang Zhang","","http://arxiv.org/abs/1611.02319v2","http://arxiv.org/pdf/1611.02319v2","","This article has been withdrawn by arXiv administrators due to
  excessive unattributed and verbatim text overlap from external sources","","","cs.NI","cs.NI"
"756","1611.02652v1","2016-11-08 18:50:13","2016-11-08 18:50:13","Space weather research and forecast in USA","  In the United States, scientific research in space weather is funded by
several Government Agencies including the National Science Foundation (NSF) and
the National Aeronautics and Space Agency (NASA). For commercial purposes,
space weather forecast is made by the Space Weather Prediction Center (SWPC) of
the National Oceanic and Atmospheric Administration (NOAA). Observations come
from the network of groundbased observatories funded via various sources, as
well as from the instruments on spacecraft. Numerical models used in forecast
are developed in the framework of individual research projects. Later, the most
promising models are selected for additional testing at SWPC. In order to
increase the application of models in research and education, NASA in
collaboration with other agencies created Community Coordinated Modeling Center
(CCMC). In mid-1990, US scientific community presented compelling evidence for
developing the National Program on Space Weather, and in 1995, such program has
been formally created. In 2015, the National Council on Science and Technology
issued two documents: the National Space Weather Strategy [1] and the Action
Plan [2]. In the near future, these two documents will define the development
of Space Weather research and forecasting activity in USA. Both documents
emphasize the need for close international collaboration in area of space
weather.
","Alexei A. Pevtsov","","http://arxiv.org/abs/1611.02652v1","http://arxiv.org/pdf/1611.02652v1","","6 pages, 2 figures, submitted for publication in Proceedings of XX
  annual Conference on Solar and Solar-Terrestrial Physics held at the Central
  (Pulkovo) Astronomical Observatory, 10-14 Oct. 2016","","","physics.space-ph","physics.space-ph|astro-ph.IM|astro-ph.SR"
"757","1611.02842v1","2016-11-09 08:12:30","2016-11-09 08:12:30","Policy-Compliant Path Diversity and Bisection Bandwidth","  How many links can be cut before a network is bisected? What is the maximal
bandwidth that can be pushed between two nodes of a network? These questions
are closely related to network resilience, path choice for multipath routing or
bisection bandwidth estimations in data centers. The answer is quantified using
metrics such as the number of edge-disjoint paths between two network nodes and
the cumulative bandwidth that can flow over these paths. In practice though,
such calculations are far from simple due to the restrictive effect of network
policies on path selection. Policies are set by network administrators to
conform to service level agreements, protect valuable resources or optimize
network performance. In this work, we introduce a general methodology for
estimating lower and upper bounds for the policy-compliant path diversity and
bisection bandwidth between two nodes of a network, effectively quantifying the
effect of policies on these metrics. Exact values can be obtained if certain
conditions hold. The approach is based on regular languages and can be applied
in a variety of use cases.
","Rowan Kloti|Vasileios Kotronis|Bernhard Ager|Xenofontas Dimitropoulos","","http://arxiv.org/abs/1611.02842v1","http://arxiv.org/pdf/1611.02842v1","","Proceedings of IEEE INFOCOM 2015, pages 675-683, 1/4/2015","","","cs.NI","cs.NI"
"758","1611.03252v1","2016-11-10 10:52:15","2016-11-10 10:52:15","Reduce positive and negative falses from attacks collected from the
  deployment of distributed honeypot network","  Current tools and systems of detecting vulnerabilities simply alert the
administrator of attempted attacks against his network or system. However,
generally, the huge number of alerts to analyze and the amount time required to
update security rules after analyzing alerts provides time and opportunity for
the attacker to inflict damages. Moreover, most of these tools generate
positive and negative falses, which may be important to the attacked network.
Otherwise, many solutions exist such as IPS, but it shows a great defect due,
fundamentally, to false positives. Indeed, attackers often make IPS block a
legitimate traffic when they detect its presence in the attacked network. In
this paper we describe an automated algorithm that gives the ability to detect
attacks before they occurrence, then reduce positive and negative falses rates.
Moreover, we use a set of data related to malicious traffic captured using a
network of honeypots to recognize potential threats sources.
","Abdeljalil Agnaou|Anas Abou El Kalam|Abdellah Ait Ouahman|Mina De Montfort","","http://arxiv.org/abs/1611.03252v1","http://arxiv.org/pdf/1611.03252v1","","","International Journal of Computer Science and Information Security
  Volume 14 No. 9, September 2016","","cs.CR","cs.CR"
"759","1611.04758v1","2016-11-15 09:49:01","2016-11-15 09:49:01","Structure and Dynamics of Brain Lobes Functional Networks at the Onset
  of Anesthesia Induced Loss of Consciousness","  Anesthetic agents are neurotropic drugs able to induce dramatic alterations
in the thalamo-cortical system, promoting a drastic reduction in awareness and
level of consciousness. There is experimental evidence that general anesthesia
impacts large scale functional networks leading to alterations in the brain
state. However, the way anesthetics affect the structure assumed by functional
connectivity in different brain regions have not been reported yet. Within this
context, the present study has sought to characterize the functional brain
networks respective to the frontal, parietal, temporal and occipital lobes. In
this experiment, electro-physiological neural activity was recorded through the
use of a dense ECoG-electrode array positioned directly over the cortical
surface of an old world monkey of the species Macaca fuscata. Networks were
serially estimated over time at each five seconds, while the animal model was
under controlled experimental conditions of an anesthetic induction process. In
each one of the four cortical brain lobes, prominent alterations on distinct
properties of the networks evidenced a transition in the networks architecture,
which occurred within about one and a half minutes after the administration of
the anesthetics. The characterization of functional brain networks performed in
this study represents important experimental evidence and brings new knowledge
towards the understanding of neural correlates of consciousness in terms of the
structure and properties of the functional brain networks.
","Eduardo C. Padovani","","http://arxiv.org/abs/1611.04758v1","http://arxiv.org/pdf/1611.04758v1","","41 pages; 30 figures; 30 tables. arXiv admin note: substantial text
  overlap with arXiv:1604.00002","","","q-bio.NC","q-bio.NC"
"760","1611.04996v1","2016-11-15 19:26:03","2016-11-15 19:26:03","Fast neutron background characterization with the Radiological
  Multi-sensor Analysis Platform (RadMAP)","  In an effort to characterize the fast neutron radiation background, 16 EJ-309
liquid scintillator cells were installed in the Radiological Multi-sensor
Analysis Platform (RadMAP) to collect data in the San Francisco Bay Area. Each
fast neutron event was associated with specific weather metrics (pressure,
temperature, absolute humidity) and GPS coordinates. The expected exponential
dependence of the fast neutron count rate on atmospheric pressure was
demonstrated and event rates were subsequently adjusted given the measured
pressure at the time of detection. Pressure adjusted data was also used to
investigate the influence of other environmental conditions on the neutron
background rate. Using National Oceanic and Atmospheric Administration (NOAA)
coastal area lidar data, an algorithm was implemented to approximate sky-view
factors (the total fraction of visible sky) for points along RadMAPs route.
Three areas analyzed in San Francisco, Downtown Oakland, and Berkeley all
demonstrated a suppression in the background rate of over 50% for the range of
sky-view factors measured. This effect, which is due to the shielding of
cosmic-ray produced neutrons by surrounding buildings, was comparable to the
pressure influence which yielded a 32% suppression in the count rate over the
range of pressures measured.
","John R. Davis|Erik Brubaker|Kai Vetter","","http://arxiv.org/abs/1611.04996v1","http://arxiv.org/pdf/1611.04996v1","http://dx.doi.org/10.1016/j.nima.2017.03.042","9 pages, 16 figures. Submitted to Nucl. Instr. Meth. Phys. Res. A","","10.1016/j.nima.2017.03.042","physics.ins-det","physics.ins-det"
"761","1611.06219v1","2016-11-18 20:49:06","2016-11-18 20:49:06","Astrophysics Source Code Library: Here we grow again!","  The Astrophysics Source Code Library (ASCL) is a free online registry of
research codes; it is indexed by ADS and Web of Science and has over 1300 code
entries. Its entries are increasingly used to cite software; citations have
been doubling each year since 2012 and every major astronomy journal accepts
citations to the ASCL. Codes in the resource cover all aspects of astrophysics
research and many programming languages are represented. In the past year, the
ASCL added dashboards for users and administrators, started minting Digital
Objective Identifiers (DOIs) for software it houses, and added metadata fields
requested by users. This presentation covers the ASCL's growth in the past year
and the opportunities afforded it as one of the few domain libraries for
science research codes.
","Alice Allen|G. Bruce Berriman|Kimberly DuPrie|Jessica Mink|Robert Nemiroff|Thomas Robitaille|Judy Schmidt|Lior Shamir|Keith Shortridge|Mark Taylor|Peter Teuben|John Wallin","","http://arxiv.org/abs/1611.06219v1","http://arxiv.org/pdf/1611.06219v1","","4 pages, 2 figures; to be published in ADASS XXVI (held October
  16-20, 2016) proceedings","","","astro-ph.IM","astro-ph.IM"
"762","1611.06904v1","2016-11-21 17:05:55","2016-11-21 17:05:55","Isolario: a Do-ut-des Approach to Improve the Appeal of BGP Route
  Collecting","  The incompleteness of data collected from BGP route collecting projects is a
well-known issue which potentially affects every research activity carried out
on the analysis of the Internet inter-domain routing. Recent works explained
that one of the possible solutions is to increase the number of ASes feeding
these projects from the Internet periphery, in order to reveal the hidden
portion of peering connectivity of their upstream providers. The main problem
is that these projects are currently not appealing enough for the network
administrators of these ASes, which are typically not aware of their existence
or not interested enough to share their data. Our contribution is Isolario, a
project based on the do-ut-des principle which aims at persuading network
administrators to share their routing information by offering services in
return, ranging from real-time analyses of the incoming BGP session(s) to
historic analyses of routing reachability. To the best of our knowledge,
Isolario is the only route collecting project publicly available which offers a
set of services to its users to encourage their participation, aiming at
increasing the amount of BGP data publicly available for research purposes.
","Enrico Gregori|Alessandro Improta|Luca Sani","","http://arxiv.org/abs/1611.06904v1","http://arxiv.org/pdf/1611.06904v1","","Technical report","","","cs.NI","cs.NI"
"763","1611.07383v2","2016-11-22 16:09:12","2016-12-07 09:00:11","A Non-Intrusive and Context-Based Vulnerability Scoring Framework for
  Cloud Services","  Understanding the severity of vulnerabilities within cloud services is
particularly important for today service administrators.Although many systems,
e.g., CVSS, have been built to evaluate and score the severity of
vulnerabilities for administrators, the scoring schemes employed by these
systems fail to take into account the contextual information of specific
services having these vulnerabilities, such as what roles they play in a
particular service. Such a deficiency makes resulting scores unhelpful. This
paper presents a practical framework, NCVS, that offers automatic and
contextual scoring mechanism to evaluate the severity of vulnerabilities for a
particular service. Specifically, for a given service S, NCVS first
automatically collects S contextual information including topology,
configurations, vulnerabilities and their dependencies. Then, NCVS uses the
collected information to build a contextual dependency graph, named CDG, to
model S context. Finally, NCVS scores and ranks all the vulnerabilities in S by
analyzing S context, such as what roles the vulnerabilities play in S, and how
critical they affect the functionality of S. NCVS is novel and useful, because
1) context-based vulnerability scoring results are highly relevant and
meaningful for administrators to understand each vulnerability importance
specific to the target service; and 2) the workflow of NCVS does not need
instrumentation or modifications to any source code. Our experimental results
demonstrate that NCVS can obtain more relevant vulnerability scoring results
than comparable system, such as CVSS.
","Hao Zhuang|Florian Pydde","","http://arxiv.org/abs/1611.07383v2","http://arxiv.org/pdf/1611.07383v2","","","","","cs.CR","cs.CR"
"764","1611.07617v2","2016-11-23 03:02:39","2016-12-06 19:25:31","A New Framework for Ranking Vulnerabilities in the Clouds","  Qualifying and ranking threat degrees of vulnerabilities in cloud service are
known to be full of challenges. Although there have been several efforts aiming
to address this problem, most of them are too simple or cannot be applied into
cloud infrastructure. This paper aims to propose a novel framework to qualify
and rank the vulnerabilities based on their threat degrees in cloud service.
Through inputting or constructing service dependency graph, our framework is
able to generate the importance degree of each service and the ranking list of
all the vulnerabilities in cloud service. Moreover, our framework can be
adopted not only into various cloud infrastructures, but also different
categories of algorithms according to concrete requirements. To evaluate our
framework, we adopt AssetRank algorithm into the framework, and present the
whole design of our work. Comprehensive experiments prove the effectiveness of
our framework on qualifying and ranking vulnerabilities in cloud service.
","He Zhu","","http://arxiv.org/abs/1611.07617v2","http://arxiv.org/pdf/1611.07617v2","","arXiv admin note: submission has been withdrawn by arXiv
  administrators due to inappropriate text reuse from external sources","","","cs.CR","cs.CR"
"765","1611.08848v1","2016-11-27 14:12:38","2016-11-27 14:12:38","Predicting drug recalls from Internet search engine queries","  Batches of pharmaceutical are sometimes recalled from the market when a
safety issue or a defect is detected in specific production runs of a drug.
Such problems are usually detected when patients or healthcare providers report
abnormalities to medical authorities. Here we test the hypothesis that
defective production lots can be detected earlier by monitoring queries to
Internet search engines.
  We extracted queries from the USA to the Bing search engine which mentioned
one of 5,195 pharmaceutical drugs during 2015 and all recall notifications
issued by the Food and Drug Administration (FDA) during that year. By using
attributes that quantify the change in query volume at the state level, we
attempted to predict if a recall of a specific drug will be ordered by FDA in a
time horizon ranging from one to 40 days in future.
  Our results show that future drug recalls can indeed be identified with an
AUC of 0.791 and a lift at 5% of approximately 6 when predicting a recall will
occur one day ahead. This performance degrades as prediction is made for longer
periods ahead. The most indicative attributes for prediction are sudden spikes
in query volume about a specific medicine in each state. Recalls of
prescription drugs and those estimated to be of medium-risk are more likely to
be identified using search query data.
  These findings suggest that aggregated Internet search engine data can be
used to facilitate in early warning of faulty batches of medicines.
","Elad Yom-Tov","","http://arxiv.org/abs/1611.08848v1","http://arxiv.org/pdf/1611.08848v1","http://dx.doi.org/10.1109/JTEHM.2017.2732945","","","10.1109/JTEHM.2017.2732945","cs.IR","cs.IR|stat.AP"
"766","1611.08981v1","2016-11-28 04:35:24","2016-11-28 04:35:24","Online tools for public engagement: case studies from Reykjavik","  With the ubiquity of Internet technologies and growing demands for
transparency and open data policies, the role of social networking and online
deliberation tools for public engagement in decision-making has increased
substantially in the last decades. In this paper, we present the analysis of
how social media are used by different public bodies to enhance public
participation in deliberative democracy. We collected and reviewed published
information on the subject and carried out a field base assessment, involving
structured interviews with different government representatives and urban
policymakers. In order to compare collected data, we used a framework for
systematic analysis and comparison of e-participation platforms called the
participatory cube. The results we got were the following. Participatory
decision-making on matters of public concern justly consumes time and
resources, therefore online tools should be applied with consideration of scale
and efficiency, i.e. on burning issues for a majority of citizens or
small-scale local platforms, and in combination with meetings in real time and
space. The budget and workforce allocated to managing online engagement tools
should be proportionate to other political and administrative efforts to bring
to execution proposed ideas and act on collected feedback in order to satisfy
the needs expressed by the communities and not undermine their beliefs about
their power to influence decisions.
","Iva Bojic|Giulia Marra|Vera Naydenova","","http://arxiv.org/abs/1611.08981v1","http://arxiv.org/pdf/1611.08981v1","","","","","cs.CY","cs.CY|cs.HC|cs.SI"
"767","1611.09812v2","2016-11-29 20:02:00","2017-02-02 20:33:12","Limiting the effects of earthquakes on gravitational-wave
  interferometers","  Ground-based gravitational wave interferometers such as the Laser
Interferometer Gravitational-wave Observatory (LIGO) are susceptible to
high-magnitude teleseismic events, which can interrupt their operation in
science mode and significantly reduce the duty cycle. It can take several hours
for a detector to stabilize enough to return to its nominal state for
scientific observations. The down time can be reduced if advance warning of
impending shaking is received and the impact is suppressed in the isolation
system with the goal of maintaining stable operation even at the expense of
increased instrumental noise. Here we describe an early warning system for
modern gravitational-wave observatories. The system relies on near real-time
earthquake alerts provided by the U.S. Geological Survey (USGS) and the
National Oceanic and Atmospheric Administration (NOAA). Hypocenter and
magnitude information is generally available in 5 to 20 minutes of a
significant earthquake depending on its magnitude and location. The alerts are
used to estimate arrival times and ground velocities at the gravitational-wave
detectors. In general, 90\% of the predictions for ground-motion amplitude are
within a factor of 5 of measured values. The error in both arrival time and
ground-motion prediction introduced by using preliminary, rather than final,
hypocenter and magnitude information is minimal. By using a machine learning
algorithm, we develop a prediction model that calculates the probability that a
given earthquake will prevent a detector from taking data. Our initial results
indicate that by using detector control configuration changes, we could prevent
interruption of operation from 40-100 earthquake events in a 6-month
time-period.
","Michael Coughlin|Paul Earle|Jan Harms|Sebastien Biscans|Christopher Buchanan|Eric Coughlin|Fred Donovan|Jeremy Fee|Hunter Gabbard|Michelle Guy|Nikhil Mukund|Matthew Perry","","http://arxiv.org/abs/1611.09812v2","http://arxiv.org/pdf/1611.09812v2","http://dx.doi.org/10.1088/1361-6382/aa5a60","","Class. Quantum Grav. 34 044004 (2017)","10.1088/1361-6382/aa5a60","gr-qc","gr-qc|astro-ph.IM|physics.ins-det"
"768","1612.00125v2","2016-12-01 03:18:46","2016-12-07 15:05:48","A Novel Artificial Fish Swarm Algorithm for Pattern Recognition with
  Convex Optimization","  Image pattern recognition is an important area in digital image processing.
An efficient pattern recognition algorithm should be able to provide correct
recognition at a reduced computational time. Off late amongst the machine
learning pattern recognition algorithms, Artificial fish swarm algorithm is one
of the swarm intelligence optimization algorithms that works based on
population and stochastic search. In order to achieve acceptable result, there
are many parameters needs to be adjusted in AFSA. Among these parameters,
visual and step are very significant in view of the fact that artificial fish
basically move based on these parameters. In standard AFSA, these two
parameters remain constant until the algorithm termination. Large values of
these parameters increase the capability of algorithm in global search, while
small values improve the local search ability of the algorithm. In this paper,
we empirically study the performance of the AFSA and different approaches to
balance between local and global exploration have been tested based on the
adaptive modification of visual and step during algorithm execution. The
proposed approaches have been evaluated based on the four well-known benchmark
functions. Experimental results show considerable positive impact on the
performance of AFSA. A Convex optimization has been integrated into the
proposed work to have an ideal segmentation of the input image which is a MR
brain image.
","Lei Shi|Rui Guo|Yuchen Ma","","http://arxiv.org/abs/1612.00125v2","http://arxiv.org/pdf/1612.00125v2","","arXiv admin note: submission has been withdrawn by arXiv
  administrators due to inappropriate text reuse from external sources","","","cs.CV","cs.CV"
"769","1612.01901v1","2016-12-06 16:44:46","2016-12-06 16:44:46","Evolving network structure of academic institutions","  Today's colleges and universities consist of highly complex structures that
dictate interactions between the administration, faculty, and student body.
These structures can play a role in dictating the efficiency of policy enacted
by the administration and determine the effect that curriculum changes in one
department have on other departments. Despite the fact that the features of
these complex structures have a strong impact on the institutions, they remain
by-and-large unknown in many cases. In this paper we study the academic
structure of our home institution of Trinity College in Hartford, CT using the
major and minor patterns between graduating students to build a temporal
multiplex network describing the interactions between different departments.
Using recent network science techniques developed for such temporal networks we
identify the evolving community structures that organize departments'
interactions, as well as quantify the interdisciplinary centrality of each
department. We implement this framework for Trinity College, finding practical
insights and applications, but also present it as a general framework for
colleges and universities to better understand their own structural makeup in
order to better inform academic and administrative policy.
","Shufan Wang|Mariam Avagyan|Per Sebastian Skardal","","http://arxiv.org/abs/1612.01901v1","http://arxiv.org/pdf/1612.01901v1","http://dx.doi.org/10.1007/s41109-016-0020-1","","","10.1007/s41109-016-0020-1","physics.soc-ph","physics.soc-ph|cs.SI|nlin.AO"
"770","1612.03373v1","2016-12-11 06:02:41","2016-12-11 06:02:41","A probabilistic graphical model approach in 30 m land cover mapping with
  multiple data sources","  There is a trend to acquire high accuracy land-cover maps using multi-source
classification methods, most of which are based on data fusion, especially
pixel- or feature-level fusions. A probabilistic graphical model (PGM) approach
is proposed in this research for 30 m resolution land-cover mapping with
multi-temporal Landsat and MODerate Resolution Imaging Spectroradiometer
(MODIS) data. Independent classifiers were applied to two single-date Landsat 8
scenes and the MODIS time-series data, respectively, for probability
estimation. A PGM was created for each pixel in Landsat 8 data. Conditional
probability distributions were computed based on data quality and reliability
by using information selectively. Using the administrative territory of Beijing
City (Area-1) and a coastal region of Shandong province, China (Area-2) as
study areas, multiple land-cover maps were generated for comparison.
Quantitative results show the effectiveness of the proposed method. Overall
accuracies promoted from 74.0% (maps acquired from single-temporal Landsat
images) to 81.8% (output of the PGM) for Area-1. Improvements can also be seen
when using MODIS data and only a single-temporal Landsat image as input
(overall accuracy: 78.4% versus 74.0% for Area-1, and 86.8% versus 83.0% for
Area-2). Information from MODIS data did not help much when the PGM was applied
to cloud free regions of. One of the advantages of the proposed method is that
it can be applied where multi-temporal data cannot be simply stacked as a
multi-layered image.
","Jie Wang|Luyan Ji|Xiaomeng Huang|Haohuan Fu|Shiming Xu|Congcong Li","","http://arxiv.org/abs/1612.03373v1","http://arxiv.org/pdf/1612.03373v1","","38 pages, 5 figures","","","stat.AP","stat.AP|cs.CV"
"771","1612.03541v1","2016-12-12 04:32:53","2016-12-12 04:32:53","A biological model of scabies infection dynamics and treatment explains
  why mass drug administration does not lead to elimination","  Despite a low global prevalence, infections with Sarcoptes scabiei, or
scabies, are still common in remote communities such as in northern Australia
and the Solomon Islands. Mass drug administration (MDA) has been utilised in
these communities, and although prevalence drops substantially initially, these
reductions have not been sustained. We develop a compartmental model of scabies
infection dynamics and incorporate both ovicidal and non-ovicidal treatment
regimes. By including the dynamics of mass drug administration, we are able to
reproduce the phenomena of an initial reduction in prevalence, followed by the
recrudescence of infection levels in the population. We show that even under a
`perfect' two-round MDA, eradication of scabies under a non-ovicidal treatment
scheme is almost impossible. We then go on to consider how the probability of
elimination varies with the number of treatment rounds delivered in an MDA. We
find that even with infeasibly large numbers of treatment rounds, elimination
remains challenging.
","M. Lydeamore|P. T. Campbell|D. G. Regan|S. Y. C. Tong|R. Andrews|A. C. Steer|L. Romani|J. M. Kaldor|J. McVernon|J. M. McCaw","","http://arxiv.org/abs/1612.03541v1","http://arxiv.org/pdf/1612.03541v1","http://dx.doi.org/10.1016/j.mbs.2018.08.007","22 pages, 10 figures","","10.1016/j.mbs.2018.08.007","q-bio.PE","q-bio.PE"
"772","1612.06200v2","2016-12-19 14:28:59","2017-03-01 13:50:09","The Price of Political Uncertainty: Evidence from the 2016 U.S.
  Presidential Election and the U.S. Stock Markets","  There is bountiful evidence that political uncertainty stemming from
presidential elections or doubt about the direction of future policy make
financial markets significantly volatile, especially in proximity to close
elections or elections that may prompt radical policy changes. Although several
studies have examined the association between presidential elections and stock
returns, very little attention has been given to the impacts of elections and
election induced uncertainty on stock markets. This paper explores, at sectoral
level, the uncertain information hypothesis (UIH) as a means of explaining the
reaction of markets to the arrival of unanticipated information. This
hypothesis postulates that political uncertainty is greater prior to the
elections (relative to pre-election period) but is resolved once the outcome of
the elections is determined (relative to post-election period). To this end, we
adopt an event-study methodology that examines abnormal return behavior around
the election date. We show that collapsing stock returns around the election
result is reversed by positive abnormal return on the next day, except some
cases where we note negative responses following the vote count. Although
Trump's win plunges US into uncertain future, positive reactions of abnormal
return are found. Therefore, our results do not support the UIH hypothesis.
Besides, the effect of political uncertainty is sector-specific. While some
sectors emerged winners (healthcare, oil and gas, real estate, defense,
financials and consumer goods and services), others took the opposite route
(technology and utilities). The winning industries are generally those that
will benefit from the new administration's focus on rebuilding infrastructure,
renegotiating trade agreements, reforming tax policy and labour laws,
increasing defense funding, easing restrictions on energy production, and
rolling back Obamacare.
","Jamal Bouoiyour|Refk Selmi","CATT|CATT","http://arxiv.org/abs/1612.06200v2","http://arxiv.org/pdf/1612.06200v2","","","","","q-fin.GN","q-fin.GN"
"773","1612.08428v2","2016-12-26 18:56:38","2017-10-25 18:55:36","Geometric uncertainty relation, the symplectic area, and the
  J-holomorphic maps for mixed quantum states","  In this paper we will establish a relation between geometric uncertainty
relation and the determinant of the quantum covariance matrix for mixed quantum
states. We will show that determinant of the covariance matrix represents the
squared metric area of a parallelogram. In this setting the geometric
uncertainty relation compares a metric area to a symplectic area. Moreover, we
will in details investigate relation between J-holomorphic maps and geometric
uncertainty relation for mixed quantum states. We will argue that determinant
of the quantum covariance matrix is equal to the harmonic energy of a
holomorphic map that minimize the areas.
","Hoshang Heydari","","http://arxiv.org/abs/1612.08428v2","http://arxiv.org/pdf/1612.08428v2","","This article has been withdrawn by arXiv administrators due to
  excessive unattributed and verbatim text overlap from external sources","","","quant-ph","quant-ph|math-ph|math.MP"
"774","1612.08702v1","2016-12-27 18:42:28","2016-12-27 18:42:28","Reduce The Wastage of Data During Movement in Data Warehouse","  In this research paper so as to handle Data in warehousing as well as reduce
the wastage of data and provide a better results which takes more and more turn
into a focal point of the data source business. Data warehousing and on-line
analytical processing (OLAP) are vital fundamentals of resolution hold, which
has more and more become a focal point of the database manufacturing. Lots of
marketable yield and services be at the present accessible, and the entire
primary database management organization vendor nowadays have contributions in
the area assessment hold up spaces some quite dissimilar necessities on record
technology compare to conventional on-line transaction giving out application.
This article gives a general idea of data warehousing and OLAP technologies,
with the highlighting on top of their latest necessities. So tools which is
used for extract, clean-up and load information into back end of a information
warehouse; multidimensional data model usual of OLAP; front end client tools
for querying and data analysis; server extension for proficient query
processing; and tools for data managing and for administration the warehouse.
In adding to survey the circumstances of the art, this article also identify a
number of capable research issue, a few which are interrelated to data wastage
troubles. In this paper use some new techniques to reduce the wastage of data,
provide better results. In this paper take some values, put in anova table and
give results through graphs which shows performance.
","Ahmed Mateen|Lareab Chaudhary","","http://arxiv.org/abs/1612.08702v1","http://arxiv.org/pdf/1612.08702v1","http://dx.doi.org/10.5120/ijca2016911907|http://dx.doi.org/10.5120/ijca2016911907","5 pages","International Journal of Computer Applications Foundation of
  Computer Science (FCS), NY, USA Volume 152 - Number 8 Year of Publication:
  2016","10.5120/ijca2016911907 10.5120/ijca2016911907","cs.DC","cs.DC"
"775","1612.08811v1","2016-12-28 06:47:56","2016-12-28 06:47:56","AZ Model for Software Development","  Know a days Computer system become essential and it is most commonly used in
every field of life. The computer saves time and use to solve complex and
extensive problem quickly in an efficient way. For this purpose software
programs are develop to facilitate the works for administrator, offices, banks
etc. so Quality is the most important factor as it mostly defines CUSTOMER
SATISFACTION which directly related to success of the project so there are many
approaches (methodologies) have been developed for this purpose occasionally.
The main study of this paper is to propose a new methodology for the
development of the software which focuses on the quality improvement of all
kind of product. This study will also discuss the features and limitation of
the traditional methodologies like waterfall iterative spiral RUP and Agile and
show how the new innovative methodology is better than previous one.
","Ahmed Mateen|Muhammad Azeem|Mohammad Shafiq","","http://arxiv.org/abs/1612.08811v1","http://arxiv.org/pdf/1612.08811v1","http://dx.doi.org/10.5120/ijca2016911701","4 pages","International Journal of Computer Applications Foundation of
  Computer Science (FCS), NY, USA Volume 151 - Number 6 Year of Publication:
  2016","10.5120/ijca2016911701","cs.SE","cs.SE"
"776","1612.09527v1","2016-12-30 16:50:45","2016-12-30 16:50:45","Ontology-based Access Control in Open Scenarios: Applications to Social
  Networks and the Cloud","  Thanks to the advent of the Internet, it is now possible to easily share vast
amounts of electronic information and computer resources (which include
hardware, computer services, etc.) in open distributed environments. These
environments serve as a common platform for heterogeneous users (e.g.,
corporate, individuals etc.) by hosting customized user applications and
systems, providing ubiquitous access to the shared resources and requiring less
administrative efforts; as a result, they enable users and companies to
increase their productivity. Unfortunately, sharing of resources in open
environments has significantly increased the privacy threats to the users.
Indeed, shared electronic data may be exploited by third parties, such as Data
Brokers, which may aggregate, infer and redistribute (sensitive) personal
features, thus potentially impairing the privacy of the individuals. A way to
palliate this problem consists on controlling the access of users over the
potentially sensitive resources. Specifically, access control management
regulates the access to the shared resources according to the credentials of
the users, the type of resource and the privacy preferences of the
resource/data owners. The efficient management of access control is crucial in
large and dynamic environments. Moreover, in order to propose a feasible and
scalable solution, we need to get rid of manual management of rules/constraints
(in which most available solutions rely) that constitutes a serious burden for
the users and the administrators. Finally, access control management should be
intuitive for the end users, who usually lack technical expertise, and they may
find access control mechanism more difficult to understand and rigid to apply
due to its complex configuration settings.
","Malik Imran-Daud","","http://arxiv.org/abs/1612.09527v1","http://arxiv.org/pdf/1612.09527v1","","Ph.D Thesis Report","","","cs.CR","cs.CR"
"777","1701.01531v3","2017-01-06 03:14:45","2018-05-14 22:52:03","Control of Dynamics in Brain Networks","  The ability to effectively control brain dynamics holds great promise for the
enhancement of cognitive function in humans, and the betterment of their
quality of life. Yet, successfully controlling dynamics in neural systems is
challenging, in part due to the immense complexity of the brain and the large
set of interactions that can drive any single change. While we have gained some
understanding of the control of single neurons, the control of large-scale
neural systems -- networks of multiply interacting components -- remains poorly
understood. Efforts to address this gap include the construction of tools for
the control of brain networks, mostly adapted from control and dynamical
systems theory. Informed by current opportunities for practical intervention,
these theoretical contributions provide models that draw from a wide array of
mathematical approaches. We present intriguing recent developments for
effective strategies of control in dynamic brain networks, and we also describe
potential mechanisms that underlie such processes. We review efforts in the
control of general neurophysiological processes with implications for brain
development and cognitive function, as well as the control of altered
neurophysiological processes in medical contexts such as anesthesia
administration, seizure suppression, and deep-brain stimulation for Parkinson's
disease. We conclude with a forward-looking discussion regarding how emerging
results from network control -- especially approaches that deal with nonlinear
dynamics or more realistic trajectories for control transitions -- could be
used to directly address pressing questions in neuroscience.
","Evelyn Tang|Danielle S. Bassett","","http://arxiv.org/abs/1701.01531v3","http://arxiv.org/pdf/1701.01531v3","http://dx.doi.org/10.1103/RevModPhys.90.031003","Intended for a Colloquium in Rev. Mod. Phys","","10.1103/RevModPhys.90.031003","q-bio.QM","q-bio.QM|cond-mat.dis-nn|cond-mat.soft|physics.bio-ph|q-bio.NC"
"778","1701.01739v1","2017-01-06 19:22:44","2017-01-06 19:22:44","In-class vs. online administration of concept inventories and
  attitudinal assessments","  This study investigates differences in student responses to in-class and
online administrations of the Force Concept Inventory (FCI), Conceptual Survey
of Electricity and Magnetism (CSEM), and the Colorado Learning Attitudes about
Science Survey (CLASS). Close to 700 physics students from 12 sections of three
different courses were instructed to complete the concept inventory relevant to
their course, either the FCI or CSEM, and the CLASS. Each student was randomly
assigned to take one of the surveys in class and the other survey online using
the LA Supported Student Outcomes (LASSO) system hosted by the Learning
Assistant Alliance (LAA). We examine how testing environments and instructor
practices affect participation rates and identify best practices for future
use.
","Manher Jariwala|Jada-Simone S. White|Ben Van Dusen|Eleanor W. Close","","http://arxiv.org/abs/1701.01739v1","http://arxiv.org/pdf/1701.01739v1","http://dx.doi.org/10.1119/perc.2016.pr.039","4 pages, 3 tables, 3 figures, Physics Education Research Conference
  proceedings","M. Jariwala, J. S. White, B. Van Dusen, and E. W. Close, In-class
  vs. Online Administration of Concept Inventories and Attitudinal Assessments,
  2016 PERC Proceedings [Sacramento, CA, July 20-21, 2016]","10.1119/perc.2016.pr.039","physics.ed-ph","physics.ed-ph"
"779","1701.02344v1","2017-01-09 20:47:21","2017-01-09 20:47:21","Database Engines: Evolution of Greenness","  Context: Information Technology consumes up to 10\% of the world's
electricity generation, contributing to CO2 emissions and high energy costs.
Data centers, particularly databases, use up to 23% of this energy. Therefore,
building an energy-efficient (green) database engine could reduce energy
consumption and CO2 emissions.
  Goal: To understand the factors driving databases' energy consumption and
execution time throughout their evolution.
  Method: We conducted an empirical case study of energy consumption by two
MySQL database engines, InnoDB and MyISAM, across 40 releases. We examined the
relationships of four software metrics to energy consumption and execution time
to determine which metrics reflect the greenness and performance of a database.
  Results: Our analysis shows that database engines' energy consumption and
execution time increase as databases evolve. Moreover, the Lines of Code metric
is correlated moderately to strongly with energy consumption and execution time
in 88% of cases.
  Conclusions: Our findings provide insights to both practitioners and
researchers. Database administrators may use them to select a fast, green
release of the MySQL database engine. MySQL database-engine developers may use
the software metric to assess products' greenness and performance. Researchers
may use our findings to further develop new hypotheses or build models to
predict greenness and performance of databases.
","Andriy V. Miranskyy|Zainab Al-zanbouri|David Godwin|Ayse Basar Bener","","http://arxiv.org/abs/1701.02344v1","http://arxiv.org/pdf/1701.02344v1","http://dx.doi.org/10.1002/smr.1915","","","10.1002/smr.1915","cs.SE","cs.SE|cs.DB"
"780","1701.03173v1","2017-01-11 21:33:31","2017-01-11 21:33:31","Depicting urban boundaries from a mobility network of spatial
  interactions: A case study of Great Britain with geo-located Twitter data","  Existing urban boundaries are usually defined by government agencies for
administrative, economic, and political purposes. Defining urban boundaries
that consider socio-economic relationships and citizen commute patterns is
important for many aspects of urban and regional planning. In this paper, we
describe a method to delineate urban boundaries based upon human interactions
with physical space inferred from social media. Specifically, we depicted the
urban boundaries of Great Britain using a mobility network of Twitter user
spatial interactions, which was inferred from over 69 million geo-located
tweets. We define the non-administrative anthropographic boundaries in a
hierarchical fashion based on different physical movement ranges of users
derived from the collective mobility patterns of Twitter users in Great
Britain. The results of strongly connected urban regions in the form of
communities in the network space yield geographically cohesive, non-overlapping
urban areas, which provide a clear delineation of the non-administrative
anthropographic urban boundaries of Great Britain. The method was applied to
both national (Great Britain) and municipal scales (the London metropolis).
While our results corresponded well with the administrative boundaries, many
unexpected and interesting boundaries were identified. Importantly, as the
depicted urban boundaries exhibited a strong instance of spatial proximity, we
employed a gravity model to understand the distance decay effects in shaping
the delineated urban boundaries. The model explains how geographical distances
found in the mobility patterns affect the interaction intensity among different
non-administrative anthropographic urban areas, which provides new insights
into human spatial interactions with urban space.
","Junjun Yin|Aiman Soliman|Dandong Yin|Shaowen Wang","","http://arxiv.org/abs/1701.03173v1","http://arxiv.org/pdf/1701.03173v1","","32 pages, 7 figures, International Journal of Geographic Information
  Science","","","cs.SI","cs.SI|physics.soc-ph"
"781","1701.03959v1","2017-01-14 19:40:13","2017-01-14 19:40:13","A test for monitoring under- and overtreatment in Dutch hospitals","  Over- and undertreatment harm patients and society and confound other
healthcare quality measures. Despite a growing body of research covering
specific conditions, we lack tools to systematically detect and measure over-
and undertreatment in hospitals. We demonstrate a test used to monitor over-
and undertreatment in Dutch hospitals, and illustrate its results applied to
the aggregated administrative treatment data of 1,836,349 patients at 89
hospitals in 2013. We employ a random effects model to create risk-adjusted
funnel plots that account for natural variation among hospitals, allowing us to
estimate a measure of overtreatment and undertreatment when hospitals fall
outside the control limits. The results of this test are not definitive,
findings were discussed with hospitals to improve the model and to enable the
hospitals to make informed treatment decisions.
","Oliver Urs Lenz|Daniel L Oberski","","http://arxiv.org/abs/1701.03959v1","http://arxiv.org/pdf/1701.03959v1","","","","","stat.AP","stat.AP"
"782","1701.05334v1","2017-01-19 08:50:37","2017-01-19 08:50:37","Fuzzy Ontology-Based Sentiment Analysis of Transportation and City
  Feature Reviews for Safe Traveling","  Traffic congestion is rapidly increasing in urban areas, particularly in mega
cities. To date, there exist a few sensor network based systems to address this
problem. However, these techniques are not suitable enough in terms of
monitoring an entire transportation system and delivering emergency services
when needed. These techniques require real-time data and intelligent ways to
quickly determine traffic activity from useful information. In addition, these
existing systems and websites on city transportation and travel rely on rating
scores for different factors (e.g., safety, low crime rate, cleanliness, etc.).
These rating scores are not efficient enough to deliver precise information,
whereas reviews or tweets are significant, because they help travelers and
transportation administrators to know about each aspect of the city. However,
it is difficult for travelers to read, and for transportation systems to
process, all reviews and tweets to obtain expressive sentiments regarding the
needs of the city. The optimum solution for this kind of problem is analyzing
the information available on social network platforms and performing sentiment
analysis. On the other hand, crisp ontology-based frameworks cannot extract
blurred information from tweets and reviews; therefore, they produce inadequate
results. In this regard, this paper proposes fuzzy ontology-based sentiment
analysis and SWRL rule-based decision-making to monitor transportation
activities and to make a city- feature polarity map for travelers. This system
retrieves reviews and tweets related to city features and transportation
activities. The feature opinions are extracted from these retrieved data, and
then fuzzy ontology is used to determine the transportation and city-feature
polarity. A fuzzy ontology and an intelligent system prototype are developed by
using Prot\'eg\'e OWL and Java, respectively.
","Farman Ali|D. Kwak|Pervez Khan|S. M. Riazul Islam|K. H. Kim|K. S. Kwak","","http://arxiv.org/abs/1701.05334v1","http://arxiv.org/pdf/1701.05334v1","","24 pages, 7 figures, Transportation Research Part C","","","cs.AI","cs.AI|cs.CL"
"783","1701.06207v1","2017-01-22 19:04:27","2017-01-22 19:04:27","Understanding Workers, Developing Effective Tasks, and Enhancing
  Marketplace Dynamics: A Study of a Large Crowdsourcing Marketplace","  We conduct an experimental analysis of a dataset comprising over 27 million
microtasks performed by over 70,000 workers issued to a large crowdsourcing
marketplace between 2012-2016. Using this data---never before analyzed in an
academic context---we shed light on three crucial aspects of crowdsourcing: (1)
Task design --- helping requesters understand what constitutes an effective
task, and how to go about designing one; (2) Marketplace dynamics --- helping
marketplace administrators and designers understand the interaction between
tasks and workers, and the corresponding marketplace load; and (3) Worker
behavior --- understanding worker attention spans, lifetimes, and general
behavior, for the improvement of the crowdsourcing ecosystem as a whole.
","Ayush Jain|Akash Das Sarma|Aditya Parameswaran|Jennifer Widom","","http://arxiv.org/abs/1701.06207v1","http://arxiv.org/pdf/1701.06207v1","","","","","cs.HC","cs.HC"
"784","1701.06225v1","2017-01-22 22:16:46","2017-01-22 22:16:46","Predicting Demographics of High-Resolution Geographies with Geotagged
  Tweets","  In this paper, we consider the problem of predicting demographics of
geographic units given geotagged Tweets that are composed within these units.
Traditional survey methods that offer demographics estimates are usually
limited in terms of geographic resolution, geographic boundaries, and time
intervals. Thus, it would be highly useful to develop computational methods
that can complement traditional survey methods by offering demographics
estimates at finer geographic resolutions, with flexible geographic boundaries
(i.e. not confined to administrative boundaries), and at different time
intervals. While prior work has focused on predicting demographics and health
statistics at relatively coarse geographic resolutions such as the county-level
or state-level, we introduce an approach to predict demographics at finer
geographic resolutions such as the blockgroup-level. For the task of predicting
gender and race/ethnicity counts at the blockgroup-level, an approach adapted
from prior work to our problem achieves an average correlation of 0.389
(gender) and 0.569 (race) on a held-out test dataset. Our approach outperforms
this prior approach with an average correlation of 0.671 (gender) and 0.692
(race).
","Omar Montasser|Daniel Kifer","","http://arxiv.org/abs/1701.06225v1","http://arxiv.org/pdf/1701.06225v1","","6 pages, AAAI-17 preprint","","","cs.LG","cs.LG|cs.SI|stat.ML"
"785","1701.06233v1","2017-01-22 23:03:11","2017-01-22 23:03:11","What the Language You Tweet Says About Your Occupation","  Many aspects of people's lives are proven to be deeply connected to their
jobs. In this paper, we first investigate the distinct characteristics of major
occupation categories based on tweets. From multiple social media platforms, we
gather several types of user information. From users' LinkedIn webpages, we
learn their proficiencies. To overcome the ambiguity of self-reported
information, a soft clustering approach is applied to extract occupations from
crowd-sourced data. Eight job categories are extracted, including Marketing,
Administrator, Start-up, Editor, Software Engineer, Public Relation, Office
Clerk, and Designer. Meanwhile, users' posts on Twitter provide cues for
understanding their linguistic styles, interests, and personalities. Our
results suggest that people of different jobs have unique tendencies in certain
language styles and interests. Our results also clearly reveal distinctive
levels in terms of Big Five Traits for different jobs. Finally, a classifier is
built to predict job types based on the features extracted from tweets. A high
accuracy indicates a strong discrimination power of language features for job
prediction task.
","Tianran Hu|Haoyuan Xiao|Thuy-vy Thi Nguyen|Jiebo Luo","","http://arxiv.org/abs/1701.06233v1","http://arxiv.org/pdf/1701.06233v1","","Published at the 10th International AAAI Conference on Web and Social
  Media (ICWSM-16)","","","cs.CY","cs.CY|cs.AI|cs.CL|cs.LG"
"786","1701.08125v1","2017-01-27 17:35:56","2017-01-27 17:35:56","Organic Computing in the Spotlight","  Organic Computing is an initiative in the field of systems engineering that
proposed to make use of concepts such as self-adaptation and self-organisation
to increase the robustness of technical systems. Based on the observation that
traditional design and operation concepts reach their limits, transferring more
autonomy to the systems themselves should result in a reduction of complexity
for users, administrators, and developers. However, there seems to be a need
for an updated definition of the term ""Organic Computing"", of desired
properties of technical, organic systems, and the objectives of the Organic
Computing initiative. With this article, we will address these points.
","Sven Tomforde|Bernhard Sick|Christian Muller-Schloer","","http://arxiv.org/abs/1701.08125v1","http://arxiv.org/pdf/1701.08125v1","","10 pages, one figure, article","","","cs.MA","cs.MA|cs.AI|68T05|I.2.8, I.2.11"
"787","1701.08303v2","2017-01-28 17:04:21","2017-08-13 12:56:03","Drug-Drug Interaction Extraction from Biomedical Text Using Long Short
  Term Memory Network","  Simultaneous administration of multiple drugs can have synergistic or
antagonistic effects as one drug can affect activities of other drugs.
Synergistic effects lead to improved therapeutic outcomes, whereas,
antagonistic effects can be life-threatening, may lead to increased healthcare
cost, or may even cause death. Thus identification of unknown drug-drug
interaction (DDI) is an important concern for efficient and effective
healthcare. Although multiple resources for DDI exist, they are often unable to
keep pace with rich amount of information available in fast growing biomedical
texts. Most existing methods model DDI extraction from text as a classification
problem and mainly rely on handcrafted features. Some of these features further
depend on domain specific tools. Recently neural network models using latent
features have been shown to give similar or better performance than the other
existing models dependent on handcrafted features. In this paper, we present
three models namely, {\it B-LSTM}, {\it AB-LSTM} and {\it Joint AB-LSTM} based
on long short-term memory (LSTM) network. All three models utilize word and
position embedding as latent features and thus do not rely on explicit feature
engineering. Further use of bidirectional long short-term memory (Bi-LSTM)
networks allow implicit feature extraction from the whole sentence. The two
models, {\it AB-LSTM} and {\it Joint AB-LSTM} also use attentive pooling in the
output of Bi-LSTM layer to assign weights to features. Our experimental results
on the SemEval-2013 DDI extraction dataset show that the {\it Joint AB-LSTM}
model outperforms all the existing methods, including those relying on
handcrafted features. The other two proposed LSTM models also perform
competitively with state-of-the-art methods.
","Sunil Kumar Sahu|Ashish Anand","","http://arxiv.org/abs/1701.08303v2","http://arxiv.org/pdf/1701.08303v2","","Under review to the Journal of Biomedical Informatics","","","cs.CL","cs.CL"
"788","1701.08668v1","2017-01-30 16:02:04","2017-01-30 16:02:04","Modeling and administration scheduling of fractional-order
  pharmacokinetic systems","  Fractional-order dynamical systems were recently introduced in the field of
pharmacokinetics where they proved powerful tools for modeling the absorption,
disposition, distribution and excretion of drugs which are liable to anomalous
diffusion, deep tissue trapping and other nonlinear phenomena. In this paper we
present several ways to simulate such fractional-order pharmacokinetic models
and we evaluate their accuracy and complexity on a fractional-order
pharmacokinetic model of Amiodarone, an anti-arrhythmic drug. We then propose
an optimal administration scheduling scheme and evaluate it on a population of
patients.
","Domagoj Herceg|Sotiris Ntouskas|Pantelis Sopasakis|Aris Dokoumetzidis|Panos Macheras|Haralambos Sarimveis|Panagiotis Patrinos","","http://arxiv.org/abs/1701.08668v1","http://arxiv.org/pdf/1701.08668v1","","","","","math.DS","math.DS|math.OC"
"789","1702.02442v1","2017-02-01 04:26:52","2017-02-01 04:26:52","A Development of Hybrid Framework for E-Government","  Governments all around the world are widely investing on the implementation
of e government to advance services to citizens and minimize costs. Governments
can progress effectiveness of their operations and can carry their
administrative operations efficiently with the help of ICT. Electronic
government perceived to provide a way for governments to renovate their
operational activities to serve their clients more competently. With
improvement in Information and Communication Technology ICT it is now time to
device electronic access to government facilities to the variously located
citizens. E governments all around the world have different objectives and
follow different models for e government development. Present models examined
and found less than satisfactory to guide e-government implementation. This
research proposed a hybrid model from Citizen comprehensive vision acknowledged
the civic idea and The strategic framework of e-government models. The
procedure of merging different computational knowledge systems to assemble a
solitary crossover show has turned out to be progressively prominent. The
execution files of these mixture models have ended up being superior to the
individual segments when utilized alone. To ensure that the proposed model is
more efficient and beneficial a survey study conducted. Survey based on
questionnaires and results calculated by applying statistics on data gathered
from survey.
","Ahmed Mateen|Sana Sabir|Kareem Ullah","","http://arxiv.org/abs/1702.02442v1","http://arxiv.org/pdf/1702.02442v1","","","International Journal of Management, IT & Engineering Vol. 7 Issue
  2, February 2017, ISSN: 2249-0558","","cs.CY","cs.CY"
"790","1702.00188v1","2017-02-01 10:16:41","2017-02-01 10:16:41","Can SDN Accelerate BGP Convergence? A Performance Analysis of
  Inter-domain Routing Centralization","  The Internet is composed of Autonomous Systems (ASes) or domains, i.e.,
networks belonging to different administrative entities. Routing between
domains/ASes is realised in a distributed way, over the Border Gateway Protocol
(BGP). Despite its global adoption, BGP has several shortcomings, like slow
convergence after routing changes, which can cause packet losses and interrupt
communication even for several minutes. To accelerate convergence, inter-domain
routing centralization approaches, based on Software Defined Networking (SDN),
have been recently proposed. Initial studies show that these approaches can
significantly improve performance and routing control over BGP. In this paper,
we complement existing system-oriented works, by analytically studying the
gains of inter-domain SDN. We propose a probabilistic framework to analyse the
effects of centralization on the inter-domain routing performance. We derive
bounds for the time needed to establish data plane connectivity between ASes
after a routing change, as well as predictions for the control-plane
convergence time. Our results provide useful insights (e.g., related to the
penetration of SDN in the Internet) that can facilitate future research. We
discuss applications of our results, and demonstrate the gains through
simulations on the Internet AS-topology.
","Pavlos Sermpezis|Xenofontas Dimitropoulos","","http://arxiv.org/abs/1702.00188v1","http://arxiv.org/pdf/1702.00188v1","","","","","cs.NI","cs.NI"
"791","1702.03226v2","2017-02-10 15:53:45","2017-03-24 11:29:52","An applied spatial agent-based model of administrative boundaries using
  SEAL","  This paper extends and adapts an existing abstract model into an empirical
metropolitan region in Brazil. The model - named SEAL: a Spatial Economic
Agent-based Lab - comprehends a framework to enable public policy ex-ante
analysis. The aim of the model is to use official data and municipalities
spatial boundaries to allow for policy experimentation. The current version
considers three markets: housing, labor and goods. Families' members age,
consume, join the labor market and trade houses. A single consumption tax is
collected by municipalities that invest back into quality of life improvements.
We test whether a single metropolitan government - which is an aggregation of
municipalities - would be in the best interest of its citizens. Preliminary
results for 20 simulation runs indicate that it may be the case. Future
developments include improving performance to enable running of higher
percentage of the population and a number of runs that make the model more
robust.
","Bernardo Alves Furtado|Isaque Daniel Eberhardt Rocha","","http://arxiv.org/abs/1702.03226v2","http://arxiv.org/pdf/1702.03226v2","","11 pages. 4 figures. Accepted ABMUS 2017 as part of AAMAS 2017, S\~ao
  Paulo, Brazil","","","cs.MA","cs.MA|q-fin.EC"
"792","1702.04143v1","2017-02-14 10:30:49","2017-02-14 10:30:49","TruSDN: Bootstrapping Trust in Cloud Network Infrastructure","  Software-Defined Networking (SDN) is a novel architectural model for cloud
network infrastructure, improving resource utilization, scalability and
administration. SDN deployments increasingly rely on virtual switches executing
on commodity operating systems with large code bases, which are prime targets
for adversaries attacking the net- work infrastructure. We describe and
implement TruSDN, a framework for bootstrapping trust in SDN infrastructure
using Intel Software Guard Extensions (SGX), allowing to securely deploy SDN
components and protect communication between network endpoints. We introduce
ephemeral flow-specific pre-shared keys and propose a novel defense against
cuckoo attacks on SGX enclaves. TruSDN is secure under a powerful adversary
model, with a minor performance overhead.
","Nicolae Paladi|Christian Gehrmann","","http://arxiv.org/abs/1702.04143v1","http://arxiv.org/pdf/1702.04143v1","","SecureComm 2016 12th EAI International Conference on Security and
  Privacy in Communication Networks October 10-12, 2016 Guangzhou, People's
  Republic of China","","","cs.NI","cs.NI|cs.CR"
"793","1702.05349v1","2017-02-17 14:11:44","2017-02-17 14:11:44","ARTEMIS: Real-Time Detection and Automatic Mitigation for BGP Prefix
  Hijacking","  Prefix hijacking is a common phenomenon in the Internet that often causes
routing problems and economic losses. In this demo, we propose ARTEMIS, a tool
that enables network administrators to detect and mitigate prefix hijacking
incidents, against their own prefixes. ARTEMIS is based on the real-time
monitoring of BGP data in the Internet, and software-defined networking (SDN)
principles, and can completely mitigate a prefix hijacking within a few minutes
(e.g., 5-6 mins in our experiments) after it has been launched.
","Gavriil Chaviaras|Petros Gigis|Pavlos Sermpezis|Xenofontas Dimitropoulos","","http://arxiv.org/abs/1702.05349v1","http://arxiv.org/pdf/1702.05349v1","http://dx.doi.org/10.1145/2934872.2959078","","Proceedings of the ACM SIGCOMM 2016 Conference (SIGCOMM '16),
  625-626","10.1145/2934872.2959078","cs.NI","cs.NI"
"794","1702.05713v1","2017-02-19 07:50:29","2017-02-19 07:50:29","Adoption of free and open source software using alternative educational
  framework in college of applied sciences","  The adoption of Free and Open Source Software (FOSS) in educational
institutions is increasing day by day. Many countries are insisting the use of
FOSS in their government sectors and few are in the process of adopting FOSS
strategies. The reasons for adopting FOSS are: total cost ownership, free to
make copies and distribution, software legality, reliability, availability,
performance, security and other pedagogical and administrative benefits. In
this research paper, the alternative educational framework has been used to
identify the FOSS tools and software by considering the various strategic
areas, polices and phased approach to identify the suitable FOSS tools for the
courses of various specializations of Information Technology programme in
College of Applied Sciences, Sultanate of Oman. As a result of this research, a
list of alternative FOSS tools and software for the courses of IT programme has
been identified and recommended.
","Raya Al-Hajri|Ghada Al-Mukhaini|Rajasekar Ramalingam","","http://arxiv.org/abs/1702.05713v1","http://arxiv.org/pdf/1702.05713v1","","4 Pages Free and Open Source Software Conference (fossc-17) Muscat
  Oman, February 14-15, 2017","","","cs.CY","cs.CY"
"795","1702.07196v1","2017-02-23 12:38:45","2017-02-23 12:38:45","Preconditioning ideas for the Augmented Lagrangian method","  A preconditioning strategy for the Powell-Hestenes-Rockafellar Augmented
Lagrangian method (ALM) is presented. The scheme exploits the structure of the
Augmented Lagrangian Hessian. It is a modular preconditioner consisting of two
blocks. The first one is associated with the Lagrangian of the objective while
the second administers the Jacobian of the constraints and possible low-rank
corrections to the Hessian. The proposed updating strategies take advantage of
ALM convergence results and avoid frequent refreshing. Constraint
administration takes into account complementarity over the Lagrange multipliers
and admits relaxation. The preconditioner is designed for problems where
constraint quantity is small compared to the search space. A virtue of the
scheme is that it is agnostic to the preconditioning technique used for the
Hessian of the Lagrangian function. The strategy described can be used for
linear and nonlinear preconditioning. Numerical experiments report on spectral
properties of preconditioned matrices from Matrix Market while some
optimization problems where taken from the CUTEst collection. Preliminary
results indicate that the proposed scheme could be attractive and further
experimentation is encouraged.
","AM Sajo-Castelli","","http://arxiv.org/abs/1702.07196v1","http://arxiv.org/pdf/1702.07196v1","","20 pages, 2 figures","","","math.OC","math.OC|65F10, 15A12, 65F35, 90C30, 90C25"
"796","1703.01237v1","2017-02-27 18:38:28","2017-02-27 18:38:28","How real is the random censorship model in medical studies?","  In survival analysis the random censorship model refers to censoring and
survival times being independent of each other. It is one of the fundamental
assumptions in the theory of survival analysis. We explain the reason for it
being so ubiquitous, and we investigate its presence in medical studies. We
differentiate two types of censoring in medical studies (dropout and
administrative), and we explain their importance in examining the existence of
the random censorship model. We show that in order to presume the random
censorship model it is not enough to have a design study which conforms to it,
but that one needs to provide evidence for its presence in the results. Blindly
presuming the random censorship model might lead to the Kaplan-Meier estimator
producing biased results, which might have serious consequences when estimating
survival in medical studies.
","Damjan Krstajic","","http://arxiv.org/abs/1703.01237v1","http://arxiv.org/pdf/1703.01237v1","","","","","stat.AP","stat.AP|math.ST|stat.TH"
"797","1703.01284v2","2017-03-03 18:51:20","2017-06-13 14:15:10","Investcoin: A System for Privacy-Preserving Investments","  This work presents a new framework for Privacy-Preserving Investment systems
in a distributed model. In this model, independent investors can transfer funds
to independent projects, in the same way as it works on crowdfunding platforms.
The framework protects the investors' single payments from being detected (by
any other party), only the sums of each investor's payments are revealed.
Likewise, the projects' single incoming payments are concealed and only the
final sums of the incoming payments for every project are revealed. In this
way, no other party than the investor (not even the system administration) can
detect how much she paid to any single project. Though it is still possible to
confidentially exchange any part of an investment between any pair of
investors, such that market liquidity is unaffected by the system. On top, our
framework allows a privacy-preserving return of a multiple of all the held
investments (e.g. interest payments or dividends) to the indivdual investors
while still revealing nothing else than the sum of all returns for every
investor. We provide reasonable security guarantees for this framework that are
based on common notions from the Secure Multi-Party Computation literature. As
instantiation for this framework we present Investcoin. It is a proper
combination of three cryptographic protocols, namely a Private Stream
Aggregation scheme, a Commitment scheme and a Range test and it is usable in
connection with any existing currency. The security of these protocols is based
on the DDH assumption. By a composition theorem from the SMPC literature, the
security of the resulting Investcoin protocol is also based on the DDH
assumption. Furthermore, we provide a simple decentralised key generation
protocol for Investcoin supporting dynamic join/leave and fault-tolarance of
investors and moreover achieves some security guarantees against malicious
investors.
","Filipp Valovich","","http://arxiv.org/abs/1703.01284v2","http://arxiv.org/pdf/1703.01284v2","","","","","cs.CR","cs.CR"
"798","1703.01534v1","2017-03-04 23:17:25","2017-03-04 23:17:25","Secure Count Query on Encrypted Genomic Data","  Capturing the vast amount of meaningful information encoded in the human
genome is a fascinating research problem. The outcome of these researches have
significant influences in a number of health related fields --- personalized
medicine, paternity testing and disease susceptibility testing are a few to be
named. To facilitate these types of large scale biomedical research projects,
it oftentimes requires to share genomic and clinical data collected by
disparate organizations among themselves. In that case, it is of utmost
importance to ensure that sharing, managing and analyzing the data does not
reveal the identity of the individuals who contribute their genomic samples.
The task of storage and computation on the shared data can be delegated to
third party cloud infrastructures, equipped with large storage and high
performance computation resources. Outsourcing these sensitive genomic data to
the third party cloud storage is associated with the challenges of the
potential loss, theft or misuse of the data as the server administrator cannot
be completely trusted as well as there is no guarantee that the security of the
server will not be breached. In this paper, we provide a model for secure
sharing and computation on genomic data in a semi-honest third party cloud
server. The security of the shared data is guaranteed through encryption while
making the overall computation fast and scalable enough for real-life
large-scale biomedical applications. We evaluated the efficiency of our
proposed model on a database of Single-Nucleotide Polymorphism (SNP) sequences
and experimental results demonstrate that a query of 50 SNPs in a database of
50000 records, where each record contains 300 SNPs, takes approximately 6
seconds.
","Mohammad Zahidul Hasan|Md Safiur Rahman Mahdi|Noman Mohammed","","http://arxiv.org/abs/1703.01534v1","http://arxiv.org/pdf/1703.01534v1","","19 pages, 7 figures, 3rd International Workshop on Genome Privacy and
  Security (GenoPri'16)","","","cs.CR","cs.CR"
"799","1703.01952v2","2017-03-06 16:23:50","2017-11-07 16:26:19","Efficient Strategy Computation in Zero-Sum Asymmetric Repeated Games","  Zero-sum asymmetric games model decision making scenarios involving two
competing players who have different information about the game being played. A
particular case is that of nested information, where one (informed) player has
superior information over the other (uninformed) player. This paper considers
the case of nested information in repeated zero-sum games and studies the
computation of strategies for both the informed and uninformed players for
finite-horizon and discounted infinite-horizon nested information games. For
finite-horizon settings, we exploit that for both players, the security
strategy, and also the opponent's corresponding best response depend only on
the informed player's history of actions. Using this property, we refine the
sequence form, and formulate an LP computation of player strategies that is
linear in the size of the uninformed player's action set. For the
infinite-horizon discounted game, we construct LP formulations to compute the
approximated security strategies for both players, and provide a bound on the
performance difference between the approximated security strategies and the
security strategies. Finally, we illustrate the results on a network
interdiction game between an informed system administrator and uniformed
intruder.
","Lichun Li|Jeff S. Shamma","","http://arxiv.org/abs/1703.01952v2","http://arxiv.org/pdf/1703.01952v2","","sumbitted to IEEE TAC, under review","","","cs.GT","cs.GT"
"800","1703.01953v4","2017-03-06 16:24:52","2017-04-07 20:45:09","Regularity in gradient constraint problem and thin film micromagnetics","  We prove some C^{1,\alpha} regularity in some gradient constraint problem and
application to Torsion problem and micromagnetic problem and variational
inequality.
","Rene Chipot","","http://arxiv.org/abs/1703.01953v4","http://arxiv.org/pdf/1703.01953v4","","This article has been withdrawn by arXiv administrators due to
  falsified authorship and affiliation","","","math.AP","math.AP"
"801","1703.02014v2","2017-03-06 18:29:15","2017-06-02 17:37:11","SoK: Cryptographically Protected Database Search","  Protected database search systems cryptographically isolate the roles of
reading from, writing to, and administering the database. This separation
limits unnecessary administrator access and protects data in the case of system
breaches. Since protected search was introduced in 2000, the area has grown
rapidly; systems are offered by academia, start-ups, and established companies.
  However, there is no best protected search system or set of techniques.
Design of such systems is a balancing act between security, functionality,
performance, and usability. This challenge is made more difficult by ongoing
database specialization, as some users will want the functionality of SQL,
NoSQL, or NewSQL databases. This database evolution will continue, and the
protected search community should be able to quickly provide functionality
consistent with newly invented databases.
  At the same time, the community must accurately and clearly characterize the
tradeoffs between different approaches. To address these challenges, we provide
the following contributions:
  1) An identification of the important primitive operations across database
paradigms. We find there are a small number of base operations that can be used
and combined to support a large number of database paradigms.
  2) An evaluation of the current state of protected search systems in
implementing these base operations. This evaluation describes the main
approaches and tradeoffs for each base operation. Furthermore, it puts
protected search in the context of unprotected search, identifying key gaps in
functionality.
  3) An analysis of attacks against protected search for different base
queries.
  4) A roadmap and tools for transforming a protected search system into a
protected database, including an open-source performance evaluation platform
and initial user opinions of protected search.
","Benjamin Fuller|Mayank Varia|Arkady Yerukhimovich|Emily Shen|Ariel Hamlin|Vijay Gadepally|Richard Shay|John Darby Mitchell|Robert K. Cunningham","","http://arxiv.org/abs/1703.02014v2","http://arxiv.org/pdf/1703.02014v2","","20 pages, to appear to IEEE Security and Privacy","","","cs.CR","cs.CR"
"802","1703.02275v3","2017-03-07 08:35:26","2017-04-11 20:53:18","Regularity and convergence of minimizer in nonlocal variational
  principle","  We prove some variational analysis of regularity and weak convergence of
nonlocal variational principle.
","Rene Chipot","","http://arxiv.org/abs/1703.02275v3","http://arxiv.org/pdf/1703.02275v3","","arXiv admin note: submission has been withdrawn by arXiv
  administrators due to inappropriate text reuse from external sources","","","math.AP","math.AP"
"803","1703.04237v1","2017-03-13 03:57:28","2017-03-13 03:57:28","Effects of Reactive Social Distancing on the 1918 Influenza Pandemic","  The 1918 influenza pandemic was characterized by multiple epidemic waves. We
investigated into reactive social distancing, a form of behavioral responses,
and its effect on the multiple influenza waves in the United Kingdom. Two forms
of reactive social distancing have been used in previous studies: Power
function, which is a function of the proportion of recent influenza mortality
in a population, and Hill function, which is a function of the actual number of
recent influenza mortality. Using a simple epidemic model with a Power function
and one common set of parameters, we provided a good model fit for the observed
multiple epidemic waves in London boroughs, Birmingham and Liverpool. Our
approach is different from previous studies where separate models are fitted to
each city. We then applied these model parameters obtained from fitting three
cities to all 334 administrative units in England and Wales and including the
population sizes of individual administrative units. We computed the Pearson's
correlation between the observed and simulated data for each administrative
unit. We achieved a median correlation of 0.636, indicating our model
predictions perform reasonably well. Our modelling approach which requires
reduced number of parameters resulted in computational efficiency gain without
over-fitting the model. Our works have both scientific and public health
significance.
","Duo Yu|Qianying Lin|Alice PY Chiu|Daihai He","","http://arxiv.org/abs/1703.04237v1","http://arxiv.org/pdf/1703.04237v1","http://dx.doi.org/10.1371/journal.pone.0180545","11 pages, 5 figures","PLoS ONE 2017 12(7): e0180545","10.1371/journal.pone.0180545","q-bio.PE","q-bio.PE"
"804","1703.06774v2","2017-03-20 14:38:09","2018-10-18 15:09:19","Stochastic geometric models with non-stationary spatial correlations in
  Lagrangian fluid flows","  Inspired by spatiotemporal observations from satellites of the trajectories
of objects drifting near the surface of the ocean in the National Oceanic and
Atmospheric Administration's `Global Drifter Program', this paper develops
data-driven stochastic models of geophysical fluid dynamics (GFD) with
non-stationary spatial correlations representing the dynamical behaviour of
oceanic currents. Three models are considered. Model 1 from Holm [2015] is
reviewed, in which the spatial correlations are time independent. Two new
models, called Model 2 and Model 3, introduce two different symmetry breaking
mechanisms by which the spatial correlations may be advected by the flow. These
models are derived using reduction by symmetry of stochastic variational
principles, leading to stochastic Hamiltonian systems, whose momentum maps,
conservation laws and Lie-Poisson bracket structures are used in developing the
new stochastic Hamiltonian models of GFD.
","Francois Gay-Balmaz|Darryl D. Holm","","http://arxiv.org/abs/1703.06774v2","http://arxiv.org/pdf/1703.06774v2","http://dx.doi.org/10.1007/s00332-017-9431-0","27 pages, 3 figures","","10.1007/s00332-017-9431-0","nlin.CD","nlin.CD"
"805","1703.07526v1","2017-03-22 04:55:00","2017-03-22 04:55:00","Networks of border zones: multiplex relations of power, religion and
  economy in South-eastern Europe, 1250-1453 CE","  We demonstrate the application of the multiplex networks-approach for the
analysis of various networks which connected individuals and communities in the
politically highly fragmented late medieval Balkans (1204-1453 AD) within and
across border zones. We present how we obtain relational data from our sources
and the integration of these data into three different networks (of roads,
state administration and ecclesiastical administration) of various topologies;
then we calculate several indicators for influences and overlaps between these
different networks which connect the same set of nodes (settlements). We
analyse changes and continuities in the topologies of the various networks for
three time-steps (1210, 1324 and 1380 CE) and demonstrate the role of these
networks as frameworks for social interactions. Finally, we combine all three
networks into one network which shows properties observed for the small world
model. Thus, we demonstrate possibilities for capturing historical complexity
with the help of the multiplex networks approach.
","Johannes Preiser-Kapeller","","http://arxiv.org/abs/1703.07526v1","http://arxiv.org/pdf/1703.07526v1","","Published in: Proceedings of the 39th Annual Conference of Computer
  Applications and Quantitative Methods in Archaeology, Revive the Past (CAA)
  in Beijing, China. Amsterdam 2012, pp. 381-393 [peer reviewed]","","","physics.soc-ph","physics.soc-ph|nlin.AO"
"806","1703.09528v4","2017-03-28 12:10:45","2019-05-22 06:39:41","Discovering Latent Covariance Structures for Multiple Time Series","  Analyzing multivariate time series data is important to predict future events
and changes of complex systems in finance, manufacturing, and administrative
decisions. The expressiveness power of Gaussian Process (GP) regression methods
has been significantly improved by compositional covariance structures. In this
paper, we present a new GP model which naturally handles multiple time series
by placing an Indian Buffet Process (IBP) prior on the presence of shared
kernels. Our selective covariance structure decomposition allows exploiting
shared parameters over a set of multiple, selected time series. We also
investigate the well-definedness of the models when infinite latent components
are introduced. We present a pragmatic search algorithm which explores a larger
structure space efficiently. Experiments conducted on five real-world data sets
demonstrate that our new model outperforms existing methods in term of
structure discoveries and predictive performances.
","Anh Tong|Jaesik Choi","","http://arxiv.org/abs/1703.09528v4","http://arxiv.org/pdf/1703.09528v4","","ICML2019, 13 pages","","","stat.ML","stat.ML"
"807","1703.09752v1","2017-03-28 19:04:11","2017-03-28 19:04:11","Collective Anomaly Detection based on Long Short Term Memory Recurrent
  Neural Network","  Intrusion detection for computer network systems becomes one of the most
critical tasks for network administrators today. It has an important role for
organizations, governments and our society due to its valuable resources on
computer networks. Traditional misuse detection strategies are unable to detect
new and unknown intrusion. Besides, anomaly detection in network security is
aim to distinguish between illegal or malicious events and normal behavior of
network systems. Anomaly detection can be considered as a classification
problem where it builds models of normal network behavior, which it uses to
detect new patterns that significantly deviate from the model. Most of the cur-
rent research on anomaly detection is based on the learning of normally and
anomaly behaviors. They do not take into account the previous, re- cent events
to detect the new incoming one. In this paper, we propose a real time
collective anomaly detection model based on neural network learning and feature
operating. Normally a Long Short Term Memory Recurrent Neural Network (LSTM
RNN) is trained only on normal data and it is capable of predicting several
time steps ahead of an input. In our approach, a LSTM RNN is trained with
normal time series data before performing a live prediction for each time step.
Instead of considering each time step separately, the observation of prediction
errors from a certain number of time steps is now proposed as a new idea for
detecting collective anomalies. The prediction errors from a number of the
latest time steps above a threshold will indicate a collective anomaly. The
model is built on a time series version of the KDD 1999 dataset. The
experiments demonstrate that it is possible to offer reliable and efficient for
collective anomaly detection.
","Loic Bontemps|Van Loi Cao|James McDermott|Nhien-An Le-Khac","","http://arxiv.org/abs/1703.09752v1","http://arxiv.org/pdf/1703.09752v1","","","","","cs.LG","cs.LG|cs.CR"
"808","1704.00691v1","2017-04-03 17:14:00","2017-04-03 17:14:00","Use of a clinical PET/CT scanner for whole body biodistribution of
  intranasal nanoparticles","  Whole body biodistribution of 100 nanometer sized polymer micellar
nanoparticles (NPs) was determined following intranasal administration using
PET/CT imaging on a clinical scanner. Nanoparticles labeled with Zirconium 89
were administered intranasally or intravenously to Sprague Dawley rats followed
by serial ex vivo PET/CT imaging in a clinical scanner. At 30 minutes and 1
hour following intranasal delivery, the animals were sacrificed and placed in
the PET/CT scanner. Images were acquired and transferred to a workstation for
post-processing. 3D regions of interest were constructed. A different set of
animals were used for ex vivo verification. These animals were also
administered 100 nm polymer micellar NPs and sacrificed at 30 min and 1 hr
following intranasal or intravenous delivery via intra-cardiac perfusion.
Various organs, including brain, lungs, heart, liver, spleen, stomach and
intestines, were procured following exsanguination. A gamma counter determined
activity in each organ for comparison with corresponding PET/CT region of
interest activity measurements. The ex vivo measurements of activity were
consistent with the image-based value determinations. Use of a clinical PET/CT
scanner is a feasible means to determine the temporal and spatial distribution
of radiolabeled agents after intranasal and intravenous delivery. This work may
allow for quantitative in vivo testing of new radionanotheranostic agents.
","Michael C Veronesi|Marta Zamora|Mohammed Bhuiyan|Bill Obrien-Penney|Chin Tu Chen|Michael W Vannier","","http://arxiv.org/abs/1704.00691v1","http://arxiv.org/pdf/1704.00691v1","","17 pages, 9 figures, 4 tables","","","q-bio.TO","q-bio.TO"
"809","1704.00694v1","2017-04-03 17:22:15","2017-04-03 17:22:15","Experimental Observations of Nuclear Activity in Deuterated Materials
  Subjected to a Low-Energy Photon Beam","  Exposure of highly deuterated materials to a low-energy (nom. 2 MeV) photon
beam resulted in nuclear activity of both the parent metals of hafnium and
erbium and a witness material (molybdenum) mixed with the reactants. Gamma
spectral analysis of all deuterated materials, ErD2.8-C36D74-Mo and
HfD2-C36D74-Mo, showed that nuclear processes had occurred as shown by unique
gamma signatures. For the deuterated erbium specimens, posttest gamma spectra
showed evidence of radioisotopes of erbium (163Er and 171Er) and of molybdenum
(99Mo and 101Mo) and by beta decay, technetium (99mTc and 101Tc). For the
deuterated hafnium specimens, posttest gamma spectra showed evidence of
radioisotopes of hafnium (180mHf and 181Hf) and molybdenum (99Mo and 101Mo),
and by beta decay, technetium (99mTc and 101Tc). In contrast, when either the
hydrogenated or non-gas-loaded erbium or hafnium materials were exposed to the
gamma flux, the gamma spectra revealed no new isotopes. Neutron activation
materials showed evidence of thermal and epithermal neutrons. CR-39 solid-state
nuclear track detectors showed evidence of fast neutrons with energies between
1.4 and 2.5 MeV and several instances of triple tracks, indicating greater than
10 MeV neutrons. Further study is required to determine the mechanism causing
the nuclear activity
","Bruce M. Steinetz|Theresa L. Benyo|Vladimir Pines|Marianna Pines|Lawrence P. Forsley|Paul A. Westmeyer|Arnon Chait|Michael D. Becks|Richard E. Martin|Robert C. Hendricks|Nicholas Penney|Annette M. Marsolais|Tracy R. Kamm","National Aeronautics and Space Administration, Glenn Research Center|National Aeronautics and Space Administration, Glenn Research Center|PineSci Consulting|PineSci Consulting|JWK Corporation|National Aeronautics and Space Administration, Headquarters|National Aeronautics and Space Administration, Glenn Research Center|Vantage Partners, LLC|Cleveland State University|National Aeronautics and Space Administration, Glenn Research Center|Ohio Aerospace Institute|Vantage Partners, LLC|Vantage Partners, LLC","http://arxiv.org/abs/1704.00694v1","http://arxiv.org/pdf/1704.00694v1","","27 pages, 9 Figures, Original Work Performed for NASA","","","nucl-ex","nucl-ex|physics.ins-det"
"810","1704.01183v1","2017-04-03 17:33:06","2017-04-03 17:33:06","Investigation of Deuterium Loaded Materials Subject to X-Ray Exposure","  Results are presented from an exploratory study involving x-ray irradiation
of select deuterated materials. Titanium deuteride (TiD2) plus deuterated
polyethylene ([-CD2-]n; DPE), DPE alone, and for control, hydrogen-based
polyethylene ([-CH2-]n; HPE) samples and nondeuterated titanium samples were
exposed to x-ray irradiation. These samples were exposed to various energy
levels from 65 to 280 kV with prescribed electron flux from 500 to 9000 micro-A
impinging on a tungsten braking target, with total exposure times ranging from
55 to 280 min. Gamma activity was measured using a high-purity germanium (HPGe)
detector, and for all samples no gamma activity above background was detected.
Alpha and beta activities were measured using a gas proportional counter, and
for select samples beta activity was measured with a liquid scintillator
spectrometer. The majority of the deuterated materials subjected to the
microfocus x-ray irradiation exhibited postexposure beta activity above
background and several showed short-lived alpha activity. The HPE and
nondeuterated titanium control samples exposed to the x-ray irradiation showed
no postexposure alpha or beta activities above background. Several of the
samples (SL10A, SL16, SL17A) showed beta activity above background with a
greater than 4-sigma confidence level, months after exposure. Portions of
SL10A, SL16, and SL17A samples were also scanned using a beta scintillator and
found to have beta activity in the tritium energy band, continuing without
noticeable decay for over 12 months. Beta scintillation investigation of
as-received materials (before x-ray exposure) showed no beta activity in the
tritium energy band, indicating the beta emitters were not in the starting
materials.
","Theresa L. Benyo|Bruce M. Steinetz|Robert C. Hendricks|Richard E. Martin|Lawrence P. Forsley|Christopher C. Daniels|Arnon Chait|Vladimir Pines|Marianna Pines|Nicholas Penney|Tracy R. Kamm|Michael D. Becks","National Aeronautics and Space Administration, Glenn Research Center|National Aeronautics and Space Administration, Glenn Research Center|National Aeronautics and Space Administration, Glenn Research Center|Cleveland State University|JWK Corporation|The University of Akron|National Aeronautics and Space Administration, Glenn Research Center|PineSci Consulting|PineSci Consulting|Ohio Aerospace Institute|Vantage Partners, LLC|Vantage Partners, LLC","http://arxiv.org/abs/1704.01183v1","http://arxiv.org/pdf/1704.01183v1","","30 pages, 10 figures, Created as work for NASA","","","physics.ins-det","physics.ins-det|nucl-ex"
"811","1704.02220v2","2017-04-07 13:22:38","2018-08-22 07:29:47","Semi-Parametric Empirical Best Prediction for small area estimation of
  unemployment indicators","  The Italian National Institute for Statistics regularly provides estimates of
unemployment indicators using data from the Labor Force Survey. However, direct
estimates of unemployment incidence cannot be released for Local Labor Market
Areas. These are unplanned domains defined as clusters of municipalities; many
are out-of-sample areas and the majority is characterized by a small sample
size, which render direct estimates inadequate. The Empirical Best Predictor
represents an appropriate, model-based, alternative. However, for non-Gaussian
responses, its computation and the computation of the analytic approximation to
its Mean Squared Error require the solution of (possibly) multiple integrals
that, generally, have not a closed form. To solve the issue, Monte Carlo
methods and parametric bootstrap are common choices, even though the
computational burden is a non trivial task. In this paper, we propose a
Semi-Parametric Empirical Best Predictor for a (possibly) non-linear mixed
effect model by leaving the distribution of the area-specific random effects
unspecified and estimating it from the observed data. This approach is known to
lead to a discrete mixing distribution which helps avoid unverifiable
parametric assumptions and heavy integral approximations. We also derive a
second-order, bias-corrected, analytic approximation to the corresponding Mean
Squared Error. Finite sample properties of the proposed approach are tested via
a large scale simulation study. Furthermore, the proposal is applied to
unit-level data from the 2012 Italian Labor Force Survey to estimate
unemployment incidence for 611 Local Labor Market Areas using auxiliary
information from administrative registers and the 2011 Census.
","Maria Francesca Marino|Maria Giovanna Ranalli|Nicola Salvati|Marco Alfo'","","http://arxiv.org/abs/1704.02220v2","http://arxiv.org/pdf/1704.02220v2","","","","","stat.ME","stat.ME"
"812","1704.04103v2","2017-04-12 13:59:26","2017-06-30 13:24:34","Orderness Predicts Academic Performance: Behavioral Analysis on Campus
  Lifestyle","  Quantitative understanding of relationships between students' behavioral
patterns and academic performances is a significant step towards personalized
education. In contrast to previous studies that mainly based on questionnaire
surveys, in this paper, we collect behavioral records from 18,960 undergraduate
students' smart cards and propose a novel metric, called orderness, which
measures the regularity of campus daily life (e.g., meals and showers) of each
student. Empirical analysis demonstrates that academic performance (GPA) is
strongly correlated with orderness. Furthermore, we show that orderness is an
important feature to predict academic performance, which remarkably improves
the prediction accuracy even at the presence of students' diligence. Based on
these analyses, education administrators could better guide students' campus
lives and implement effective interventions in an early stage when necessary.
","Yi Cao|Jian Gao|Defu Lian|Zhihai Rong|Jiatu Shi|Qing Wang|Yifan Wu|Huaxiu Yao|Tao Zhou","","http://arxiv.org/abs/1704.04103v2","http://arxiv.org/pdf/1704.04103v2","http://dx.doi.org/10.1098/rsif.2018.0210","11 pages, 6 figures and 2 table, Appendix included","J. R. Soc. Interface 15 (2018) 20180210","10.1098/rsif.2018.0210","physics.soc-ph","physics.soc-ph"
"813","1704.04301v1","2017-04-13 23:26:26","2017-04-13 23:26:26","A Tree-based Approach for Detecting Redundant Business Rules in very
  Large Financial Datasets","  Net Asset Value (NAV) calculation and validation is the principle task of a
fund administrator. If the NAV of a fund is calculated incorrectly then there
is huge impact on the fund administrator; such as monetary compensation,
reputational loss, or loss of business. In general, these companies use the
same methodology to calculate the NAV of a fund, however the type of fund in
question dictates the set of business rules used to validate this. Today, most
Fund Administrators depend heavily on human resources due to the lack of an
automated standardized solutions, however due to economic climate and the need
for efficiency and costs reduction many banks are now looking for an automated
solution with minimal human interaction; i.e., straight through processing
(STP). Within the scope of a collaboration project that focuses on building an
optimal solution for NAV validation, in this paper, we will present a new
approach for detecting correlated business rules. We also show how we evaluate
this approach using real-world financial data.
","Nhien-An Le-Khac|Sammer Markos|M-Tahar Kechadi","","http://arxiv.org/abs/1704.04301v1","http://arxiv.org/pdf/1704.04301v1","","","","","cs.DB","cs.DB"
"814","1704.04782v1","2017-04-16 14:59:21","2017-04-16 14:59:21","A Security Monitoring Framework For Virtualization Based HEP
  Infrastructures","  High Energy Physics (HEP) distributed computing infrastructures require
automatic tools to monitor, analyze and react to potential security incidents.
These tools should collect and inspect data such as resource consumption, logs
and sequence of system calls for detecting anomalies that indicate the presence
of a malicious agent. They should also be able to perform automated reactions
to attacks without administrator intervention. We describe a novel framework
that accomplishes these requirements, with a proof of concept implementation
for the ALICE experiment at CERN. We show how we achieve a fully virtualized
environment that improves the security by isolating services and Jobs without a
significant performance impact. We also describe a collected dataset for
Machine Learning based Intrusion Prevention and Detection Systems on Grid
computing. This dataset is composed of resource consumption measurements (such
as CPU, RAM and network traffic), logfiles from operating system services, and
system call data collected from production Jobs running in an ALICE Grid test
site and a big set of malware. This malware was collected from security
research sites. Based on this dataset, we will proceed to develop Machine
Learning algorithms able to detect malicious Jobs.
","A. Gomez Ramirez|M. Martinez Pedreira|C. Grigoras|L. Betev|C. Lara|U. Kebschull","for the ALICE Collaboration|for the ALICE Collaboration|for the ALICE Collaboration|for the ALICE Collaboration|for the ALICE Collaboration|for the ALICE Collaboration","http://arxiv.org/abs/1704.04782v1","http://arxiv.org/pdf/1704.04782v1","http://dx.doi.org/10.1088/1742-6596/898/10/102004","Proceedings of the 22nd International Conference on Computing in High
  Energy and Nuclear Physics, CHEP 2016, 10-14 October 2016, San Francisco.
  Submitted to Journal of Physics: Conference Series (JPCS)","","10.1088/1742-6596/898/10/102004","cs.DC","cs.DC|cs.AI|cs.CR|hep-ex"
"815","1704.05045v1","2017-04-17 17:52:41","2017-04-17 17:52:41","Sub-100 nm 3-D fluorescence lifetime imaging using time correlated
  single photon counting detection and multifocal multiphoton excitation","  Abstract removed.
","Suhas Kumar","","http://arxiv.org/abs/1704.05045v1","http://arxiv.org/pdf/1704.05045v1","http://dx.doi.org/10.1364/OE.15.012548","This submission removed by arXiv administrators as the submitter
  Suhas Kumar was not an author of the paper","Optics Express 15, 12548 (2007)","10.1364/OE.15.012548","physics.optics","physics.optics|cond-mat.mes-hall|physics.ins-det"
"816","1704.06549v1","2017-04-21 14:04:28","2017-04-21 14:04:28","LiftUpp: Support to develop learner performance","  Various motivations exist to move away from the simple assessment of
knowledge towards the more complex assessment and development of competence.
However, to accommodate such a change, high demands are put on the supporting
e-infrastructure in terms of intelligently collecting and analysing data. In
this paper, we discuss these challenges and how they are being addressed by
LiftUpp, a system that is now used in 70% of UK dental schools, and is finding
wider applications in physiotherapy, medicine and veterinary science. We
describe how data is collected for workplace-based development in dentistry
using a dedicated iPad app, which enables an integrated approach to linking and
assessing work flows, skills and learning outcomes. Furthermore, we detail how
the various forms of collected data can be fused, visualized and integrated
with conventional forms of assessment. This enables curriculum integration,
improved real-time student feedback, support for administration, and informed
instructional planning. Together these facets contribute to better support for
the development of learners' competence in situated learning setting, as well
as an improved experience. Finally, we discuss several directions for future
research on intelligent teaching systems that are afforded by using the design
present within LiftUpp.
","Frans A. Oliehoek|Rahul Savani|Elliot Adderton|Xia Cui|David Jackson|Phil Jimmieson|John Christopher Jones|Keith Kennedy|Ben Mason|Adam Plumbley|Luke Dawson","","http://arxiv.org/abs/1704.06549v1","http://arxiv.org/pdf/1704.06549v1","","Short 4-page version to appear at AIED 2017","","","cs.CY","cs.CY"
"817","1704.07823v1","2017-04-25 01:07:36","2017-04-25 01:07:36","Towards Understanding the Impact of Human Mobility on Police Allocation","  Motivated by recent findings that human mobility is proxy for crime behavior
in big cities and that there is a superlinear relationship between the people's
movement and crime, this article aims to evaluate the impact of how these
findings influence police allocation. More precisely, we shed light on the
differences between an allocation strategy, in which the resources are
distributed by clusters of floating population, and conventional allocation
strategies, in which the police resources are distributed by an Administrative
Area (typically based on resident population). We observed a substantial
difference in the distributions of police resources allocated following these
strategies, what evidences the imprecision of conventional police allocation
methods.
","Carlos Caminha|Vasco Furtado","","http://arxiv.org/abs/1704.07823v1","http://arxiv.org/pdf/1704.07823v1","","6 pages and 6 figures","","","physics.soc-ph","physics.soc-ph|cs.SI"
"818","1705.01591v1","2017-05-03 19:35:16","2017-05-03 19:35:16","Demonstrating research subcommunities in mathematical networks","  We propose a method for demonstrating sub community structure in scientific
networks of relatively small size from analyzing databases of publications.
Research relationships between the network members can be visualized as a graph
with vertices corresponding to authors and with edges indicating joint
authorship. Using a fast clustering algorithm combined with a graph layout
algorithm, we demonstrate how to display these clustering results in an
attractive and informative way. The small size of the graph allows us to
develop tools that keep track of how these research sub communities evolve in
time, as well as to present the research articles that create the links between
the network members. These tools are included in a web app, where the visitor
can easily identify the various sub communities, providing also valuable
information for administrational purposes. Our method was developed for the
GEAR mathematical network and it can be applied to other networks.
","Steven B. Bradlow|Konstantinos Kapenekakis|Georgios Kydonakis|Xinwei Li|Jiarui Xu","","http://arxiv.org/abs/1705.01591v1","http://arxiv.org/pdf/1705.01591v1","","4 pages, 3 figures, 1 table","","","cs.SI","cs.SI|cs.DS|physics.soc-ph"
"819","1705.09404v1","2017-05-26 01:01:49","2017-05-26 01:01:49","Confidentiality-Preserving Publish/Subscribe: A Survey","  Publish/subscribe (pub/sub) is an attractive communication paradigm for
large-scale distributed applications running across multiple administrative
domains. Pub/sub allows event-based information dissemination based on
constraints on the nature of the data rather than on pre-established
communication channels. It is a natural fit for deployment in untrusted
environments such as public clouds linking applications across multiple sites.
However, pub/sub in untrusted environments lead to major confidentiality
concerns stemming from the content-centric nature of the communications. This
survey classifies and analyzes different approaches to confidentiality
preservation for pub/sub, from applications of trust and access control models
to novel encryption techniques. It provides an overview of the current
challenges posed by confidentiality concerns and points to future research
directions in this promising field.
","Emanuel Onica|Pascal Felber|Hugues Mercier|Etienne Riviere","","http://arxiv.org/abs/1705.09404v1","http://arxiv.org/pdf/1705.09404v1","http://dx.doi.org/10.1145/2940296","","ACM Computing Surveys, Volume 49, Issue 2, November 2016","10.1145/2940296","cs.CR","cs.CR"
"820","1705.10063v1","2017-05-29 07:59:17","2017-05-29 07:59:17","Small Area Quantile Estimation","  Sample surveys are widely used to obtain information about totals, means,
medians, and other parameters of finite populations. In many applications,
similar information is desired for subpopulations such as individuals in
specific geographic areas and socio-demographic groups. When the surveys are
conducted at national or similarly high levels, a probability sampling can
result in just a few sampling units from many unplanned subpopulations at the
design stage. Cost considerations may also lead to low sample sizes from
individual small areas. Estimating the parameters of these subpopulations with
satisfactory precision and evaluating their accuracy are serious challenges for
statisticians. To overcome the difficulties, statisticians resort to pooling
information across the small areas via suitable model assumptions,
administrative archives, and census data. In this paper, we develop an array of
small area quantile estimators. The novelty is the introduction of a
semiparametric density ratio model for the error distribution in the unit-level
nested error regression model. In contrast, the existing methods are usually
most effective when the response values are jointly normal. We also propose a
resampling procedure for estimating the mean square errors of these estimators.
Simulation results indicate that the new methods have superior performance when
the population distributions are skewed and remain competitive otherwise.
","Jiahua Chen|Yukun Liu","","http://arxiv.org/abs/1705.10063v1","http://arxiv.org/pdf/1705.10063v1","","26 pages, 3 figures, 3 tables","","","stat.ME","stat.ME"
"821","1706.01513v2","2017-06-01 20:34:41","2018-01-26 15:05:34","Beyond Volume: The Impact of Complex Healthcare Data on the Machine
  Learning Pipeline","  From medical charts to national census, healthcare has traditionally operated
under a paper-based paradigm. However, the past decade has marked a long and
arduous transformation bringing healthcare into the digital age. Ranging from
electronic health records, to digitized imaging and laboratory reports, to
public health datasets, today, healthcare now generates an incredible amount of
digital information. Such a wealth of data presents an exciting opportunity for
integrated machine learning solutions to address problems across multiple
facets of healthcare practice and administration. Unfortunately, the ability to
derive accurate and informative insights requires more than the ability to
execute machine learning models. Rather, a deeper understanding of the data on
which the models are run is imperative for their success. While a significant
effort has been undertaken to develop models able to process the volume of data
obtained during the analysis of millions of digitalized patient records, it is
important to remember that volume represents only one aspect of the data. In
fact, drawing on data from an increasingly diverse set of sources, healthcare
data presents an incredibly complex set of attributes that must be accounted
for throughout the machine learning pipeline. This chapter focuses on
highlighting such challenges, and is broken down into three distinct
components, each representing a phase of the pipeline. We begin with attributes
of the data accounted for during preprocessing, then move to considerations
during model building, and end with challenges to the interpretation of model
output. For each component, we present a discussion around data as it relates
to the healthcare domain and offer insight into the challenges each may impose
on the efficiency of machine learning techniques.
","Keith Feldman|Louis Faust|Xian Wu|Chao Huang|Nitesh V. Chawla","","http://arxiv.org/abs/1706.01513v2","http://arxiv.org/pdf/1706.01513v2","http://dx.doi.org/10.1007/978-3-319-69775-8_9","Healthcare Informatics, Machine Learning, Knowledge Discovery: 20
  Pages, 1 Figure","Towards Integrative Machine Learning and Knowledge Extraction,
  LNCS vol 10344 (2017) 150-169","10.1007/978-3-319-69775-8_9","cs.CY","cs.CY|cs.LG|stat.ML"
"822","1706.00804v1","2017-06-02 18:35:05","2017-06-02 18:35:05","Estimating the prevalence of infectious diseases from under-reported
  age-dependent compulsorily notification databases","  Background: National or local laws, norms or regulations (sometimes and in
some countries) require medical providers to report notifiable diseases to
public health authorities. Reporting, however, is almost always incomplete.
This is due to a variety of reasons, ranging from not recognizing the diseased
to failures in the technical or administrative steps leading to the final
official register in the disease notification system. The reported fraction
varies from 9% to 99% and is strongly associated with the disease being
reported.
  Methods: In this paper we propose a method to approximately estimate the full
prevalence (and any other variable or parameter related to transmission
intensity) of infectious diseases. The model assumes incomplete notification of
incidence and allows the estimation of the non-notified number of infections
and it is illustrated by the case of hepatitis C in Brazil. The method has the
advantage that it can be corrected iteratively by comparing its findings with
empirical results.
  Results: The application of the model for the case of hepatitis C in Brazil
resulted in a prevalence of notified cases that varied between 163,902 and
169,382 cases; a prevalence of non-notified cases that varied between 1,433,638
and 1,446,771; and a total prevalence of infections that varied between
1,597,540 and 1,616,153 cases.
  Conclusions: We conclude that that the model proposed can be useful for
estimation of the actual magnitude of endemic states of infectious diseases,
particularly for those where the number of notified cases is only the tip of
the iceberg. In addition, the method can be applied to other situations, such
as the well known underreported incidence of criminality (for example rape),
among others.
","Marcos Amaku|Marcelo Nascimento Burattini|Eleazar Chaib|Francisco Antonio Bezerra Coutinho|David Greenhalgh|Luis Fernandez Lopez|Eduardo Massad","","http://arxiv.org/abs/1706.00804v1","http://arxiv.org/pdf/1706.00804v1","","22 pages, 4 figures","","","q-bio.PE","q-bio.PE"
"823","1706.01432v2","2017-06-05 17:37:28","2018-05-16 15:49:06","Strategic Equilibria in Queues with Dynamic Service Rate and Full
  Information","  We consider the problem of customer equilibrium behavior of a single server
Markovian queue with dynamic control of the service rate. Customers arrive
according a Poisson procedure and the system administrator makes a service rate
choice between a low and a high value according to a $T$-threshold dynamic
service policy, where the decision for switching to the higher service rate is
made when the number of customers exceeds T without any additional cost. We
assume that customers are identical and they are making join decisions
regarding the maximization of their expected net benefit, receiving a fixed
reward for service completion and incurring a waiting cost. In addition, we
consider the observable case of the model where customers are fully informed on
the service policy and the queue length upon arrival.
","Apostolos Burnetas|Yiannis Dimitrakopoulos","","http://arxiv.org/abs/1706.01432v2","http://arxiv.org/pdf/1706.01432v2","","","","","math.OC","math.OC"
"824","1706.01985v1","2017-06-06 21:16:32","2017-06-06 21:16:32","Privacy in Information-Rich Intelligent Infrastructure","  Intelligent infrastructure will critically rely on the dense instrumentation
of sensors and actuators that constantly transmit streaming data to cloud-based
analytics for real-time monitoring. For example, driverless cars communicate
real-time location and other data to companies like Google, which aggregate
regional data in order to provide real-time traffic maps. Such traffic maps can
be extremely useful to the driver (for optimal travel routing), as well as to
city transportation administrators for real-time accident response that can
have an impact on traffic capacity. Intelligent infrastructure monitoring
compromises the privacy of drivers who continuously share their location to
cloud aggregators, with unpredictable consequences.
  Without a framework for protecting the privacy of the driver's data, drivers
may be very conservative about sharing their data with cloud-based analytics
that will be responsible for adding the intelligence to intelligent
infrastructure. In the energy sector, the Smart Grid revolution relies
critically on real-time metering of energy supply and demand with very high
granularity. This is turn enables real-time demand response and creates a new
energy market that can incorporate unpredictable renewable energy sources while
ensuring grid stability and reliability. However, real-time streaming data
captured by smart meters contain a lot of private information, such as our home
activities or lack of, which can be easily inferred by anyone that has access
to the smart meter data, resulting not only in loss of privacy but potentially
also putting us at risk.
","Cynthia Dwork|George J. Pappas","","http://arxiv.org/abs/1706.01985v1","http://arxiv.org/pdf/1706.01985v1","","A Computing Community Consortium (CCC) white paper, 5 pages","","","cs.CY","cs.CY"
"825","1706.03171v2","2017-06-10 02:33:09","2017-07-02 16:57:26","Attribute Based Administration of Role Based Access Control : A Detailed
  Description","  Administrative Role Based Access Control (ARBAC) models deal with how to
manage user-role assignments (URA), permission-role assignments (PRA), and
role-role assignments (RRA). A wide variety of approaches has been proposed in
the literature for URA, PRA, and RRA. In this paper, we propose attribute-based
administrative models that unify many prior approaches for URA and PRA. The
motivating factor is that attributes of various RBAC entities such as admin
users, regular users and permissions can be used to administer URA and PRA in a
highly flexible manner. We develop an attribute-based URA model called AURA and
an attribute-based PRA model called ARPA. We demonstrate that AURA and ARPA can
express and unify many prior URA and PRA models.
","Jiwan Ninglekhu|Ram Krishnan","","http://arxiv.org/abs/1706.03171v2","http://arxiv.org/pdf/1706.03171v2","","32 Pages, 1 Figure A subset of this article submitting to IEEE CIC
  2017","","","cs.CR","cs.CR"
"826","1706.03206v1","2017-06-10 09:00:51","2017-06-10 09:00:51","Analysis of Anomalies in the Internet Traffic Observed at the Campus
  Network Gateway","  A considerable portion of the machine learning literature applied to
intrusion detection uses outdated data sets based on a simulated network with a
limited environment. Moreover, flaws usually appear in datasets and the way we
handle them may impact on measurements. Finally, the detection capacity of
intrusion detection is highly influenced by the system configuration. We focus
on a topic rarely investigated: the characterization of anomalies in a large
network environment. Intrusion Detection System (IDS) are used to detect
exploits or other attacks that raise alarms. These anomalous events usually
receive less attention than attack alarms, causing them to be frequently
overlooked by security administrators. However, the observation of this
activity contributes to understand the traffic network characteristics. On one
hand, abnormal behaviors may be legitimate, e.g., misinterpreted protocols or
malfunctioning network equipment, but on the other hand an attacker may
intentionally craft packets to introduce anomalies to evade monitoring systems.
Anomalies found in operational network environments may indicate cases of
evasion attacks, application bugs, and a wide variety of factors that highly
influence intrusion detection performance. This study explores the nature of
anomalies found in U-Tokyo Network using cooperatively Bro and Snort IDS among
other resources. We analyze 6.5 TB of compressed binary tcpdump data
representing 12 hours of network traffic. Our major contributions can be
summarized in: 1) reporting the anomalies observed in real, up-to-date traffic
from a large academic network environment, and documenting problems in research
that may lead to wrong results due to misinterpretations of data or
misconfigurations in software; 2) assessing the quality of data by analyzing
the potential and the real problems in the capture process.
","Veronica del Carmen Estrada","","http://arxiv.org/abs/1706.03206v1","http://arxiv.org/pdf/1706.03206v1","","Master Thesis, January 14th 2011, Graduate School of
  Interdisciplinary Information Studies, University of Tokyo","","","cs.CR","cs.CR"
"827","1706.03446v2","2017-06-12 03:03:15","2018-02-24 01:41:18","Deep EHR: A Survey of Recent Advances in Deep Learning Techniques for
  Electronic Health Record (EHR) Analysis","  The past decade has seen an explosion in the amount of digital information
stored in electronic health records (EHR). While primarily designed for
archiving patient clinical information and administrative healthcare tasks,
many researchers have found secondary use of these records for various clinical
informatics tasks. Over the same period, the machine learning community has
seen widespread advances in deep learning techniques, which also have been
successfully applied to the vast amount of EHR data. In this paper, we review
these deep EHR systems, examining architectures, technical aspects, and
clinical applications. We also identify shortcomings of current techniques and
discuss avenues of future research for EHR-based deep learning.
","Benjamin Shickel|Patrick Tighe|Azra Bihorac|Parisa Rashidi","","http://arxiv.org/abs/1706.03446v2","http://arxiv.org/pdf/1706.03446v2","http://dx.doi.org/10.1109/JBHI.2017.2767063","Accepted for publication with Journal of Biomedical and Health
  Informatics: http://ieeexplore.ieee.org/abstract/document/8086133/","","10.1109/JBHI.2017.2767063","cs.LG","cs.LG|stat.ML"
"828","1706.04345v1","2017-06-14 07:49:49","2017-06-14 07:49:49","Towards Adaptive Resilience in High Performance Computing","  Failure rates in high performance computers rapidly increase due to the
growth in system size and complexity. Hence, failures became the norm rather
than the exception. Different approaches on high performance computing (HPC)
systems have been introduced, to prevent failures (e. g., redundancy) or at
least minimize their impacts (e. g., checkpoint and restart). In most cases,
when these approaches are employed to increase the resilience of certain parts
of a system, energy consumption rapidly increases, or performance significantly
degrades. To address this challenge, we propose on-demand resilience as an
approach to achieve adaptive resilience in HPC systems. In this work, the HPC
system is considered in its entirety and resilience mechanisms such as
checkpointing, isolation, and migration, are activated on-demand. Using the
proposed approach, the unavoidable increase in total energy consumption and
system performance degradation is decreased compared to the typical
checkpoint/restart and redundant resilience mechanisms. Our work aims to
mitigate a large number of failures occurring at various layers in the system,
to prevent their propagation, and to minimize their impact, all of this in an
energy-saving manner. In the case of failures that are estimated to occur but
cannot be mitigated using the proposed on-demand resilience approach, the
system administrators will be notified in view of performing further
investigations into the causes of these failures and their impacts.
","Siavash Ghiasvand|Florina M. Ciorba","","http://arxiv.org/abs/1706.04345v1","http://arxiv.org/pdf/1706.04345v1","","2 pages, to be published in Proceedings of the Work in Progress
  Session held in connection with the 25th EUROMICRO International Conference
  on Parallel, Distributed and Network-based Processing, PDP 2017","","","cs.DC","cs.DC|cs.PF|C.1.4; C.2.4; C.4"
"829","1706.05272v1","2017-06-16 13:36:04","2017-06-16 13:36:04","Declarative Modeling for Building a Cloud Federation and Cloud
  Applications","  The paper illustrates how we built a federated cloud computing platform
dedicated to the Italian research community. Building a cloud platform is a
daunting task, that requires coordinating the deployment of many services,
interrelated and dependent on each other. Provisioning, servicing and
maintaining the platform must be automated. For our deployment, we chose a
declarative modeling tool, that allows describing the parts that compose the
system and their relations of supplier/consumer of specific interfaces. The
tool arranges the steps to bring the deployment to convergence by transforming
the state of the system until it reaches a configuration that satisfies all
constraints. We chose a declarative service modeling approach for orchestrating
both the deployment of the platform by the administrators and the deployment of
applications by users. The cloud platform has been designed so that it can be
managed by this kind of automation, facilitating the deployment of federated
regions by anyone wishing to join and to contribute resources to the
federation. Federated resources are integrated into a single cloud platform
available to any user of the federation. The federation can also seamlessly
include public clouds. We describe the architectural choices, how we adapted
the OpenStack basic facilities to the needs of a federation of multiple
independent organizations, how we control resource allocation according to
committed plans and correspondingly how we handle accounting and billing of
resource usage. Besides providing traditional IaaS services, the cloud supports
self-service deployment of cloud applications. The cloud thus addresses the
long tail of science, allowing researchers of any discipline, without expertise
in system or cloud administration, to deploy applications readily available for
their perusal.
","Giuseppe Attardi|Alex Barchiesi|Alberto Colla|Fulvio Galeazzi|Giovanni Marzulli|Mario Reale","","http://arxiv.org/abs/1706.05272v1","http://arxiv.org/pdf/1706.05272v1","","","","","cs.DC","cs.DC|C.2.4"
"830","1706.06191v1","2017-06-19 21:59:24","2017-06-19 21:59:24","A flexible multidimensional rectangular mesh administration and
  refinement technique with application in cancer invasion models","  We present a mesh-structure data administration technique for the bookkeeping
of adaptive mesh refinement, in particular h-refinement, of rectangular
parallelepiped meshes.
  Our technique is a unified approach for 1-, 2- and 3D domains, that can be
extended to even higher dimensions. It is easy to use and avoids the traversing
of the connectivity graph for the ancestry. Due to the rectangular structure of
the mesh, the identification of the siblings and the neighboring cells is also
greatly simplified. It is particularly designed for smooth meshes, and uses
their smoothness dynamically in the matrix operations. It has a small memory
footprint that makes it affordable for a wide range of mesh resolutions over a
large class of problems.
  We present three applications of this technique; most notably, the first
application of an h-refinement a 2D cancer growth and invasion problem.
","Niklas Kolbe|Nikolaos Sfakianakis","","http://arxiv.org/abs/1706.06191v1","http://arxiv.org/pdf/1706.06191v1","","","","","math.NA","math.NA"
"831","1706.08620v1","2017-06-26 22:53:34","2017-06-26 22:53:34","Viral infection model with diffusion and state-dependent delay:
  stability of classical solutions","  A class of reaction-diffusion virus dynamics models with intracellular
state-dependent delay and a general non-linear infection rate functional
response is investigated. We are interested in classical solutions with
Lipschitz in-time initial functions which are adequate to the discontinuous
change of parameters due to, for example, drug administration. The Lyapunov
functions technique is used to analyse stability of interior infection
equilibria which describe the cases of a chronic disease.
","Alexander Rezounenko","","http://arxiv.org/abs/1706.08620v1","http://arxiv.org/pdf/1706.08620v1","http://dx.doi.org/10.3934/dcdsb.2018143","17 pages","Discrete and Continuous Dynamical Systems - Series B, Volume 23,
  Issue 3, May 2018, Pages 1091-1105","10.3934/dcdsb.2018143","math.DS","math.DS|math.AP|93C23, 34K20, 35K57, 97M60"
"832","1706.09481v1","2017-06-28 20:48:25","2017-06-28 20:48:25","A Markov decision process approach to optimizing cancer therapy using
  multiple modalities","  There are several different modalities, e.g., surgery, chemotherapy, and
radiotherapy, that are currently used to treat cancer. It is common practice to
use a combination of these modalities to maximize clinical outcomes, which are
often measured by a balance between maximizing tumor damage and minimizing
normal tissue side effects due to treatment. However, multi-modality treatment
policies are mostly empirical in current practice, and are therefore subject to
individual clinicians' experiences and intuition. We present a novel
formulation of optimal multi-modality cancer management using a finite-horizon
Markov decision process approach. Specifically, at each decision epoch, the
clinician chooses an optimal treatment modality based on the patient's observed
state, which we define as a combination of tumor progression and normal tissue
side effect. Treatment modalities are categorized as (1) Type 1, which has a
high risk and high reward, but is restricted in the frequency of administration
during a treatment course, (2) Type 2, which has a lower risk and lower reward
than Type 1, but may be repeated without restriction, and (3) Type 3, no
treatment (surveillance), which has the possibility of reducing normal tissue
side effect at the risk of worsening tumor progression. Numerical simulations
using various intuitive, concave reward functions show the structural insights
of optimal policies and demonstrate the potential applications of using a
rigorous approach to optimizing multi-modality cancer management.
","Kelsey Maass|Minsun Kim","","http://arxiv.org/abs/1706.09481v1","http://arxiv.org/pdf/1706.09481v1","http://dx.doi.org/10.1093/imammb/dqz004","15 pages, 11 figures","","10.1093/imammb/dqz004","math.OC","math.OC|physics.med-ph"
"833","1706.10274v2","2017-06-30 17:04:35","2018-12-24 19:06:01","A Model for Attribute Based Role-Role Assignment (ARRA)","  Administrative Role Based Access Control (ARBAC) models specify how to manage
user-role assignments (URA), permission-role assignments (PRA), and role-role
assignments (RRA). Many approaches have been proposed in the literature for
URA, PRA, and RRA. In this paper, we propose a model for attribute-based
role-role assignment (ARRA), a novel way to unify prior RRA approaches. We
leverage the idea that attributes of various RBAC entities such as admin users
and regular roles can be used to administer RRA in a highly flexible manner. We
demonstrate that ARRA can express and unify prior RRA models.
","Jiwan Ninglekhu|Ram Krishnan","","http://arxiv.org/abs/1706.10274v2","http://arxiv.org/pdf/1706.10274v2","","1 Table, 1 Figure This paper was published on ""Secure Knowledge
  Management Workshop 2017, St. Pete, FL, Oct 6-7 2017"" Website:
  https://www.csiac.org/event/secure-knowledge-management-workshop-2017-skm-2017/","","","cs.CR","cs.CR"
"834","1707.02901v1","2017-07-10 15:14:25","2017-07-10 15:14:25","Diclofenac sodium ion exchange resin complex loaded melt cast films for
  sustained release ocular delivery","  The goal of the present study is to develop polymeric matrix films loaded
with a combination of free diclofenac sodium (DFSfree) and DFS:Ion exchange
resin complexes (DFS:IR) for immediate and sustained release profiles,
respectively. Effect of ratio of DFS and IR on the DFS:IR complexation
efficiency was studied using batch processing. DFS:IR complex, DFSfree, or a
combination of DFSfree+DFS:IR loaded matrix films were prepared by melt-cast
technology. DFS content was 20% w/w in these matrix films. In vitro
transcorneal permeability from the film formulations were compared against DFS
solution, using a side-by-side diffusion apparatus, over a 6 h period. Ocular
disposition of DFS from the solution, films and corresponding suspensions were
evaluated in conscious New Zealand albino rabbits, 4 h and 8 h post-topical
administration. All in vivo studies were carried out as per the University of
Mississippi IACUC approved protocol. Complexation efficiency of DFS:IR was
found to be 99% with a 1:1 ratio of DFS:IR. DFS release from DFS:IR suspension
and the film were best-fit to a Higuchi model. In vitro transcorneal flux with
the DFSfree+DFS:IR(1:1)(1 + 1) was twice that of only DFS:IR(1:1) film. In
vivo, DFS solution and DFS:IR(1:1) suspension formulations were not able to
maintain therapeutic DFS levels in the aqueous humor (AH). Both DFSfree and
DFSfree+DFS:IR(1:1)(3 + 1) loaded matrix films were able to achieve and
maintain high DFS concentrations in the AH, but elimination of DFS from the
ocular tissues was much faster with the DFSfree formulation. DFSfree+DFS:IR
combination loaded matrix films were able to deliver and maintain therapeutic
DFS concentrations in the anterior ocular chamber for up to 8 h. Thus, free
drug/IR complex loaded matrix films could be a potential topical ocular
delivery platform for achieving immediate and sustained release
characteristics.
","Goutham R. Adelli|Sai Prachetan Balguri|Prakash Bhagav|Vijayasankar Raman|Soumyajit Majumdar","","http://arxiv.org/abs/1707.02901v1","http://arxiv.org/pdf/1707.02901v1","","","","","physics.bio-ph","physics.bio-ph"
"835","1707.03341v1","2017-07-11 16:12:08","2017-07-11 16:12:08","Use of Docker for deployment and testing of astronomy software","  We describe preliminary investigations of using Docker for the deployment and
testing of astronomy software. Docker is a relatively new containerisation
technology that is developing rapidly and being adopted across a range of
domains. It is based upon virtualization at operating system level, which
presents many advantages in comparison to the more traditional hardware
virtualization that underpins most cloud computing infrastructure today. A
particular strength of Docker is its simple format for describing and managing
software containers, which has benefits for software developers, system
administrators and end users.
  We report on our experiences from two projects -- a simple activity to
demonstrate how Docker works, and a more elaborate set of services that
demonstrates more of its capabilities and what they can achieve within an
astronomical context -- and include an account of how we solved problems
through interaction with Docker's very active open source development
community, which is currently the key to the most effective use of this
rapidly-changing technology.
","D. Morris|S. Voutsinas|N. C. Hambly|R. G. Mann","","http://arxiv.org/abs/1707.03341v1","http://arxiv.org/pdf/1707.03341v1","","29 pages, 9 figures, accepted for publication in Astronomy and
  Computing, ref ASCOM199","","","cs.SE","cs.SE|astro-ph.IM"
"836","1707.06210v1","2017-07-12 02:30:05","2017-07-12 02:30:05","College Student Retention: When Do We Losing Them?","  One of the long term goals of any college or university is increasing the
student retention. The negative impact of student dropout are clear to
students, parents, universities and society. The positive effect of decreasing
student attrition is also self-evident including higher chance of having a
better career and higher standard of life for college graduate. In view of
these reasons, directors in higher education feel increasingly pressurized to
outline and implement strategies to increase student retention. In this paper,
we provide a detailed analysis of the student attrition problem and use
statistical methods to predict when students are going to dropout from school
using real case data. Our work has a number of advantages with the potential of
being employed by higher education administrator of universities. We take
advantage of multiple kinds of information about different aspects of student's
characteristic and efficiently utilize them to make a personalized decision
about the risk of dropout for a particular student.
","Mehrdad J. Bani|Mina Haji","","http://arxiv.org/abs/1707.06210v1","http://arxiv.org/pdf/1707.06210v1","","Submitted to Proceedings of the World Congress on Engineering and
  Computer Science 2017","","","cs.CY","cs.CY"
"837","1709.06516v1","2017-07-19 10:49:16","2017-07-19 10:49:16","Model-driven Engineering IDE for Quality Assessment of Data-intensive
  Applications","  This article introduces a model-driven engineering (MDE) integrated
development environment (IDE) for Data-Intensive Cloud Applications (DIA) with
iterative quality enhancements. As part of the H2020 DICE project (ICT-9-2014,
id 644869), a framework is being constructed and it is composed of a set of
tools developed to support a new MDE methodology. One of these tools is the IDE
which acts as the front-end of the methodology and plays a pivotal role in
integrating the other tools of the framework. The IDE enables designers to
produce from the architectural structure of the general application along with
their properties and QoS/QoD annotations up to the deployment model.
Administrators, quality assurance engineers or software architects may also run
and examine the output of the design and analysis tools in addition to the
designer in order to assess the DIA quality in an iterative process.
","Marc Gil|Christophe Joubert|Ismael Torres","","http://arxiv.org/abs/1709.06516v1","http://arxiv.org/pdf/1709.06516v1","http://dx.doi.org/10.1145/3053600.3053633","","","10.1145/3053600.3053633","cs.SE","cs.SE"
"838","1707.06505v1","2017-07-20 13:51:41","2017-07-20 13:51:41","Design and Implementation Aspects of Mobile Derived Identities","  With the ongoing digitalisation of our everyday tasks, more and more
eGovernment services make it possible for citizens to take care of their
administrative obligations online. This type of services requires a certain
assurance level for user authentication. To meet these requirements, a digital
identity issued to the citizen is essential. Nowadays, due to the widespread
use of smartphones, mobile user authentication is often favoured. This
naturally supports two-factor authentication schemes (2FA). We use the term
mobile derived identity to stress two aspects: a) the identity is enabled for
mobile usage and b) the identity is somehow derived from a physical or digital
proof of identity. This work reviews 21 systems that support mobile derived
identities. One subset of the considered systems is already in place (public or
private sector in Europe), another subset is subject to research. Our goal is
to identify prevalent design and implementation aspects for these systems in
order to gain a better understanding on best practises and common views on
mobile derived identities. We found, that research prefers storing identity
data on the mobile device itself whereas real world systems usually rely on
cloud storage. 2FA is common in both worlds, however biometrics as second
factor is the exception.
","Daniel Trader|Alexander Zeier|Andreas Heinemann","","http://arxiv.org/abs/1707.06505v1","http://arxiv.org/pdf/1707.06505v1","","","","","cs.CY","cs.CY"
"839","1707.07796v3","2017-07-25 02:40:44","2018-02-12 13:38:54","Stationary waves and slowly moving features in the night upper clouds of
  Venus","  At the cloud top level of Venus (65-70 km altitude) the atmosphere rotates 60
times faster than the underlying surface, a phenomenon known as superrotation.
Whereas on Venus's dayside the cloud top motions are well determined and Venus
general circulation models predict a mean zonal flow at the upper clouds
similar on both day and nightside, the nightside circulation remains poorly
studied except for the polar region. Here we report global measurements of the
nightside circulation at the upper cloud level. We tracked individual features
in thermal emission images at 3.8 and 5.0 $\mathrm{\mu m}$ obtained between
2006 and 2008 by the Visible and Infrared Thermal Imaging Spectrometer
(VIRTIS-M) onboard Venus Express and in 2015 by ground-based measurements with
the Medium-Resolution 0.8-5.5 Micron Spectrograph and Imager (SpeX) at the
National Aeronautics and Space Administration Infrared Telescope Facility
(NASA/IRTF). The zonal motions range from -110 to -60 m s$^{-1}$, consistent
with those found for the dayside but with larger dispersion. Slow motions (-50
to -20 m s$^{-1}$) were also found and remain unexplained. In addition,
abundant stationary wave patterns with zonal speeds from -10 to +10 m s$^{-1}$
dominate the night upper clouds and concentrate over the regions of higher
surface elevation.
","J. Peralta|R. Hueso|A. Sanchez-Lavega|Y. J. Lee|A. Garcia-Munoz|T. Kouyama|H. Sagawa|T. M. Sato|G. Piccioni|S. Tellmann|T. Imamura|T. Satoh","","http://arxiv.org/abs/1707.07796v3","http://arxiv.org/pdf/1707.07796v3","http://dx.doi.org/10.1038/s41550-017-0187","15 pages, 4 figures, 6 supplementary figures","Nat. Astron. 1, 0187 (2017)","10.1038/s41550-017-0187","astro-ph.EP","astro-ph.EP|physics.ao-ph|85A20"
"840","1707.08342v1","2017-07-26 09:49:21","2017-07-26 09:49:21","Declarative Sequential Pattern Mining of Care Pathways","  Sequential pattern mining algorithms are widely used to explore care pathways
database, but they generate a deluge of patterns, mostly redundant or useless.
Clinicians need tools to express complex mining queries in order to generate
less but more significant patterns. These algorithms are not versatile enough
to answer complex clinician queries. This article proposes to apply a
declarative pattern mining approach based on Answer Set Programming paradigm.
It is exemplified by a pharmaco-epidemiological study investigating the
possible association between hospitalization for seizure and antiepileptic drug
switch from a french medico-administrative database.
","Thomas Guyet|Andre Happe|Yann Dauxais","LACODAM|UR1|UR1","http://arxiv.org/abs/1707.08342v1","http://arxiv.org/pdf/1707.08342v1","http://dx.doi.org/10.1002/pds.3879","","Conference on Artificial Intelligence in Medicine in Europe, Jun
  2017, Vienna, Austria. 24, pp.1161 - 266, 2017","10.1002/pds.3879","cs.AI","cs.AI"
"841","1707.09300v2","2017-07-28 16:05:17","2018-01-16 15:56:42","From 4G to 5G: Self-organized Network Management meets Machine Learning","  In this paper, we provide an analysis of self-organized network management,
with an end-to-end perspective of the network. Self-organization as applied to
cellular networks is usually referred to Self-organizing Networks (SONs), and
it is a key driver for improving Operations, Administration, and Maintenance
(OAM) activities. SON aims at reducing the cost of installation and management
of 4G and future 5G networks, by simplifying operational tasks through the
capability to configure, optimize and heal itself. To satisfy 5G network
management requirements, this autonomous management vision has to be extended
to the end to end network. In literature and also in some instances of products
available in the market, Machine Learning (ML) has been identified as the key
tool to implement autonomous adaptability and take advantage of experience when
making decisions. In this paper, we survey how network management can
significantly benefit from ML solutions. We review and provide the basic
concepts and taxonomy for SON, network management and ML. We analyse the
available state of the art in the literature, standardization, and in the
market. We pay special attention to 3rd Generation Partnership Project (3GPP)
evolution in the area of network management and to the data that can be
extracted from 3GPP networks, in order to gain knowledge and experience in how
the network is working, and improve network performance in a proactive way.
Finally, we go through the main challenges associated with this line of
research, in both 4G and in what 5G is getting designed, while identifying new
directions for research.
","Jessica Moysen|Lorenza Giupponi","","http://arxiv.org/abs/1707.09300v2","http://arxiv.org/pdf/1707.09300v2","","23 pages, 3 figures, Survey","","","cs.NI","cs.NI"
"842","1707.09421v1","2017-07-28 21:21:28","2017-07-28 21:21:28","Scaling of the Detonation Product State with Reactant Kinetic Energy","  This submissions has been withdrawn by arXiv administrators because the
submitter did not have the right to agree to our license.
","Scott I. Jackson","","http://arxiv.org/abs/1707.09421v1","http://arxiv.org/pdf/1707.09421v1","","","","","physics.flu-dyn","physics.flu-dyn|cond-mat.soft"
"843","1707.09850v1","2017-07-31 14:02:15","2017-07-31 14:02:15","CODE-RADE - Community Infrastructure for the Delivery of Physics
  Applications","  Scientific computing can in some sense be distilled to the execution of an
application - or rather sets of applications which are combined into complex
workflows. Due to the complexity and number both of scientific packages as well
as computing platforms, delivering these applications to end users has always
been a significant challenge through the grid era, and remains so in the cloud
era. In this contribution we describe a platform for user-driven, continuous
integration and delivery of research applications in a distributed environment
- project CODE-RADE. Starting with 6 hypotheses describing the problem at hand,
we put forward technical and social solutions to these. Combining widely-used
and thoroughly-tested tools, we show how it is possible to manage the
dependencies and configurations of a wide range of scientific applications, in
an almost fully-automated way. The CODE-RADE platform is a means for developing
trust between public computing and data infrastructures on the one hand and
various developer and scientific communities on the other hand. Predefined
integration tests are specified for any new application, allowing the system to
be user-driven. This greatly accelerates time-to-production for scientific
applications, while reducing the workload for administrators of HPC, grid and
cloud installations. Finally, we will give some insight into how this platform
could be extended to address issues of reproducibility and collaboration in
scientific research in Africa.
","Bruce Becker|Sean Murray","","http://arxiv.org/abs/1707.09850v1","http://arxiv.org/pdf/1707.09850v1","","Article submitted to SAIP 2016","","","cs.DC","cs.DC"
"844","1708.02022v1","2017-08-07 07:51:48","2017-08-07 07:51:48","Active galactic nuclei in the era of the Imaging X-ray Polarimetry
  Explorer","  In about four years, the National Aeronautics and Space Administration (NASA)
will launch a small explorer mission named the Imaging X-ray Polarimetry
Explorer (IXPE). IXPE is a satellite dedicated to the observation of X-ray
polarization from bright astronomical sources in the 2-8 keV energy range.
Using Gas Pixel Detectors (GPD), the mission will allow for the first time to
acquire X-ray polarimetric imaging and spectroscopy of about a hundred of
sources during its first two years of operation. Among them are the most
powerful sources of light in the Universe: active galactic nuclei (AGN). In
this proceedings, we summarize the scientific exploration we aim to achieve in
the field of AGN using IXPE, describing the main discoveries that this new
generation of X-ray polarimeters will be able to make. Among these discoveries,
we expect to detect indisputable signatures of strong gravity, quantifying the
amount and importance of scattering on distant cold material onto the iron
K_alpha line observed at 6.4 keV. IXPE will also be able to probe the
morphology of parsec-scale AGN regions, the magnetic field strength and
direction in quasar jets, and, among the most important results, deliver an
independent measurement of the spin of black holes.
","F. Marin|M. C. Weisskopf","","http://arxiv.org/abs/1708.02022v1","http://arxiv.org/pdf/1708.02022v1","","5 pages, 3 figures. To appear in the Proceedings of the French
  Society of Astronomy & Astrophysics (SF2A)","","","astro-ph.IM","astro-ph.IM|astro-ph.HE|85-06|J.2"
"845","1708.02432v1","2017-08-08 09:56:08","2017-08-08 09:56:08","Migration patterns across the life course of families: Gender
  differences and proximity with parents and siblings in Finland","  Family members' life course tendencies to remain geographically close to each
other or to migrate due to education or job opportunities have been studied
relatively little. Here we investigate migration patterns of parents and their
children between 19 administrative regions of Finland from 1970 to 2012. Using
the FinnFamily register dataset of 60 000 index individuals and their family
members, we investigate the patterns of regional migration and regional
co-residence of parents and their children. Specifically, we analyse how likely
it is for children to reside in the same region as their parents at any
specific age, whether parents and children who live in different regions are
likely to reunite, and whether siblings function as regional attractors to each
other. Results show an intense regional migration of people to the capital
area. The migration propensity of individuals is high in early childhood and
peaks in early adulthood. About two thirds of Finnish children live in the same
region as their parents throughout their adult lives. Females show higher
propensity to migrate than males, since daughters move away from their parents
earlier and with a higher rate than sons do. The propensity for two full
sibling brothers to be in the same region is higher than that for other types
of sibling dyads. We conclude that family members serve as important
geographical attractors to each other through the life course and that family
attraction is stronger for sons and brothers than for daughters and sisters in
contemporary Finland.
","Asim Ghosh|Venla Berg|Kunal Bhattacharya|Daniel Monsivais|Janos Kertesz|Kimmo Kaski|Anna Rotkirch","","http://arxiv.org/abs/1708.02432v1","http://arxiv.org/pdf/1708.02432v1","","17 pages (14 from the main text and 3 from the SI), 10 figures in the
  main text, 2 in the SI. 1 table in the main text, 3 in the SI","","","physics.soc-ph","physics.soc-ph"
"846","1708.09404v1","2017-08-09 19:33:03","2017-08-09 19:33:03","Success Criteria For Implementing Technology in Special Education: a
  Case Study","  The Kingdom of Saudi Arabia (KSA) has made a large investment in deploying
technology to develop the infrastructure and resources for special education.
The aims of the present research were to find out the rate of return of these
investments in terms of success and, based on the findings, to propose a
framework for success criteria. To achieve these aims, a mixed
methodology-based research was conducted. Our study found that the use of
technology in special education could not reach the desired level of
implementation. We found that various success criteria such as professional
experience and technology skills of special educators, administrative support,
assistive hardware issues and assistive software issues, pedagogical issues,
and teaching style are the key influencing factors of the implementation
process.
","Mohammed Fakrudeen|Mahdi H. Miraz|Peter Excell","","http://arxiv.org/abs/1708.09404v1","http://arxiv.org/pdf/1708.09404v1","http://dx.doi.org/10.13140/2.1.2650.6567","","the proceedings of the fifth international conference on Internet
  Technologies and Applications (ITA 13) held at Glynd\^wr University in
  Wrexham, UK, 10-13 September 2013,ISBN-10: 0-946881-81-2, ISBN-13:
  978-0-946881-81-9, pp. 226-235","10.13140/2.1.2650.6567","cs.CY","cs.CY|cs.HC"
"847","1708.03765v2","2017-08-12 11:13:14","2018-03-29 08:01:49","Spatial heterogeneity analyses identify limitations of epidemic alert
  systems: Monitoring influenza-like illness in France","  Surveillance data serving for epidemic alert systems are typically fully
aggregated in space. However, epidemics may be spatially heterogeneous,
undergoing distinct dynamics in distinct regions of the surveillance area. We
unveil this in retrospective analyses by classifying incidence time series. We
use Pearson correlation to quantify the similarity between local time series
and then classify them using modularity maximization. The surveillance area is
thus divided into regions with different incidence patterns. We analyzed 31
years of data on influenza-like-illness from the French system Sentinelles and
found spatial heterogeneity in 19/31 influenza seasons. However, distinct
epidemic regions could be identified only 4-5 weeks after the nationwide alert.
The impact of spatial heterogeneity on influenza epidemiology was complex.
First, when the nationwide alert was triggered, 32-41% of the administrative
regions were experiencing an epidemic, while the others were not. Second, the
nationwide alert was timely for the whole surveillance area, but, subsequently,
regions experienced distinct epidemic dynamics. Third, the epidemic dynamics
were homogeneous in space. Spatial heterogeneity analyses can provide the
timing of the epidemic peak and finish, in various regions, to tailor disease
monitoring and control.
","Pavel Polyakov|Cecile Souty|Pierre-Yves Boelle|Romulus Breban","","http://arxiv.org/abs/1708.03765v2","http://arxiv.org/pdf/1708.03765v2","","24 pages, 1 table, 4 figures","","","q-bio.PE","q-bio.PE"
"848","1708.05401v2","2017-08-17 18:15:52","2017-08-30 13:32:14","Deformable Modeling for Human Body Acquired from Depth Sensors","  This paper presents a novel approach to reconstruct complete 3D deformable
models over time by a single depth camera. These are the steps employed for
deforming objects from single depth camera. The partial surfaces reconstructed
from various times of capture are assembled together to form a complete 3D
surface. A mesh warping algorithm is used to align different partial surfaces
based on linear mesh deformation. A volumetric method is then applied to
combine partial surfaces, fix missing holes and smooth alignment errors.
","Vamshhi Pavan Kumar Varma Vegeshna","","http://arxiv.org/abs/1708.05401v2","http://arxiv.org/pdf/1708.05401v2","","arXiv admin note: submission has been withdrawn by arXiv
  administrators due to inappropriate text reuse from external sources","","","cs.CV","cs.CV"
"849","1708.05477v2","2017-08-18 01:06:53","2018-07-07 00:34:51","WedgeTail: An Intrusion Prevention System for the Data Plane of Software
  Defined Networks","  Networks are vulnerable to disruptions caused by malicious forwarding
devices. The situation is likely to worsen in Software Defined Networks (SDNs)
with the incompatibility of existing solutions, use of programmable soft
switches and the potential of bringing down an entire network through
compromised forwarding devices. In this paper, we present WedgeTail, an
Intrusion Prevention System (IPS) designed to secure the SDN data plane.
WedgeTail regards forwarding devices as points within a geometric space and
stores the path packets take when traversing the network as trajectories. To be
efficient, it prioritizes forwarding devices before inspection using an
unsupervised trajectory-based sampling mechanism. For each of the forwarding
device, WedgeTail computes the expected and actual trajectories of packets and
`hunts' for any forwarding device not processing packets as expected. Compared
to related work, WedgeTail is also capable of distinguishing between malicious
actions such as packet drop and generation. Moreover, WedgeTail employs a
radically different methodology that enables detecting threats autonomously. In
fact, it has no reliance on pre-defined rules by an administrator and may be
easily imported to protect SDN networks with different setups, forwarding
devices, and controllers. We have evaluated WedgeTail in simulated
environments, and it has been capable of detecting and responding to all
implanted malicious forwarding devices within a reasonable time-frame. We
report on the design, implementation, and evaluation of WedgeTail in this
manuscript.
","Arash Shaghaghi|Mohamed Ali Kaafar|Sanjay Jha","","http://arxiv.org/abs/1708.05477v2","http://arxiv.org/pdf/1708.05477v2","http://dx.doi.org/10.1145/3052973.3053039","Accepted to ASIACCS - Final version, Proceedings of the 2017 ACM on
  Asia Conference on Computer and Communications Security. ACM, 2017","","10.1145/3052973.3053039","cs.CR","cs.CR"
"850","1708.05926v1","2017-08-20 03:26:40","2017-08-20 03:26:40","Tamper-Evident Complex Genomic Networks","  Networks are important storage data structures now used to store personal
information of individuals around the globe. With the advent of personal genome
sequencing, networks are going to be used to store personal genomic sequencing
of people. In contrast to social media networks, the importance of
relationships in this genomic network is extremely significant. Losing
connections between individuals thus implies losing relationship information
(E.g. father or son etc.). There currently exists a considerably serious
problem in the current approach to storing network data. Simply stated, network
data is not tamper-evident. In other words, if some links or nodes were
changed/removed/added by a malicious attacker, it would be impossible for the
administrator to detect such changes. While, in the current age of social media
networks, change in node characteristics and links can be bad in terms of
relationships, in the case of networks for storing personal genomes, the
results could be truly devastating. Here we present a scheme for building
tamper-evident networks using a combination of Cryptographic and Ego-based
Network analytic methods. Using actual published data-sets, we also demonstrate
the utility and validity of the scheme besides demonstrating its working in
various possible scenarios of usage. Results from the extensive experiments
demonstrate the validity of the proposed approach.
","Komal Batool|Muaz A. Niazi","","http://arxiv.org/abs/1708.05926v1","http://arxiv.org/pdf/1708.05926v1","","14 pages, 8 figures","","","cs.SI","cs.SI|cs.CR|cs.DB|cs.NI|F.2; G.2.2; J.3; K.4.4; K.6.5; C.2.0; C.2.1; J.4"
"851","1708.06546v2","2017-08-22 09:18:39","2018-02-12 16:42:22","Probabilistic Tri-level Market Models for Demand Side Management in
  Power Distribution Systems","  The emerging interest in deployment of renewable energy resources (RESs) in
smart system represents a great challenge to both system planners and owners of
Microgrids (MGs) operators. In this regard, we propose a Tri-level power market
models for designing demand side management systems to match power supply and
shape renewable power generations. We characterize the resulting equilibria in
competitive as well as oligopolistic market, and propose distributed demand
response algorithms to achieve the equilibria. The models serve as a starting
point to include the appliance-level details and constraints for designing
practical demand response schemes for smart power grids. In order to show the
usefulness of proposed model, two various case studies are considered in this
paper: uncoordinated and coordinated load demand. A novel mathematical model is
further developed whereby the behavior of RES, in response to different
electricity prices owing to demand response programs, is considered in
generating the energy consumption of MGs.
","Mohammd Hamdi","","http://arxiv.org/abs/1708.06546v2","http://arxiv.org/pdf/1708.06546v2","","arXiv admin note: submission has been withdrawn by arXiv
  administrators due to inappropriate text reuse from external sources","","","math.OC","math.OC|91|F.2.2"
"852","1708.06884v1","2017-08-23 04:41:47","2017-08-23 04:41:47","Big Data Meets HPC Log Analytics: Scalable Approach to Understanding
  Systems at Extreme Scale","  Today's high-performance computing (HPC) systems are heavily instrumented,
generating logs containing information about abnormal events, such as critical
conditions, faults, errors and failures, system resource utilization, and about
the resource usage of user applications. These logs, once fully analyzed and
correlated, can produce detailed information about the system health, root
causes of failures, and analyze an application's interactions with the system,
providing valuable insights to domain scientists and system administrators.
However, processing HPC logs requires a deep understanding of hardware and
software components at multiple layers of the system stack. Moreover, most log
data is unstructured and voluminous, making it more difficult for system users
and administrators to manually inspect the data. With rapid increases in the
scale and complexity of HPC systems, log data processing is becoming a big data
challenge. This paper introduces a HPC log data analytics framework that is
based on a distributed NoSQL database technology, which provides scalability
and high availability, and the Apache Spark framework for rapid in-memory
processing of the log data. The analytics framework enables the extraction of a
range of information about the system so that system administrators and end
users alike can obtain necessary insights for their specific needs. We describe
our experience with using this framework to glean insights from the log data
about system behavior from the Titan supercomputer at the Oak Ridge National
Laboratory.
","Byung H. Park|Saurabh Hukerikar|Ryan Adamson|Christian Engelmann","","http://arxiv.org/abs/1708.06884v1","http://arxiv.org/pdf/1708.06884v1","","IEEE Cluster 2017 at Workshop on Monitoring and Analysis for High
  Performance Computing Systems Plus Applications","","","cs.DC","cs.DC|cs.DB"
"853","1708.07569v1","2017-08-24 22:30:10","2017-08-24 22:30:10","Secure by default - the case of TLS","  Default configuration of various software applications often neglects
security objectives. We tested the default configuration of TLS in dozen web
and application servers. The results show that ""secure by default"" principle
should be adopted more broadly by developers and package maintainers. In
addition, system administrators cannot rely blindly on default security
options.
","Martin Stanek","","http://arxiv.org/abs/1708.07569v1","http://arxiv.org/pdf/1708.07569v1","","5 pages","","","cs.CR","cs.CR"
"854","1708.07987v2","2017-08-26 15:43:39","2017-08-30 13:32:04","Stereo Matching With Color-Weighted Correlation, Hierarchical Belief
  Propagation And Occlusion Handling","  In this paper, we contrive a stereo matching algorithm with careful handling
of disparity, discontinuity and occlusion. This algorithm works a worldwide
matching stereo model which is based on minimization of energy. The global
energy comprises two terms, firstly the data term and secondly the smoothness
term. The data term is approximated by a color-weighted correlation, then
refined in obstruct and low-texture areas in many applications of hierarchical
loopy belief propagation algorithm. The results during the experiment are
evaluated on the Middlebury data sets, showing that out algorithm is the top
performer among all the algorithms listed there
","Vamshhi Pavan Kumar Varma Vegeshna","","http://arxiv.org/abs/1708.07987v2","http://arxiv.org/pdf/1708.07987v2","","arXiv admin note: submission has been withdrawn by arXiv
  administrators due to inappropriate text reuse from external sources","","","cs.CV","cs.CV"
"855","1708.08228v1","2017-08-28 08:05:44","2017-08-28 08:05:44","Provably Secure Networks: Methodology and Toolset for Configuration
  Management","  Network administration is an inherently complex task, in particular with
regard to security. Using the Isabelle interactive proof assistant, we develop
two automated, formally verified tools which help uncovering and preventing
bugs in network-level access control configurations. Our first tool guides the
process of designing networks from scratch. Our second tool facilitates the
analysis of existing iptables configurations. Combined, the two form a powerful
toolset.
","Cornelius Diekmann","","http://arxiv.org/abs/1708.08228v1","http://arxiv.org/pdf/1708.08228v1","","PhD thesis","","","cs.NI","cs.NI|cs.CR"
"856","1708.08261v2","2017-08-28 10:08:19","2018-02-12 16:42:00","Congestion-Driven Transmission Expansion Planning Considering Wind Power
  Generation in Spot Markets","  The integration of a massive number of large-scale wind turbines brought
about urgent technical challenge to power transmission network operators in
terms of secure power supply and energy dispatching optimization. In this
paper, an optimal framework is proposed for transmission expansion planning in
a deregulated power market environment. The level of congestion in the network
is utilized as the driving signal for the need of network expansion. A
compromise between the congestion cost and the investment cost is used to
determine the optimal expansion scheme. The long-term network expansion problem
is formed as the decoupled combination of: 1) the master problem (minimization
of investment costs subject to investment constraints and the Benders cuts
generated by the operational problem (power pool) and 2) the operational
problem, whose solution provides congestion details and associated multipliers.
A proper power-pool model is developed and solved for congestion cost,
congestion revenue, and transmission shadow prices. Also, we proposed to use a
set of metrics to rate the effect of the expansion on the generators, demands,
and the system as a whole. The proposed model is applied to the Garver six-bus
system and to the IEEE 24-bus Reliability Test System.
","Mohammd Hamdi","","http://arxiv.org/abs/1708.08261v2","http://arxiv.org/pdf/1708.08261v2","","arXiv admin note: submission has been withdrawn by arXiv
  administrators due to inappropriate text reuse from external sources","","","math.OC","math.OC|60|F.2.2"
"857","1708.08435v2","2017-08-28 17:44:44","2018-05-29 22:56:59","Analyzing Query Performance and Attributing Blame for Contentions in a
  Cluster Computing Framework","  There are many approaches is use today to either prevent or minimize the
impact of inter-query interactions on a shared cluster. Despite these measures,
performance issues due to concurrent executions of mixed workloads still
prevail causing undue waiting times for queries. Analyzing these resource
interferences is thus critical in order to answer time sensitive questions like
'who is causing my query to slowdown' in a multi-tenant environment. More
importantly, dignosing whether the slowdown of a query is a result of resource
contentions caused by other queries or some other external factor can help an
admin narrow down the many possibilities of performance degradation. This
process of investigating the symptoms of resource contentions and attributing
blame to concurrent queries is non-trivial and tedious, and involves hours of
manually debugging through a cycle of query interactions.
  In this paper, we present ProtoXplore - a Proto or first system to eXplore
contentions, that helps administrators determine whether the blame for resource
bottlenecks can be attributed to concurrent queries, and uses a methodology
called Resource Acquire Time Penalty (RATP) to quantify this blame towards
contentious sources accurately. Further, ProtoXplore builds on the theory of
explanations and enables a step-wise deep exploration of various levels of
performance bottlenecks faced by a query during its execution using a
multi-level directed acyclic graph called ProtoGraph. Our experimental
evaluation uses ProtoXplore to analyze the interactions between TPC-DS queries
on Apache Spark to show how ProtoXplore provides explanations that help in
diagnosing contention related issues and better managing a changing mixed
workload in a shared cluster.
","Prajakta Kalmegh|Shivnath Babu|Sudeepa Roy","","http://arxiv.org/abs/1708.08435v2","http://arxiv.org/pdf/1708.08435v2","","","","","cs.DC","cs.DC|cs.DB"
"858","1709.02380v1","2017-09-06 18:42:54","2017-09-06 18:42:54","Independent Experimentation on the Activation of Deuterium-Loaded
  Materials by X-Ray Exposure","  In an attempt to replicate work performed elsewhere [1], we have searched for
x-ray induced nuclear activation of deuterated materials. Our first results,
reported in September 2015, showed no evidence of nuclear activation, contrary
to the results reported in [1]. The primary shortcoming of our first attempt,
however, was that the x-ray tube we used was capable of a maximum accelerating
potential of only 160kV, less than the 200kV used on the comparison test sample
from [1]. Thus, our results could not exclude processes initiated by x-rays
near 200 keV in energy. A second potential shortcoming was that our x-ray tube
did not have a microfocus beam, like the one used in [1]. Recently, we have
irradiated additional samples with an x-ray tube matching the specifications of
the tube used in [1]. By matching the test conditions with essentially
identical equipment, we have overcome both of the shortcomings of our previous
measurements. As before, we find no evidence of activation. The bulk of this
report, pages 2 through 35, contain the results and analysis from the first
round of testing. The extension to 200kV is found in the final appendix, namely
Appendix A5.
","Rob Davies","","http://arxiv.org/abs/1709.02380v1","http://arxiv.org/pdf/1709.02380v1","","The research was carried out at the Jet Propulsion Laboratory,
  California Institute of Technology, under a contract with the National
  Aeronautics and Space Administration","","","physics.ins-det","physics.ins-det|nucl-ex"
"859","1709.02209v1","2017-09-07 12:41:43","2017-09-07 12:41:43","Discovering Neighbor Devices in Computer Network: Development of CDP and
  LLDP Simulation Modules for OMNeT++","  The purpose of data-link layer discovery protocols is to provide the network
administrator with the current information (i.e., various Layer 2 and 3
parameters) about neighbor devices. These protocols are invaluable for network
monitoring, maintenance, and troubleshooting. However, they start to play an
important role in the operation of data-centers and other high-availability
networks. This paper outlines design, implementation and deployment of Cisco
Discovery Protocol and Link Layer Discovery Protocol simulation modules in
OMNeT++ simulator.
","Vladimir Vesely|Toma<U+0161> Rajca","","http://arxiv.org/abs/1709.02209v1","http://arxiv.org/pdf/1709.02209v1","","Published in: A. Foerster, A. Udugama, A. Koensgen, A. Virdis, M.
  Kirsche (Eds.), Proc. of the 4th OMNeT++ Community Summit, University of
  Bremen - Germany - September 7-8, 2017","","","cs.NI","cs.NI|I.6; I.6.5"
"860","1709.03309v1","2017-09-11 09:37:07","2017-09-11 09:37:07","Discriminant chronicles mining: Application to care pathways analytics","  Pharmaco-epidemiology (PE) is the study of uses and effects of drugs in well
defined populations. As medico-administrative databases cover a large part of
the population, they have become very interesting to carry PE studies. Such
databases provide longitudinal care pathways in real condition containing
timestamped care events, especially drug deliveries. Temporal pattern mining
becomes a strategic choice to gain valuable insights about drug uses. In this
paper we propose DCM, a new discriminant temporal pattern mining algorithm. It
extracts chronicle patterns that occur more in a studied population than in a
control population. We present results on the identification of possible
associations between hospitalizations for seizure and anti-epileptic drug
switches in care pathway of epileptic patients.
","Yann Dauxais|Thomas Guyet|David Gross-Amblard|Andre Happe","UR1, LACODAM|LACODAM|DRUID|","http://arxiv.org/abs/1709.03309v1","http://arxiv.org/pdf/1709.03309v1","","Artificial Intelligence in Medicine, Jun 2017, Vienna, Austria. 2017,
  16th Conference on Artificial Intelligence in Medicine","","","cs.AI","cs.AI|cs.DS"
"861","1709.05022v3","2017-09-15 00:59:08","2018-01-30 20:53:30","Network Controllability in the IFG Relates to Controlled Language
  Variability and Susceptibility to TMS","  In language production, humans are confronted with considerable word
selection demands. Often, we must select a word from among similar, acceptable,
and competing alternative words in order to construct a sentence that conveys
an intended meaning. In recent years, the left inferior frontal gyrus (LIFG)
has been identified as critical to this ability. Despite a recent emphasis on
network approaches to understanding language, how the LIFG interacts with the
brain's complex networks to facilitate controlled language performance remains
unknown. Here, we take a novel approach to understand word selection as a
network control process in the brain. Using an anatomical brain network derived
from high-resolution diffusion spectrum imaging (DSI), we computed network
controllability underlying the site of transcranial magnetic stimulation in the
LIFG between administrations of two word selection tasks. We find that a
statistic that quantifies the LIFG's theoretically predicted control of
difficult-to-reach states explains vulnerability to TMS in language tasks that
vary in response (cognitive control) demands: open-response (word generation)
vs. closed-response (number naming) tasks. Moreover, we find that a statistic
that quantifies the LIFG's theoretically predicted control of communication
across modules in the human connectome explains TMS-induced changes in
open-response language task performance only. These findings establish a link
between network controllability, cognitive function, and TMS effects.
","John D. Medaglia|Denise Y. Harvey|Nicole White|Danielle S. Bassett|Roy H. Hamilton","","http://arxiv.org/abs/1709.05022v3","http://arxiv.org/pdf/1709.05022v3","","","","","q-bio.NC","q-bio.NC"
"862","1709.05231v1","2017-09-15 14:32:36","2017-09-15 14:32:36","A Spectral Method for Activity Shaping in Continuous-Time Information
  Cascades","  Information Cascades Model captures dynamical properties of user activity in
a social network. In this work, we develop a novel framework for activity
shaping under the Continuous-Time Information Cascades Model which allows the
administrator for local control actions by allocating targeted resources that
can alter the spread of the process. Our framework employs the optimization of
the spectral radius of the Hazard matrix, a quantity that has been shown to
drive the maximum influence in a network, while enjoying a simple convex
relaxation when used to minimize the influence of the cascade. In addition,
use-cases such as quarantine and node immunization are discussed to highlight
the generality of the proposed activity shaping framework. Finally, we present
the NetShape influence minimization method which is compared favorably to
baseline and state-of-the-art approaches through simulations on real social
networks.
","Kevin Scaman|Argyris Kalogeratos|Luca Corinzia|Nicolas Vayatis","","http://arxiv.org/abs/1709.05231v1","http://arxiv.org/pdf/1709.05231v1","","","","","stat.ML","stat.ML|cs.AI|cs.LG|cs.SI|math.OC|93E20, 91D30|I.2.6"
"863","1709.05366v1","2017-09-15 18:47:37","2017-09-15 18:47:37","Tortoise: Interactive System Configuration Repair","  System configuration languages provide powerful abstractions that simplify
managing large-scale, networked systems. Thousands of organizations now use
configuration languages, such as Puppet. However, specifications written in
configuration languages can have bugs and the shell remains the simplest way to
debug a misconfigured system. Unfortunately, it is unsafe to use the shell to
fix problems when a system configuration language is in use: a fix applied from
the shell may cause the system to drift from the state specified by the
configuration language. Thus, despite their advantages, configuration languages
force system administrators to give up the simplicity and familiarity of the
shell.
  This paper presents a synthesis-based technique that allows administrators to
use configuration languages and the shell in harmony. Administrators can fix
errors using the shell and the technique automatically repairs the higher-level
specification written in the configuration language. The approach (1) produces
repairs that are consistent with the fix made using the shell; (2) produces
repairs that are maintainable by minimizing edits made to the original
specification; (3) ranks and presents multiple repairs when relevant; and (4)
supports all shells the administrator may wish to use. We implement our
technique for Puppet, a widely used system configuration language, and evaluate
it on a suite of benchmarks under 42 repair scenarios. The top-ranked repair is
selected by humans 76% of the time and the human-equivalent repair is ranked
1.31 on average.
","Aaron Weiss|Arjun Guha|Yuriy Brun","","http://arxiv.org/abs/1709.05366v1","http://arxiv.org/pdf/1709.05366v1","","Published version in proceedings of IEEE/ACM International Conference
  on Automated Software Engineering (ASE) 2017","","","cs.SE","cs.SE"
"864","1709.05551v1","2017-09-16 18:57:37","2017-09-16 18:57:37","Applying Machine Learning Methods to Enhance the Distribution of Social
  Services in Mexico","  The Government of Mexico's social development agency, SEDESOL, is responsible
for the administration of social services and has the mission of lifting
Mexican families out of poverty. One key challenge they face is matching people
who have social service needs with the services SEDESOL can provide accurately
and efficiently. In this work we describe two specific applications implemented
in collaboration with SEDESOL to enhance their distribution of social services.
The first problem relates to systematic underreporting on applications for
social services, which makes it difficult to identify where to prioritize
outreach. Responding that five people reside in a home when only three do is a
type of underreporting that could occur while a social worker conducts a home
survey with a family to determine their eligibility for services. The second
involves approximating multidimensional poverty profiles across households.
That is, can we characterize different types of vulnerabilities -- for example,
food insecurity and lack of health services -- faced by those in poverty?
  We detail the problem context, available data, our machine learning
formulation, experimental results, and effective feature sets. As far as we are
aware this is the first time government data of this scale has been used to
combat poverty within Mexico. We found that survey data alone can suggest
potential underreporting.
  Further, we found geographic features useful for housing and service related
indicators and transactional data informative for other dimensions of poverty.
The results from our machine learning system for estimating poverty profiles
will directly help better match 7.4 million individuals to social programs.
","Kris Sankaran|Diego Garcia-Olano|Mobin Javed|Maria Fernanda Alcala-Durand|Adolfo De Unanue|Paul van der Boor|Eric Potash|Roberto Sanchez Avalos|Luis Inaki Alberro Encinas|Rayid Ghani","","http://arxiv.org/abs/1709.05551v1","http://arxiv.org/pdf/1709.05551v1","","This work was done as part of the 2016 Eric & Wendy Schmidt Data
  Science for Social Good Summer Fellowship at the University of Chicago","","","stat.AP","stat.AP"
"865","1709.06132v2","2017-09-18 19:21:05","2018-02-12 16:41:27","Reserve Requirements in Ancillary Markets Using Consensus-Based
  Cooperative Model Considering Renewable Resources","  From an economic point of view, a common criterion for assessing the merits
of a reserve investment is its impacts on social welfare. The underlying
assumption in using this criterion is that side payments may be used to
distribute the social gains among all market players. In reality, however,
since the impacts of an electricity reserve project on different players may
vary, such side payments are rather difficult to implement. We present a
three-stage scenario-based programming model for committing reserves in power
markets with large amounts of wind power. We describe wind power generation in
terms of a representative set of appropriately weighted scenarios, and we
present a dual decomposition algorithm for solving the resulting stochastic
program. We test our scenario generation methodology on a model of 118 bus
system, and we show that the stochastic programming unit commitment policy
outperforms common reserve rules.
","Yan Xiu|Subior Magounder|Stefanous Barose|Mohammd Hamdi","","http://arxiv.org/abs/1709.06132v2","http://arxiv.org/pdf/1709.06132v2","","arXiv admin note: submission has been withdrawn by arXiv
  administrators due to inappropriate text reuse from external sources","","","math.OC","math.OC|60|F.2.2"
"866","1709.06713v1","2017-09-20 03:41:48","2017-09-20 03:41:48","Dissecting the Spatial Structure of Cities from Human Mobility Patterns
  to Define Functional Urban Boundaries","  Since the industrial revolution, accelerated urban growth has overflown
administrative divisions, merged cities into large built extensions, and
blurred the boundaries between urban and rural land-uses. These traits, present
in most of contemporary metropolis, complicate the definition of cities, a
crucial issue considering that objective and comparable metrics are the basic
inputs needed for the planning and design of sustainable urban environments. In
this context, city definitions that respond to administrative or political
criteria usually overlook human dynamics, a key factor that could help to make
cities comparable across the urban fabric of diverse social, cultural and
economic realities. Using a technique based on the spectral analysis of complex
networks, we rank places in 11 of the major Chilean urban regions from a
high-resolution human mobility dataset: Official origin-destination (OD)
surveys. We propose a method for further distinguishing urban and rural
land-uses within these regions, by means of a network centrality measure from
which we construct a spectre of geographic places. This spectre, constructed
from the ranking of locations as measured by their approximate number of
embedded human flows, allows us to probe several urban boundaries. From the
analysis of the urban scaling exponent of trips in relation to the population
across these city delineations, we identify two clearly distinct scaling
regimes occurring in urban and rural areas. The comparison of our results with
land cover derived from remote sensing suggests that, for the case of trips,
the scaling exponent in urban areas is close to linear. We conclude with
estimations for well-formed cities in the Chilean urban system, which according
to our analysis could emerge from clusters composed by places that capture at
least ~138 trips (over the expectation) of the underlying mobility network.
","Francisco J. Humeres|Horacio Samaniego","","http://arxiv.org/abs/1709.06713v1","http://arxiv.org/pdf/1709.06713v1","","23 pages, 5 figures, 3 Tables, Text by Francisco J. Humeres and
  Horacio Samaniego, Analysis and Figures by Francisco J. Humeres","","","cs.SI","cs.SI|physics.soc-ph|91C20 (Primary), 91B72 (Secondary)"
"867","1709.09093v2","2017-09-26 15:38:26","2017-10-01 15:24:56","Beyond opening up the black box: Investigating the role of algorithmic
  systems in Wikipedian organizational culture","  Scholars and practitioners across domains are increasingly concerned with
algorithmic transparency and opacity, interrogating the values and assumptions
embedded in automated, black-boxed systems, particularly in user-generated
content platforms. I report from an ethnography of infrastructure in Wikipedia
to discuss an often understudied aspect of this topic: the local, contextual,
learned expertise involved in participating in a highly automated
social-technical environment. Today, the organizational culture of Wikipedia is
deeply intertwined with various data-driven algorithmic systems, which
Wikipedians rely on to help manage and govern the ""anyone can edit""
encyclopedia at a massive scale. These bots, scripts, tools, plugins, and
dashboards make Wikipedia more efficient for those who know how to work with
them, but like all organizational culture, newcomers must learn them if they
want to fully participate. I illustrate how cultural and organizational
expertise is enacted around algorithmic agents by discussing two
autoethnographic vignettes, which relate my personal experience as a veteran in
Wikipedia. I present thick descriptions of how governance and gatekeeping
practices are articulated through and in alignment with these automated
infrastructures. Over the past 15 years, Wikipedian veterans and administrators
have made specific decisions to support administrative and editorial workflows
with automation in particular ways and not others. I use these cases of
Wikipedia's bot-supported bureaucracy to discuss several issues in the fields
of critical algorithms studies, critical data studies, and fairness,
accountability, and transparency in machine learning -- most principally
arguing that scholarship and practice must go beyond trying to ""open up the
black box"" of such systems and also examine sociocultural processes like
newcomer socialization.
","R. Stuart Geiger","","http://arxiv.org/abs/1709.09093v2","http://arxiv.org/pdf/1709.09093v2","http://dx.doi.org/10.1177/2053951717730735","14 pages, typo fixed in v2","Big Data & Society 4(2). 2017","10.1177/2053951717730735","cs.CY","cs.CY|cs.AI|cs.HC"
"868","1709.09409v2","2017-09-27 09:31:35","2017-09-29 10:17:07","e-Sem: Dynamic Seminar Management System for Primary, Secondary and
  Tertiary Education","  This paper describes the dynamic seminar management system named 'e-Sem',
developed according to the opensource software philosophy. Due to its dynamic
management functionality, it can equally adapt to any education environment
(Primary, Secondary, Tertiary). The purpose of the proposed dynamic system is
ease of use and handling, by any class of users, without the need of special
guidance. Also, students are given the opportunity to: a) register as users; b)
enroll in seminars in a simple way; c) receive e-learning material at any time
of day any day of week, and d) be informed of new announcements concerning the
seminar in which they are enrolled . In addition, the administrator and the
tutors have a number of tools such as : management seminars and trainees in a
friendly way, sending educational material as well as new announcements to the
trainees; the possibility of electronic recording of presence or absence of the
trainees in a seminar, and direct printing of a certificate of successful
attendance of a seminar for each trainee. The application also offers features
such as electronic organization, storage and presentation of educational
material, overcoming the limiting factors of space and time of classical
teaching, thus creating a dynamic environment
","Ioannis A. Skordas|Nikolaos Tsirekas|Nestoras Kolovos|George F. Fragulis|Athanasios G. Triantafyllou|Maria G. Bouliou","","http://arxiv.org/abs/1709.09409v2","http://arxiv.org/pdf/1709.09409v2","","","","","cs.CY","cs.CY"
"869","1709.09768v1","2017-09-28 00:31:21","2017-09-28 00:31:21","Colonel Blotto Game for Secure State Estimation in Interdependent
  Critical Infrastructure","  Securing the physical components of a city's interdependent critical
infrastructure (ICI) such as power, natural gas, and water systems is a
challenging task due to their interdependence and large number of involved
sensors. Using a novel integrated state-space model that captures the
interdependence, a two-stage cyber attack on ICI is studied in which the
attacker first compromises the ICI's sensors by decoding their messages, and,
subsequently, it alters the compromised sensors' data to cause state estimation
errors. To thwart such attacks, the administrator of the CIs must assign
protection levels to the sensors based on their importance in the state
estimation process. To capture the interdependence between the attacker and the
ICI administrator's actions and analyze their interactions, a Colonel Blotto
game framework is proposed. The mixed-strategy Nash equilibrium of this game is
derived analytically. At this equilibrium, it is shown that the administrator
can strategically randomize between the protection levels of the sensors to
deceive the attacker. Simulation results coupled with theoretical analysis show
that, using the proposed game, the administrator can reduce the state
estimation error by at least $ 50\% $ compared to any non-strategic action. The
results also show that the ICI's administrator must consider the CIs inside a
city as a unified ICI for security analysis instead of assigning independent
protection levels to each individual CI, as is conventionally done.
","Aidin Ferdowsi|Walid Saad|Narayan B. Mandayam","","http://arxiv.org/abs/1709.09768v1","http://arxiv.org/pdf/1709.09768v1","","","","","cs.SY","cs.SY|cs.GT|cs.IT|math.IT"
"870","1709.10279v2","2017-09-29 08:21:08","2018-05-12 03:31:47","Heterogeneous Employment Effects of Job Search Programmes: A Machine
  Learning Approach","  We systematically investigate the effect heterogeneity of job search
programmes for unemployed workers. To investigate possibly heterogeneous
employment effects, we combine non-experimental causal empirical models with
Lasso-type estimators. The empirical analyses are based on rich administrative
data from Swiss social security records. We find considerable heterogeneities
only during the first six months after the start of training. Consistent with
previous results of the literature, unemployed persons with fewer employment
opportunities profit more from participating in these programmes. Furthermore,
we also document heterogeneous employment effects by residence status. Finally,
we show the potential of easy-to-implement programme participation rules for
improving average employment effects of these active labour market programmes.
","Michael Knaus|Michael Lechner|Anthony Strittmatter","","http://arxiv.org/abs/1709.10279v2","http://arxiv.org/pdf/1709.10279v2","","","","","econ.EM","econ.EM"
"871","1710.01359v2","2017-10-03 19:37:10","2018-02-12 16:41:13","Multi-Period Coordinated Management of Electric Vehicles in Zonal Power
  Markets: A Convex Relaxation Approach","  In recent years, developments in plug in hybrid electric vehicles have
provided various environmental and economic advantages. In the future smart
grids, electric vehicles are seen as an important means of transportation to
reduce greenhouse gas emissions. One of the main issues regarding to this sort
of vehicles is managing their charging time to prevent high peak loads over
time. Deploying advanced metering and automatic chargers can be a practical way
not only for the vehicle owners to manage their energy consumption, but also
for the utilities to manage the electricity load during the day by shifting the
charging loads to the off-peak periods. Additionally, an efficient charging
schedule can reduce the user's electricity bill cost. In this paper we propose
a new coordinated multi-period management model for electric vehicles charging
scheduling based on multi-objective approach, aiming at optimizing customers
charging cost. In the proposed method, a stochastic model is given for starting
time of charging, which makes the method a practical tool for simulating the
vehicle owners charging behavior effectively. In order to verify the
effectiveness of the proposed method, two market policies are used as case
studies. The computation results can be used to evaluate the impact of electric
vehicles scheduling on economic performance of smart grid.
","Shanxuei Chen|Elizabeth Olivioufski|Braian Dods|Mohammd Hamdi","","http://arxiv.org/abs/1710.01359v2","http://arxiv.org/pdf/1710.01359v2","","arXiv admin note: submission has been withdrawn by arXiv
  administrators due to inappropriate text reuse from external sources","","","math.OC","math.OC|91|F.2.2"
"872","1710.01404v1","2017-10-03 22:18:13","2017-10-03 22:18:13","Survey of the physics landscape and attempts to improve diversity","  Students from statistically underrepresented minority (URM) groups and women
earn a smaller fraction of undergraduate and graduate degrees in most physical
sciences, particularly physics. This underrepresentation is also prevalent at
the faculty level and in higher administration roles at most physics
departments, universities, and national laboratories. This proceedings
summarizes a presentation that presented statistics on participation in
physical sciences, and discussed the outcomes of initiatives such as Bridge
Programs which aim to improve diversity in physics graduate programs.
","Brian Beckford","","http://arxiv.org/abs/1710.01404v1","http://arxiv.org/pdf/1710.01404v1","","Talk presented at the APS Division of Particles and Fields Meeting
  (DPF 2017), July 31-August 4, 2017, Fermilab. C170731","","","physics.ed-ph","physics.ed-ph"
"873","1710.01576v1","2017-10-04 12:46:27","2017-10-04 12:46:27","The Parameterized Complexity of Centrality Improvement in Networks","  The centrality of a vertex v in a network intuitively captures how important
v is for communication in the network. The task of improving the centrality of
a vertex has many applications, as a higher centrality often implies a larger
impact on the network or less transportation or administration cost. In this
work we study the parameterized complexity of the NP-complete problems
Closeness Improvement and Betweenness Improvement in which we ask to improve a
given vertex' closeness or betweenness centrality by a given amount through
adding a given number of edges to the network. Herein, the closeness of a
vertex v sums the multiplicative inverses of distances of other vertices to v
and the betweenness sums for each pair of vertices the fraction of shortest
paths going through v. Unfortunately, for the natural parameter ""number of
edges to add"" we obtain hardness results, even in rather restricted cases. On
the positive side, we also give an island of tractability for the parameter
measuring the vertex deletion distance to cluster graphs.
","Clemens Hoffmann|Hendrik Molter|Manuel Sorge","","http://arxiv.org/abs/1710.01576v1","http://arxiv.org/pdf/1710.01576v1","","","","","cs.DM","cs.DM|cs.CC"
"874","1710.01986v1","2017-10-05 12:39:49","2017-10-05 12:39:49","Job Management and Task Bundling","  High Performance Computing is often performed on scarce and shared computing
resources. To ensure computers are used to their full capacity, administrators
often incentivize large workloads that are not possible on smaller systems.
Measurements in Lattice QCD frequently do not scale to machine-size workloads.
By bundling tasks together we can create large jobs suitable for gigantic
partitions. We discuss METAQ and mpi_jm, software developed to dynamically
group computational tasks together, that can intelligently backfill to consume
idle time without substantial changes to users' current workflows or
executables.
","Evan Berkowitz|Gustav R. Jansen|Kenneth McElvain|Andre Walker-Loud","","http://arxiv.org/abs/1710.01986v1","http://arxiv.org/pdf/1710.01986v1","http://dx.doi.org/10.1051/epjconf/201817509007","8 pages, 3 figures, LATTICE 2017 proceedings","","10.1051/epjconf/201817509007","hep-lat","hep-lat|cs.DC|physics.comp-ph"
"875","1710.03127v2","2017-10-09 14:51:10","2017-11-28 09:09:35","Group Sequential Clinical Trial Designs for Normally Distributed Outcome
  Variables","  In a group sequential clinical trial, accumulated data are analysed at
numerous time-points in order to allow early decisions about a hypothesis of
interest. These designs have historically been recommended for their ethical,
administrative and economic benefits. In this work, we discuss a collection of
new Stata commands for computing the stopping boundaries and required group
size of various classical group sequential designs, assuming a normally
distributed outcome variable. Following this, we demonstrate how the
performance of several designs can be compared graphically.
","Michael Grayling|James Wason|Adrian Mander","","http://arxiv.org/abs/1710.03127v2","http://arxiv.org/pdf/1710.03127v2","","","","","stat.CO","stat.CO"
"876","1710.03999v1","2017-10-11 10:57:46","2017-10-11 10:57:46","Gap junction plasticity can lead to spindle oscillations","  Patterns of waxing and waning oscillations, called spindles, are observed in
multiple brain regions during sleep. Spindle are thought to be involved in
memory consolidation. The origin of spindle oscillations is ongoing work but
experimental results point towards the thalamic reticular nucleus (TRN) as a
likely candidate. The TRN is rich in electrical synapses, also called gap
junctions, which promote synchrony in neural activity. Moreover, gap junctions
undergo activity-dependent long-term plasticity. We hypothesized that gap
junction plasticity can modulate spindle oscillations. We developed a
computational model of gap junction plasticity in recurrent networks of TRN and
thalamocortical neurons (TC). We showed that gap junction coupling can modulate
the TRN-TC network synchrony and that gap junction plasticity is a plausible
mechanism for the generation of sleep-spindles. Finally, our results are robust
to the simulation of pharmacological manipulation of spindles, such as the
administration of propofol, an anesthetics known to generate spindles in
humans.
","Guillaume Pernelle|Wilten Nicola|Claudia Clopath","","http://arxiv.org/abs/1710.03999v1","http://arxiv.org/pdf/1710.03999v1","","arXiv admin note: text overlap with arXiv:1707.02324","","","q-bio.NC","q-bio.NC"
"877","1710.04144v1","2017-10-11 15:58:21","2017-10-11 15:58:21","GUIDES - Geospatial Urban Infrastructure Data Engineering Solutions","  As the underground infrastructure systems of cities age, maintenance and
repair become an increasing concern. Cities face difficulties in planning
maintenance, predicting and responding to infrastructure related issues, and in
realizing their vision to be a smart city due to their incomplete understanding
of the existing state of the infrastructure. Only few cities have accurate and
complete digital information on their underground infrastructure (e.g.,
electricity, water, natural gas) systems, which poses problems to those
planning and performing construction projects. To address these issues, we
introduce GUIDES as a new data conversion and management framework for urban
underground infrastructure systems that enable city administrators, workers,
and contractors along with the general public and other users to query
digitized and integrated data to make smarter decisions. This demo paper
presents the GUIDES architecture and describes two of its central components:
(i) mapping of underground infrastructure systems, and (ii) integration of
heterogeneous geospatial data.
","Booma Sowkarthiga Balasubramani|Omar Belingheri|Eric S. Boria|Isabel F. Cruz|Sybil Derrible|Michael D. Siciliano","","http://arxiv.org/abs/1710.04144v1","http://arxiv.org/pdf/1710.04144v1","","4 pages, SIGSPATIAL'17, November 7-10, 2017, Los Angeles Area, CA,
  USA","","","cs.CY","cs.CY|cs.DB"
"878","1710.04749v2","2017-10-12 23:42:00","2018-02-12 05:16:08","Explaining Aviation Safety Incidents Using Deep Temporal Multiple
  Instance Learning","  Although aviation accidents are rare, safety incidents occur more frequently
and require a careful analysis to detect and mitigate risks in a timely manner.
Analyzing safety incidents using operational data and producing event-based
explanations is invaluable to airline companies as well as to governing
organizations such as the Federal Aviation Administration (FAA) in the United
States. However, this task is challenging because of the complexity involved in
mining multi-dimensional heterogeneous time series data, the lack of
time-step-wise annotation of events in a flight, and the lack of scalable tools
to perform analysis over a large number of events. In this work, we propose a
precursor mining algorithm that identifies events in the multidimensional time
series that are correlated with the safety incident. Precursors are valuable to
systems health and safety monitoring and in explaining and forecasting safety
incidents. Current methods suffer from poor scalability to high dimensional
time series data and are inefficient in capturing temporal behavior. We propose
an approach by combining multiple-instance learning (MIL) and deep recurrent
neural networks (DRNN) to take advantage of MIL's ability to learn using weakly
supervised data and DRNN's ability to model temporal behavior. We describe the
algorithm, the data, the intuition behind taking a MIL approach, and a
comparative analysis of the proposed algorithm with baseline models. We also
discuss the application to a real-world aviation safety problem using data from
a commercial airline company and discuss the model's abilities and
shortcomings, with some final remarks about possible deployment directions.
","Vijay Manikandan Janakiraman","","http://arxiv.org/abs/1710.04749v2","http://arxiv.org/pdf/1710.04749v2","","","","","cs.CV","cs.CV|cs.AI|stat.AP|stat.ML"
"879","1710.06197v2","2017-10-17 10:33:39","2017-11-17 13:52:25","Transverse Momentum and Multiplicity Fluctuations in Ar+Sc Collisions at
  the CERN SPS from NA61/SHINE","  The NA61/SHINE experiment aims to discover the critical point of strongly
interacting matter and study the properties of the onset of deconfinement. For
these goals a scan of the two dimensional phase diagram ($T$-$\mu_{B}$) is
being performed at the SPS by measurements of hadron production in
proton-proton, proton-nucleus and nucleus-nucleus interactions as a function of
collision energy and system size. In this contribution preliminary results on
transverse momentum and multiplicity fluctuations expressed in terms of
strongly intensive quantities from the Ar+Sc energy scan will be presented.
These fluctuations are expected to be sensitive to the existence of a critical
point. The results are compared with results from the p+p and Be+Be energy scan
as well as with NA49 measurements.
","Evgeny Andronov","for the NA61/SHINE Collaboration","http://arxiv.org/abs/1710.06197v2","http://arxiv.org/pdf/1710.06197v2","http://dx.doi.org/10.5506/APhysPolBSupp.10.449","This submission has been withdrawn by arXiv administrators because it
  is a duplicate of arXiv:1610.05569","Acta Physica Polonica B Proceedings Supplement, Vol. 10, No. 3
  (2017) 449-453","10.5506/APhysPolBSupp.10.449","nucl-ex","nucl-ex"
"880","1710.06200v2","2017-10-17 10:52:47","2017-12-27 01:01:26","Composition of Jupiter irregular satellites sheds light on their origin","  Irregular satellites of Jupiter with their highly eccentric, inclined and
distant orbits suggest that their capture took place just before the giant
planet migration. We aim to improve our understanding of the surface
composition of irregular satellites of Jupiter to gain insight into a narrow
time window when our Solar System was forming. We observed three Jovian
irregular satellites, Himalia, Elara, and Carme, using a medium-resolution
0.8-5.5 micro m spectrograph on the National Aeronautics and Space
Administration (NASA) Infrared Telescope Facility (IRTF). Using a linear
spectral unmixing model we have constrained the major mineral phases on the
surface of these three bodies. Our results confirm that the surface of Himalia,
Elara, and Carme are dominated by opaque materials such as those seen in
carbonaceous chondrite meteorites. Our spectral modeling of NIR spectra of
Himalia and Elara confirm that their surface composition is the same and
magnetite is the dominant mineral. A comparison of the spectral shape of
Himalia with the two large main C-type asteroids, Themis (D 176 km) and Europa
(D 352 km), suggests surface composition similar to Europa. The NIR spectrum of
Carme exhibits blue slope up to 1.5 microm and is spectrally distinct from
those of Himalia and Elara. Our model suggests that it is compositionally
similar to amorphous carbon. Himalia and Elara are compositionally similar but
differ significantly from Carme. These results support the hypotheses that the
Jupiter irregular satellites are captured bodies that were subject to further
breakup events and clustered as families based on their similar physical and
surface compositions.
","M. Bhatt|V. Reddy|K. Schindler|E. Cloutis|A. Bhardwaj|L. L. Corre|P. Mann","","http://arxiv.org/abs/1710.06200v2","http://arxiv.org/pdf/1710.06200v2","http://dx.doi.org/10.1051/0004-6361/201630361","","A&A 608, A67 (2017)","10.1051/0004-6361/201630361","astro-ph.EP","astro-ph.EP"
"881","1710.07213v2","2017-10-17 15:57:59","2018-07-09 17:15:08","Graphene quantum dots prevent alpha-synucleinopathy in Parkinson's
  disease","  While the emerging evidence indicates that the pathogenesis of Parkinson's
disease (PD) is strongly correlated to the accumulation of alpha-synuclein
({\alpha}-syn) aggregates, there has been no clinical success in
anti-aggregation agents for the disease to date. Here we show that graphene
quantum dots (GQDs) exhibit anti-amyloid activity via direct interaction with
{\alpha}-syn. Employing biophysical, biochemical, and cell-based assays as well
as molecular dynamics (MD) simulation, we find that GQDs have notable potency
in not only inhibiting fibrillization of {\alpha}-syn but also disaggregating
mature fibrils in a time-dependent manner. Remarkably, GQDs rescue neuronal
death and synaptic loss, reduce Lewy body (LB)/Lewy neurite (LN) formation,
ameliorate mitochondrial dysfunctions, and prevent neuron-to-neuron
transmission of {\alpha}-syn pathology induced by {\alpha}-syn preformed
fibrils (PFFs) in neurons. In addition, in vivo administration of GQDs protects
against {\alpha}-syn PFFs-induced loss of dopamine neurons, LB/LN pathology,
and behavioural deficits through the penetration of the blood-brain barrier
(BBB). The finding that GQDs function as an anti-aggregation agent provides a
promising novel therapeutic target for the treatment of PD and related
{\alpha}-synucleinopathies.
","Donghoon Kim|Je Min Yoo|Heehong Hwang|Junghee Lee|Su Hyun Lee|Seung Pil Yun|Myung Jin Park|MinJun Lee|Seulah Choi|Sang Ho Kwon|Saebom Lee|Seung-Hwan Kwon|Sangjune Kim|Yong Joo Park|Misaki Kinoshita|Young-Ho Lee|Seokmin Shin|Seung R. Paik|Sung Joong Lee|Seulki Lee|Byung Hee Hong|Han Seok Ko","","http://arxiv.org/abs/1710.07213v2","http://arxiv.org/pdf/1710.07213v2","http://dx.doi.org/10.1038/s41565-018-0179-y","","","10.1038/s41565-018-0179-y","physics.med-ph","physics.med-ph|cond-mat.mtrl-sci|physics.bio-ph"
"882","1710.06811v1","2017-10-18 16:11:43","2017-10-18 16:11:43","Visual Progression Analysis of Student Records Data","  University curriculum, both on a campus level and on a per-major level, are
affected in a complex way by many decisions of many administrators and faculty
over time. As universities across the United States share an urgency to
significantly improve student success and success retention, there is a
pressing need to better understand how the student population is progressing
through the curriculum, and how to provide better supporting infrastructure and
refine the curriculum for the purpose of improving student outcomes. This work
has developed a visual knowledge discovery system called eCamp that pulls
together a variety of populationscale data products, including student grades,
major descriptions, and graduation records. These datasets were previously
disconnected and only available to and maintained by independent campus
offices. The framework models and analyzes the multi-level relationships hidden
within these data products, and visualizes the student flow patterns through
individual majors as well as through a hierarchy of majors. These results
support analytical tasks involving student outcomes, student retention, and
curriculum design. It is shown how eCamp has revealed student progression
information that was previously unavailable.
","Mohammad Raji|John Duggan|Blaise DeCotes|Jian Huang|Bradley Vander Zanden","","http://arxiv.org/abs/1710.06811v1","http://arxiv.org/pdf/1710.06811v1","","8 pages, 7 figures, Published in Visualization in Data Science (VDS
  2017)","","","cs.CY","cs.CY"
"883","1710.07039v2","2017-10-19 08:40:24","2018-05-10 08:35:00","Causal inference for binary non-independent outcomes","  Causal inference on multiple non-independent outcomes raises serious
challenges, because multivariate techniques that properly account for the
outcome's dependence structure need to be considered. We focus on the case of
binary outcomes framing our discussion in the potential outcome approach to
causal inference. We define causal effects of treatment on joint outcomes
introducing the notion of product outcomes. We also discuss a decomposition of
the causal effect on product outcomes into intrinsic and extrinsic causal
effects, which respectively provide information on treatment effect on the
intrinsic (product) structure of the product outcomes and on the outcomes'
dependence structure. We propose a log-mean linear regression approach for
modeling the distribution of the potential outcomes, which is particularly
appealing because all the causal estimands of interest and the decomposition
into intrinsic and extrinsic causal effects can be easily derived by model
parameters. The method is illustrated in two randomized experiments concerning
(i) the effect of the administration of oral pre-surgery morphine on pain
intensity after surgery; and (ii) the effect of honey on nocturnal cough and
sleep difficulty associated with childhood upper respiratory tract infections.
","Monia Lupparelli|Alessandra Mattei","","http://arxiv.org/abs/1710.07039v2","http://arxiv.org/pdf/1710.07039v2","","","","","stat.ME","stat.ME"
"884","1710.08023v1","2017-10-22 21:59:47","2017-10-22 21:59:47","A Brief Comparison of Two Enterprise-Class RDBMSs","  This paper is an extended version of a report from a student-developed study
to compare Microsoft SQL Server and PostgreSQL, two widely-used
enterprise-class relational database management systems (RDBMSs). The study
followed an introductory undergraduate course in relational systems and was
designed to help gain practical understanding of specific DBMSs. During this
study, we implemented three non-trivial schemas in each system, identified 26
common database design, development, and administration activities while
implementing the schemas, and compared the support each system offers to carry
out the identified activities. Where relevant, we also compared each system
against the SQL standard.
  In this report, we present a summary of the similarities and differences we
found between the two systems, and we provide a quantitative measure ranking
both systems' implementations of the 26 activities. We also briefly discuss the
""technical suitability"" of PostgreSQL to enterprise applications. Although this
report is not comprehensive and is too general to comment on the suitability of
either system to a specific enterprise application, it can nevertheless provide
an initial set of considerations and criteria to choose a system for most
enterprise applications.
","Andrew Figueroa|Steven Rollo|Sean Murthy","","http://arxiv.org/abs/1710.08023v1","http://arxiv.org/pdf/1710.08023v1","","14 pages, 16 figures, 2 tables","","","cs.DB","cs.DB|H.2.4"
"885","1710.08831v1","2017-10-24 15:11:58","2017-10-24 15:11:58","BaHaMAS: A Bash Handler to Monitor and Administrate Simulations","  Numerical QCD is often extremely resource demanding and it is not rare to run
hundreds of simulations at the same time. Each of these can last for days or
even months and it typically requires a job-script file as well as an input
file with the physical parameters for the application to be run. Moreover, some
monitoring operations (i.e. copying, moving, deleting or modifying files,
resume crashed jobs, etc.) are often required to guarantee that the final
statistics is correctly accumulated. Proceeding manually in handling
simulations is probably the most error-prone way and it is deadly uncomfortable
and inefficient! BaHaMAS was developed and successfully used in the last years
as a tool to automatically monitor and administrate simulations.
","Alessandro Sciarra","","http://arxiv.org/abs/1710.08831v1","http://arxiv.org/pdf/1710.08831v1","http://dx.doi.org/10.1051/epjconf/201817509003","8 pages, 3 figures, proceedings of the 35th International Symposium
  on Lattice Field Theory (Lattice 2017), 18-24 June 2017, Granada, Spain","","10.1051/epjconf/201817509003","hep-lat","hep-lat|physics.comp-ph"
"886","1710.08977v2","2017-10-24 20:18:12","2018-07-17 21:35:36","Making Physics Courses Accessible for Blind Students: strategies for
  course administration, class meetings and course materials","  The Americans with Disabilities Act (ADA) mandates that U.S. institutions of
higher education provide ""reasonable accommodations"" to students with
disabilities to ensure equal educational opportunities. However, despite the
key role of physics as a gateway to Science, Technology, Engineering and
Mathematics (STEM) studies, only limited resources exist for teaching physics
to students who are blind or visually impaired. Here we share lessons from our
experience creating an accessible physics curriculum for a blind physics major.
The authors include the student himself, a blind physics B.S. who graduated
from a different institution, a PhD chemist and consultant on STEM
accessibility who is himself blind, and several sighted educators and course
assistants who worked regularly with the students. Throughout this effort, we
learned that many of the principles of universal design described herein
enhanced learning for all of our students.
","Megan Holt|Daniel Gillen|Chelsea Cook|Christa Hixson Miller|Sacha D. Nandlall|Kevin Setter|Cary Supalo|Paul Thorman|Suzanne Amador Kane","","http://arxiv.org/abs/1710.08977v2","http://arxiv.org/pdf/1710.08977v2","","","","","physics.ed-ph","physics.ed-ph"
"887","1710.09918v1","2017-10-26 21:28:13","2017-10-26 21:28:13","EduCTX: A blockchain-based higher education credit platform","  Blockchain technology enables the creation of a decentralized environment
where transactions and data are not under the control of any third party
organization. Any transaction ever completed is recorded in a public ledger in
a verifiable and permanent way. Based on blockchain technology, we propose a
global higher education credit platform, named EduCTX. This platform is based
on the concept of the European Credit Transfer and Accumulation System (ECTS).
It constitutes a globally trusted, decentralized higher education credit and
grading system that can offer a globally unified viewpoint for students and
higher education institutions (HEIs), as well as for other potential
stakeholders such as companies, institutions, and organizations. As a proof of
concept, we present a prototype implementation of the environment, based on the
open-source Ark Blockchain Platform. Based on a globally distributed
peer-to-peer network, EduCTX will process, manage and control ECTX tokens,
which represent credits that students gain for completed courses such as ECTS.
HEIs are the peers of the blockchain network. The platform is a first step
towards a more transparent and technologically advanced form of higher
education systems. The EduCTX platform represents the basis of the EduCTX
initiative which anticipates that various HEIs would join forces in order to
create a globally efficient, simplified and ubiquitous environment in order to
avoid language and administrative barriers. Therefore we invite and encourage
HEIs to join the EduCTX initiative and the EduCTX blockchain network.
","Muhamed Turkanovi<U+0107>|Marko Holbl|Kristjan Ko<U+0161>i<U+010D>|Marjan Heri<U+010D>ko|Aida Kami<U+0161>ali<U+0107>","","http://arxiv.org/abs/1710.09918v1","http://arxiv.org/pdf/1710.09918v1","http://dx.doi.org/10.1109/ACCESS.2018.2789929","20 pages, 6 figures","","10.1109/ACCESS.2018.2789929","cs.CY","cs.CY"
"888","1710.10688v1","2017-10-29 20:41:55","2017-10-29 20:41:55","Nikolay Luzin, his students, adversaries, and defenders (notes on the
  history of Moscow mathematics, 1914-1936)","  This is historical-mathematical and historical notes on Moscow mathematics
1914-1936. Nikolay Luzin was a central figure of that time. Pavel Alexandroff,
Nina Bari, Alexandr Khinchin, Andrey Kolmogorov, Mikhail Lavrentiev, Lazar
Lyusternik, Dmitry Menshov, Petr Novikov, Lev Schnirelman, Mikhail Suslin, and
Pavel Urysohn were his students. We discuss the time of the great intellectual
influence of Luzin (1915-1924), the time of decay of his school (1922-1930), a
moment of his administrative power (1934-1936), and his fall in July 1936.
","Yury A. Neretin","","http://arxiv.org/abs/1710.10688v1","http://arxiv.org/pdf/1710.10688v1","","English, Russian; 378p., 683 refs","","","math.HO","math.HO|01A60, 01A70, 01A72, 01A73, 01A80, 01-02, 97A30"
"889","1711.00618v1","2017-11-02 05:54:31","2017-11-02 05:54:31","ThrottleBot - Performance without Insight","  Large scale applications are increasingly built by composing sets of
microservices. In this model the functionality for a single application might
be split across 100s or 1000s of microservices. Resource provisioning for these
applications is complex, requiring administrators to understand both the
functioning of each microservice, and dependencies between microservices in an
application. In this paper we present ThrottleBot, a system that automates the
process of determining what resource when allocated to which microservice is
likely to have the greatest impact on application performance. We demonstrate
the efficacy of our approach by applying ThrottleBot to both synthetic and real
world applications. We believe that ThrottleBot when combined with existing
microservice orchestrators, e.g., Kubernetes, enables push-button deployment of
web scale applications.
","Michael Alan Chang|Aurojit Panda|Yuan-Cheng Tsai|Hantao Wang|Scott Shenker","","http://arxiv.org/abs/1711.00618v1","http://arxiv.org/pdf/1711.00618v1","","","","","cs.DC","cs.DC|cs.PF"
"890","1711.00959v1","2017-11-02 22:04:22","2017-11-02 22:04:22","Transition from Plan Driven to SAFe : Periodic Team Self-Assessment","  Context: How to adopt, scale and tailor agile methods depends on several
factors such as the size of the organization, business goals, operative model,
and needs. The Scaled Agile Framework (SAFe) was developed to support
organizations to scale agile practices across the enterprise. Problem: Early
adopters of SAFe tend to be large multi-national enterprises who report that
the adoption of SAFe has led to significant productivity and quality gains.
However, little is known about whether these benefits translate to small to
medium sized enterprises (SMEs). Method: As part of a longitudinal study of an
SME transitioning to SAFe we ask, to what extent are SAFe practices adopted at
the team level? We targeted all team members and administrated a mixed method
survey in February, 2017 and in July, 2017 to identify and evaluate the
adoption rate of SAFe practices. Results: Initially in Quarter 1, teams were
struggling with PI/Release health and Technical health throughout the
organization as most of the teams were transitioning from plan-driven to SAFe .
But, during the transition period in Quarter 3, we observed discernible
improvements in different areas of SAFe practice adoption. Conclusion: The
observed improvement might be due to teams merely becoming more familiar with
the practices over-time. However, management had also made some structural
changes to the teams that may account for the change.
","Mohammad Abdur Razzak|John Noll|Ita Richardson|Clodagh Nic Canna|Sarah Beecham","","http://arxiv.org/abs/1711.00959v1","http://arxiv.org/pdf/1711.00959v1","","10, QuASD, Profes 2017","","","cs.SE","cs.SE"
"891","1711.02742v1","2017-11-07 21:52:08","2017-11-07 21:52:08","The VACCINE Framework for Building DLP Systems","  Conventional Data Leakage Prevention (DLP) systems suffer from the following
major drawback: Privacy policies that define what constitutes data leakage
cannot be seamlessly defined and enforced across heterogeneous forms of
communication. Administrators have the dual burden of: (1) manually
self-interpreting policies from handbooks to specify rules (which is
error-prone); (2) extracting relevant information flows from heterogeneous
communication protocols and enforcing policies to determine which flows should
be admissible. To address these issues, we present the Verifiable and
ACtionable Contextual Integrity Norms Engine (VACCINE), a framework for
building adaptable and modular DLP systems. VACCINE relies on (1) the theory of
contextual integrity to provide an abstraction layer suitable for specifying
reusable protocol-agnostic leakage prevention rules and (2) programming
language techniques to check these rules against correctness properties and to
enforce them faithfully within a DLP system implementation. We applied VACCINE
to the Family Educational Rights and Privacy Act and Enron Corporation privacy
regulations. We show that by using contextual integrity in conjunction with
verification techniques, we can effectively create reusable privacy rules with
specific correctness guarantees, and check the integrity of information flows
against these rules. Our experiments in emulated enterprise settings indicate
that VACCINE improves over current DLP system design approaches and can be
deployed in enterprises involving tens of thousands of actors.
","Yan Shvartzshnaider|Zvonimir Pavlinovic|Thomas Wies|Lakshminarayanan Subramanian|Prateek Mittal|Helen Nissenbaum","","http://arxiv.org/abs/1711.02742v1","http://arxiv.org/pdf/1711.02742v1","","","","","cs.CR","cs.CR"
"892","1711.10883v1","2017-11-09 07:30:26","2017-11-09 07:30:26","An Analytical Framework for Understanding the Intensity of Religious
  Fundamentalism","  This paper examines the process of emergence of religious fundamentalism
through development parameters. Therefore this research work reflects an
analytical discussion on how the level of religious fundamentalism can be
explained by the economic, political administrative and legal parameters such
as GDP, Employment to Population ratio, Government Effectiveness, Voice &
Accountability, Rule of Law (World Justice Project Report) and Rule of law
(Governance Indicators).
","Navonil Bhattacharya|Arabinda Bhattacharya","","http://arxiv.org/abs/1711.10883v1","http://arxiv.org/pdf/1711.10883v1","","","","","physics.soc-ph","physics.soc-ph"
"893","1711.03386v1","2017-11-09 14:28:12","2017-11-09 14:28:12","Performance Evaluation of Deep Learning Tools in Docker Containers","  With the success of deep learning techniques in a broad range of application
domains, many deep learning software frameworks have been developed and are
being updated frequently to adapt to new hardware features and software
libraries, which bring a big challenge for end users and system administrators.
To address this problem, container techniques are widely used to simplify the
deployment and management of deep learning software. However, it remains
unknown whether container techniques bring any performance penalty to deep
learning applications. The purpose of this work is to systematically evaluate
the impact of docker container on the performance of deep learning
applications. We first benchmark the performance of system components (IO, CPU
and GPU) in a docker container and the host system and compare the results to
see if there's any difference. According to our results, we find that
computational intensive jobs, either running on CPU or GPU, have small overhead
indicating docker containers can be applied to deep learning programs. Then we
evaluate the performance of some popular deep learning tools deployed in a
docker container and the host system. It turns out that the docker container
will not cause noticeable drawbacks while running those deep learning tools. So
encapsulating deep learning tool in a container is a feasible solution.
","Pengfei Xu|Shaohuai Shi|Xiaowen Chu","","http://arxiv.org/abs/1711.03386v1","http://arxiv.org/pdf/1711.03386v1","","Conference: BIgCom2017, 9 pages","","","cs.DC","cs.DC|cs.LG|cs.PF"
"894","1711.03723v1","2017-11-10 08:23:52","2017-11-10 08:23:52","A Relationship between the Solar Rotation and Activity Analysed by
  Tracing Sunspot Groups","  The sunspot position from Greenwich Photoheliographic Results (GPR), US Air
Force Solar Optical Observing Network and National Oceanic and Atmospheric
Administration (USAF/NOAA), and Debrecen Photoheliographic Data (DPD) data
bases in the period 1874 to 2016 were used to calculate yearly values of the
solar differential-rotation parameters $A$ and $B$. The calculated
differential-rotation parameters were compared with the solar-activity level.
We found that the Sun rotates more differentially at the minimum than at the
maximum of activity during the 1977 - 2016 epoch. An inverse correlation
between equatorial rotation and solar activity was found using the recently
revised sunspot number. The secular decrease of equatorial rotation rate
accompanying the increase of activity stopped in the last part of the 20th
century. It was noted that when a significant peak of equatorial rotation
velocity is observed during minimum of activity, the strength of the next
maximum is smaller than the previous one.
","Domagoj Ru<U+017E>djak|Roman Braj<U+0161>a|Davor Sudar|Ivica Skoki<U+0107>|Ivana Poljan<U+010D>i<U+0107> Beljan","","http://arxiv.org/abs/1711.03723v1","http://arxiv.org/pdf/1711.03723v1","http://dx.doi.org/10.1007/s11207-017-1199-8","13pages, 3 figures","","10.1007/s11207-017-1199-8","astro-ph.SR","astro-ph.SR"
"895","1711.03759v3","2017-11-10 10:18:02","2018-05-25 13:24:56","YEDDA: A Lightweight Collaborative Text Span Annotation Tool","  In this paper, we introduce \textsc{Yedda}, a lightweight but efficient and
comprehensive open-source tool for text span annotation. \textsc{Yedda}
provides a systematic solution for text span annotation, ranging from
collaborative user annotation to administrator evaluation and analysis. It
overcomes the low efficiency of traditional text annotation tools by annotating
entities through both command line and shortcut keys, which are configurable
with custom labels. \textsc{Yedda} also gives intelligent recommendations by
learning the up-to-date annotated text. An administrator client is developed to
evaluate annotation quality of multiple annotators and generate detailed
comparison report for each annotator pair. Experiments show that the proposed
system can reduce the annotation time by half compared with existing annotation
tools. And the annotation time can be further compressed by 16.47\% through
intelligent recommendation.
","Jie Yang|Yue Zhang|Linwei Li|Xingxuan Li","","http://arxiv.org/abs/1711.03759v3","http://arxiv.org/pdf/1711.03759v3","","Accepted by ACL 2018 as demonstration paper","","","cs.CL","cs.CL"
"896","1711.04235v2","2017-11-12 04:21:27","2018-01-09 06:05:11","Bitcoin and quantum computing","  Bitcoin is a digital currency and payment system based on classical
cryptographic technologies which works without a central administrator such as
in traditional currencies. It has long been questioned what the impact of
quantum computing would be on Bitcoin, and cryptocurrencies in general. Here,
we analyse three primary directions that quantum computers might have an impact
in: mining, security, and forks. We find that in the near-term the impact of
quantum computers appear to be rather small for all three directions. The
impact of quantum computers would require considerably larger number of qubits
and breakthroughs in quantum algorithms to reverse existing hash functions.
","Louis Tessler|Tim Byrnes","","http://arxiv.org/abs/1711.04235v2","http://arxiv.org/pdf/1711.04235v2","","","","","quant-ph","quant-ph|cs.CR"
"897","1711.05656v2","2017-11-15 16:36:14","2017-11-23 19:09:15","Learning to Predict with Highly Granular Temporal Data: Estimating
  individual behavioral profiles with smart meter data","  Big spatio-temporal datasets, available through both open and administrative
data sources, offer significant potential for social science research. The
magnitude of the data allows for increased resolution and analysis at
individual level. While there are recent advances in forecasting techniques for
highly granular temporal data, little attention is given to segmenting the time
series and finding homogeneous patterns. In this paper, it is proposed to
estimate behavioral profiles of individuals' activities over time using
Gaussian Process-based models. In particular, the aim is to investigate how
individuals or groups may be clustered according to the model parameters. Such
a Bayesian non-parametric method is then tested by looking at the
predictability of the segments using a combination of models to fit different
parts of the temporal profiles. Model validity is then tested on a set of
holdout data. The dataset consists of half hourly energy consumption records
from smart meters from more than 100,000 households in the UK and covers the
period from 2015 to 2016. The methodological approach developed in the paper
may be easily applied to datasets of similar structure and granularity, for
example social media data, and may lead to improved accuracy in the prediction
of social dynamics and behavior.
","Anastasia Ushakova|Slava J. Mikhaylov","","http://arxiv.org/abs/1711.05656v2","http://arxiv.org/pdf/1711.05656v2","","","","","stat.AP","stat.AP|stat.ML"
"898","1711.05696v1","2017-11-15 17:48:13","2017-11-15 17:48:13","An approach to evaluation of common DNS misconfigurations","  DNS is a basic Internet service which almost all other user services depend
on. However, what has been perceived in practice are a lot of inconsistencies
and errors in the configuration of servers that cause different problems. The
majority of such cases are included in this research with the aim of
identifying and classifying the major problems of DNS availability, performance
and security. In order to analyze these problems in correlation with DNS
administrators working practice, we have developed a methodology and tool for
testing, quantifying and analysis of DNS misconfigurations. The methodology and
tool were applied on three heterogeneous domain categories - the most popular
Internet domains, academic domains and one national top level domain. Our
results confirm relatively high percentage of misconfigured domains, especially
in the academic and national categories. However, we have shown that fixing the
configuration on relatively small number of name servers can have significant
impact to great number of domains. Proper domain management, permanent testing
and collaboration with other administrators are identified as measures to
improve domains operation, stability and security.
","Petar D. Bojovi<U+0107>|Slavko Gajin","","http://arxiv.org/abs/1711.05696v1","http://arxiv.org/pdf/1711.05696v1","","","","","cs.NI","cs.NI"
"899","1711.05838v1","2017-11-15 22:51:51","2017-11-15 22:51:51","Participation rates of in-class vs. online administration of low-stakes
  research-based assessments","  This study investigates differences in student participation rates between
in-class and online administrations of research-based assessments. A sample of
1,310 students from 25 sections of 3 different introductory physics courses
over two semesters were instructed to complete the CLASS attitudinal survey and
the concept inventory relevant to their course, either the FCI or the CSEM.
Each student was randomly assigned to take one of the surveys in class and the
other survey online at home using the Learning About STEM Student Outcomes
(LASSO) platform. Results indicate large variations in participation rates
across both test conditions (online and in class). A hierarchical generalized
linear model (HGLM) of the student data utilizing logistic regression indicates
that student grades in the course and faculty assessment administration
practices were both significant predictors of student participation. When the
recommended online assessments administration practices were implemented,
participation rates were similar across test conditions. Implications for
student and course assessment methodologies will be discussed.
","Manher Jariwala|Jayson Nissen|Xochith Herrera|Eleanor W. Close|Ben Van Dusen","","http://arxiv.org/abs/1711.05838v1","http://arxiv.org/pdf/1711.05838v1","","","","","physics.ed-ph","physics.ed-ph"
"900","1711.06070v1","2017-11-16 13:05:56","2017-11-16 13:05:56","Adjusting for selective non-participation with re-contact data in the
  FINRISK 2012 survey","  Aims: A common objective of epidemiological surveys is to provide
population-level estimates of health indicators. Survey results tend to be
biased under selective non-participation. One approach to bias reduction is to
collect information about non-participants by contacting them again and asking
them to fill in a questionnaire. This information is called re-contact data,
and it allows to adjust the estimates for non-participation.
  Methods: We analyse data from the FINRISK 2012 survey, where re-contact data
were collected. We assume that the respondents of the re-contact survey are
similar to the remaining non-participants with respect to the health given
their available background information. Validity of this assumption is
evaluated based on the hospitalization data obtained through record linkage of
survey data to the administrative registers. Using this assumption and multiple
imputation, we estimate the prevalences of daily smoking and heavy alcohol
consumption and compare them to estimates obtained with a commonly used
assumption that the participants represent the entire target group.
  Results: This approach produces higher prevalence estimates than what is
estimated from participants only. Among men, smoking prevalence estimate was
28.5% (23.2% for participants), heavy alcohol consumption prevalence was 9.4%
(6.8% for participants). Among women, smoking prevalence was 19.0% (16.5% for
participants) and heavy alcohol consumption 4.8% (3.0% for participants).
Conclusion: Utilization of re-contact data is a useful method to adjust for
non-participation bias on population estimates in epidemiological surveys.
","Juho Kopra|Tommi Harkanen|Hanna Tolonen|Pekka Jousilahti|Kari Kuulasmaa|Jaakko Reinikainen|Juha Karvanen","","http://arxiv.org/abs/1711.06070v1","http://arxiv.org/pdf/1711.06070v1","http://dx.doi.org/10.1177/1403494817734774","16 pages, 4 tables, 0 figures","Scandinavian Journal of Public Health, 2017","10.1177/1403494817734774","stat.AP","stat.AP"
"901","1711.06332v2","2017-11-16 22:20:30","2018-05-23 13:49:41","A worldwide model for boundaries of urban settlements","  The shape of urban settlements plays a fundamental role in their sustainable
planning. Properly defining the boundaries of cities is challenging and remains
an open problem in the Science of Cities. Here, we propose a worldwide model to
define urban settlements beyond their administrative boundaries through a
bottom-up approach that takes into account geographical biases intrinsically
associated with most societies around the world, and reflected in their
different regional growing dynamics. The generality of the model allows to
study the scaling laws of cities at all geographical levels: countries,
continents, and the entire world. Our definition of cities is robust and holds
to one of the most famous results in Social Sciences: Zipf's law. According to
our results, the largest cities in the world are not in line with what was
recently reported by the United Nations. For example, we find that the largest
city in the world is an agglomeration of several small settlements close to
each other, connecting three large settlements: Alexandria, Cairo, and Luxor.
Our definition of cities opens the doors to the study of the economy of cities
in a systematic way independently of arbitrary definitions that employ
administrative boundaries.
","Erneson A. Oliveira|Vasco Furtado|Jose S. Andrade Jr.|Hernan A. Makse","","http://arxiv.org/abs/1711.06332v2","http://arxiv.org/pdf/1711.06332v2","http://dx.doi.org/10.1098/rsos.180468","","","10.1098/rsos.180468","physics.soc-ph","physics.soc-ph"
"902","1711.06587v1","2017-11-17 15:23:06","2017-11-17 15:23:06","Performance differences for in-class and online administration of
  low-stakes research-based assessments","  Research-based assessments (RBAs), such as the Force Concept Inventory, have
played central roles in many course transformations from traditional
lecture-based instruction to research-based teaching methods. In order to
support instructors in assessing their courses, the online Learning About STEM
Student Outcomes (LASSO) platform simplifies administering, scoring, and
interpreting RBAs. Reducing the barriers to using RBAs will support more
instructors in objectively assessing the efficacy of their courses and,
subsequently, transforming their courses to improve student outcomes. The
purpose of this study was to investigate the extent to which RBAs administered
online and outside of class with the LASSO platform provided equivalent data to
traditional paper and pencil tests administered in class. Research indicates
that these two modes of administering assessments provide equivalent data for
graded exams that are administered in class. However, little research has
focused on ungraded (low-stakes) exams that are administered outside of class.
We used an experimental design to investigate the differences between these two
test modes. Results indicated that the LASSO platform provided equivalent data
to paper and pencil tests.
","Jayson M. Nissen|Manher Jariwala|Xochith Herrera|Eleanor W. Close|Ben Van Dusen","","http://arxiv.org/abs/1711.06587v1","http://arxiv.org/pdf/1711.06587v1","","","","","physics.ed-ph","physics.ed-ph"
"903","1711.06595v1","2017-11-17 15:39:12","2017-11-17 15:39:12","Participation and Performance on Paper- and Computer-Based Low-Stakes
  Assessments","  High-stakes assessments, such the Graduate Records Examination, have
transitioned from paper to computer administration. Low-stakes Research-Based
Assessments (RBAs), such as the Force Concept Inventory, have only recently
begun this transition to computer administration with online services. These
online services can simplify administering, scoring, and interpreting
assessments, thereby reducing barriers to instructors' use of RBAs. By
supporting instructors' objective assessment of the efficacy of their courses,
these services can stimulate instructors to transform their courses to improve
student outcomes. We investigate the extent to which RBAs administered outside
of class with the online Learning About STEM Student Outcomes (LASSO) platform
provide equivalent data to tests administered on paper in class, in terms of
both student participation and performance. We use an experimental design to
investigate the differences between these two assessment conditions with 1,310
students in 25 sections of 3 college physics courses spanning 2 semesters.
Analysis conducted using Hierarchical Linear Models indicates that student
performance on low-stakes RBAs is equivalent for online (out-of-class) and
paper-and-pencil (in-class) administrations. The models also show differences
in participation rates across assessment conditions and student grades, but
that instructors can achieve participation rates with online assessments
equivalent to paper assessments by offering students credit for participating
and by providing multiple reminders to complete the assessment. We conclude
that online out-of-class administration of RBAs can save class and instructor
time while providing participation rates and performance results equivalent to
in-class paper-and-pencil tests.
","Jayson M. Nissen|Manher Jariwala|Eleanor W. Close|Ben Van Dusen","","http://arxiv.org/abs/1711.06595v1","http://arxiv.org/pdf/1711.06595v1","","","","","physics.ed-ph","physics.ed-ph"
"904","1711.06654v3","2017-11-17 18:05:26","2018-02-06 22:01:24","Towards the Adoption of Anti-spoofing Protocols","  Email spoofing is a critical step of phishing, where the attacker
impersonates someone the victim knows or trusts. In this paper, we conduct a
qualitative study to explore why email spoofing is still possible after years
of efforts to develop and deploy anti-spoofing protocols (e.g., SPF, DKIM,
DMARC). First, we measure the protocol adoption by scanning 1 million Internet
domains. We find the adoption rates are still low, especially for the new DMARC
(3.1%). Second, to understand the reasons behind the low-adoption rate, we
collect 4293 discussion threads (25.7K messages) from the Internet Engineering
Task Force (IETF), a working group formed to develop and promote Internet
standards. Our analysis shows key security and usability limitations in the
protocol design, which makes it difficult to generate a positive ""net effect""
for a wide adoption. We validate our results by interviewing email
administrators and discuss key implications for future anti-spoofing solutions.
","Hang Hu|Peng Peng|Gang Wang","","http://arxiv.org/abs/1711.06654v3","http://arxiv.org/pdf/1711.06654v3","","","","","cs.CR","cs.CR"
"905","1711.07245v2","2017-11-20 10:33:29","2018-12-25 18:33:15","Optical Character Recognition (OCR) for Telugu: Database, Algorithm and
  Application","  Telugu is a Dravidian language spoken by more than 80 million people
worldwide. The optical character recognition (OCR) of the Telugu script has
wide ranging applications including education, health-care, administration etc.
The beautiful Telugu script however is very different from Germanic scripts
like English and German. This makes the use of transfer learning of Germanic
OCR solutions to Telugu a non-trivial task. To address the challenge of OCR for
Telugu, we make three contributions in this work: (i) a database of Telugu
characters, (ii) a deep learning based OCR algorithm, and (iii) a client server
solution for the online deployment of the algorithm. For the benefit of the
Telugu people and the research community, we will make our code freely
available at https://gayamtrishal.github.io/OCR_Telugu.github.io/
","Chandra Prakash Konkimalla|Manikanta Srikar Yellapragada|Trishal Gayam|Souraj Mandal|Sumohana S. Channappayya","","http://arxiv.org/abs/1711.07245v2","http://arxiv.org/pdf/1711.07245v2","","Accepted to IEEE International Conference on Image Processing 2018","","","cs.CV","cs.CV"
"906","1711.09747v1","2017-11-20 18:04:59","2017-11-20 18:04:59","Hyper Converged Infrastructures: Beyond virtualization","  Hyper Convergence has brought virtualization and IT strategies to a new
level. Datacenters are undergoing a deep paradigm shift from a hardware-centric
to an application-centric approach which leverages on software defined
architectures, while IT is more and more being delivered as services rather
than assets or products. Throughout different evolving phases since the initial
attempts to convergence, the concept has been refined down to a level
where,ultimately, a whole datacenter could be fully managed from a centralized
single point, abstracting the whole hardware layer and exposing it to the
administrators as a transparent pool of resources. This paper analyzes the
evolution of infrastructures and tries to dig into the reality and convenience
of Hyper Convergence.
","Alberto Perez Veiga","","http://arxiv.org/abs/1711.09747v1","http://arxiv.org/pdf/1711.09747v1","","","","","cs.CY","cs.CY"
"907","1711.09061v2","2017-11-21 17:33:32","2017-12-20 13:26:44","The Expected Achievable Distortion of Two-User Decentralized
  Interference Channels","  This paper concerns the transmission of two independent Gaussian sources over
a two-user decentralized interference channel, assuming that the transmitters
are unaware of the instantaneous CSIs. The availability of the channel state
information at receivers (CSIR) is considered in two scenarios of perfect and
imperfect CSIR. In the imperfect CSIR case, we consider a more practical
assumption of having an MMSE estimation of the channel gain at the receivers.
In this case, minimizing the expected achievable distortion associated with
each link is considered. Due to the absence of CSI at the transmitters, the
Gaussian sources are encoded in a successively refinable manner and the
resulting code words are transmitted over the channel using a multi-layer
coding technique. Accordingly, the optimal power assignment between code layers
leading to the least expected achievable distortion, under a mean-square error
criterion is derived for both, the perfect and imperfect CSIR scenarios.
Finally, some numerical examples are provided and it is demonstrated that the
proposed method results in better performance as compared with the conventional
single-layer approach, termed as outage approach.
","Mohammadreza Darabi","","http://arxiv.org/abs/1711.09061v2","http://arxiv.org/pdf/1711.09061v2","","arXiv admin note: submission has been withdrawn by arXiv
  administrators due to inappropriate text reuse from external sources","","","eess.SP","eess.SP|cs.IT|math.IT"
"908","1711.08075v1","2017-11-21 22:48:33","2017-11-21 22:48:33","Intrusion Detection and Ubiquitous Host to Host Encryption","  Growing concern for individual privacy, driven by an increased public
awareness of the degree to which many of our electronic activities are tracked
by interested third parties (e.g. Google knows what I am thinking before I
finish entering my search query), is driving the development anonymizing
technologies (e.g. Tor). The coming mass migration to IPv6 as the primary
transport of Internet traffic promises to make one such technology, end-to-end
host based encryption, more readily available to the average user. In a world
where end-to-end encryption is ubiquitous, what can replace the existing models
for network intrusion detection? How can network administrators and operators,
responsible for securing networks against hostile activity, protect a network
they cannot see? In an encrypted world, signature based event detection is
unlikely to prove useful. In order to secure a network in such an environment,
without trampling the privacy afforded to users by end-to-end encryption, our
threat detection model needs to evolve from signature based detection to a
heuristic model that flags deviations from normal network-wide behavior for
further investigation. In this paper we present such a heuristic model and test
its effectiveness for detecting intrusions in an entirely encrypted network
environment. Our results demonstrate the network intrusion detection system's
ability to monitor a network carrying only host-to-host encrypted traffic. This
work indicates that a broad perspective change is required. Network security
models need to evolve from endeavoring to define attack signatures to
describing what the network looks like under normal conditions and searching
for deviations from the norm.
","Aaron Gibson|Hamilton Scott Clouse","","http://arxiv.org/abs/1711.08075v1","http://arxiv.org/pdf/1711.08075v1","","10 pages, 8 figures, 3 tables","","","cs.CR","cs.CR"
"909","1711.08941v1","2017-11-24 12:30:48","2017-11-24 12:30:48","Neuropeptide Y and its involvement in chronic pain","  Chronic pain is a serious condition that significantly impairs the quality of
life, affecting an estimate of 1.5 billion people worldwide. Despite the
physiological, emotional and financial burden of chronic pain, there is still a
lack of efficient treatments. Neuropeptide Y (NPY) is a highly conserved
endogenous peptide in the central and peripheral nervous system of all mammals,
which has been implicated in both pro- and antinociceptive effects. NPY is
expressed in the superficial laminae of the dorsal horn of the spinal cord,
where it appears to mediate its antinociceptive actions via the Y1 and Y2
receptors. Intrathecal administration of NPY in animal models of neuropathic,
inflammatory or post-operative pain has been shown to cause analgesia, even
though its exact mechanisms are still unclear. It remains to be seen whether
these promising central antinociceptive effects of NPY can be transferred into
a future treatment for chronic pain.
","Marta Diaz-delCastillo|David P. D. Woldbye|Anne Marie Heegaard","","http://arxiv.org/abs/1711.08941v1","http://arxiv.org/pdf/1711.08941v1","http://dx.doi.org/10.1016/j.neuroscience.2017.08.050","18 pages, 1 figure. In press","2017,Neuroscience","10.1016/j.neuroscience.2017.08.050","q-bio.NC","q-bio.NC|q-bio.TO"
"910","1711.09260v2","2017-11-25 16:55:40","2017-12-08 10:02:57","Privacy Risks from Public Data Sources","  In the fight against tax evaders and other cheats, governments seek to gather
more information about their citizens. In this paper we claim that this
increased transparency, combined with ineptitude, or corruption, can lead to
widespread violations of privacy, ultimately harming law-abiding individuals
while helping those engaged in criminal activities such as stalking, identity
theft and so on. In this paper we survey a number of data sources administrated
by the Greek state, offered as web services, to investigate whether they can
lead to leakage of sensitive information. Our study shows that we were able to
download significant portions of the data stored in some of these data sources
(scraping). Moreover, for those data sources that were not amenable to scraping
we looked at ways of extracting information for specific individuals that we
had identified by looking at other data sources. The vulnerabilities we have
discovered enable the collection of personal data and, thus, open the way for a
variety of impersonation attacks, identity theft, confidence trickster attacks
and so on. We believe that the lack of a big picture which was caused by the
piecemeal development of these data sources hides the true extent of the
threat. Hence, by looking at all these data sources together, we outline a
number of mitigation strategies that can alleviate some of the most obvious
attack strategies. Finally, we look at measures that can be taken in the longer
term to safeguard the privacy of the citizens.
","Zacharias Tzermias|Panagiotis Papadopoulos|Sotiris Ioannidis|Vassilis Prevelakis","","http://arxiv.org/abs/1711.09260v2","http://arxiv.org/pdf/1711.09260v2","","","","","cs.CY","cs.CY"
"911","1711.10160v1","2017-11-28 07:48:05","2017-11-28 07:48:05","Snorkel: Rapid Training Data Creation with Weak Supervision","  Labeling training data is increasingly the largest bottleneck in deploying
machine learning systems. We present Snorkel, a first-of-its-kind system that
enables users to train state-of-the-art models without hand labeling any
training data. Instead, users write labeling functions that express arbitrary
heuristics, which can have unknown accuracies and correlations. Snorkel
denoises their outputs without access to ground truth by incorporating the
first end-to-end implementation of our recently proposed machine learning
paradigm, data programming. We present a flexible interface layer for writing
labeling functions based on our experience over the past year collaborating
with companies, agencies, and research labs. In a user study, subject matter
experts build models 2.8x faster and increase predictive performance an average
45.5% versus seven hours of hand labeling. We study the modeling tradeoffs in
this new setting and propose an optimizer for automating tradeoff decisions
that gives up to 1.8x speedup per pipeline execution. In two collaborations,
with the U.S. Department of Veterans Affairs and the U.S. Food and Drug
Administration, and on four open-source text and image data sets representative
of other deployments, Snorkel provides 132% average improvements to predictive
performance over prior heuristic approaches and comes within an average 3.60%
of the predictive performance of large hand-curated training sets.
","Alexander Ratner|Stephen H. Bach|Henry Ehrenberg|Jason Fries|Sen Wu|Christopher Re","","http://arxiv.org/abs/1711.10160v1","http://arxiv.org/pdf/1711.10160v1","http://dx.doi.org/10.14778/3157794.3157797","","Proceedings of the VLDB Endowment, 11(3), 269-282, 2017","10.14778/3157794.3157797","cs.LG","cs.LG|stat.ML"
"912","1711.11005v1","2017-11-29 18:30:11","2017-11-29 18:30:11","A Centralized Reputation Management Scheme for Isolating Malicious
  Controller(s) in Distributed Software-Defined Networks","  Software-Defined Networks have seen an increasing in their deployment because
they offer better network manageability compared to traditional networks.
Despite their immense success and popularity, various security issues in SDN
remain open problems for research. Particularly, the problem of securing the
controllers in distributed environment is still short of any solutions. This
paper proposes a scheme to identify any rogue/malicious controller(s) in a
distributed environment. Our scheme is based on trust and reputation system
which is centrally managed. As such, our scheme identifies any controllers
acting maliciously by comparing the state of installed flows/policies with
policies that should be installed. Controllers rate each other on this basis
and report the results to a central entity, which reports it to the network
administrator.
","Bilal Karim Mughal|Sufian Hameed|Ghulam Muhammad Shaikh","","http://arxiv.org/abs/1711.11005v1","http://arxiv.org/pdf/1711.11005v1","http://dx.doi.org/10.14569/IJACSA.2016.071248","6 pages, 4 figures","International Journal of Advanced Computer Science and
  Applications(ijacsa), 7(12), 2016","10.14569/IJACSA.2016.071248","cs.NI","cs.NI|cs.CR|cs.DC"
"913","1712.00644v1","2017-12-02 17:35:40","2017-12-02 17:35:40","Short-term Mortality Prediction for Elderly Patients Using Medicare
  Claims Data","  Risk prediction is central to both clinical medicine and public health. While
many machine learning models have been developed to predict mortality, they are
rarely applied in the clinical literature, where classification tasks typically
rely on logistic regression. One reason for this is that existing machine
learning models often seek to optimize predictions by incorporating features
that are not present in the databases readily available to providers and policy
makers, limiting generalizability and implementation. Here we tested a number
of machine learning classifiers for prediction of six-month mortality in a
population of elderly Medicare beneficiaries, using an administrative claims
database of the kind available to the majority of health care payers and
providers. We show that machine learning classifiers substantially outperform
current widely-used methods of risk prediction but only when used with an
improved feature set incorporating insights from clinical medicine, developed
for this study. Our work has applications to supporting patient and provider
decision making at the end of life, as well as population health-oriented
efforts to identify patients at high risk of poor outcomes.
","Maggie Makar|Marzyeh Ghassemi|David Cutler|Ziad Obermeyer","","http://arxiv.org/abs/1712.00644v1","http://arxiv.org/pdf/1712.00644v1","","","","","stat.ML","stat.ML|cs.LG"
"914","1712.00841v1","2017-12-03 21:17:24","2017-12-03 21:17:24","Kidemonas: The Silent Guardian","  Advanced Persistent Threats or APTs are big challenges to the security of
government organizations or industry systems. These threats may result in
stealth attacks, but if the attack is confronted before the attacker end goal
has been achieved, the attackers could become aggressive by changing the mode
of attack or by resorting to some form of contingency plan, which might cause
unexpected damage. Therefore, the attack detection and the notification to the
system administrator should be done surreptitiously. This paper presents an
architecture, called Kidemonas, to silently detect the threat and secretly
report it to the user or the system administrator. This way the attacker is
deceived into carrying out the attack, without sending any clear signal so that
the defender can buy time to develop countermeasures to deal with the attack.
We consider several attack scenarios and perform a security analysis to
demonstrate the features of Kidemonas.
","Rudra Prasad Baksi|Shambhu J. Upadhyaya","","http://arxiv.org/abs/1712.00841v1","http://arxiv.org/pdf/1712.00841v1","","Secure Knowledge Management 2017, Tampa, FL,
  http://skmworkshop.wixsite.com/home/program","","","cs.CR","cs.CR"
"915","1712.02549v1","2017-12-07 09:30:12","2017-12-07 09:30:12","A multiplicative masking method for preserving the skewness of the
  original micro-records","  Masking methods for the safe dissemination of microdata consist of distorting
the original data while preserving a pre-defined set of statistical properties
in the microdata. For continuous variables, available methodologies rely
essentially on matrix masking and in particular on adding noise to the original
values, using more or less refined procedures depending on the extent of
information that one seeks to preserve. Almost all of these methods make use of
the critical assumption that the original datasets follow a normal distribution
and/or that the noise has such a distribution. This assumption is, however,
restrictive in the sense that few variables follow empirically a Gaussian
pattern: the distribution of household income, for example, is positively
skewed, and this skewness is essential information that has to be considered
and preserved. This paper addresses these issues by presenting a simple
multiplicative masking method that preserves skewness of the original data
while offering a sufficient level of disclosure risk control. Numerical
examples are provided, leading to the suggestion that this method could be
well-suited for the dissemination of a broad range of microdata, including
those based on administrative and business records.
","Nicolas Ruiz","","http://arxiv.org/abs/1712.02549v1","http://arxiv.org/pdf/1712.02549v1","","","","","cs.CR","cs.CR"
"916","1712.04309v1","2017-12-12 14:31:13","2017-12-12 14:31:13","Mining the Social Media Data for a Bottom-Up Evaluation of Walkability","  Urbanization represents a huge opportunity for computer applications enabling
cities to be managed more efficiently while, at the same time, improving the
life quality of their citizens. One of the potential application of this kind
of systems is a bottom-up evaluation of the level of walkability of the city
(namely the level of usefulness, comfort, safety and attractiveness of an urban
area for walking). This is based on the usage of data from social media for the
computation of structured indicators describing the actual usage of areas by
pedestrians. This paper will present an experimentation of analysis of data
about the city of Milano (Italy) acquired from Flickr and Foursquare. The over
500 thousand points, which represent the photos and the POIs collected from the
above mentioned social meda, were clustered through an iterative approach based
on the DBSCAN algorithm, in order to achieve homogeneous areas defined by the
actual activity of inhabitants and tourists rather than by a top down
administrative procedure and to supply useful indications on the level of
walkability of the city of Milan.
","Christian Berzi|Andrea Gorrini|Giuseppe Vizzari","","http://arxiv.org/abs/1712.04309v1","http://arxiv.org/pdf/1712.04309v1","","Pre-print of a paper presented at the 12th International Conference
  on Traffic and Granular Flow - TGF 2017, 19-22 July 2017, Washington DC, USA
  (2017)","","","cs.CY","cs.CY"
"917","1712.04706v2","2017-12-13 11:14:51","2017-12-14 17:21:02","A High-Level Rule-based Language for Software Defined Network
  Programming based on OpenFlow","  This paper proposes XML-Defined Network policies (XDNP), a new high-level
language based on XML notation, to describe network control rules in Software
Defined Network environments. We rely on existing OpenFlow controllers
specifically Floodlight but the novelty of this project is to separate
complicated language- and framework-specific APIs from policy descriptions.
This separation makes it possible to extend the current work as a northbound
higher level abstraction that can support a wide range of controllers who are
based on different programming languages. By this approach, we believe that
network administrators can develop and deploy network control policies easier
and faster.
","Mehdi Mohammadi|Ala Al-Fuqaha|Zijiang James Yang","","http://arxiv.org/abs/1712.04706v2","http://arxiv.org/pdf/1712.04706v2","","4 pages. This paper has been presented in the poster section of GENI
  Engineering Conference 22 (GEC 22), Washington D.C., March 23-26, 2015","","","cs.NI","cs.NI|cs.PL"
"918","1712.05813v1","2017-12-15 19:16:17","2017-12-15 19:16:17","Realistic Traffic Generation for Web Robots","  Critical to evaluating the capacity, scalability, and availability of web
systems are realistic web traffic generators. Web traffic generation is a
classic research problem, no generator accounts for the characteristics of web
robots or crawlers that are now the dominant source of traffic to a web server.
Administrators are thus unable to test, stress, and evaluate how their systems
perform in the face of ever increasing levels of web robot traffic. To resolve
this problem, this paper introduces a novel approach to generate synthetic web
robot traffic with high fidelity. It generates traffic that accounts for both
the temporal and behavioral qualities of robot traffic by statistical and
Bayesian models that are fitted to the properties of robot traffic seen in web
logs from North America and Europe. We evaluate our traffic generator by
comparing the characteristics of generated traffic to those of the original
data. We look at session arrival rates, inter-arrival times and session
lengths, comparing and contrasting them between generated and real traffic.
Finally, we show that our generated traffic affects cache performance similarly
to actual traffic, using the common LRU and LFU eviction policies.
","Kyle Brown|Derek Doran","","http://arxiv.org/abs/1712.05813v1","http://arxiv.org/pdf/1712.05813v1","http://dx.doi.org/10.1109/ICMLA.2017.0-161","8 pages","","10.1109/ICMLA.2017.0-161","cs.NI","cs.NI|stat.ML"
"919","1712.06790v1","2017-12-19 05:29:37","2017-12-19 05:29:37","Docker-Enabled Build and Execution Environment (BEE): an Encapsulated
  Environment Enabling HPC Applications Running Everywhere","  Variations in High Performance Computing (HPC) system software configurations
mean that applications are typically configured and built for specific HPC
environments. Building applications can require a significant investment of
time and effort for application users and requires application users to have
additional technical knowledge. Container technologies like Docker bring great
benefits to the application development, build and deployment processes. While
much cloud computing infrastructure is already designed to support Docker,
little work has been done to support production Docker deployment on HPC
systems. In this work, we propose a Docker-enabled Build and Execution
Environment (BEE) for HPC systems and detail a standard backend for BEE using
virtual machines, the BEE-VM. This brings many of the benefits of Docker to
existing HPC machines in user-space without the requirement of specialized
pre-installed software and with no system administrator configuration. We show
that current HPC application can be easily configured to run within BEE,
eliminating the need to reconfigure and rebuild applications for different
systems while preserving comparable performance.
","Jieyang Chen|Qiang Guan|Xin Liang|Louis James Vernon|Allen McPherson|Li-Ta Lo|Zizhong Chen|James Paul Ahrens","","http://arxiv.org/abs/1712.06790v1","http://arxiv.org/pdf/1712.06790v1","","","","","cs.DC","cs.DC"
"920","1712.07029v1","2017-12-19 16:32:02","2017-12-19 16:32:02","Sonification of Network Traffic Flow for Monitoring and Situational
  Awareness","  Maintaining situational awareness of what is happening within a network is
challenging, not least because the behaviour happens within computers and
communications networks, but also because data traffic speeds and volumes are
beyond human ability to process. Visualisation is widely used to present
information about the dynamics of network traffic dynamics. Although it
provides operators with an overall view and specific information about
particular traffic or attacks on the network, it often fails to represent the
events in an understandable way. Visualisations require visual attention and so
are not well suited to continuous monitoring scenarios in which network
administrators must carry out other tasks. Situational awareness is critical
and essential for decision-making in the domain of computer network monitoring
where it is vital to be able to identify and recognize network environment
behaviours.Here we present SoNSTAR (Sonification of Networks for SiTuational
AwaReness), a real-time sonification system to be used in the monitoring of
computer networks to support the situational awareness of network
administrators. SoNSTAR provides an auditory representation of all the TCP/IP
protocol traffic within a network based on the different traffic flows between
between network hosts. SoNSTAR raises situational awareness levels for computer
network defence by allowing operators to achieve better understanding and
performance while imposing less workload compared to visual techniques. SoNSTAR
identifies the features of network traffic flows by inspecting the status flags
of TCP/IP packet headers and mapping traffic events to recorded sounds to
generate a soundscape representing the real-time status of the network traffic
environment. Listening to the soundscape allows the administrator to recognise
anomalous behaviour quickly and without having to continuously watch a computer
screen.
","Mohamed Debashi|Paul Vickers","","http://arxiv.org/abs/1712.07029v1","http://arxiv.org/pdf/1712.07029v1","http://dx.doi.org/10.1371/journal.pone.0195948","17 pages, 7 figures plus supplemental material in Github repository","","10.1371/journal.pone.0195948","cs.HC","cs.HC|H.5.2; C.2.3; H.5.5"
"921","1712.08522v1","2017-12-20 21:31:14","2017-12-20 21:31:14","Linking Administrative Data: An Evolutionary Schema","  Statistics New Zealand (Stats NZ) has committed unreservedly to an
administrative data first policy. Thus, all new methods used at Stats NZ are to
be viewed within this context and discussing strategies for using
administrative data is an integral part of every working day. As statistical
methodologists, the three authors were drawn into these discussions. Like most
methodologists, the authors see surveys and the publications of their results
as a process where estimation is the key tool to achieve the final goal of an
accurate statistical output. Randomness and sampling exists to support this
goal, and early on it was clear to us that the incoming it-is-what-it-is data
sources were not randomly selected. These sources were obviously biased and
thus would produce biased estimates. So, we set out to design a strategy to
deal with this issue. This led us to the concept of representativeness which is
closely related to statistical bias but has a wider context invoking both
randomness and judgement. The representativeness issue was the principal
question that we set out to answer. The necessary components that we gathered
for our solution are summarized in the paper.
  Keywords: Representativeness, Timeline Databases, Statistical Registers,
Estimation
","Jack Lothian|Anders Holmberg|Allyson Seyb","","http://arxiv.org/abs/1712.08522v1","http://arxiv.org/pdf/1712.08522v1","","","","","stat.ME","stat.ME"
"922","1712.08875v4","2017-12-24 04:43:46","2018-03-12 16:34:16","Predicting Rich Drug-Drug Interactions via Biomedical Knowledge Graphs
  and Text Jointly Embedding","  Minimizing adverse reactions caused by drug-drug interactions has always been
a momentous research topic in clinical pharmacology. Detecting all possible
interactions through clinical studies before a drug is released to the market
is a demanding task. The power of big data is opening up new approaches to
discover various drug-drug interactions. However, these discoveries contain a
huge amount of noise and provide knowledge bases far from complete and
trustworthy ones to be utilized. Most existing studies focus on predicting
binary drug-drug interactions between drug pairs but ignore other interactions.
In this paper, we propose a novel framework, called PRD, to predict drug-drug
interactions. The framework uses the graph embedding that can overcome data
incompleteness and sparsity issues to achieve multiple DDI label prediction.
First, a large-scale drug knowledge graph is generated from different sources.
Then, the knowledge graph is embedded with comprehensive biomedical text into a
common low dimensional space. Finally, the learned embeddings are used to
efficiently compute rich DDI information through a link prediction process. To
validate the effectiveness of the proposed framework, extensive experiments
were conducted on real-world datasets. The results demonstrate that our model
outperforms several state-of-the-art baseline methods in terms of capability
and accuracy.
","Meng Wang","","http://arxiv.org/abs/1712.08875v4","http://arxiv.org/pdf/1712.08875v4","","This article has been withdrawn by arXiv administrators due to an
  unresolvable authorship dispute","","","cs.AI","cs.AI"
"923","1712.09605v1","2017-12-27 15:57:49","2017-12-27 15:57:49","Accelerators in macroeconomics: Comparison of discrete and continuous
  approaches","  We prove that the standard discrete-time accelerator equation cannot be
considered as an exact discrete analog of the continuous-time accelerator
equation. This leads to fact that the standard discrete-time macroeconomic
models cannot be considered as exact discretization of the corresponding
continuous-time models. As a result, the equations of the continuous and
standard discrete models have different solutions and can predict the different
behavior of the economy. In this paper, we propose a self-consistent
discrete-time description of the economic accelerators that is based on the
exact finite differences. For discrete-time approach, the model equations with
exact differences have the same solutions as the corresponding continuous-time
models and these discrete and continuous models describe the same behavior of
the economy. Using the Harrod-Domar growth model as an example, we show that
equations of the continuous-time model and the suggested exact discrete model
have the same solutions and these models predict the same behavior of the
economy.
","Valentina V. Tarasova|Vasily E. Tarasov","","http://arxiv.org/abs/1712.09605v1","http://arxiv.org/pdf/1712.09605v1","http://dx.doi.org/10.3844/ajebasp.2017.47.55","12 pages, pdf","American Journal of Economics and Business Administration. 2017.
  Vol.9. No.3. P.47-55","10.3844/ajebasp.2017.47.55","q-fin.EC","q-fin.EC|91B02"
"924","1801.00505v2","2018-01-01 20:16:05","2018-02-12 16:40:56","Optimal Stochastic Management of Distributed Energy Storage Embedded
  with Wind Farms","  Increasing wind turbines (WT) penetration and low carbon demand can
potentially lead to two different flow peaks, generation and load, within
distribution networks. This will not only constrain WT penetration but also
pose serious threats to network reliability. This paper proposes energy storage
(ES) to reduce system congestion cost caused by the two peaks by sending
cost-reflective economic signals to affect ES operation in responding to
network conditions. Firstly, a new charging and discharging (C/D) strategy
based on Binary Search Method is designed for ES, which responds to system
congestion cost over time. Then, a novel pricing method, based on Location
Marginal Pricing, is designed for ES. The pricing model is derived by
evaluating ES impact on the network power flows and congestion from the loss
and congestion components in Location Marginal Pricing. The impact is then
converted into an hourly economic signal to reflect ES operation. The proposed
ES C/D strategy and pricing methods are validated on a real local Grid Supply
Point area. Results show that the proposed Location Marginal Pricing-based
pricing is efficient to capture the feature of ES and provide signals for
affecting its operation. This work can further increase network flexibility and
the capability of networks to accommodate increasing WT penetration.
","Xiao Yanchi|Bruce Vargas|Mohammd Hamdi","","http://arxiv.org/abs/1801.00505v2","http://arxiv.org/pdf/1801.00505v2","","arXiv admin note: submission has been withdrawn by arXiv
  administrators due to inappropriate text reuse from external sources","","","math.OC","math.OC|90|F.2.2"
"925","1801.00815v1","2018-01-02 19:36:22","2018-01-02 19:36:22","Advice from the Oracle: Really Intelligent Information Retrieval","  What is ""intelligent"" information retrieval? Essentially this is asking what
is intelligence, in this article I will attempt to show some of the aspects of
human intelligence, as related to information retrieval. I will do this by the
device of a semi-imaginary Oracle. Every Observatory has an oracle, someone who
is a distinguished scientist, has great administrative responsibilities, acts
as mentor to a number of less senior people, and as trusted advisor to even the
most accomplished scientists, and knows essentially everyone in the field. In
an appendix I will present a brief summary of the Statistical Factor Space
method for text indexing and retrieval, and indicate how it will be used in the
Astrophysics Data System Abstract Service. 2018 Keywords: Personal Digital
Assistant; Supervised Topic Models
","Michael J. Kurtz","","http://arxiv.org/abs/1801.00815v1","http://arxiv.org/pdf/1801.00815v1","http://dx.doi.org/10.1007/978-0-585-33110-2_3","Author copy; published 25 years ago at the beginning of the
  Astrophysics Data System; 2018 keywords added","In: Heck A., Murtagh F. (eds) Intelligent Information Retrieval:
  The Case of Astronomy and Related Space Sciences. Astrophysics and Space
  Science Library, vol 182. Springer, Dordrecht (1993)","10.1007/978-0-585-33110-2_3","cs.AI","cs.AI|astro-ph.IM|physics.soc-ph"
"926","1801.01048v1","2018-01-03 15:23:26","2018-01-03 15:23:26","Impact Assessment of Hypothesized Cyberattacks on Interconnected Bulk
  Power Systems","  The first-ever Ukraine cyberattack on power grid has proven its devastation
by hacking into their critical cyber assets. With administrative privileges
accessing substation networks/local control centers, one intelligent way of
coordinated cyberattacks is to execute a series of disruptive switching
executions on multiple substations using compromised supervisory control and
data acquisition (SCADA) systems. These actions can cause significant impacts
to an interconnected power grid. Unlike the previous power blackouts, such
high-impact initiating events can aggravate operating conditions, initiating
instability that may lead to system-wide cascading failure. A systemic
evaluation of ""nightmare"" scenarios is highly desirable for asset owners to
manage and prioritize the maintenance and investment in protecting their
cyberinfrastructure. This survey paper is a conceptual expansion of real-time
monitoring, anomaly detection, impact analyses, and mitigation (RAIM) framework
that emphasizes on the resulting impacts, both on steady-state and dynamic
aspects of power system stability. Hypothetically, we associate the
combinatorial analyses of steady state on substations/components outages and
dynamics of the sequential switching orders as part of the permutation. The
expanded framework includes (1) critical/noncritical combination verification,
(2) cascade confirmation, and (3) combination re-evaluation. This paper ends
with a discussion of the open issues for metrics and future design pertaining
the impact quantification of cyber-related contingencies.
","Chee-Wooi Ten|Koji Yamashita|Zhiyuan Yang|Athanasios V. Vasilakos|Andrew Ginter","","http://arxiv.org/abs/1801.01048v1","http://arxiv.org/pdf/1801.01048v1","http://dx.doi.org/10.1109/TSG.2017.2656068","","","10.1109/TSG.2017.2656068","cs.CR","cs.CR|cs.SY"
"927","1801.01190v1","2018-01-03 21:49:04","2018-01-03 21:49:04","The formation of the heaviest elements","  The rapid neutron-capture process needed to build up many of the elements
heavier than iron seems to take place primarily in neutron-star mergers, not
supernova explosions.
","Anna Frebel|Timothy C. Beers","MIT|Notre Dame","http://arxiv.org/abs/1801.01190v1","http://arxiv.org/pdf/1801.01190v1","","This version removed by arXiv administrators for inappropriate format","","","astro-ph.SR","astro-ph.SR|astro-ph.CO|astro-ph.GA|nucl-ex|nucl-th"
"928","1801.07331v2","2018-01-07 08:57:31","2019-01-31 20:19:19","Simplified Inequalities for Fractional Wavelet Frames on L^2(R)","  In this paper, we provide inequalities for fractional wavelets in a
simplified form on the Hilbert space over Euclidean space R
","M. Younus Bhat","","http://arxiv.org/abs/1801.07331v2","http://arxiv.org/pdf/1801.07331v2","","This article has been withdrawn by arXiv administrators due to
  excessive unattributed and verbatim text overlap from external sources","","","math.FA","math.FA"
"929","1801.07332v2","2018-01-07 09:08:59","2019-01-31 20:19:26","Sufficient Conditions for Composite Wavelet Frames in L^2(R^n)","  In this paper, we provide conditions which are sufficient to form composite
wavelet frames on the Hilbert space of Euclidean space over R^n
","M. Younus Bhat","","http://arxiv.org/abs/1801.07332v2","http://arxiv.org/pdf/1801.07332v2","","This article has been withdrawn by arXiv administrators due to
  excessive unattributed and verbatim text overlap from external sources","","","math.FA","math.FA"
"930","1801.04179v1","2018-01-12 14:35:19","2018-01-12 14:35:19","Arhuaco: Deep Learning and Isolation Based Security for Distributed
  High-Throughput Computing","  Grid computing systems require innovative methods and tools to identify
cybersecurity incidents and perform autonomous actions i.e. without
administrator intervention. They also require methods to isolate and trace job
payload activity in order to protect users and find evidence of malicious
behavior. We introduce an integrated approach of security monitoring via
Security by Isolation with Linux Containers and Deep Learning methods for the
analysis of real time data in Grid jobs running inside virtualized
High-Throughput Computing infrastructure in order to detect and prevent
intrusions. A dataset for malware detection in Grid computing is described. We
show in addition the utilization of generative methods with Recurrent Neural
Networks to improve the collected dataset. We present Arhuaco, a prototype
implementation of the proposed methods. We empirically study the performance of
our technique. The results show that Arhuaco outperforms other methods used in
Intrusion Detection Systems for Grid Computing. The study is carried out in the
ALICE Collaboration Grid, part of the Worldwide LHC Computing Grid.
","A. Gomez Ramirez|C. Lara|L. Betev|D. Bilanovic|U. Kebschull","and for the ALICE Collaboration|and for the ALICE Collaboration|and for the ALICE Collaboration|and for the ALICE Collaboration|and for the ALICE Collaboration","http://arxiv.org/abs/1801.04179v1","http://arxiv.org/pdf/1801.04179v1","","Manuscript submitted to the Journal of Grid Computing","","","cs.DC","cs.DC|cs.CR|cs.LG"
"931","1801.04267v1","2018-01-12 18:57:10","2018-01-12 18:57:10","Management's Perspective on Critical Success Factors Affecting Mobile
  Learning in Higher Education Institutions - An Empirical Study","  Mobile learning (m-Learning) is considered to be one of the fastest growing
learning platforms. The immense interest in m-Learning is attributed to the
incredible rate of growth of mobile technology and its proliferation into every
aspect of modern life. Despite this, m-Learning has not experienced a similar
adoption rate in the education sector, chiefly higher education. Researchers
have attempted to explain this anomaly by conducting several studies in the
area. However, mostly the research in m-Learning is examined from the
perspective of the students and educators. In this research, it is contended
that there is a third important stakeholder group whose opinion is equally
important in determining the success of m-Learning: the university management.
Although diversified by nature, heads of departments, deans, and IT system
administrators are nevertheless considered members of any university
management. The results of the research show that university commitment to
m-Learning, university learning practices, and change management practices were
the factors critical to the success of m-Learning, from the university
management perspective.
","Muasaad Alrasheedi|Luiz Fernando Capretz|Arif Raza","","http://arxiv.org/abs/1801.04267v1","http://arxiv.org/pdf/1801.04267v1","http://dx.doi.org/10.1177/0735633115620387","","Volume 2015, pp. 1-22","10.1177/0735633115620387","cs.CY","cs.CY"
"932","1801.04824v1","2018-01-15 14:39:10","2018-01-15 14:39:10","Susceptibility of Methicillin Resistant Staphylococcus aureus to
  Vancomycin using Liposomal Drug Delivery System","  Staphylococcus aureus responsible for nosocomial infections is a significant
threat to the public health. The increasing resistance of S.aureus to various
antibiotics has drawn it to a prime focus for research on designing an
appropriate drug delivery system. Emergence of Methicillin Resistant
Staphylococcus aureus (MRSA) in 1961, necessitated the use of vancomycin ""the
drug of last resort"" to treat these infections. Unfortunately, S.aureus has
already started gaining resistances to vancomycin. Liposome encapsulation of
drugs have been earlier shown to provide an efficient method of microbial
inhibition in many cases. We have studied the effect of liposome encapsulated
vancomycin on MRSA and evaluated the antibacterial activity of the
liposome-entrapped drug in comparison to that of the free drug based on the
minimum inhibitory concentration (MIC) of the drug. The MIC for liposomal
vancomycin was found to be about half of that of free vancomycin. The growth
response of MRSA showed that the liposomal vancomycin induced the culture to go
into bacteriostatic state and phagocytic killing was enhanced. Administration
of the antibiotic encapsulated in liposome thus was shown to greatly improve
the drug delivery as well as the drug resistance caused by MRSA.
","Kiran Vishwasrao|Arjumanara Surti|S. Radha","","http://arxiv.org/abs/1801.04824v1","http://arxiv.org/pdf/1801.04824v1","","","","","q-bio.TO","q-bio.TO|cond-mat.soft|physics.bio-ph"
"933","1801.05244v1","2018-01-16 13:20:31","2018-01-16 13:20:31","Assessing Bayesian Nonparametric Log-Linear Models: an application to
  Disclosure Risk estimation","  We present a method for identification of models with good predictive
performances in the family of Bayesian log-linear mixed models with Dirichlet
process random effects. Such a problem arises in many different applications;
here we consider it in the context of disclosure risk estimation, an
increasingly relevant issue raised by the increasing demand for data collected
under a pledge of confidentiality. Two different criteria are proposed and
jointly used via a two-stage selection procedure, in a M-open view. The first
stage is devoted to identifying a path of search; then, at the second, a small
number of nonparametric models is evaluated through an application-specific
score based Bayesian information criterion. We test our method on a variety of
contingency tables based on microdata samples from the US Census Bureau and the
Italian National Security Administration, treated here as populations, and
carefully discuss its features. This leads us to a journey around different
forms and sources of bias along which we show that (i) while based on the so
called ""score+search"" paradigm, our method is by construction well protected
from the selection-induced bias, and (ii) models with good performances are
invariably characterized by an extraordinarily simple structure of fixed
effects. The complexity of model selection - a very challenging and difficult
task in a strictly parametric context with large and sparse tables - is
therefore significantly defused by our approach. An attractive collateral
result of our analysis are fruitful new ideas about modeling in small area
estimation problems, where interest is in total counts over cells with a small
number of observations.
","Cinzia Carota|Maurizio Filippone|Silvia Polettini","","http://arxiv.org/abs/1801.05244v1","http://arxiv.org/pdf/1801.05244v1","","32 pages, 7 figures","","","stat.ME","stat.ME|stat.AP"
"934","1801.05643v1","2018-01-17 12:51:01","2018-01-17 12:51:01","The Case for Automatic Database Administration using Deep Reinforcement
  Learning","  Like any large software system, a full-fledged DBMS offers an overwhelming
amount of configuration knobs. These range from static initialisation
parameters like buffer sizes, degree of concurrency, or level of replication to
complex runtime decisions like creating a secondary index on a particular
column or reorganising the physical layout of the store. To simplify the
configuration, industry grade DBMSs are usually shipped with various advisory
tools, that provide recommendations for given workloads and machines. However,
reality shows that the actual configuration, tuning, and maintenance is usually
still done by a human administrator, relying on intuition and experience.
Recent work on deep reinforcement learning has shown very promising results in
solving problems, that require such a sense of intuition. For instance, it has
been applied very successfully in learning how to play complicated games with
enormous search spaces. Motivated by these achievements, in this work we
explore how deep reinforcement learning can be used to administer a DBMS.
First, we will describe how deep reinforcement learning can be used to
automatically tune an arbitrary software system like a DBMS by defining a
problem environment. Second, we showcase our concept of NoDBA at the concrete
example of index selection and evaluate how well it recommends indexes for
given workloads.
","Ankur Sharma|Felix Martin Schuhknecht|Jens Dittrich","","http://arxiv.org/abs/1801.05643v1","http://arxiv.org/pdf/1801.05643v1","","","","","cs.DB","cs.DB|cs.AI"
"935","1801.05764v1","2018-01-17 17:32:00","2018-01-17 17:32:00","M-STAR: A Modular, Evidence-based Software Trustworthiness Framework","  Despite years of intensive research in the field of software vulnerabilities
discovery, exploits are becoming ever more common. Consequently, it is more
necessary than ever to choose software configurations that minimize systems'
exposure surface to these threats. In order to support users in assessing the
security risks induced by their software configurations and in making informed
decisions, we introduce M-STAR, a Modular Software Trustworthiness ARchitecture
and framework for probabilistically assessing the trustworthiness of software
systems, based on evidence, such as their vulnerability history and source code
properties.
  Integral to M-STAR is a software trustworthiness model, consistent with the
concept of computational trust. Computational trust models are rooted in
Bayesian probability and Dempster-Shafer Belief theory, offering mathematical
soundness and expressiveness to our framework. To evaluate our framework, we
instantiate M-STAR for Debian Linux packages, and investigate real-world
deployment scenarios. In our experiments with real-world data, M-STAR could
assess the relative trustworthiness of complete software configurations with an
error of less than 10%. Due to its modular design, our proposed framework is
agile, as it can incorporate future advances in the field of code analysis and
vulnerability prediction. Our results point out that M-STAR can be a valuable
tool for system administrators, regular users and developers, helping them
assess and manage risks associated with their software configurations.
","Nikolaos Alexopoulos|Sheikh Mahbub Habib|Steffen Schulz|Max Muhlhauser","","http://arxiv.org/abs/1801.05764v1","http://arxiv.org/pdf/1801.05764v1","","18 pages, 13 figures","","","cs.CR","cs.CR"
"936","1801.05916v2","2018-01-18 02:57:25","2018-01-24 12:32:25","Citation Analysis of Innovative ICT and Advances of Governance
  (2008-2017)","  This paper opens by introducing the Internet Plus Government (IPG), a new
government initiative emerging in the last decade. To understand benefits and
challenges associated with this initiative worldwide, we conducted analyses on
research articles published in the e-governance area between 2008 and 2017.
Content analysis and citation analysis were performed on 2105 articles to
address three questions: (1) What types of new ICT have been adopted in the IPG
initiative in the past decade? (2) How did scholars investigate interactions
between the new ICTs and governance core to IPG? (3) How did the new ICTs
interact and shape while also being shaped by the evolution of governance in
the past decade? Our analysis suggests that IPG initiative has enriched the
government information infrastructure. It presented opportunities to accumulate
and use huge volume of data for better decision making and proactive
government-citizen interaction. At the same time, the advance of open data, the
widespread use of social media and the potential of data analytics also
generated great pressure to address challenging questions and issues in the
domain of e-democracy.
","Shuhua Monica Liu|Liting Pan|Xiaowei Chen","Department of Public Administration, Fudan University, Shanghai, China|Department of Public Administration, Fudan University, Shanghai, China|Department of Public Administration, Fudan University, Shanghai, China","http://arxiv.org/abs/1801.05916v2","http://arxiv.org/pdf/1801.05916v2","","Corrected first author's name spelling and added authors' affiliation
  in the metadata","","","cs.SI","cs.SI|cs.CY|cs.DL"
"937","1801.05958v1","2018-01-18 07:38:13","2018-01-18 07:38:13","On a Generic Security Game Model","  To protect the systems exposed to the Internet against attacks, a security
system with the capability to engage with the attacker is needed. There have
been attempts to model the engagement/interactions between users, both benign
and malicious, and network administrators as games. Building on such works, we
present a game model which is generic enough to capture various modes of such
interactions. The model facilitates stochastic games with imperfect
information. The information is imperfect due to erroneous sensors leading to
incorrect perception of the current state by the players. To model this error
in perception distributed over other multiple states, we use Euclidean
distances between the outputs of the sensors. We build a 5-state game to
represent the interaction of the administrator with the user. The states
correspond to 1) the user being out of the system in the Internet, and after
logging in to the system; 2) having low privileges; 3) having high privileges;
4) when he successfully attacks and 5) gets trapped in a honeypot by the
administrator. Each state has its own action set. We present the game with a
distinct perceived action set corresponding to each distinct information set of
these states. The model facilitates stochastic games with imperfect
information. The imperfect information is due to erroneous sensors leading to
incorrect perception of the current state by the players. To model this error
in perception distributed over the states, we use Euclidean distances between
outputs of the sensors. A numerical simulation of an example game is presented
to show the evaluation of rewards to the players and the preferred strategies.
We also present the conditions for formulating the strategies when dealing with
more than one attacker and making collaborations.
","Vivek Shandilya|Sajjan Shiva","","http://arxiv.org/abs/1801.05958v1","http://arxiv.org/pdf/1801.05958v1","http://dx.doi.org/10.4236/ijcns.2017.107008","31 pages","Shandilya, V. and Shiva, S. (2017) On a Generic Security Game
  Model. Int. J. Communications , Network and System Sciences , 10, 142-172","10.4236/ijcns.2017.107008","cs.CR","cs.CR|cs.GT"
"938","1801.06874v2","2018-01-21 18:49:28","2018-02-12 16:40:40","Optimal Co-Optimizing of Distributed Generation and ESS Facilities
  Considering Voltage Stability","  The paper deals with the optimal sizing and allocation of dispersed
generation, and distributed storage systems. The optimization aims at
minimizing the sum of the costs sustained by the distributor for the power
losses, for network upgrading, for carrying out the reactive power service and
the costs of storage and capacitor installation, over a planning period of
several years. Also, analysis of the effect of distributed generation capacity
and location on voltage stability enhancement of distribution networks are
presented in this paper. The analysis is performed using a steady state voltage
stability index which can be evaluated at each node of the distribution system.
Different optimal capacities and locations are used to check this effect. The
location of distributed generation has the main effect on the system voltage
stability. Voltage stability should be taken into account as one of the
objectives of distributed generation optimal allocation techniques. The
analysis is conducted on a 69-node distribution system in order to show the
feasibility of the proposed procedure.
","Maichel Linotive|Bruce Vargas|Mohammd Hamdi","","http://arxiv.org/abs/1801.06874v2","http://arxiv.org/pdf/1801.06874v2","","arXiv admin note: submission has been withdrawn by arXiv
  administrators due to inappropriate text reuse from external sources","","","math.OC","math.OC|90|F.2.2"
"939","1801.07138v1","2018-01-22 15:34:38","2018-01-22 15:34:38","Wikipedia in academia as a teaching tool: from averse to proactive
  faculty profiles","  This study concerned the active use of Wikipedia as a teaching tool in the
classroom in higher education, trying to identify different usage profiles and
their characterization. A questionnaire survey was administrated to all
full-time and part-time teachers at the Universitat Oberta de Catalunya and the
Universitat Pompeu Fabra, both in Barcelona, Spain. The questionnaire was
designed using the Technology Acceptance Model as a reference, including items
about teachers web 2.0 profile, Wikipedia usage, expertise, perceived
usefulness, easiness of use, visibility and quality, as well as Wikipedia
status among colleagues and incentives to use it more actively. Clustering and
statistical analysis were carried out using the k-medoids algorithm and
differences between clusters were assessed by means of contingency tables and
generalized linear models (logit). The respondents were classified in four
clusters, from less to more likely to adopt and use Wikipedia in the classroom,
namely averse (25.4%), reluctant (17.9%), open (29.5%) and proactive (27.2%).
Proactive faculty are mostly men teaching part-time in STEM fields, mainly
engineering, while averse faculty are mostly women teaching full-time in
non-STEM fields. Nevertheless, questionnaire items related to visibility,
quality, image, usefulness and expertise determine the main differences between
clusters, rather than age, gender or domain. Clusters involving a positive view
of Wikipedia and at least some frequency of use clearly outnumber those with a
strictly negative stance. This goes against the common view that faculty
members are mostly sceptical about Wikipedia. Environmental factors such as
academic culture and colleagues opinion are more important than faculty
personal characteristics, especially with respect to what they think about
Wikipedia quality.
","Julia Minguillon|Eduard Aibar|Maura Lerga|Josep Llados|Antoni Meseguer-Artola","","http://arxiv.org/abs/1801.07138v1","http://arxiv.org/pdf/1801.07138v1","","16 pages, 1 figure, 8 tables","","","cs.SI","cs.SI|cs.CY|K.3.1; H.3.5"
"940","1801.07215v1","2018-01-22 17:42:32","2018-01-22 17:42:32","Get Your Workload in Order: Game Theoretic Prioritization of Database
  Auditing","  For enhancing the privacy protections of databases, where the increasing
amount of detailed personal data is stored and processed, multiple mechanisms
have been developed, such as audit logging and alert triggers, which notify
administrators about suspicious activities; however, the two main limitations
in common are: 1) the volume of such alerts is often substantially greater than
the capabilities of resource-constrained organizations, and 2) strategic
attackers may disguise their actions or carefully choosing which records they
touch, making incompetent the statistical detection models. For solving them,
we introduce a novel approach to database auditing that explicitly accounts for
adversarial behavior by 1) prioritizing the order in which types of alerts are
investigated and 2) providing an upper bound on how much resource to allocate
for each type. We model the interaction between a database auditor and
potential attackers as a Stackelberg game in which the auditor chooses an
auditing policy and attackers choose which records to target. A corresponding
approach combining linear programming, column generation, and heuristic search
is proposed to derive an auditing policy. For testing the policy-searching
performance, a publicly available credit card application dataset are adopted,
on which it shows that our methods produce high-quality mixed strategies as
database audit policies, and our general approach significantly outperforms
non-game-theoretic baselines.
","Chao Yan|Bo Li|Yevgeniy Vorobeychik|Aron Laszka|Daniel Fabbri|Bradley Malin","","http://arxiv.org/abs/1801.07215v1","http://arxiv.org/pdf/1801.07215v1","","","","","cs.AI","cs.AI|cs.CR|cs.DB|cs.GT|cs.MA|D.4.6; H.2.0; K.6.5; J.1; I.2"
"941","1801.08248v1","2018-01-25 00:46:30","2018-01-25 00:46:30","Generating survival times using Cox proportional hazards models with
  cyclic time-varying covariates, with application to a multiple-dose
  monoclonal antibody clinical trial","  In two harmonized efficacy studies to prevent HIV infection through multiple
infusions of the monoclonal antibody VRC01, a key objective is to evaluate
whether the serum concentration of VRC01, which changes cyclically over time
along with the infusion schedule, is associated with the rate of HIV infection.
Simulation studies are needed in the development of such survival models. In
this paper, we consider simulating event time data with a continuous
time-varying covariate whose values vary with time through multiple drug
administration cycles, and whose effect on survival changes differently before
and after a threshold within each cycle. The latter accommodates settings with
a zero-protection biomarker threshold above which the drug provides a varying
level of protection depending on the biomarker level, but below which the drug
provides no protection. We propose two simulation approaches: one based on
simulating survival data under a single-dose regimen first before data are
aggregated over multiple doses, and another based on simulating survival data
directly under a multiple-dose regimen. We generate time-to-event data
following a Cox proportional hazards model based on inverting the cumulative
hazard function and a log link function for relating the hazard function to the
covariates. The method's validity is assessed in two sets of simulation
experiments. The results indicate that the proposed procedures perform well in
producing data that conform to their cyclic nature and assumptions of the Cox
proportional hazards model.
","Yunda Huang|Yuanyuan Zhang|Zong Zhang|Peter B. Gilbert","","http://arxiv.org/abs/1801.08248v1","http://arxiv.org/pdf/1801.08248v1","","","","","stat.ME","stat.ME"
"942","1801.09061v3","2018-01-27 09:36:22","2018-12-04 07:31:21","SWRL2SPIN: A tool for transforming SWRL rule bases in OWL ontologies to
  object-oriented SPIN rules","  Semantic Web Rule Language (SWRL) combines OWL (Web Ontology Language)
ontologies with Horn Logic rules of the Rule Markup Language (RuleML) family.
Being supported by ontology editors, rule engines and ontology reasoners, it
has become a very popular choice for developing rule-based applications on top
of ontologies. However, SWRL is probably not go-ing to become a WWW Consortium
standard, prohibiting industrial acceptance. On the other hand, SPIN (SPARQL
Inferencing Notation) has become a de-facto industry standard to rep-resent
SPARQL rules and constraints on Semantic Web models, building on the widespread
acceptance of SPARQL (SPARQL Protocol and RDF Query Language). In this paper,
we ar-gue that the life of existing SWRL rule-based ontology applications can
be prolonged by con-verting them to SPIN. To this end, we have developed the
SWRL2SPIN tool in Prolog that transforms SWRL rules into SPIN rules,
considering the object-orientation of SPIN, i.e. linking rules to the
appropriate ontology classes and optimizing them, as derived by analysing the
rule conditions.
","Nick Bassiliades","","http://arxiv.org/abs/1801.09061v3","http://arxiv.org/pdf/1801.09061v3","","arXiv admin note: This version has been removed by arXiv
  administrators due to copyright infringement. Previous versions available
  arXiv:1801.09061v2 and arXiv:1801.09061v1","","","cs.AI","cs.AI"
"943","1801.09528v2","2018-01-29 14:37:04","2018-05-15 16:41:32","Similarities vs. key discrepancy between tuberculosis and cancer","  In 2015 in the United States 612,000 persons died from cancer whereas only
470 died from tuberculosis (TB), a disease which was the main cause of death
around 1900. How can one explain such a key discrepancy in treatment progress?
A statistical comparison between TB and cancer will give some clues. However,
TB and cancer also share several important features. Both TB and cancer can
affect several organs, e.g. lungs, brain, bones, intestines, skin. What in
cancer is called malignant neoplasm (tumor) is called granuloma in TB. By
isolating malignant cells (versus bacteria) from the rest of the body, such
clusters protect the host's organism but at the same time they are ""secure
beachheads"" from where malignant cells (versus infected macrophages) can wander
off to new locations. Thus, metastatic tumors have a TB parallel in the form of
secondary granulomas. In order to investigate more closely this parallel we use
the age-specific response of organs. Called spectrometric analysis in a
previous paper (Berrut et al. 2017), this method provides information about how
fast tumors develop and how serious they become. A convenient characterization
of the response to TB of organ j is given by the following (age-dependent)
death ratio: Tj(t)=(death by TB of type j)/(all TB deaths) The development of
cancer tumors can be described by a similar function Cj(t). We compare the
organs' responses in all cases for which specific death data are available. It
appears that for the same organ Tj(t) is similar in shape to Cj(t). For
instance, with regard to brain tumors, both TB and cancer mortality peak around
the age of 10. Such observations may bring to light vulnerabilities in the way
the immune system provides protection to various organs.
","Peter Richmond|Bertrand M. Roehner","","http://arxiv.org/abs/1801.09528v2","http://arxiv.org/pdf/1801.09528v2","","22p, 8 figures. An old version was replaced by a new one which is
  about twice as long. At first the new version was submitted as a new paper,
  but at the request of the administration a replacement seemed a more
  appropriate solution","","","q-bio.TO","q-bio.TO|physics.bio-ph"
"944","1802.00324v1","2018-01-31 10:38:07","2018-01-31 10:38:07","One-class Collective Anomaly Detection based on Long Short-Term Memory
  Recurrent Neural Networks","  Intrusion detection for computer network systems has been becoming one of the
most critical tasks for network administrators today. It has an important role
for organizations, governments and our society due to the valuable resources
hosted on computer networks. Traditional misuse detection strategies are unable
to detect new and unknown intrusion types. In contrast, anomaly detection in
network security aims to distinguish between illegal or malicious events and
normal behavior of network systems. Anomaly detection can be considered as a
classification problem where it builds models of normal network behavior, of
which it uses to detect new patterns that significantly deviate from the model.
Most of the current approaches on anomaly detection is based on the learning of
normal behavior and anomalous actions. They do not include memory that is they
do not take into account previous events classify new ones. In this paper, we
propose a one class collective anomaly detection model based on neural network
learning. Normally a Long Short Term Memory Recurrent Neural Network (LSTM RNN)
is trained only on normal data, and it is capable of predicting several time
steps ahead of an input. In our approach, a LSTM RNN is trained on normal time
series data before performing a prediction for each time step. Instead of
considering each time-step separately, the observation of prediction errors
from a certain number of time-steps is now proposed as a new idea for detecting
collective anomalies. The prediction errors of a certain number of the latest
time-steps above a threshold will indicate a collective anomaly. The model is
evaluated on a time series version of the KDD 1999 dataset. The experiments
demonstrate that the proposed model is capable to detect collective anomaly
efficiently
","Nga Nguyen Thi|Van Loi Cao|Nhien-An Le-Khac","","http://arxiv.org/abs/1802.00324v1","http://arxiv.org/pdf/1802.00324v1","","arXiv admin note: substantial text overlap with arXiv:1703.09752","","","cs.LG","cs.LG|stat.ML"
"945","1802.00230v1","2018-02-01 10:30:20","2018-02-01 10:30:20","Integrity Coded Databases: An Evaluation of Performance, Efficiency, and
  Practicality","  In recent years, cloud database storage has become an inexpensive and
convenient option for businesses and individuals to store information. While
its positive aspects make the cloud extremely attractive for data storage, it
is a relatively new area of service, making it vulnerable to cyber-attacks and
security breaches. Storing data in a foreign location also requires the owner
to relinquish control of their information to system administrators of these
online database services. This opens the possibility for malicious, internal
attacks on the data that may involve the manipulation, omission, or addition of
data. The retention of the data as it was intended to be stored is referred to
as the database's integrity. Our research tests a potential solution for
maintaining the integrity of these cloud-storage databases by converting the
original databases to Integrity Coded Databases (ICDB). ICDBs utilize Integrity
Codes: cryptographic codes created alongside the data by a private key that
only the data owner has access to. When the database is queried, an integrity
code is returned along with the queried information. The owner is then able to
verify that the information is correct, complete, and fresh. Consequently,
ICDBs also incur performance and memory penalties. In our research, we explore,
test, and benchmark ICDBs to determine the costs and benefits of maintaining an
ICDB versus a standard database.
","Dan Kondratyuk|Jake Rodden|Elmer Duran","","http://arxiv.org/abs/1802.00230v1","http://arxiv.org/pdf/1802.00230v1","","11 pages, 7 figures. Research Experience for Undergraduates in
  Software Security, Boise State University, July 2015","","","cs.DB","cs.DB|cs.CR"
"946","1802.00577v1","2018-02-02 06:43:46","2018-02-02 06:43:46","Ensuring Data Integrity in Electronic Health Records: A Quality Health
  Care Implication","  An Electronic Health Record (EHR) system must enable efficient availability
of meaningful, accurate and complete data to assist improved clinical
administration through the development, implementation and optimisation of
clinical pathways. Therefore data integrity is the driving force in EHR systems
and is an essential aspect of service delivery at all levels. However,
preserving data integrity in EHR systems has become a major problem because of
its consequences in promoting high standards of patient care. In this paper, we
review and address the impact of data integrity of the use of EHR system and
its associated issues. We determine and analyse three phases of data integrity
of an EHR system. Finally, we also present an appropriate method to preserve
the integrity in EHR systems. To analyse and evaluate the data integrity, one
of the major clinical systems in Australia is considered. This will demonstrate
the impact on quality and safety of patient care.
","P. Vimalachandran|H. Wang|Y. Zhang|B. Heyward|F. Whittaker","","http://arxiv.org/abs/1802.00577v1","http://arxiv.org/pdf/1802.00577v1","","8 pages, ICOT2016: http://www.icot-conference.org/2016/","","","cs.CY","cs.CY|cs.CR"
"947","1802.02953v2","2018-02-08 16:32:53","2018-09-27 17:41:44","Open Data, Grey Data, and Stewardship: Universities at the Privacy
  Frontier","  As universities recognize the inherent value in the data they collect and
hold, they encounter unforeseen challenges in stewarding those data in ways
that balance accountability, transparency, and protection of privacy, academic
freedom, and intellectual property. Two parallel developments in academic data
collection are converging: (1) open access requirements, whereby researchers
must provide access to their data as a condition of obtaining grant funding or
publishing results in journals; and (2) the vast accumulation of 'grey data'
about individuals in their daily activities of research, teaching, learning,
services, and administration. The boundaries between research and grey data are
blurring, making it more difficult to assess the risks and responsibilities
associated with any data collection. Many sets of data, both research and grey,
fall outside privacy regulations such as HIPAA, FERPA, and PII. Universities
are exploiting these data for research, learning analytics, faculty evaluation,
strategic decisions, and other sensitive matters. Commercial entities are
besieging universities with requests for access to data or for partnerships to
mine them. The privacy frontier facing research universities spans open access
practices, uses and misuses of data, public records requests, cyber risk, and
curating data for privacy protection. This paper explores the competing values
inherent in data stewardship and makes recommendations for practice, drawing on
the pioneering work of the University of California in privacy and information
security, data governance, and cyber risk.
","Christine L. Borgman","","http://arxiv.org/abs/1802.02953v2","http://arxiv.org/pdf/1802.02953v2","http://dx.doi.org/10.15779/Z38B56D489","Final published version, Sept 30, 2018","Borgman, C.L. (2018). Open data, grey data, and stewardship:
  Universities at the privacy frontier. Berkeley Technology Law Journal, 33:2,
  365-412","10.15779/Z38B56D489","cs.DL","cs.DL|cs.CR|cs.CY"
"948","1802.03343v2","2018-02-09 16:48:15","2018-05-02 09:25:57","Long-Term Unemployed hirings: Should targeted or untargeted policies be
  preferred?","  To what extent, hiring incentives targeting a specific group of vulnerable
unemployed (i.e. long term unemployed) are more effective, with respect to
generalised incentives (without a definite target), to increase hirings of the
targeted group? Are generalized incentives able to influence hirings of the
vulnerable group? Do targeted policies have negative side effects too important
to accept them? Even though there is a huge literature on hiring subsidies,
these questions remained unresolved. We tried to answer them, comparing the
impact of two similar hiring policies, one oriented towards a target group and
one generalised, implemented on the italian labour market. We used
administrative data on job contracts, and counterfactual analysis methods. The
targeted policy had a positive and significant impact, while the generalized
policy didn't have a significant impact on the vulnerable group. Moreover, we
concluded the targeted policy didn't have any indirect negative side effect.
","Alessandra Pasquini|Marco Centra|Guido Pellegrini","MEMOTEF Department Sapienza University of Rome, INAPP, Scienze Sociali ed Eonomiche Department Sapienza University of Rome|MEMOTEF Department Sapienza University of Rome, INAPP, Scienze Sociali ed Eonomiche Department Sapienza University of Rome|MEMOTEF Department Sapienza University of Rome, INAPP, Scienze Sociali ed Eonomiche Department Sapienza University of Rome","http://arxiv.org/abs/1802.03343v2","http://arxiv.org/pdf/1802.03343v2","","30 pages including the appendix, 6 figures, 14 tables, previous
  versions presented at IWCEE2017 and Astril 2017 Conferences","","","econ.EM","econ.EM|stat.AP"
"949","1802.03418v3","2018-02-09 19:19:17","2019-01-12 16:13:33","Predicting University Students' Academic Success and Major using Random
  Forests","  In this article, a large data set containing every course taken by every
undergraduate student in a major university in Canada over 10 years is
analysed. Modern machine learning algorithms can use large data sets to build
useful tools for the data provider, in this case, the university. In this
article, two classifiers are constructed using random forests. To begin, the
first two semesters of courses completed by a student are used to predict if
they will obtain an undergraduate degree. Secondly, for the students that
completed a program, their major is predicted using once again the first few
courses they have registered to. A classification tree is an intuitive and
powerful classifier and building a random forest of trees improves this
classifier. Random forests also allow for reliable variable importance
measurements. These measures explain what variables are useful to the
classifiers and can be used to better understand what is statistically related
to the students' situation. The results are two accurate classifiers and a
variable importance analysis that provides useful information to university
administrations.
","Cedric Beaulac|Jeffrey S. Rosenthal","","http://arxiv.org/abs/1802.03418v3","http://arxiv.org/pdf/1802.03418v3","http://dx.doi.org/10.1007/s11162-019-09546-y","","","10.1007/s11162-019-09546-y","stat.ML","stat.ML|cs.LG"
"950","1802.03613v2","2018-02-10 16:10:10","2018-05-30 17:47:32","Security level analysis of academic information systems based on
  standard ISO 27002:2003 using SSE-CMM","  This research was conducted to find out the level of information security in
an organization to give recommendations improvements in information security
management at the organization. This research uses the ISO 27002 by involving
the entire clause that exists in ISO 27002 check-lists. Based on the analysis
results, 13 objective controls and 43 security controls were scattered in 3
clauses of ISO 27002. From the analysis it was concluded that the maturity
level of information system security governance was 2.51, which means the level
of security is still at level 2 planned and tracked is planned and tracked
actively) but is approaching level 3 well defined.
","Endang Kurniawan|Imam Riadi","","http://arxiv.org/abs/1802.03613v2","http://arxiv.org/pdf/1802.03613v2","","arXiv admin note: submission has been withdrawn by arXiv
  administrators due to inappropriate overlap with external sources","","","cs.CY","cs.CY"
"951","1802.05329v2","2018-02-14 21:39:54","2018-03-01 15:21:19","Beam shaping algorithm with optimization parameters","  Withdrawn by arXiv administrators because the author did not have the right
to agree to our license at the time of submission.
","Nicolas Barre","","http://arxiv.org/abs/1802.05329v2","http://arxiv.org/pdf/1802.05329v2","","This version withdrawn by arXiv administrators because the author did
  not have the right to agree to our license at the time of submission","","","physics.comp-ph","physics.comp-ph"
"952","1802.06043v2","2018-02-16 17:34:38","2018-02-26 18:59:00","Flawed Waveform Design of Augusto Aubry, Antonio DeMaio et al","  arXiv admin note: This submission has been withdrawn by arXiv administrators
due to unprofessional personal attack.
","Pawan Setlur|Muralidhar Rangaswamy","","http://arxiv.org/abs/1802.06043v2","http://arxiv.org/pdf/1802.06043v2","","arXiv admin note: This submission has been withdrawn by arXiv
  administrators due to unprofessional personal attack","","","eess.SP","eess.SP"
"953","1802.06270v1","2018-02-17 18:15:53","2018-02-17 18:15:53","MAVIS: Managing Datacenters using Smartphones","  Distributed monitoring plays a crucial role in managing the activities of
cloud-based datacenters. System administrators have long relied on monitoring
systems such as Nagios and Ganglia to obtain status alerts on their
desktop-class machines. However, the popularity of mobile devices is pushing
the community to develop datacenter monitoring solutions for smartphone-class
devices. Here we lay out desirable characteristics of such smartphone-based
monitoring and identify quantitatively the shortcomings from directly applying
existing solutions to this domain. Then we introduce a possible design that
addresses some of these shortcomings and provide results from an early
prototype, called MAVIS, using one month of monitoring data from approximately
3,000 machines hosted by Purdue's central IT organization.
","Raghav Shankar|Benjamin Kobin|Saurabh Bagchi|Michael Kistler|Jan Rellermeyer","","http://arxiv.org/abs/1802.06270v1","http://arxiv.org/pdf/1802.06270v1","","ACM Classification (2012): Data center networks; System management;
  Ubiquitous and mobile computing systems and tools","","","cs.DC","cs.DC"
"954","1802.07982v1","2018-02-22 11:23:36","2018-02-22 11:23:36","Shared Services Center for E-Government Policy","  It is a general opinion that applicative cooperation represents a useful
vehicle for the development of e-government. At the architectural level,
solutions for applicative cooperation are quite stable, but organizational and
methodological problems prevent the expected and needed development of
cooperation among different administrations. Moreover, the introduction of the
digital government requires a considerable involvement of resources that can be
unsustainable for small public administrations. This work shows how the above
mentioned problems can be (partially) solved with the introduction of a Shared
Services Center (SSC).
","Flavio Corradini|Lucio Forastieri|Alberto Polzonetti|Oliviero Riganelli|Andrea Sergiacomi","","http://arxiv.org/abs/1802.07982v1","http://arxiv.org/pdf/1802.07982v1","","Proceedings of eGov-Interop'05 Conference 23-24 February 2005, Geneva
  (Switzerland)","","","cs.SE","cs.SE"
"955","1802.09307v1","2018-02-22 15:51:43","2018-02-22 15:51:43","Digital Scientific Notations as a Human-Computer Interface in
  Computer-Aided Research","  Most of today's scientific research relies on computers and software not only
for administrational tasks, but also for processing scientific information.
Examples of such computer-aided research are the analysis of experimental data
or the simulation of phenomena based on theoretical models. With the rapid
increase of computational power, scientific software has integrated more and
more complex scientific knowledge in a black-box fashion. As a consequence, its
users do not know, and don't even have a chance of finding out, which models or
assumptions their computations are based on. The black-box nature of scientific
software has thereby become a major cause of mistakes. The present work starts
with an analysis of this situation from the point of view of human-computer
interaction in scientific research. It identifies the key role of digital
scientific notations at the human-computer interface, and describes a
proof-of-concept implementation of such a digital scientific notation for
scientific models formulated as mathematical equations.
","Konrad Hinsen","","http://arxiv.org/abs/1802.09307v1","http://arxiv.org/pdf/1802.09307v1","http://dx.doi.org/10.7717/peerj-cs.158","","","10.7717/peerj-cs.158","cs.HC","cs.HC|cs.CY"
"956","1802.08484v1","2018-02-23 11:14:03","2018-02-23 11:14:03","Business Rules in e-Government Applications","  The introduction of Information and Communication Technologies (ICT) into
public administrations has been radically changing the way organizations
cooperate and, more generally, the way to think about business processes over
organizational boundaries. In this paper we describe our approach to combining
business processes with business rules in order to integrate effectively single
units in an inter- or intra-organizational cooperation. Business rules
represent the knowledge that an administration has about its business; with
regard to this, they can express strategies, contracts and can influence not
only staff relations, but, finally, citizen relations, as well. In other words,
business rules are the core of an administration and affect either the business
processes or the behaviours of the system participants. They are typically
expressed implicitly in business contracts and they are embedded within the
source code of many application modules. So a concise and declarative statement
of business behaviour is converted into a set of programming instructions,
which are spread widely throughout the whole information system. In this way,
business rules are difficult to change and keep consistent over the time. For
this reason, it is necessary to reengineer the system in order to logically and
perhaps physically externalize rules from the application code. In our proposed
approach, we describe a cooperation as a collection of tasks combined in
certain ways according to the organization logic specified by business rules.
Our rule-driven methodology has the goal to make the business process design
more adaptable to the changes of internal or external environment.
","Flavio Corradini|Alberto Polzonetti|Oliviero Riganelli","","http://arxiv.org/abs/1802.08484v1","http://arxiv.org/pdf/1802.08484v1","","","Electronic Journal of e- Government Volume 7 Issue 1 2009, pp. 45
  - 54,","","cs.SE","cs.SE"
"957","1802.09118v1","2018-02-26 01:07:32","2018-02-26 01:07:32","Multi-Commodity Flow with In-Network Processing","  Modern networks run ""middleboxes"" that offer services ranging from network
address translation and server load balancing to firewalls, encryption, and
compression. In an industry trend known as Network Functions Virtualization
(NFV), these middleboxes run as virtual machines on any commodity server, and
the switches steer traffic through the relevant chain of services. Network
administrators must decide how many middleboxes to run, where to place them,
and how to direct traffic through them, based on the traffic load and the
server and network capacity. Rather than placing specific kinds of middleboxes
on each processing node, we argue that server virtualization allows each server
node to host all middlebox functions, and simply vary the fraction of resources
devoted to each one. This extra flexibility fundamentally changes the
optimization problem the network administrators must solve to a new kind of
multi-commodity flow problem, where the traffic flows consume bandwidth on the
links as well as processing resources on the nodes. We show that allocating
resources to maximize the processed flow can be optimized exactly via a linear
programming formulation, and to arbitrary accuracy via an efficient
combinatorial algorithm. Our experiments with real traffic and topologies show
that a joint optimization of node and link resources leads to an efficient use
of bandwidth and processing capacity. We also study a class of design problems
that decide where to provide node capacity to best process and route a given
set of demands, and demonstrate both approximation algorithms and hardness
results for these problems.
","Moses Charikar|Yonatan Naamad|Jennifer Rexford|X. Kelvin Zou","","http://arxiv.org/abs/1802.09118v1","http://arxiv.org/pdf/1802.09118v1","","","","","cs.DS","cs.DS|cs.NI"
"958","1802.09220v2","2018-02-26 09:50:27","2018-07-27 08:15:21","The Trusted Server: A secure computational environment for privacy
  compliant evaluations on plain personal data","  A growing framework of legal and ethical requirements limit scientific and
commercial evalua-tion of personal data. Typically, pseudonymization,
encryption, or methods of distributed com-puting try to protect individual
privacy. However, computational infrastructures still depend on human system
administrators. This introduces severe security risks and has strong impact on
privacy: system administrators have unlimited access to the computers that they
manage in-cluding encryption keys and pseudonymization-tables. Distributed
computing and data obfuscation technologies reduce but do not eliminate the
risk of privacy leakage by administrators. They produce higher implementation
effort and possible data quality degradation. This paper proposes the Trusted
Server as an alternative approach that provides a sealed and inaccessible
computational environment in a cryptographically strict sense. During operation
or by direct physical access to storage media, data stored and processed inside
the Trusted Server can by no means be read, manipulated or leaked, other than
by brute-force. Thus, secure and privacy-compliant data processing or
evaluation of plain person-related data becomes possible even from multiple
sources, which want their data kept mutually secret.
","Nikolaus von Bomhard|Bernd Ahlborn|Catherine Mason|Ulrich Mansmann","","http://arxiv.org/abs/1802.09220v2","http://arxiv.org/pdf/1802.09220v2","http://dx.doi.org/10.1371/journal.pone.0202752","29 pages, 6 figures","","10.1371/journal.pone.0202752","cs.CR","cs.CR|cs.CY"
"959","1803.03571v3","2018-03-09 15:45:12","2019-02-20 15:29:36","City-wide Analysis of Electronic Health Records Reveals Gender and Age
  Biases in the Administration of Known Drug-Drug Interactions","  The occurrence of drug-drug-interactions (DDI) from multiple drug
dispensations is a serious problem, both for individuals and health-care
systems, since patients with complications due to DDI are likely to re-enter
the system at a costlier level. We present a large-scale longitudinal study (18
months) of the DDI phenomenon at the primary- and secondary-care level using
electronic health records (EHR) from the city of Blumenau in Southern Brazil
(pop. ~340,000). This is the first study of DDI we are aware of that follows an
entire city longitudinally for more than 3 months. We found that 181 distinct
drug pairs known to interact were dispensed concomitantly to 12% of the
patients in the city's public health-care system. Further, 4% of the patients
were dispensed DDI combinations, likely to result in major adverse reactions
with costs estimated to be larger than previously reported in smaller studies.
DDI results are integrated into associative networks for inference and
visualization, revealing key medications and interactions involved in the DDI
phenomenon. Analysis reveals that women have a 60% increased risk of DDI as
compared to men; the increase becomes 90% when only major DDI are considered.
Furthermore, DDI risk increases substantially with age. Patients aged 70-79
years have a 34% risk of DDI when they are dispensed two or more drugs
concomitantly. Interestingly, a null model demonstrates that age- and
women-specific risks from increased polypharmacy fail by far to explain the
observed risks of DDI in those populations. This suggests that social and
biological factors are at play. These results demonstrate that considerable
gender and age biases exist, but that accurate warning systems for known DDI
can be devised for health-care systems and public-health policy management, to
reduce DDI-related adverse reactions and health-care costs.
","Rion Brattig Correia|Luciana P. de Araujo|Mauro M. Mattos|Luis M. Rocha","","http://arxiv.org/abs/1803.03571v3","http://arxiv.org/pdf/1803.03571v3","","","","","cs.SI","cs.SI|cs.CY|cs.IR|q-bio.QM|stat.ML|62-07 (Primary) 05C82, 92C42, 92C60 (Secondary)|J.3; G.3"
"960","1803.04967v1","2018-03-13 08:09:20","2018-03-13 08:09:20","Recurrent Neural Network Attention Mechanisms for Interpretable System
  Log Anomaly Detection","  Deep learning has recently demonstrated state-of-the art performance on key
tasks related to the maintenance of computer systems, such as intrusion
detection, denial of service attack detection, hardware and software system
failures, and malware detection. In these contexts, model interpretability is
vital for administrator and analyst to trust and act on the automated analysis
of machine learning models. Deep learning methods have been criticized as black
box oracles which allow limited insight into decision factors. In this work we
seek to ""bridge the gap"" between the impressive performance of deep learning
models and the need for interpretable model introspection. To this end we
present recurrent neural network (RNN) language models augmented with attention
for anomaly detection in system logs. Our methods are generally applicable to
any computer system and logging source.
  By incorporating attention variants into our RNN language models we create
opportunities for model introspection and analysis without sacrificing
state-of-the art performance.
  We demonstrate model performance and illustrate model interpretability on an
intrusion detection task using the Los Alamos National Laboratory (LANL) cyber
security dataset, reporting upward of 0.99 area under the receiver operator
characteristic curve despite being trained only on a single day's worth of
data.
","Andy Brown|Aaron Tuor|Brian Hutchinson|Nicole Nichols","","http://arxiv.org/abs/1803.04967v1","http://arxiv.org/pdf/1803.04967v1","","Submitted to the First Workshop On Machine Learning for Computer
  Systems, ACM HPDC 2018","","","cs.LG","cs.LG|cs.NE|stat.ML"
"961","1803.04837v4","2018-03-13 14:32:38","2018-11-17 06:20:12","Learning the Joint Representation of Heterogeneous Temporal Events for
  Clinical Endpoint Prediction","  The availability of a large amount of electronic health records (EHR)
provides huge opportunities to improve health care service by mining these
data. One important application is clinical endpoint prediction, which aims to
predict whether a disease, a symptom or an abnormal lab test will happen in the
future according to patients' history records. This paper develops deep
learning techniques for clinical endpoint prediction, which are effective in
many practical applications. However, the problem is very challenging since
patients' history records contain multiple heterogeneous temporal events such
as lab tests, diagnosis, and drug administrations. The visiting patterns of
different types of events vary significantly, and there exist complex nonlinear
relationships between different events. In this paper, we propose a novel model
for learning the joint representation of heterogeneous temporal events. The
model adds a new gate to control the visiting rates of different events which
effectively models the irregular patterns of different events and their
nonlinear correlations. Experiment results with real-world clinical data on the
tasks of predicting death and abnormal lab tests prove the effectiveness of our
proposed approach over competitive baselines.
","Luchen Liu|Jianhao Shen|Ming Zhang|Zichang Wang|Jian Tang","","http://arxiv.org/abs/1803.04837v4","http://arxiv.org/pdf/1803.04837v4","","8 pages, this paper has been accepted by AAAI 2018","","","cs.AI","cs.AI|cs.LG|stat.ML"
"962","1804.01576v1","2018-03-13 22:32:16","2018-03-13 22:32:16","A Bayesian Model for False Information Belief Impact, Optimal Design,
  and Fake News Containment","  This work is a technical approach to modeling false information nature,
design, belief impact and containment in multi-agent networks. We present a
Bayesian mathematical model for source information and viewer's belief, and how
the former impacts the latter in a media (network) of broadcasters and viewers.
Given the proposed model, we study how a particular information (true or false)
can be optimally designed into a report, so that on average it conveys the most
amount of the original intended information to the viewers of the network.
Consequently, the model allows us to study susceptibility of a particular group
of viewers to false information, as a function of statistical metrics of the
their prior beliefs (e.g. bias, hesitation, open-mindedness, credibility
assessment etc.). In addition, based on the same model we can study false
information ""containment"" strategies imposed by network administrators.
Specifically, we study a credibility assessment strategy, where every
disseminated report must be within a certain distance of the truth. We study
the trade-off between false and true information-belief convergence using this
scheme which leads to ways for optimally deciding how truth sensitive an
information dissemination network should operate.
","Amin Khajehnejad|Shima Hajimirza","","http://arxiv.org/abs/1804.01576v1","http://arxiv.org/pdf/1804.01576v1","","","","","cs.SI","cs.SI"
"963","1803.05297v2","2018-03-14 14:18:13","2018-04-19 12:50:54","A quantitative analysis of the 2017 Honduran election and the argument
  used to defend its outcome","  The Honduran incumbent president and his administration recently declared
victory in an election riddled with irregularities and indicators of fraud.
Perhaps most curious, however, was a numerical anomaly: the primary challenger
carried a very significant lead of five percentage points more than half way
through the election but was ultimately defeated by the incumbent. The
incumbent (Hernandez) offered a plausible explanation for the surprising
turnaround in the ballots: his popularity is greater in remote areas of the
country but votes from remote areas were not counted until later in the
election. Here, we mathematically formalize this argument, which we will call
the Hernandez conjecture, and employ the resulting formulae together with
geodemographic data from Honduras to quantitatively assess the conjectures
veracity. When the departamentos were analyzed individually, three
sparsely-populated departamentos (of 18 total) showed small but non-negligible
probability of the conjectures veracity; however, when the country was analyzed
as a whole, the overall probability of the conjectures veracity was calculated
to be less than 0.0001 under a wide range of different assumptions. Results of
our three-pronged analysis, taken together, indicate a negligible probability
of a fair win by the incumbent.
","Philip Gerrish|Benjamin Zepeda|Theophilus Okosun|Irene A Gerrish|Rosemary Joyce","","http://arxiv.org/abs/1803.05297v2","http://arxiv.org/pdf/1803.05297v2","","","","","stat.AP","stat.AP"
"964","1803.05874v2","2018-03-15 17:21:25","2018-09-26 17:30:52","Synthesizing geocodes to facilitate access to detailed geographical
  information in large scale administrative data","  In this paper we investigate if generating synthetic data can be a viable
strategy for providing access to detailed geocoding information for external
researchers without compromising the confidentiality of the units included in
the database. This research was motivated by a recent project at the Institute
for Employment Research (IAB) in Germany that linked exact geocodes to the
Integrated Employment Biographies, a large administrative database containing
several million records. Based on these data we evaluate the performance of
several synthesizers in terms of addressing the trade-off between preserving
analytical validity and limiting the risk of disclosure. We propose strategies
for making the synthesizers scalable for such large files, present analytical
validity measures for the generated data and provide general recommendations
for statistical agencies considering the synthetic data approach for
disseminating detailed geographical information.We also illustrate that the
commonly used disclosure avoidance strategy of providing geographical
information only on an aggregated level will not offer substantial improvements
in disclosure protection if coupled with synthesis. As we show in the online
supplement accompanying this manuscript that synthesizing additional variables
should be preferred if the level of protection from synthesizing only the
geocodes is not considered sufficient.
","Joerg Drechsler|Jingchen Hu","","http://arxiv.org/abs/1803.05874v2","http://arxiv.org/pdf/1803.05874v2","","","","","stat.AP","stat.AP|stat.ME"
"965","1803.06454v2","2018-03-17 04:04:28","2018-03-28 16:19:28","Some Questions in $l-$adic Cohomology","  The comparison theorem for a smooth projective variety $X$ over $\mathbb{C}$
tells us that the Betti numbers are independent of $l$. We aim to understand
the $l$ independence of Betti numbers for smooth projective varieties $X$ over
$k$, where $k$ is an algebraic extension of $\mathbb{F}_p$.
","Jagannathan Arjun Sathyamoorthy","","http://arxiv.org/abs/1803.06454v2","http://arxiv.org/pdf/1803.06454v2","","arXiv admin note: submission has been withdrawn by arXiv
  administrators due to inappropriate overlap with external sources","","","math.AG","math.AG"
"966","1803.06596v4","2018-03-18 03:16:41","2019-05-17 13:24:31","Network Service Orchestration: A Survey","  Business models of network service providers are undergoing an evolving
transformation fueled by vertical customer demands and technological advances
such as 5G, Software Defined Networking~(SDN), and Network Function
Virtualization~(NFV). Emerging scenarios call for agile network services
consuming network, storage, and compute resources across heterogeneous
infrastructures and administrative domains. Coordinating resource control and
service creation across interconnected domains and diverse technologies becomes
a grand challenge. Research and development efforts are being devoted to
enabling orchestration processes to automate, coordinate, and manage the
deployment and operation of network services. In this survey, we delve into the
topic of Network Service Orchestration~(NSO) by reviewing the historical
background, relevant research projects, enabling technologies, and
standardization activities. We define key concepts and propose a taxonomy of
NSO approaches and solutions to pave the way towards a common understanding of
the various ongoing efforts around the realization of diverse NSO application
scenarios. Based on the analysis of the state of affairs, we present a series
of open challenges and research opportunities, altogether contributing to a
timely and comprehensive survey on the vibrant and strategic topic of network
service orchestration.
","Nathan F. Saraiva de Sousa|Danny A. Lachos Perez|Raphael V. Rosa|Mateus A. S. Santos|Christian Esteve Rothenberg","","http://arxiv.org/abs/1803.06596v4","http://arxiv.org/pdf/1803.06596v4","http://dx.doi.org/10.1016/j.comcom.2019.04.008","Accepted for publication at Computer Communications Journal","","10.1016/j.comcom.2019.04.008","cs.NI","cs.NI"
"967","1803.06613v3","2018-03-18 06:40:34","2019-06-16 14:30:11","Trajectory-based Scene Understanding using Dirichlet Process Mixture
  Model","  Appropriate modeling of a surveillance scene is essential for detection of
anomalies in road traffic. Learning usual paths can provide valuable insight
into road traffic conditions and thus can help in identifying unusual routes
taken by commuters/vehicles. If usual traffic paths are learned in a
nonparametric way, manual interventions in road marking road can be avoided. In
this paper, we propose an unsupervised and nonparametric method to learn
frequently used paths from the tracks of moving objects in $\Theta(kn)$ time,
where $k$ denotes the number of paths and $n$ represents the number of tracks.
In the proposed method, temporal dependencies of the moving objects are
considered to make the clustering meaningful using Temporally Incremental
Gravity Model (TIGM). In addition, the distance-based scene learning makes it
intuitive to estimate the model parameters. Further, we have extended TIGM
hierarchically as Dynamically Evolving Model (DEM) to represent notable traffic
dynamics of a scene. Experimental validation reveals that the proposed method
can learn a scene quickly without prior knowledge about the number of paths
($k$). We have compared the results with various state-of-the-art methods. We
have also highlighted the advantages of the proposed method over existing
techniques popularly used for designing traffic monitoring applications. It can
be used for administrative decision making to control traffic at junctions or
crowded places and generate alarm signals, if necessary.
","Santhosh Kelathodi Kumaran|Debi Prosad Dogra|Partha Pratim Roy|Bidyut Baran Chaudhuri","","http://arxiv.org/abs/1803.06613v3","http://arxiv.org/pdf/1803.06613v3","","14 pages, 27 figures","","","cs.CV","cs.CV"
"968","1803.06740v1","2018-03-18 21:19:11","2018-03-18 21:19:11","Metabolomic signature of type 1 diabetes-induced sensory loss and nerve
  damage in diabetic neuropathy","  Diabetic-induced peripheral neuropathy (DPN) is a diabetic late complication.
The molecular mechanisms underlying the pathophysiology of nerve damage &
sensory loss remain largely unclear. Recently, alterations in metabolic flux
have gained attention a basis for organ damage in diabetes; however, peripheral
sensory neurons have not been adequately analyzed. In the present study, we
attempted to delineate the role of alteration of metabolic pathways in relation
to nerve damage & sensory loss. We employed STZ-injected mouse model of type1
diabetes. To investigate the progression of DPN by behavioral measurements of
sensitivity to thermal & mechanical stimuli and quantitative assessment of
intraepidermal nerve fiber density. We employed a MS-based screen to address
alterations in levels of metabolites in peripheral sciatic nerve (SN) & amino
acids (AA) in serum over several months post-STZ administration. Although
hyperglycemia & body weight changes occurred early, sensory loss & reduced
intraepithelial branching of nociceptive nerves was only evident at 22 wks
post-STZ. The longitudinal metabolites screen in SN demonstrated that mice at
12 and 22 wks post-STZ showed an early impairment the tricarboxylic acid. We
found that levels of citric acid, ketoglutaric acid, succinic acid, fumaric
acid & malic acid were observed to be significantly reduced in SN at 22 wks
post-STZ. In addition, we also found the increase in levels of sorbitol &
L-Lactate in SN from 12 wks post-STZ injection. AA screen in serum showed that
the amino acids Val, Ile and Leu, increased more than 2-fold from 12 wks
post-STZ. Similarly, the levels of Tyr, Asn, Ser, His, Ala, & Pro showed
progressive increase. Our results indicate that the impaired TCA cycle
metabolites in peripheral nerve is the primary cause of shunting metabolic
substrate to compensatory pathways which leads to mitochondrial dysfunction &
nerve damage.
","Daniel Rangel Rojas|Rohini Kuner|Nitin Agarwal","","http://arxiv.org/abs/1803.06740v1","http://arxiv.org/pdf/1803.06740v1","","17 pages, 3 figures","","","q-bio.CB","q-bio.CB|q-bio.TO"
"969","1803.06854v2","2018-03-19 10:05:41","2018-05-15 11:11:44","MONICA in Hamburg: Towards Large-Scale IoT Deployments in a Smart City","  Modern cities and metropolitan areas all over the world face new management
challenges in the 21st century primarily due to increasing demands on living
standards by the urban population. These challenges range from climate change,
pollution, transportation, and citizen engagement, to urban planning, and
security threats. The primary goal of a Smart City is to counteract these
problems and mitigate their effects by means of modern ICT to improve urban
administration and infrastructure. Key ideas are to utilise network
communication to inter-connect public authorities; but also to deploy and
integrate numerous sensors and actuators throughout the city infrastructure -
which is also widely known as the Internet of Things (IoT). Thus, IoT
technologies will be an integral part and key enabler to achieve many
objectives of the Smart City vision.
  The contributions of this paper are as follows. We first examine a number of
IoT platforms, technologies and network standards that can help to foster a
Smart City environment. Second, we introduce the EU project MONICA which aims
for demonstration of large-scale IoT deployments at public, inner-city events
and give an overview on its IoT platform architecture. And third, we provide a
case-study report on SmartCity activities by the City of Hamburg and provide
insights on recent (on-going) field tests of a vertically integrated,
end-to-end IoT sensor application.
","Sebastian Meiling|Dorothea Purnomo|Julia-Ann Shiraishi|Michael Fischer|Thomas C. Schmidt","","http://arxiv.org/abs/1803.06854v2","http://arxiv.org/pdf/1803.06854v2","","6 pages","Proceedings of the European Conference on Networks and
  Communications, EuCNC, 2018","","cs.NI","cs.NI"
"970","1803.07540v2","2018-03-20 17:27:03","2018-07-02 08:39:07","Enslaving the Algorithm: From a ""Right to an Explanation"" to a ""Right to
  Better Decisions""?","  As concerns about unfairness and discrimination in ""black box"" machine
learning systems rise, a legal ""right to an explanation"" has emerged as a
compellingly attractive approach for challenge and redress. We outline recent
debates on the limited provisions in European data protection law, and
introduce and analyze newer explanation rights in French administrative law and
the draft modernized Council of Europe Convention 108. While individual rights
can be useful, in privacy law they have historically unreasonably burdened the
average data subject. ""Meaningful information"" about algorithmic logics is more
technically possible than commonly thought, but this exacerbates a new
""transparency fallacy""---an illusion of remedy rather than anything
substantively helpful. While rights-based approaches deserve a firm place in
the toolbox, other forms of governance, such as impact assessments, ""soft law,""
judicial review, and model repositories deserve more attention, alongside
catalyzing agencies acting for users to control algorithmic system design.
","Lilian Edwards|Michael Veale","","http://arxiv.org/abs/1803.07540v2","http://arxiv.org/pdf/1803.07540v2","http://dx.doi.org/10.1109/MSP.2018.2701152","14 pages, 0 figures","IEEE Security & Privacy (2018) 16(3), 46--54","10.1109/MSP.2018.2701152","cs.AI","cs.AI|cs.HC"
"971","1803.09002v3","2018-03-23 22:40:52","2018-09-04 19:15:19","Socio-spatial Self-organizing Maps: Using Social Media to Assess
  Relevant Geographies for Exposure to Social Processes","  Social media offers a unique window into attitudes like racism and
homophobia, exposure to which are important, hard to measure and understudied
social determinants of health. However, individual geo-located observations
from social media are noisy and geographically inconsistent. Existing areas by
which exposures are measured, like Zip codes, average over irrelevant
administratively-defined boundaries. Hence, in order to enable studies of
online social environmental measures like attitudes on social media and their
possible relationship to health outcomes, first there is a need for a method to
define the collective, underlying degree of social media attitudes by region.
To address this, we create the Socio-spatial-Self organizing map, ""SS-SOM""
pipeline to best identify regions by their latent social attitude from Twitter
posts. SS-SOMs use neural embedding for text-classification, and augment
traditional SOMs to generate a controlled number of non-overlapping,
topologically-constrained and topically-similar clusters. We find that not only
are SS-SOMs robust to missing data, the exposure of a cohort of men who are
susceptible to multiple racism and homophobia-linked health outcomes, changes
by up to 42% using SS-SOM measures as compared to using Zip code-based
measures.
","Kunal Relia|Mohammad Akbari|Dustin Duncan|Rumi Chunara","","http://arxiv.org/abs/1803.09002v3","http://arxiv.org/pdf/1803.09002v3","http://dx.doi.org/10.1145/3274414","23 pages, 4 figures, 3 tables","Proc. ACM Hum.-Comput.Interact.2, CSCW, Article 145 (November
  2018), 23 pages","10.1145/3274414","cs.SI","cs.SI|cs.CY|I.5.3; I.5.1; H.3.3; J.4"
"972","1803.09274v2","2018-03-25 15:18:26","2018-04-19 12:29:55","Quantum nonlinear four-wave mixing with a single atom in an optical
  cavity","  Single atom cavity quantum electrodynamics grants access to nonclassical
photon statistics, while electromagnetically induced transparency exhibits a
dark state of long coherence time. The combination of the two produces a new
light field via four-wave mixing that shows long-lived quantum statistics. We
observe the new field in the emission from the cavity as a beat with the probe
light that together with the control beam and the cavity vacuum is driving the
four-wave mixing process. Moreover, the control field allows us to tune the new
light field from antibunching to bunching, demonstrating our all-optical
control over the photon-pair emission.
","Haytham Chibani","","http://arxiv.org/abs/1803.09274v2","http://arxiv.org/pdf/1803.09274v2","","arXiv admin note: submission has been withdrawn by arXiv
  administrators due to ex parte submission of a flawed and incomplete
  manuscript. Author list is truncated","","","quant-ph","quant-ph"
"973","1803.11181v1","2018-03-29 17:54:00","2018-03-29 17:54:00","The American Space Exploration Narrative from the Cold War through the
  Obama Administration","  We document how the narrative and the policies of space exploration in the
U.S. have changed over the past 50 years. We first examine the history of the
U.S. space exploration program and also assess three current conditions of
space exploration including: (1) the increasing role of the private sector, (2)
the influence of global politics, and (3) the focus on a human mission to Mars.
We identify five rhetorical themes: competition, prestige, collaboration,
leadership, and a new paradigm. These themes are then used to analyze the
content of forty documents from eight presidential administrations. The
historical narrative and content analysis together suggest that space
exploration has developed from a discourse about a bipolar world composed of
the U.S. and U.S.S.R. into a complicated field that encompass many new players.
We make three observations: (1) there is a disconnect between stated U.S.
policy goals and the implementation of those goals, (2) the U.S. communicates
mixed messages regarding its intent to be both the dominant leader in space
exploration and also a committed participant in international collaborations,
and (3) the U.S. cannot remain a true pioneer in space exploration if it does
not embrace the realities of globalization and the changing dynamics within
space exploration. We conclude with three suggestions: (1) the U.S. government
and NASA should critically examine space exploration priorities and commit to
implementing a program that will further realistic policy and goals, (2) the
U.S. should re-examine its intention to play a dominant leadership role in
space exploration and consider emphasizing a commitment toward active
participation in international collaboration in space, and (3) the U.S. should
fully embrace the new paradigm of space exploration by lowering barriers that
hinder competitiveness.
","Dora Holland|Jack O. Burns","","http://arxiv.org/abs/1803.11181v1","http://arxiv.org/pdf/1803.11181v1","","19 pages, 1 figure, 2 tables, 1 appendix, To appear in the journal
  Space Policy","","","astro-ph.IM","astro-ph.IM|physics.soc-ph"
"974","1804.00263v1","2018-04-01 06:29:51","2018-04-01 06:29:51","A Novel Approach for Network Attack Classification Based on Sequential
  Questions","  With the development of incipient technologies, user devices becoming more
exposed and ill-used by foes. In upcoming decades, traditional security
measures will not be sufficient enough to handle this huge threat towards
distributed hardware and software. Lack of standard network attack taxonomy has
become an indispensable dispute on developing a clear understanding about the
attacks in order to have an operative protection mechanism. Present attack
categorization techniques protect a specific group of threat which has either
messed the entire taxonomy structure or ambiguous when one network attacks get
blended with few others attacks. Hence, this raises concerns about developing a
common and general purpose taxonomy. In this study, a sequential
question-answer based model of categorization is proposed. In this article, an
intrusion detection framework and threat grouping schema are proposed on the
basis of four sequential questions (Who, Where, How and What). We have used our
method for classifying traditional network attacks in order to identify
initiator, source, attack style and seriousness of an attack. Another focus of
the paper is to provide a preventive list of actions for network administrator
as a guideline to reduce overall attack consequence. Recommended taxonomy is
designed to detect common attacks rather than any particular type of attack
which can have a practical effect in real life attack classification. From the
analysis of the classifications obtained from few infamous attacks, it is
obvious that the proposed system holds certain benefits related to the
prevailing taxonomies. Future research directions have also been well
acknowledged.
","Md Mehedi Hassan Onik|Nasr Al-Zaben|Hung Phan Hoo|Chul-Soo Kim","","http://arxiv.org/abs/1804.00263v1","http://arxiv.org/pdf/1804.00263v1","http://dx.doi.org/10.33166/AETiC.2018.02.001","","Annals of Emerging Technologies in Computing (AETiC), Print ISSN:
  2516-0281, Online ISSN: 2516029X, Print ISSN: 2516-0281, Online ISSN:
  2516029X, pp. 1-14, Vol. 2, No. 2, 1st April 2018.
  http://www.aetic.theiaer.org/archive/v2n2/p1.html","10.33166/AETiC.2018.02.001","cs.CR","cs.CR"
"975","1804.01138v1","2018-04-03 19:20:33","2018-04-03 19:20:33","Designing a Micro-Benchmark Suite to Evaluate gRPC for TensorFlow: Early
  Experiences","  Remote procedure call (RPC) is the backbone of many modern distributed
systems. Google's gRPC is one of the most popular open source RPC frameworks
available in the community. gRPC is the main communication engine for Google's
Deep Learning framework TensorFlow. TensorFlow primarily uses gRPC for
communicating tensors and administrative tasks among different processes.
Tensor updates during the training phase are communication intensive and thus
TensorFlow's performance is heavily dependent on the underlying network and the
efficacy of the communication engine. Training deep learning models on
TensorFlow can take significant time ranging from several minutes to several
hours, even several days. Thus system researchers need to devote a lot of time
to understand the impact of communication on the overall performance. Clearly,
there is lack of benchmarks available for system researchers. Therefore, we
propose TF-gRPC-Bench micro-benchmark suite that enables system researchers to
quickly understand the impact of the underlying network and communication
runtime on deep learning workloads. To achieve this, we first analyze the
characteristics of TensorFlow workload over gRPC by training popular deep
learning models. Then, we propose three micro-benchmarks that take account
these workload characteristics. In addition, we comprehensively evaluate gRPC
with TF-gRPC-Bench micro-benchmark suite on different clusters over Ethernet,
IPoIB, and RDMA, and present the results.
","Rajarshi Biswas|Xiaoyi Lu|Dhabaleswar K. Panda","","http://arxiv.org/abs/1804.01138v1","http://arxiv.org/pdf/1804.01138v1","","9 Pages, 14 Figures, This paper was presented at BPOE - 9 @ ASPLOS
  2018","","","cs.DC","cs.DC"
"976","1804.01468v1","2018-04-04 15:29:31","2018-04-04 15:29:31","P4K: A Formal Semantics of P4 and Applications","  Programmable packet processors and P4 as a programming language for such
devices have gained significant interest, because their flexibility enables
rapid development of a diverse set of applications that work at line rate.
However, this flexibility, combined with the complexity of devices and
networks, increases the chance of introducing subtle bugs that are hard to
discover manually. Worse, this is a domain where bugs can have catastrophic
consequences, yet formal analysis tools for P4 programs / networks are missing.
  We argue that formal analysis tools must be based on a formal semantics of
the target language, rather than on its informal specification. To this end, we
provide an executable formal semantics of the P4 language in the K framework.
Based on this semantics, K provides an interpreter and various analysis tools
including a symbolic model checker and a deductive program verifier for P4.
  This paper overviews our formal K semantics of P4, as well as several P4
language design issues that we found during our formalization process. We also
discuss some applications resulting from the tools provided by K for P4
programmers and network administrators as well as language designers and
compiler developers, such as detection of unportable code, state space
exploration of P4 programs and of networks, bug finding using symbolic
execution, data plane verification, program verification, and translation
validation.
","Ali Kheradmand|Grigore Rosu","","http://arxiv.org/abs/1804.01468v1","http://arxiv.org/pdf/1804.01468v1","","","","","cs.NI","cs.NI|cs.PL"
"977","1805.12107v1","2018-04-05 13:49:09","2018-04-05 13:49:09","Information Technologies in Public Administration","  There are visible changes in the world organization, environment and health
of national conscience that create a background for discussion on possible
redefinition of global, state and regional management goals. The author applies
the sustainable development criteria to a hierarchical management scheme that
is to lead the world community to non-contradictory growth. Concrete
definitions are discussed in respect of decision-making process representing
the state mostly. With the help of systems analysis it is highlighted how to
understand who would carry the distinctive sign of world leadership in the
nearest future.
","V. I. Gorelov","","http://arxiv.org/abs/1805.12107v1","http://arxiv.org/pdf/1805.12107v1","","10 pages, 4 tables, 18 references","","","econ.GN","econ.GN|q-fin.EC|93C95"
"978","1804.02745v2","2018-04-08 19:36:39","2018-06-12 11:34:14","Direct Estimation of Pharmacokinetic Parameters from DCE-MRI using Deep
  CNN with Forward Physical Model Loss","  Dynamic contrast-enhanced (DCE) MRI is an evolving imaging technique that
provides a quantitative measure of pharmacokinetic (PK) parameters in body
tissues, in which series of T1-weighted images are collected following the
administration of a paramagnetic contrast agent. Unfortunately, in many
applications, conventional clinical DCE-MRI suffers from low spatiotemporal
resolution and insufficient volume coverage. In this paper, we propose a novel
deep learning based approach to directly estimate the PK parameters from
undersampled DCE-MRI data. Specifically, we design a custom loss function where
we incorporate a forward physical model that relates the PK parameters to
corrupted image-time series obtained due to subsampling in k-space. This allows
the network to directly exploit the knowledge of true contrast agent kinetics
in the training phase, and hence provide more accurate restoration of PK
parameters. Experiments on clinical brain DCE datasets demonstrate the efficacy
of our approach in terms of fidelity of PK parameter reconstruction and
significantly faster parameter inference compared to a model-based iterative
reconstruction method.
","Cagdas Ulas|Giles Tetteh|Michael J. Thrippleton|Paul A. Armitage|Stephen D. Makin|Joanna M. Wardlaw|Mike E. Davies|Bjoern H. Menze","","http://arxiv.org/abs/1804.02745v2","http://arxiv.org/pdf/1804.02745v2","","Accepted at MICCAI 2018. 9 pages, 6 figures","","","cs.CV","cs.CV"
"979","1804.03918v1","2018-04-11 10:43:34","2018-04-11 10:43:34","A Management Framework for Secure Multiparty Computation in Dynamic
  Environments","  Secure multiparty computation (SMC) is a promising technology for
privacy-preserving collaborative computation. In the last years several
feasibility studies have shown its practical applicability in different fields.
However, it is recognized that administration and management overhead of SMC
solutions are still a problem. A vital next step is the incorporation of SMC in
the emerging fields of the Internet of Things and (smart) dynamic environments.
In these settings, the properties of these contexts make utilization of SMC
even more challenging since some of its vital premises regarding environmental
stability and preliminary configuration are not initially fulfilled. We bridge
this gap by providing FlexSMC, a management and orchestration framework for SMC
which supports the discovery of nodes, supports a trust establishment between
them and realizes robustness of SMC session by handling nodes failures and
communication interruptions. The practical evaluation of FlexSMC shows that it
enables the application of SMC in dynamic environments with reasonable
performance penalties and computation durations allowing soft real-time and
interactive use cases.
","Marcel von Maltitz|Stefan Smarzly|Holger Kinkelin|Georg Carle","","http://arxiv.org/abs/1804.03918v1","http://arxiv.org/pdf/1804.03918v1","http://dx.doi.org/10.1109/NOMS.2018.8406322","","","10.1109/NOMS.2018.8406322","cs.CR","cs.CR"
"980","1804.04177v2","2018-04-11 19:16:03","2018-04-14 10:01:48","Detecting Malicious PowerShell Commands using Deep Neural Networks","  Microsoft's PowerShell is a command-line shell and scripting language that is
installed by default on Windows machines. While PowerShell can be configured by
administrators for restricting access and reducing vulnerabilities, these
restrictions can be bypassed. Moreover, PowerShell commands can be easily
generated dynamically, executed from memory, encoded and obfuscated, thus
making the logging and forensic analysis of code executed by PowerShell
challenging.For all these reasons, PowerShell is increasingly used by
cybercriminals as part of their attacks' tool chain, mainly for downloading
malicious contents and for lateral movement. Indeed, a recent comprehensive
technical report by Symantec dedicated to PowerShell's abuse by cybercrimials
reported on a sharp increase in the number of malicious PowerShell samples they
received and in the number of penetration tools and frameworks that use
PowerShell. This highlights the urgent need of developing effective methods for
detecting malicious PowerShell commands.In this work, we address this challenge
by implementing several novel detectors of malicious PowerShell commands and
evaluating their performance. We implemented both ""traditional"" natural
language processing (NLP) based detectors and detectors based on
character-level convolutional neural networks (CNNs). Detectors' performance
was evaluated using a large real-world dataset.Our evaluation results show
that, although our detectors individually yield high performance, an ensemble
detector that combines an NLP-based classifier with a CNN-based classifier
provides the best performance, since the latter classifier is able to detect
malicious commands that succeed in evading the former. Our analysis of these
evasive commands reveals that some obfuscation patterns automatically detected
by the CNN classifier are intrinsically difficult to detect using the NLP
techniques we applied.
","Danny Hendler|Shay Kels|Amir Rubin","","http://arxiv.org/abs/1804.04177v2","http://arxiv.org/pdf/1804.04177v2","","19 pages, 5 figures","","","cs.CR","cs.CR|cs.NE"
"981","1804.04798v2","2018-04-13 06:43:29","2018-05-08 16:27:59","Trustworthy Configuration Management for Networked Devices using
  Distributed Ledgers","  Numerous IoT applications, like building automation or process control of
industrial sites, exist today. These applications inherently have a strong
connection to the physical world. Hence, IT security threats cannot only cause
problems like data leaks but also safety issues which might harm people.
Attacks on IT systems are not only performed by outside attackers but also
insiders like administrators. For this reason, we present ongoing work on a
configuration management system (CMS) that provides control over
administrators, restrains their rights, and enforces separation of concerns. We
reach this goal by conducting a configuration management process that requires
multi-party authorization for critical configurations to achieve Byzantine
fault tolerance against attacks and faults by administrators. Only after a
configuration has been authorized by multiple experts, it is applied to the
targeted devices. For the whole configuration management process, our CMS
guarantees accountability and traceability. Lastly, our system is
tamper-resistant as we leverage Hyperledger Fabric, which provides a
distributed execution environment for our CMS and a blockchain-based
distributed ledger that we use to store the configurations. A beneficial side
effect of this approach is that our CMS is also suitable to manage
configurations for infrastructure shared across different organizations that do
not need to trust each other.
","Holger Kinkelin|Valentin Hauner|Heiko Niedermayer|Georg Carle","","http://arxiv.org/abs/1804.04798v2","http://arxiv.org/pdf/1804.04798v2","","Author's version -- Final paper to appear in proceedings of 2018
  IEEE/IFIP International Workshop on Decentralized Orchestration and
  Management of Distributed Heterogeneous Things (DOMINOS) co-located with the
  Network Operations and Management Symposium (NOMS)","","","cs.CR","cs.CR|cs.NI"
"982","1804.05497v1","2018-04-16 03:55:42","2018-04-16 03:55:42","Deep Learning on Key Performance Indicators for Predictive Maintenance
  in SAP HANA","  With a new era of cloud and big data, Database Management Systems (DBMSs)
have become more crucial in numerous enterprise business applications in all
the industries. Accordingly, the importance of their proactive and preventive
maintenance has also increased. However, detecting problems by predefined rules
or stochastic modeling has limitations, particularly when analyzing the data on
high-dimensional Key Performance Indicators (KPIs) from a DBMS. In recent
years, Deep Learning (DL) has opened new opportunities for this complex
analysis. In this paper, we present two complementary DL approaches to detect
anomalies in SAP HANA. A temporal learning approach is used to detect abnormal
patterns based on unlabeled historical data, whereas a spatial learning
approach is used to classify known anomalies based on labeled data. We
implement a system in SAP HANA integrated with Google TensorFlow. The
experimental results with real-world data confirm the effectiveness of the
system and models.
","Jaekoo Lee|Byunghan Lee|Jongyoon Song|Jaesik Yoon|Yongsik Lee|Donghun Lee|Sungroh Yoon","","http://arxiv.org/abs/1804.05497v1","http://arxiv.org/pdf/1804.05497v1","","This version withdrawn by arXiv administrators because the author did
  not have the right to agree to our license at the time of submission","","","cs.LG","cs.LG|stat.ML"
"983","1804.06691v1","2018-04-18 12:48:31","2018-04-18 12:48:31","Modelling Evaluation of MPLS using Physical and virtual Network on GNS3","  The Multi-Protocol Label Switching (MPLS) is an emerging technology which
have quality, effectiveness and administration quality. On the contrary,
traditional network inside part passage steering conventions ruin the viable
acknowledgment of modern activity designing approaches in legacy IP systems.
Virtualization of the network could easily be assuring the network performance
and virtual network are logically connected with one physical machine so that
data could easily be send and get information from one virtual machine to the
next machine. The purpose of this Paper is to analyse the traffic of MPLS using
physical and virtual networks. This Paper will show that MPLS could also be run
on physical and virtual networks. MPLS is running nowadays to provide local
area network speed into the wide area Network.
","Abdul Ahad Abro|Abdul Basit Abro|Mehvish Abro|Asad Aslam Siddique","","http://arxiv.org/abs/1804.06691v1","http://arxiv.org/pdf/1804.06691v1","","conference ICETAS 2017 -- Strongly Recommended","","","cs.NI","cs.NI"
"984","1804.07354v1","2018-04-19 19:56:38","2018-04-19 19:56:38","The Aftermath of Disbanding an Online Hateful Community","  Harassing and hateful speech in online spaces has become a common problem for
platform maintainers and their users. The toxicity created by such content can
discourage user participation and engagement. Therefore, it is crucial for and
a common goal of platform managers to diminish hateful and harmful content.
Over the last year, Reddit, a major online platform, enacted a policy of
banning sub-communities (subreddits) that they deem harassing, with the goal of
diminishing such activities. We studied the effects of banning the largest
hateful subreddit (r/fatpeoplehate or FPH) on the users and other subreddits
that were associated with it. We found that, while a number of outcomes were
possible --- in this case the subreddit ban led to a sustained reduced
interaction of its members (FPH users) with the Reddit platform. We also found
that the many counter-actions taken by FPH users were short-lived and promptly
neutralized by both Reddit administrators and the admins of individual
subreddits. Our findings show that forum-banning can be an effective means by
which to diminish objectionable content. Moreover, our detailed analysis of the
post-banning behavior of FPH users highlights a number of the behavioral
patterns that banning can create.
","Haji Mohammad Saleem|Derek Ruths","","http://arxiv.org/abs/1804.07354v1","http://arxiv.org/pdf/1804.07354v1","","10 pages","","","cs.SI","cs.SI"
"985","1804.07450v1","2018-04-20 05:09:56","2018-04-20 05:09:56","Personal vs. Know-How Contacts: Which Matter More in Wiki Elections?","  The use of social media affects the real world as well. This study relies on
specific social network measures to investigate the interactions between
election participants and the importance of their contacts. It investigates
whether personal contacts matter more than know-how contacts in wiki election
nominations and voting participation by using standard tools such as Pajek and
Gephi. It further evaluates the significance of a personal contacts in online
wiki elections through a number of different graph-based influence
identification methods. Additionally, the basic characteristics and cohesive
groups in the wiki vote network are explored. This work contributes by
discovering the significance of personal contacts over know-how contacts of a
person in online elections. It is found that personal contacts, i.e. immediate
neighbors (degree centrality) and neighborhood (k-neighbors) of a person have a
positive effect on a person's nomination as an administrator and also
contribute to the active participation of voters in voting. Moreover, know-how
contacts, analyzed by means of measures such as betweenness and closeness
centralities, have a relatively insignificant effect on the selection of a
person. However, know-how contacts in terms of betweenness centrality for
passing information in the network can positively contribute only to the voting
process. These contacts also measured in terms of influence domain and PageRank
can play a vital role in the selection of an admin. Additionally, such contacts
in terms of reachability and brokerage roles have a positive association with
the voting process.
","Yousra Asim|Muaz A. Niazi|Basit Raza|Ahmad Kamran Malik","","http://arxiv.org/abs/1804.07450v1","http://arxiv.org/pdf/1804.07450v1","http://dx.doi.org/10.1186/s40294-018-0054-6","21 pages, 11 figures","Y. Asim, M. A. Niazi, B. Raza et al., ""Personal vs. know-how
  contacts: which matter more in wiki elections?"" Complex Adaptive Systems
  Modeling, vol. 6, no. 1, pp. 4, April 18, 2018","10.1186/s40294-018-0054-6","cs.SI","cs.SI|cs.CY"
"986","1804.08822v1","2018-04-24 03:02:54","2018-04-24 03:02:54","In-Browser Split-Execution Support for Interactive Analytics in the
  Cloud","  The canonical analytics architecture today consists of a browser connected to
a backend in the cloud. In all deployments that we are aware of, the browser is
simply a dumb rendering endpoint. As an alternative, this paper explores
split-execution architectures that push analytics capabilities into the
browser. We show that, by taking advantage of typed arrays and asm.js, it is
possible to build an analytical RDBMS in JavaScript that runs in a browser,
achieving performance rivaling native databases. To support interactive data
exploration, our Afterburner prototype automatically generates local
materialized views from a backend database that are then shipped to the browser
to facilitate subsequent interactions seamlessly and efficiently. We compare
this architecture to several alternative deployments, experimentally
demonstrating performance parity, while at the same time providing additional
advantages in terms of administrative and operational simplicity.
","Kareem El Gebaly|Jimmy Lin","","http://arxiv.org/abs/1804.08822v1","http://arxiv.org/pdf/1804.08822v1","","","","","cs.DB","cs.DB|cs.DC"
"987","1804.09174v1","2018-04-24 17:59:23","2018-04-24 17:59:23","Central Exclusive Production at LHCb","  The search for central exclusive production of muon pairs is performed using
the LHCb data acquired on 2011 ($\sqrt{s} = 7$ TeV) and 2012 ($\sqrt{s} = 8$
TeV). Preliminary measurements of cross section using these samples are showed
and a first comparison with theoretical previews is presented.
","L. G. S. de Oliveira","","http://arxiv.org/abs/1804.09174v1","http://arxiv.org/pdf/1804.09174v1","","This version withdrawn by arXiv administrators because the author did
  not have the right to agree to our license at the time of submission. Author
  list is truncated","","","hep-ex","hep-ex"
"988","1804.09442v1","2018-04-25 09:23:30","2018-04-25 09:23:30","Der Trusted Connector im Industrial Data Space","  Digitalization affects all industrial domains and causes disruption of
various business models. Especially in domains such as logistics and
manufacturing, inter-connected devices and near-realtime exchange of sensor
data across enterprises allows to speed up processes, reduce costs and respond
to customer's needs. However, the advent of the Industrial Internet of Things
(IIoT) also raises challenges with respect to security and privacy of sensitive
and personal data that is created by sensors and processed by services hosted
in different administrative domains. The Industrial Data Space initiative
addresses these challenges and proposes a secure edge gateway platform named
""Trusted Connector"". In this report, we introduce the main security building
blocks of the Trusted Connector and point out how they help protecting
business-critical data and preserving the user's privacy.
","Julian Schutte|Gerd Brost|Sascha Wessel","","http://arxiv.org/abs/1804.09442v1","http://arxiv.org/pdf/1804.09442v1","","in German. IDS, IoT, Industrial Data Space, Edge Device, Usage
  Control","","","cs.CR","cs.CR|cs.DC"
"989","1804.09908v1","2018-04-26 06:38:42","2018-04-26 06:38:42","Ultrasensitive Detection Enabled by Nonlinear Magnetization of
  Nanomagnetic Labels","  Geometrically confined magnetic particles due to their unique response to
external magnetic fields find a variety of applications, including magnetic
guidance, heat and drug delivery, magneto-mechanical actuation, and contrast
enhancement. Highly sensitive detection and imaging techniques based on
nonlinear properties of nanomagnets were recently proposed as innovative
strong-translational potential methods applicable in complex, often opaque,
biological systems. Here we report on significant enhancement of the detection
capability using optical-lithography-defined, ferromagnetic iron-nickel alloy
disk-shaped particles. We show that an irreversible transition between a
strongly non-collinear (vortex) and single domain states, driven by an
alternating magnetic field translates into a nonlinear magnetic response that
enables ultrasensitive detection of these particles. The record sensitivity of
~ 3.5x10-9 emu, which is equivalent to ~39 pg of magnetic material is
demonstrated at room temperature for arrays of patterned disks. We also show
that unbound disks re-suspended in aqueous buffer can be successfully detected
and quantified in real-time when administrated into a live animal allowing for
tracing their biodistribution. Use of nanoscale ferromagnetic particles with
engineered nonlinear properties opens prospects for further enhancing
sensitivity, scalability and tunability of noise-free magnetic tag detection in
high-background environments for various applications spanning from biosensing
and medical imaging to anti-counterfeiting technologies.
","M. P. Nikitin|A. V. Orlov|I. L. Sokolov|A. A. Minakov|P. I. Nikitin|J. Ding|S. D. Bader|E. A. Rozhkova|V. Novosad","","http://arxiv.org/abs/1804.09908v1","http://arxiv.org/pdf/1804.09908v1","","36 pages, 9 figures","","","cond-mat.mtrl-sci","cond-mat.mtrl-sci"
"990","1804.10312v2","2018-04-27 01:06:37","2019-07-11 06:47:30","Emergence of integrated institutions in a large population of
  self-governing communities","  Most aspects of our lives are governed by large, highly developed
institutions that integrate several governance tasks under one authority
structure. But theorists differ as to the mechanisms that drive the development
of such concentrated governance systems from rudimentary beginnings. Is the
emergence of integrated governance schemes a symptom of consolidation of
authority by small status groups? Or does integration occur because a complex
institution has more potential responses to a complex environment? Here we
examine the emergence of complex governance regimes in 5,000 sovereign,
resource-constrained, self-governing online communities, ranging in scale from
one to thousands of users. Each community begins with no community members and
no governance infrastructure. As communities grow, they are subject to
selection pressures that keep better managed servers better populated. We
identify predictors of community success and test the hypothesis that
governance complexity can enhance community fitness. We find that what predicts
success depends on size: changes in complexity predict increased success with
larger population servers. Specifically, governance rules in a large successful
community are more numerous and broader in scope. They also tend to rely more
on rules that concentrate power in administrators, and on rules that manage bad
behavior and limited server resources. Overall, this work is consistent with
theories that formal integrated governance systems emerge to organize
collective responses to interdependent resource management problems, especially
as factors such as population size exacerbate those problems.
","Seth Frey|Robert W Sumner","","http://arxiv.org/abs/1804.10312v2","http://arxiv.org/pdf/1804.10312v2","http://dx.doi.org/10.1371/journal.pone.0216335","contains supplement","","10.1371/journal.pone.0216335","cs.SI","cs.SI|cs.CY|H.5.3; J.4; K.6.4"
"991","1804.11315v2","2018-04-30 16:40:57","2018-11-13 20:58:00","Comment on ""A Spin Entanglement Witness for Quantum Gravity"" and on
  ""Gravitationally Induced Entanglement between Two Massive Particles is
  Sufficient Evidence of Quantum Effects in Gravity""","  This is a comment on articles Phys. Rev. Lett. 119, 240401 (2017)
[arXiv:1707.06050] and Phys. Rev. Lett. 119, 240402 (2017) [arXiv:1707.06036].
We argue that gravity-induced entanglement by Newtonian forces is agnostic to
the quantum or classical nature of the gravitational true degrees of freedom.
","C. Anastopoulos|Bei-Lok Hu","","http://arxiv.org/abs/1804.11315v2","http://arxiv.org/pdf/1804.11315v2","","This version has been removed by arXiv administrators because the
  submission contained work over which the submitters do not have copyright","","","quant-ph","quant-ph|gr-qc"
"992","1805.00471v1","2018-05-01 05:24:40","2018-05-01 05:24:40","""I ain't tellin' white folks nuthin"": A quantitative exploration of the
  race-related problem of candour in the WPA slave narratives","  From 1936-38, the Works Progress Administration interviewed thousands of
former slaves about their life experiences. While these interviews are crucial
to understanding the ""peculiar institution"" from the standpoint of the slave
himself, issues relating to bias cloud analyses of these interviews. The
problem I investigate is the problem of candour in the WPA slave narratives: it
is widely held in the historical community that the strict racial caste system
of the Deep South compelled black ex-slaves to tell white interviewers what
they thought they wanted to hear, suggesting that there was a significant
difference candour depending on whether their interviewer was white or black.
In this work, I attempt to quantitatively characterise this race-related
problem of candour. Prior work has either been of an impressionistic,
qualitative nature, or utilised exceedingly simple quantitative methodology. In
contrast, I use more sophisticated statistical methods: in particular word
frequency and sentiment analysis and comparative topic modelling with LDA to
try and identify differences in the content and sentiment expressed by
ex-slaves in front of white interviewers versus black interviewers. While my
sentiment analysis methodology was ultimately unsuccessful due to the
complexity of the task, my word frequency analysis and comparative topic
modelling methods both showed strong evidence that the content expressed in
front of white interviewers was different from that of black interviewers. In
particular, I found that the ex-slaves spoke much more about unfavourable
aspects of slavery like whipping and slave patrollers in front of interviewers
of their own race. I hope that my more-sophisticated statistical methodology
helps improve the robustness of the argument for the existence of this problem
of candour in the slave narratives, which some would seek to deny for
revisionist purposes.
","Soumya Kambhampati","","http://arxiv.org/abs/1805.00471v1","http://arxiv.org/pdf/1805.00471v1","","A thesis presented in partial fulfilment of the requirements of the
  degree of Bachelor of Arts in Statistics & Data Science at Yale University","","","cs.CL","cs.CL"
"993","1805.01130v1","2018-05-03 06:00:30","2018-05-03 06:00:30","Eat & Tell: A Randomized Trial of Random-Loss Incentive to Increase
  Dietary Self-Tracking Compliance","  A growing body of evidence has shown that incorporating behavioral economics
principles into the design of financial incentive programs helps improve their
cost-effectiveness, promote individuals' short-term engagement, and increase
compliance in health behavior interventions. Yet, their effects on long-term
engagement have not been fully examined. In study designs where repeated
administration of incentives is required to ensure the regularity of behaviors,
the effectiveness of subsequent incentives may decrease as a result of the law
of diminishing marginal utility. In this paper, we introduce random-loss
incentive -- a new financial incentive based on loss aversion and
unpredictability principles -- to address the problem of individuals' growing
insensitivity to repeated interventions over time. We evaluate the new
incentive design by conducting a randomized controlled trial to measure the
influences of random losses on participants' dietary self-tracking and
self-reporting compliance using a mobile web application called Eat & Tell. The
results show that random losses are significantly more effective than fixed
losses in encouraging long-term engagement.
","Palakorn Achananuparp|Ee-Peng Lim|Vibhanshu Abhishek|Tianjiao Yun","","http://arxiv.org/abs/1805.01130v1","http://arxiv.org/pdf/1805.01130v1","","Published at Digital Health 2018","","","cs.SI","cs.SI"
"994","1805.02044v1","2018-05-05 11:35:08","2018-05-05 11:35:08","Conditional and marginal relative risk parameters for a class of
  recursive regression graph models","  In linear regression modelling the distortion of effects after marginalizing
over variables of the conditioning set has been widely studied in several
contexts. For Gaussian variables, the relationship between marginal and partial
regression coefficients is well-established and the issue is often addressed as
a result of W. G. Cochran. Possible generalizations beyond the linear Gaussian
case have been developed, nevertheless the case of discrete variables is still
challenging, in particular in medical and social science settings. A
multivariate regression framework is proposed for binary data with regression
coefficients given by the logarithm of relative risks and a multivariate
Relative Risk formula is derived to define the relationship between marginal
and conditional relative risks. The method is illustrated through the analysis
of the morphine data in order to assess the effect of preoperative oral
morphine administration on the postoperative pain relief.
","Monia Lupparelli","","http://arxiv.org/abs/1805.02044v1","http://arxiv.org/pdf/1805.02044v1","","","","","stat.ME","stat.ME"
"995","1805.02241v1","2018-05-06 16:32:19","2018-05-06 16:32:19","Acquisition and use of knowledge over a restricted domain by intelligent
  agents","  This short paper provides a description of an architecture to acquisition and
use of knowledge by intelligent agents over a restricted domain of the Internet
Infrastructure. The proposed architecture is added to an intelligent agent
deployment model over a very useful server for Internet Autonomous System
administrators. Such servers, which are heavily dependent on arbitrary and
eventual updates of human beings, become unreliable. This is a position paper
that proposes three research questions that are still in progress.
","Juliao Braga|Nizam Omar|Luciana F. Thome","","http://arxiv.org/abs/1805.02241v1","http://arxiv.org/pdf/1805.02241v1","http://dx.doi.org/10.1145/3077286.3077293","5 pages","","10.1145/3077286.3077293","cs.AI","cs.AI|cs.MA|cs.NI"
"996","1805.03513v1","2018-05-07 19:46:29","2018-05-07 19:46:29","Monitoring Dynamic Mobile Ad-Hoc Networks: A Fully Distributed Hybrid
  Architecture","  The mobile ad-hoc networks (MANETs) represent a broad area of study and
market interest. They provide a wide set of applications in multiple domains.
In that context, the functional and non-functional monitoring of these networks
is crucial. For that purpose, monitoring techniques have been deeply studied in
wired networks using gossip-based or hierarchical-based approaches. However,
when applied to a MANET, several problematics arise mainly due to the absence
of a centralized administration, the inherent MANETs constraints and the nodes
mobility. In this paper, we present a hybrid distributed monitoring
architecture for mobile adhoc networks in context of mobility pattern. We get
inspired of gossip-based and hierarchical-based algorithms for query
dissemination and data aggregation. We define gossip-based mechanisms that help
our virtual hierarchical topology to complete the data aggregation, and then
ensure the stability and robustness of our approach in dynamic environments.
Further, we propose a fully distributed monitoring protocol that ease the nodes
communications. We evaluate our approach through a simulated testbed by using
NS3 and Docker, and illustrate the efficiency of our mechanisms.
","Jose Alvarez|Stephane Maag|Fatiha Zaidi","","http://arxiv.org/abs/1805.03513v1","http://arxiv.org/pdf/1805.03513v1","http://dx.doi.org/10.1109/AINA.2017.74","8 pages, 10 figures, presented in AINA 2017. arXiv admin note: text
  overlap with arXiv:1712.01676, arXiv:1805.02717","Advanced Information Networking and Applications (AINA), 2017 IEEE
  31st International Conference on","10.1109/AINA.2017.74","cs.NI","cs.NI"
"997","1805.03329v2","2018-05-09 00:42:15","2018-05-15 19:14:08","LogIn: Unlock Journaling System for Personal Informatics","  In situ self-report is widely used in human-computer interaction, ubiquitous
computing, and for assessment and intervention in health and wellness.
Unfortunately, it remains limited by high burdens. We examine unlock journaling
as an alternative. Specifically, we build upon recent work to introduce single
slide unlock journaling gestures appropriate for health and wellness measures.
We then present the first field study comparing unlock journaling with
traditional diaries and notification based reminders in self report of health
and wellness measures. We find unlock journaling is less intrusive than
reminders, dramatically improves frequency of journaling, and can provide equal
or better timeliness. Where appropriate to broader design needs, unlock
journaling is thus an overall promising method for in situ self report.
","Michael Jacob|Zack Zheng","","http://arxiv.org/abs/1805.03329v2","http://arxiv.org/pdf/1805.03329v2","","arXiv admin note: submission has been withdrawn by arXiv
  administrators due to inappropriate overlap with external sources","","","cs.HC","cs.HC"
"998","1805.04002v1","2018-05-10 14:29:31","2018-05-10 14:29:31","Liquid Metal Enabled Electrobiology: A Generalized Easy Going Way to
  Tackle Disease Challenges","  In this article, a new conceptual biomedical engineering strategy to tackle
modern disease challenges, termed as liquid metal enabled electrobiology, is
proposed. This generalized and easy going way is based on the physiological
fact that specially administrated electricity would induce a series of
subsequent desired biological effects, either shortly, transitional or
permanently. Owing high compliance within any part of the biological tissues,
the liquid metal would aid to mold a pervasive way to treat physiological or
psychological diseases. As highly conductive and non-toxic multifunctional
flexible materials, such liquid metals (LMs) consist of the core to generate
any requested electric treating fields (ETFields) which can adapt to various
sites inside the human body. The basic mechanisms of electrobiology in
delivering electricity to the target tissues and then inducing expected outputs
for disease treatment are interpreted. The methods toward realizing soft and
conformable electronics based on liquid metal are illustrated. Further, a group
of typical disease challenges were taken to illustrate the basic strategies to
perform liquid metal electrobiology therapy, which include but are not limited
to: tissue electronics, brain disorder, immunotherapy, neural functional
recovery, muscle stimulation, skin rejuvenation, cosmetology and dieting,
artificial organs, cardiac pacing and cancer therapy etc. Some practical issues
involved in the electrobiology for future disease therapy were discussed.
Perspective along this direction to incubate an easy-going biomedical tool for
health care was pointed out.
","Xuelin Wang|Yi Ren|Jing Liu","","http://arxiv.org/abs/1805.04002v1","http://arxiv.org/pdf/1805.04002v1","","25 pages, 9 figures, 1 table","","","physics.med-ph","physics.med-ph"
"999","1805.05409v2","2018-05-11 14:30:30","2018-09-11 15:32:10","Machine Learning for Public Administration Research, with Application to
  Organizational Reputation","  Machine learning methods have gained a great deal of popularity in recent
years among public administration scholars and practitioners. These techniques
open the door to the analysis of text, image and other types of data that allow
us to test foundational theories of public administration and to develop new
theories. Despite the excitement surrounding machine learning methods, clarity
regarding their proper use and potential pitfalls is lacking. This paper
attempts to fill this gap in the literature through providing a machine
learning ""guide to practice"" for public administration scholars and
practitioners. Here, we take a foundational view of machine learning and
describe how these methods can enrich public administration research and
practice through their ability develop new measures, tap into new sources of
data and conduct statistical inference and causal inference in a principled
manner. We then turn our attention to the pitfalls of using these methods such
as unvalidated measures and lack of interpretability. Finally, we demonstrate
how machine learning techniques can help us learn about organizational
reputation in federal agencies through an illustrated example using tweets from
13 executive federal agencies.
","L. Jason Anastasopoulos|Andrew B. Whitford","","http://arxiv.org/abs/1805.05409v2","http://arxiv.org/pdf/1805.05409v2","","","","","cs.CY","cs.CY|cs.LG|stat.ML"
"1000","1805.05434v2","2018-05-14 20:40:47","2018-09-20 18:36:05","Response of an oscillatory delay differential equation to a periodic
  stimulus","  Periodic hematological diseases such as cyclical neutropenia or cyclical
thrombocytopenia, with their characteristic oscillations of circulating
neutrophils or platelets, may pose grave problems for patients. Likewise,
periodically administered chemotherapy has the unintended side effect of
establishing periodic fluctuations in circulating white cells, red cell
precursors and/or platelets. These fluctuations, either spontaneous or induced,
often have serious consequences for the patient (e.g. neutropenia, anemia, or
thrombocytopenia respectively) which exogenously administered cytokines can
partially correct. The question of when and how to administer these drugs is a
difficult one for clinicians and not easily answered. In this paper we use a
simple model consisting of a delay differential equation with a piecewise
linear nonlinearity, that has a periodic solution, to model the effect of a
periodic disease or periodic chemotherapy. We then examine the response of this
toy model to both single and periodic perturbations, meant to mimic the drug
administration, as a function of the drug dose and the duration and frequency
of its administration to best determine how to avoid side effects.
","Daniel C. De Souza|Michael C. Mackey","","http://arxiv.org/abs/1805.05434v2","http://arxiv.org/pdf/1805.05434v2","http://dx.doi.org/10.1007/s00285-018-1322-y","28 pages","","10.1007/s00285-018-1322-y","math.DS","math.DS|92B25, 92C45, 34K27, 34K13"
