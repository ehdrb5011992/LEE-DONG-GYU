---
title: "HW5 : 석사 2학기 이동규"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

#### **Library**

```{r message=FALSE}
library(boot) #chapter8
library(MASS) #chapter16
library(FactoMineR) #chapter16
library(gclus) #chapter17
library(e1071) #chapter17
library(kernlab) #chapter17
library(rpart) #chapter18
library(randomForest) #chapter18
library(ipred) #chapter18
```

# 8장 붓스트랩 방법

###[붓스트랩 사례: 상관계수]
```{r}
x <- c(15,26,10,9,15,20,18,11,8,20,7,9,10,11,11,10,12,17,11,10)
y <- c(95,71,83,91,102,87,93,100,104,94,113,96,83,84,102,100,105,121,86,100)
cor.X <- function(data, indices) cor(data[indices,1],data[indices,2])

boot.log <- boot(data=cbind(x,y), statistic=cor.X, R=1000)
print(boot.log)
```

우리는 상관계수에 대한 추정치의 신뢰구간 및 표준편차, 편향을 구하기 위해 붓스트랩을 시행하였다.
붓스트랩은 1000번 시행하였으며, 그값은 위에 제시되어 있다. 또한, original은 추정치 값으로 원래 데이터에서 구할 수 있다. 

```{r}
hist(boot.log$t, xlim=c(-1,1), nclass=20, main="bootstrap correlations", xlab="corr")
boot.ci(boot.log, type = c("perc", "bca"))
```

그리고 히스토그램을 그렸을 때, 분포는 비 대칭적인 형태를 띠고있다. 그러므로, BCa 신뢰구간을 읽어주는 것이 좋다.
BCa방법과 Percentile방법으로 얻은 추정치의 신뢰구간은 위에서 확인할 수 있다.
<br><br>
###[붓스트랩 사례: 회귀분석]
```{r}
data(Affairs, package = "AER")
str(Affairs)
```

이번에는 [붓스트랩 사례: 로지스틱 회귀]에 대해 알아본다. Affairs 데이터 구조는 위와 같다.
그리고 반응변수로 ynaffair , 설명변수로 gender, age, yearsmarried, religiousness, rating을 선택한다. 이때
ynaffair은 afaairs변수를 가지고 만들며 1이상이면 1 , 0이면 0을 갖는 이항형 반응변수이다.


```{r}
Affairs$ynaffair[Affairs$affairs > 0] <- 1
Affairs$ynaffair[Affairs$affairs == 0] <- 0
# Affairs$ynaffair <- factor(Affairs$ynaffair, levels = c(0,1), labels = c("No", "Yes"))
logistic.model <- glm(ynaffair ~ gender + age + yearsmarried + religiousness + rating, family=binomial(), data = Affairs)
summary(logistic.model)
```

그리고 이를 통해 로지스틱 회귀분석을 적용하면 위의 결과와 같다. 우리는 위 추정치에 대해 붓스트랩방법을 사용하여 편향과 붓스트랩 표준편차를 
계산하려고 한다.


```{r}
coefs <- function(data, indices, formula) {
  data.1 <- data[indices, ]
  fit <- glm(formula, data = data.1, family=binomial())
  return(coef(fit))
}
logistic.log <- boot(data = Affairs, statistic = coefs, R = 1000, formula = ynaffair ~ gender + age + yearsmarried + religiousness + rating)
print(logistic.log, index=1:6, digits=2)
```

위는 붓스트랩을 시행한 결과이다. 1000번 재표집 하였으며 회귀 계수에 대한 값을 통계량으로 삼았다. 편차와 표준편차를 확인할 수 있다.
이어서 아래는 각 index별 붓스트랩 신뢰구간을 구한 결과이다.

<br><br>
###[붓스트랩 사례: 두 독립표본의 중심 간 차이]

```{r}
set.seed(123)
x <- rgamma(100,5,0.8)
y <- rgamma(100,5,1.0)
x.and.y <- data.frame(x, y)
z <- stack(x.and.y)
t.test(x, y, var.equal=T)
```

이번엔 두 독립표본의 중심간의 차이에 대해 붓스트랩 하는 방법을 알아보자. 먼저 two sample t-test를 시행하면 위의 결과와 같다.

```{r}
diff.means <- function(data, n1, indices) {
  m1 <- mean(data[indices[indices <= n1],1])
  m2 <- mean(data[indices[indices >  n1],1])
  c(m1 - m2)
}

z.boot <- boot(data=z, statistic=diff.means, n1=100, R=1000, strata=z[,2])
boot.ci(z.boot, type = c("perc", "bca"))
```

그리고 이어서 붓스트랩을 적용한 결과이다. strata=z[,2]는 표본별로 복원추출을 하라는 옵션의미이다. 그리고 얻어진 percentile과 BCa의 두가지
붓스트랩 신뢰구간을 얻을 수 있다.


# 16장 행렬도와 대응분석

###[행렬도]
```{r}
setwd("C:\\Users\\82104\\Desktop\\대학원 수업\\1학년 2학기\\통계분석방법론\\응용데이터분석\\Applied Data Analysis Using R\\Applied Data Analysis Using R\\16_biplot and correspondence analysis")

# r biplot for protein data

X.1 <- read.table("protein.txt",header=T)
str(X.1)
X <- X.1[,2:10]
n <- nrow(X)

# Using R function princomp()
pca.X <- princomp(X, cor=T)
pca.X$scores <- pca.X$scores*sqrt((n-1)/n)
# round(pca.X$scores,3)
# round(apply(pca.X$scores,2,sd),3)
biplot(pca.X,scale=0,cex=0.8,xlab="First Dimension",ylab="Second Dimension")
X.1$Country
```

위처럼 작성하면 행렬도를 얻어낼 수 있다. 또한 행렬도에서 개체번호와 국가이름의 연결은 위와 같다.
다음은 주성분 분석의 요약이다.

```{r}
summary(pca.X,loadings=T)
```

표준편차와 분산비, 누적분산값을 볼 있으며 이에대한 loading(계수벡터) 값도 볼 수 있다.
<br><br>

###[대응분석]
```{r}
# Correspondence analysis for 2007 Presidential Election Survey data

F0 <- read.table("C:\\Users\\82104\\Desktop\\대학원 수업\\1학년 2학기\\통계분석방법론\\응용데이터분석\\Applied Data Analysis Using R\\Applied Data Analysis Using R\\16_biplot and correspondence analysis\\election.txt", header=T)
F <- as.matrix(F0[,-1])
rownames(F) <- F0[,1]
addmargins(F)
r.margin <- addmargins(F)[,7]
c.margin <- addmargins(F)[8,]
```

위의 결과는 대응분석 데이터이다. 

```{r}
corresp.F <- corresp(F,nf=2)
attach(corresp.F)
ROW <- rscore%*%diag(cor)
COL <- cscore
plot(ROW[,2]~ROW[,1],type="n",xlim=c(-2,2),ylim=c(-2,2),xlab="First Dimension",ylab="Second Dimension",main="Presidential Election 2007")
text(ROW[,1],ROW[,2],labels=rownames(ROW),cex=1,col="blue")
par(new=T)
plot(COL[,2]~COL[,1],type="n",xlim=c(-2,2),ylim=c(-2,2),xlab="",ylab="")
text(COL[,1],COL[,2],labels=rownames(COL),cex=1,col="red")
```

그리고 가중 표현 없이 평면에 펼쳐진 대응분석 그래프를 그리면 위와 같다. 그리고 심플렉스에 각 꼭지점에 해당하는 빨간색 점들과 파란색 점들이 얼마나 유사한지를 확인할 수 있다. 다만 크기가 반영이 되어있지 않다는 점에서 아래와 같이 빈도를 가중치로 하여 대응분석을 다시 할 수 있다.


```{r}
plot(ROW[,2]~ROW[,1],type="n",xlim=c(-2,2),ylim=c(-2,2),xlab="First Dimension",ylab="Second Dimension",main="Presidential Election 2007")
text(ROW[,1],ROW[,2],labels=rownames(ROW),cex=r.margin/sum(r.margin)*12,col="blue")
par(new=T)
plot(COL[,2]~COL[,1],type="n",xlim=c(-2,2),ylim=c(-2,2),xlab="",ylab="")
text(COL[,1],COL[,2],labels=rownames(COL),cex=c.margin/sum(c.margin)*12,col="red")
```

해석은 위에서 했던것과 마찬가지로 진행된다. 행범주와 열범주의 크기를 표현한 결과이다. 


```{r}
par(mfrow=c(1,1))
plot(rscore[,2]~rscore[,1],type="n",xlim=c(-3.5,3.5),ylim=c(-3.5,3.5),xlab="First Dimension",ylab="Second Dimension",main="Presidential Election 2007")
text(rscore[,1],rscore[,2],labels=rownames(rscore),cex=0.8,col="blue")
par(new=T)
plot(cscore[,2]~cscore[,1],type="n",xlim=c(-3.5,3.5),ylim=c(-3.5,3.5),xlab="",ylab="")
text(cscore[,1],cscore[,2],labels=rownames(cscore),cex=0.8,col="red")
```

또한 대칭적 대응분석을 위해 plot을 그린 그림이다. 행과 열이 대칭인경우 행변수와 열변수간 상관의 최대화에 분석목표를 두게 된다.

```{r}
# Alternative Biplot
biplot(rscore,cscore,cex=1,xlab="First Dimension",ylab="Second Dimension",main="Presidential Election 2007")
 
```

그리고 대칭형 대응분석을 위를 통해 시행할 수 있다. 
<br><br> 

###[다중대응분석]
다음은 다중대응분석에 대한 이야기이다. 다중대응분석은 범주형의 PCA버전이다. 먼저 tea데이터에서 선별한 변수의 데이터 구조는 아래와 같다.

```{r}
data(tea)
tea.1 <- tea[,c("Tea","How","sugar","how","where","always")]
str(tea.1)
```

또한 아래와 같이 tea데이터에 대해 다중대응분석 그래프를 그리면 아래와 같다. 17개의 빨간 점들이 찍히며, 행점은 회색으로, 열범주는 빨간색으로 찍혀있다. 

```{r}
mca.tea.1 <- mca(tea.1, nf=2, abbrev=T)
mca.tea.1
windows(heigh=8,width=7)
plot(mca.tea.1,cex=0.8,xlim=c(-0.05,0.05),ylim=c(-0.05,0.05),col=c("gray","red"),main="Tea Data")
```

그리고 아래와 같이 MCA행 프로파일 플롯과 tea 자료의 MCA 열 범주 플롯을 나눠 그릴수 있다.

```{r}
windows(heigh=8,width=7)
plot(mca.tea.1$rs,type="n",xlab="dim.1",ylab="dim.2",xlim=c(-0.02,0.02),ylim=c(-0.02,0.02),main="Respondents")
text(mca.tea.1$rs+matrix(rnorm(600,0,0.0005),300,2),label=1:300,cex=0.7,col="gray")

windows(heigh=8,width=7)
plot(mca.tea.1$cs,pch=20,xlab="dim.1",ylab="dim.2",xlim=c(-0.05,0.05),ylim=c(-0.05,0.05),main="Answer Categories")
text(mca.tea.1$cs+0.0025,rownames(mca.tea.1$cs),cex=0.8,col="red")

```



# 17장 SVM


###[비선형 SVM 분류]



```{r}
# nonlinear classification using SVM

set.seed(123)
x <- matrix(rnorm(200),100,2)
grp <- ifelse(apply(x*x,1,sum) <= 1.16, 1, 2)
y <- as.factor(grp)
table(grp)

svm.model <- svm(y ~ x, kernel="radial", scale=F, gamma=0.5)
summary(svm.model)

windows(height=8,width=7)
plot(x,pch=c(20,21)[grp],col=c("blue","red")[svm.model$fitted],xlim=c(-3,3),ylim=c(-3,3),main="Simulated Bivariate Data 3",xlab="x1",ylab="x2")
theta <- seq(0,1,0.01)*2*pi
r <- sqrt(1.16)
par(new=T); plot(r*cos(theta),r*sin(theta),lty="dotted",type="l",xlim=c(-3,3),ylim=c(-3,3),xlab="",ylab="")

table(svm.model$fitted)
```

비선형 svm 분류를 시행해보자. 데이터는 50, 50개를 생성하였으며, 이를 통해 적합을 시켜 비선형 분류를 시행하면 49, 51개로 매우 분류를 잘 하고 있음을 확인할 수 있다. 감마 모수는 0.5를 주었고, 커널함수로 rbf커널을 사용하였다.

###[스팸 메일 사례]

```{r}
data(spam); str(spam)
svm.model <- svm(type ~ ., data=spam, gamma=1, cost=1) 
summary(svm.model) 
```

spam데이터를 가지고 선형분류를 해보자. 데이터를 요약하면 위와 같고,svm을 적용시켜보았다. 그리고 분류를 시행하면 매우 잘 되었다는 걸 확인할 수 있다.

```{r}
addmargins(table(spam$type, svm.model$fitted))
```

하지만 앞의 분석에서 분류 정확도는 과장되어 있다. 테스트자료와 트레이닝자료가 같기 때문이다. 이를 고려해 데이터를 3:1로 분할하고 데이터 적합을 해서 svm을 해보자. 먼저 아래의 데이터는 테스트데이터에서의 정확도 결과이다. 

```{r}
n <- nrow(spam)
sub <- sample(1:n, round(0.75*n))
spam.1 <- spam[sub,]
spam.2 <- spam[-sub,]

svm.model.1 <- svm(type ~ ., data=spam.1, gamma=1, cost=1)
summary(svm.model.1) 
addmargins(table(spam.1$type, svm.model.1$fitted))
```

하지만 우리는 위보다 다음에 이어질 테스트 데이터에 대한 결과에 더 관심이 많다. 확인하면 아래와 같다. 


```{r}
svm.predict.2 <- predict(svm.model.1, newdata=spam.2)
addmargins(table(spam.2$type, svm.predict.2))
```

보다시피 총 오류율이 얼마나 과소평가했었는지 실감할 수 있다. 다음은 9가지 모수조합 쌍에서 10cv를 통해 최적의 모수쌍을 찾아보자. 
그러면 결과는 아래와 같다. (이 결과는 컴퓨터 성능이 떨어져 데이터가 돌아가는데 시간이 많이 걸립니다. 결과는 생략하겠습니다.)

```{r}
# tuning
#p.time <- proc.time()
#tune.svm <- tune(svm, type ~ .,  data=spam.1, ranges=list(gamma=c(0.1,1,10),cost=c(0.1,1,10)))
#summary(tune.svm)  # best parameters are 0.1 and 10
#proc.time()-p.time
```


### [선형 SVM회귀]

다음은 모의생성한 2변량 자료에서 선형 svm회귀 에 대해 알아보자. 선형 svm의 개념도에 대한 그림은 아래와 같다. 

```{r}
set.seed(12345)
x <- rnorm(100)
y <- 0.8*x + rnorm(100,0,0.6)  # runif(100,-1,1)
sd(x)
sd(y)
windows(height=7.5,width=7)
plot(y ~ x, main="simulated bivariate data",xlim=c(-3,3),ylim=c(-3,3))
abline(c(0,0.8),col="blue")
abline(c(-1,0.8),col="red",lty="dotted")
abline(c(1,0.8),col="red",lty="dotted")
for (i in 1:100){
  if (y[i]-0.8*x[i] >  1) segments(x[i],y[i],x[i],0.8*x[i]+1)
  if (y[i]-0.8*x[i] < -1) segments(x[i],y[i],x[i],0.8*x[i]-1)
}

```

즉, 바깥에 있는 점들은 이상치로써 간주하고 대략적인 점들의 분포에서 적당한 회귀선을 찾는 것이 svm 선형회귀의 목적이다.
그리고 실제로 svm회귀를 적용시켜 그림을 찾으면 아래와 같이 얻어낼 수 있다. 

```{r}
svm.model <- svm(y ~ x, kernel="linear", epsilon=1, scale=F)  # kernel="radial", epsilon=1, gamma=0.5
summary(svm.model)

windows(height=7.5,width=7)
plot(y ~ x, main="simulated bivariate data",xlim=c(-3,3),ylim=c(-3,3))
par(new=T)
plot(svm.model$fitted ~ x, main="",xlim=c(-3,3),ylim=c(-3,3),xlab="",ylab="",col="red", pch=20)
points(x[svm.model$index],y[svm.model$index],pch=20)
```


### [비선형 SVM회귀]

그렇다면 이제는 비선형회귀에 대해 알아보자. 데이터의 분포가 곡선패턴을 가지고 있을 때 회귀곡선과 띠의 안팎으로 개념도를 그리면 대략 아래와 같이 그릴 수 있다.

```{r}

# nonlinear SVM regression

set.seed(12345)
x <- rnorm(100)
y.fit <- 0.8*x^2
y <- y.fit + rnorm(100,0,0.6)  # runif(100,-1,1)

windows(height=7.5,width=7)
plot(y ~ x, main="simulated bivariate data 2",xlim=c(-3,3),ylim=c(-2,6))
par(new=T)
plot(y.fit ~ x, main="",xlim=c(-3,3),ylim=c(-2,6),xlab="",ylab="",col="red", pch=20)
index <- (1:100)[abs(y-y.fit)>=1]
points(x[index],y[index],pch=20,col="blue")
```

그리고 실제 비선형 회귀 svm을 적용시키면 아래와 같이 그릴 수 있다. 

```{r}
svm.model <- svm(y ~ x, gamma=0.5, epsilon=1, scale=F)  # kernel="radial", epsilon=1, gamma=0.5
summary(svm.model)

windows(height=7.5,width=7)
plot(y ~ x, main="simulated bivariate data 2",xlim=c(-3,3),ylim=c(-2,6))
par(new=T)
plot(svm.model$fitted ~ x, main="",xlim=c(-3,3),ylim=c(-2,6),xlab="",ylab="",col="red", pch=20)
points(x[svm.model$index],y[svm.model$index],pch=20)

 
```

검은색으로 채워진 점들은 폭이 $\epsilon=1$인 띠 밖에 있는 개체들이며, 적색점들은 적합한 회귀함수이다. 커널함수는 rbf를 사용하였다. 

<br><br>

### [오존 연구사례]
다음은 오존 데이터 사례연구를 살펴보자.

```{r}
# example 1. Poisson observations
data(ozone)
str(ozone)
library(e1071)
svm.ozone <- svm(Ozone ~ ., data=ozone, cost=1)
summary(svm.ozone)
```

오존데이터를 돌려보면 위와 같다. 그리고 이를 svm에 적용시킨 결과이다. 서포트벡터의 수는 250개, SVM회귀이며 비선형으로 적합하였다.
<br><br>
그리고 plot을 그리면 아래와 같다. 

```{r}
windows(height=7.6, width=7)
plot(ozone$Ozone ~ svm.ozone$fitted, main="ozone study",xlim=c(0,40),ylim=c(0,40))
```

이번엔 10-cv를 통해 최적의 모수 조합을 찾아낸 결과이다. 최고의 모수 조합과 이때의 correlation은 아래에서 확인할 수 있다.

```{r}
tune.svm <- tune(svm, Ozone ~ ., data=ozone, ranges=list(epsilon=c(0.1,1),gamma=c(0.125,0.5),cost=c(0.1,1)))
summary(tune.svm)
cor(svm.ozone$fitted,ozone$Ozone)
```

# 18장 나무알고리즘
### [Kyphosis 사례]

```{r}
# Tree model via rpart package
# kyophosis data
data(kyphosis)
str(kyphosis)
table(kyphosis$Kyphosis)
```

이제 나무알고리즘에 대해 알아보자. 위에서는 kyphosis데이터(분류)를 사용하였으며, 이 데이터는 불균형자료이다. 그리고 나무모형을 가지고 분류를 할 에정이다.


```{r}
tree.1 <- rpart(Kyphosis ~ Age + Number + Start, data = kyphosis)
tree.1
par(mar=c(1,1,1,1), xpd = TRUE); plot(tree.1)
text(tree.1, use.n = TRUE)
```

먼저 불순도지수로 지니지수를 이용할 계획이며, 이에따라 분류를 진행하면 위와 같은 결과를 이끌어낸다. 다음은 불순도 지수로 정보지수(엔트로피) 지수를 사용한 결과이다. 이에따분 분류를 진행하면 아래와 같다.

```{r}

tree.2 <- rpart(Kyphosis ~ Age + Number + Start, data = kyphosis, parms = list(split = "information"))
tree.2
par(mar=c(1,1,1,1), xpd = TRUE); plot(tree.2)
text(tree.2, use.n = TRUE)
```

이때 아래의 결과를 확인하면 전체 오분류에 대해서는 문제가 없지만, 데이터들의 내용에 있어서는 다소 다르다. (보다 구체적으로 false positive와 false negative에 대해서 다르다.)

```{r}
table(kyphosis$Kyphosis, predict(tree.1, type="class"))
table(kyphosis$Kyphosis, predict(tree.2, type="class"))
```

이는 불균형 데이터에 기반하였기 때문인데, 이를 고려해서 가중치를 다르게 주어 엔트로피 불순도지수를 다시사용해서 모형을 적합시키면 아래와 같이 지니지수와 유사하게 결과를 이끌어 낼 수 있다. 

```{r}
kyphosis$wts <- ifelse(kyphosis$Kyphosis=="present", 0.79, 0.21)

tree.2a <- rpart(Kyphosis ~ Age + Number + Start, data = kyphosis, parms = list(split = "information"), weights=wts)
par(mar=c(1,1,1,1), xpd = TRUE); plot(tree.2a)
text(tree.2a, use.n = TRUE)

table(kyphosis$Kyphosis, predict(tree.2a, type="class"))

```

### [컴퓨터 성능 사례]

이번에는 cpus자료(회귀) 에 대해 나무모형을 적합해보도록 하자. 먼저 아래의 히스토그램을 보면 심하게 기울어져있는 것을 볼 수 있다. 이에따라 로그변환을 한다. 

```{r}

# bagging for cpus data
data(cpus, package="MASS"); str(cpus)
hist(cpus$perf)
cpus.tree.1 <- rpart(log10(perf) ~ syct+mmin+mmax+cach+chmin+chmax, data=cpus)
windows(height=8,width=8)
par(mar=c(1,1,1,1), xpd = TRUE)
plot(cpus.tree.1, uniform=F)
text(cpus.tree.1, use.n=T, cex=0.8)
```

그리고 나무모형을 적합하면 위와 같다. 이의 평균잔차제곱은 아래에서 확인할 수 있듯 0.03이다.


```{r}
pred.err <- log10(cpus$perf) - predict(cpus.tree.1)
mean(pred.err*pred.err)
```

그러나, training데이터와 test데이터가 같이 사용되었으므로 이는 부적절하다. 즉, 우리는 복원 추출을 통해 훈련자료를 뽑아내고, 앞에서 뽑히않은 나머지 데이터(oob)를 가지고 모형을 테스트하려고 한다. 


```{r}
subsample <- sample(1:209,replace=T)
cpus.train <- cpus[subsample,]
cpus.test <- cpus[-subsample,]
cpus.tree.1 <- rpart(log10(perf) ~ syct+mmin+mmax+cach+chmin+chmax, data=cpus.train, control = rpart.control(cp=0.02))
pred.err <- log10(cpus.test$perf) - predict(cpus.tree.1,newdata=cpus.test)
mean(pred.err*pred.err)
length((1:209)[-subsample])
```

이때 sampling 결과에 따라 오분류율이 다르게 나올 수 있지만, 0.03의 값보다는 크게 나타난다. 추가로 위에서 사용한 oob(out of bags)의 셈플수도 확인할 수 있다.


```{r}
subsample <- sample(1:209,replace=T)
cpus.train <- cpus[subsample,]
cpus.test <- cpus[-subsample,]
cpus.tree.1 <- rpart(log10(perf) ~ syct+mmin+mmax+cach+chmin+chmax, data=cpus.train, control = rpart.control(cp=0.02))
pred.err <- log10(cpus.test$perf) - predict(cpus.tree.1,newdata=cpus.test)
mean(pred.err*pred.err)
length((1:209)[-subsample])
```


### [배깅(Bagging)]

이번엔 배깅에 대해 알아보자. 배깅모형은 랜덤포레스트의 일부로, 붓스트랩을 하여 나무모형을 뽑는다. 이때, 변수에서는 건드리지 않고 전부 다 사용하게 되면 배깅이다. 그리고 out-of-bag 오분류율과 오분류표를 통해 모형의 성능을 파악할 수 있다. 

```{r}
data(kyphosis); str(kyphosis)
kyphosis.present <- kyphosis[kyphosis$Kyphosis=="present",]
kyphosis.absent <- kyphosis[kyphosis$Kyphosis=="absent",]
kyphosis.balanced <- rbind(kyphosis.present, kyphosis.present, kyphosis.present, kyphosis.present, kyphosis.absent)


RF.1 <- randomForest(Kyphosis ~ Age+Number+Start, data=kyphosis.balanced, var.importance=T)
RF.1$importance
 
```

kyphosis 자료(분류) 에 대 해 배깅모형을 만들어 보았다. 이때 데이터는 약 4:1로 불균형 데이터 이기 때문에 개체를 4배로 만들어서 데이터의 불균형 문제를 해소한다. 이후 배깅을 시행하며, 배깅에서 반복횟수의 수는 25번이다. (디폴트값)

```{r}

# for cpus
data(cpus)
cpus.bag <- bagging(log10(perf) ~ syct+mmin+mmax+cach+chmin+chmax, data=cpus, coob=T, nbagg=100)
cpus.bag

 
```

그리고 같은 행위를 cpus 자료(회귀)를 통해 적합시키면 위와 같다. 이 경우는 배깅알고리즘을 회귀로 적용시켜본 결과이다. 그러므로 오분류표는 제공되지 않고, 평균제곱오차 값의 루트값을 제공한다.


### [랜덤포레스트(Random Forest)]

마지막으로 랜덤포레스트에 대해 적용해보자. 랜덤포레스트는 배깅과 유사한데, 배깅 알고리즘을 그대로 따라가다 선택할 변수의 수만 다르게 뽑으면 된다. 또한 위에서 변수 중요도를 확인할 수 있으며, 지니평균감소량(MeanDecreaseGini) 기준에 따라 Start , Age, Number순서로 중요하다. 

```{r}
# for cpus data

data(cpus, package="MASS"); str(cpus)
RF.2 <- randomForest(log10(perf) ~ syct+mmin+mmax+cach+chmin+chmax, data=cpus,importance=TRUE)
RF.2
 
```

그리고 이를 cpus 데이터에 대해 적용하면(회귀) 유사한 결과를 얻는다. 500번 반복으로 통해 얻은 모형의 평균제곱잔차값을 구할 수 있다. 


```{r}
varImpPlot(RF.2)
```

이어서 위 그림은 예측변수의 중요도를 보여준다. y축 값이 큰 값부터중요한 변수로 생각하면 되고, 변수중요도는 MSE의 퍼센트 증가 기준과 노드 순도의 증가 기준이 서로 다을 확인할 있다. 


