---
title: "HW5 : 석사 2학기 이동규"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

###  **Library**

```{r message=FALSE}
library(tidyverse)
library(lubridate)
library(aRxiv)
library(topicmodels) # lda
library(tidytext)
library(tidyverse)
library(scales) # ggplot scale
library(e1071) # svm
library(randomForest) # randomforest
library(caret) # confusion matrix
library(Epi)   # ROC, AUC
library(MLmetrics) # F1 score
```


### **Data Import**

```{r}
rm(list=ls())
gc()

# bio <- arxiv_search(query = '"biology"', limit = 1000)
# write.csv(bio,file='bio.csv') # write csv.
# adm <- arxiv_search(query = '"administration"', limit = 1000)
# write.csv(adm,file='adm.csv') # write csv.

bio <- read.csv('C:\\Users\\82104\\Desktop\\bio.csv') # data import
bio <- bio[,-1] %>% 
  as_tibble()
adm <- read.csv('C:\\Users\\82104\\Desktop\\adm.csv') # data import
adm <- adm[,-1] %>% 
  as_tibble()
```

## 1. Combine two data sets with additional categroical variable that represents the source, bio or adm.

```{r}
bio <- bio %>% 
  select(title,abstract) %>% 
  as_tibble() %>% 
  mutate(title=as.character.Date(title),
         abstract=as.character.Date(abstract),
         category='bio')
adm <- adm %>% 
  select(title,abstract) %>% 
  as_tibble() %>% 
  mutate(title=as.character.Date(title),
         abstract=as.character.Date(abstract),
         category='adm')

data <- bio %>% 
  bind_rows(adm)

data
```

새로운 변수 category를 만들고, 여기에 각 논문의 class를 저장한 뒤에 행으로 붙였다. 
이로써 2000개의 행을 지닌 데이터set이 만들어진다.
<br><br><br>

## 2. (Document Modeling) Apply the LDA with 2 topics and evaluate the model performance.

```{r}
data_dtm <- data %>% 
  mutate(title = str_c(category,title,sep="::")) %>% 
  unnest_tokens(word, abstract) %>% 
  anti_join(stop_words,by='word') %>% 
  filter(!str_detect(word,'[\\d]') ) %>% 
  filter(!str_detect(word,'^_[a-z]+')) %>%  #eliminating digit
  count(title,word) %>% 
  cast_dtm(title,word,n)

data_lda <- LDA(data_dtm, k = 2,
              control = list(seed = 1234))

```

lda를 적용한다. 이때 topic의 수는 2이다.
<br><br>

#### 2-1) Document & Topic

```{r}
#gamma
data_gamma <- tidy(data_lda, matrix = "gamma") %>% 
  mutate(document = reorder(document, gamma * topic))


docu_gamma <- data_gamma %>%
  separate(document, c("subject", "document"), sep = "::", convert = T)

docu_gamma
```

데이터 셋은 위처럼 되어있다.
<br><br>

```{r}
docu_gamma %>%
  mutate(subject = reorder(subject, gamma * topic)) %>%
  ggplot(aes(factor(topic), gamma)) +
  geom_boxplot() +
  facet_wrap(~ subject)

```

위의 그림에서 보면 box plot이 극명하게 갈림을 확인 할수 있다.
<br><br>

#### 2-1: Bonus

어느 문서에서 극명하게 갈리고 있는지를 확인해보자.

```{r}
docu_classifications <- docu_gamma %>%
  group_by(subject, document) %>%
  top_n(1, gamma) %>%
  ungroup()

subject_topics <- docu_classifications %>%
  count(subject, topic) %>%
  group_by(subject) %>%
  top_n(1, n) %>%
  ungroup() %>%
  transmute(consensus = subject, topic)

subject_topics  #topic classification through the lda
```

lda를 통해 topic이 어떻게 분류되었는지를 확인할 수 있다. 

```{r}
wrong_docu <- docu_classifications %>%
  inner_join(subject_topics, by = "topic") %>%
  filter(subject != consensus) %>% 
  count(subject, consensus,document) %>%
  ungroup() %>%
  arrange(desc(n))

wrong_docu
```

잘못 분류된 문서들의 목록을 확인 할 수 있다. 
<br><br>

```{r}
accuracy_docu_assign <- 1- sum(wrong_docu$n)/nrow(docu_classifications)
str_c("Documents & Topic Accuracy:",percent(accuracy_docu_assign),sep=' ')
```

정확도는 위와 같다. 
<br><br><br>


#### 2-2) Term & Topic

```{r}
assignments <- augment(data_lda, data = data_dtm)
assignments


assignments <- assignments %>%
  separate(document, c("subject", "document"), sep = "::", convert =T) %>% 
  inner_join(subject_topics, by = c(".topic" = "topic"))
print(assignments, n = 7)
```

subject와 consensus가 다른 것의 비율을 살필 예정이다. 

<br><br>

```{r}
assignments %>%
  count(subject,document, consensus, wt = count) %>%
  group_by(document) %>%
  mutate(percent = n / sum(n)) %>%
  ggplot(aes(consensus, subject, fill = percent)) +
  geom_tile() +
  scale_fill_gradient2(high = "red", label = percent_format()) +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 90, hjust = 1),
        panel.grid = element_blank()) +
  labs(x = "Paper words were assigned to",
       y = "Paper words came from",
       fill = "% of assignments")
```

heatmap으로 보면 위와 같다. 
<br><br>

```{r}
wrong_words <- assignments %>%
  filter(subject != consensus) %>%
  count(subject, consensus, term, wt = count) %>%
  ungroup() %>%
  arrange(desc(n))

accuracy_word_assign <- 1- sum(wrong_words$n)/nrow(assignments)
str_c("Words & Topic Accuracy:",percent(accuracy_word_assign),sep=' ')
```

정확도는 위와 같다. 
<br><br><br>

## 3. (Document Classification) Train LDA to the pooled data with 10 topics, and apply logistic regression, random forest, and SVM. Then compare their performance.


```{r}
data_lda2 <- LDA(data_dtm, k = 10,
                control = list(seed = 1234))
data_gamma2 <- tidy(data_lda2, matrix = "gamma") %>% 
  mutate(document = reorder(document, gamma * topic)) %>% 
  spread(topic,gamma)
docu_gamma2 <- data_gamma2 %>%
  separate(document, c("subject", "document"), sep = "::", convert = T) %>% 
  mutate(subject=as.factor(subject))
```

```{r}
f <- as.formula(paste('subject','~',str_c(paste0("`",1:10,"`"),collapse = '+')))
```

2-3에서 사용할 데이터를 목적에 맞게 추출하고 정비하였다. lda를 10개의 topic으로 나누어 보고, 이때 각 topic은 변수이다. 우리는 documnet & topic 에서 얻어지는 gamma값(확률) 을 사용할 계획이다.
<br><br>


#### 3-1) Logistic

```{r}
#logistic
logistic <- glm(f, data=docu_gamma2, family = 'binomial')
pred_logistic <- as.factor(ifelse(logistic$fitted.values >=0.5 , 'bio','adm'))
confusion_logistic <- confusionMatrix(pred_logistic,docu_gamma2$subject)
confusion_logistic
```

Logistic의 오분류표는 위와 같다. 
<br><br>

```{r}

ROC(test=pred_logistic, stat=docu_gamma2$subject, plot="ROC", AUC=T, main="Logistic")
```

Logistic의 ROC곡선은 위와 같다. 

#### 3-2) SVM

```{r}
#svm
support_vector_machine <- svm(f,data=docu_gamma2)
pred_svm <- as.factor(support_vector_machine$fitted)
confusion_svm <- confusionMatrix(pred_svm,docu_gamma2$subject)
confusion_svm
```

SVM의 오분류표는 위와 같다. 
<br><br>

```{r}
ROC(test=pred_svm, stat=docu_gamma2$subject, plot="ROC", AUC=T, main="SVM")
```

SVM의 ROC곡선은 위와 같다. 

#### 3-3) Random Forest

```{r}
#random forest
dat_rf <- docu_gamma2 %>% 
  rename(factor1=`1`,factor2=`2`,factor3=`3`,factor4=`4`,factor5=`5`,
         factor6=`6`,factor7=`7`,factor8=`8`,factor9=`9`,factor10=`10`)
set.seed(1234)

rf = randomForest(subject~factor1+factor2+factor3+factor4+factor5+factor6+
                    factor7+factor8+factor9+factor10, data=dat_rf,mtry = floor(sqrt(10)))
pred_rf <- rf$predicted
confusion_rf <- confusionMatrix(pred_rf,docu_gamma2$subject)
confusion_rf
```

Random Forest의 오분류표는 위와 같다. 
<br><br>

```{r}
ROC(test=pred_rf, stat=docu_gamma2$subject, plot="ROC", AUC=T, main="Random Forest")
```

Random Forest의 ROC곡선은 위와 같다.

세개의 차이는 대동소이하며, Random Forest에서 AUC값이 가장 큼을 확인할 수 있다. 
또한 정확도를 봐도 무방하며, $F1 = \frac{2}{ \frac{1}{Precision} + \frac{1}{Recall}}$의 계산을 통해 모형비교를 해도 좋다.

그 F1값은 아래와 같다.
<br><br>

```{r}
#F1 score
f1_logistic <- round(F1_Score(docu_gamma2$subject,pred_logistic),3)
f1_svm <- round(F1_Score(docu_gamma2$subject,pred_svm),3)
f1_rf <- round(F1_Score(docu_gamma2$subject,pred_rf),3)

X <- cbind(f1_logistic,f1_svm,f1_rf)
rownames(X) <- "F1 Score"
colnames(X) <- c('Logistic','SVM','Random_Forest')
X
```
